{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Further Topics in Multiple Regression\n",
    "\n",
    "**metricsAI: An Introduction to Econometrics with Python and AI in the Cloud**\n",
    "\n",
    "*[Carlos Mendez](https://carlos-mendez.org)*\n",
    "\n",
    "This notebook provides an interactive introduction to advanced topics in regression inference and prediction. All code runs directly in Google Colab without any local setup.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quarcs-lab/aed/blob/main/notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Overview\n",
    "\n",
    "This chapter extends our understanding of regression inference beyond the standard classical assumptions. You'll learn when and how to use robust methods and make accurate predictions.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Heteroskedasticity-robust standard errors (White's correction)\n",
    "- HAC (Newey-West) standard errors for time series\n",
    "- Distinguishing between predicting conditional means vs. actual values\n",
    "- Constructing prediction intervals with proper uncertainty measures\n",
    "- Understanding nonrepresentative sampling issues\n",
    "- Best estimation methods (GLS, FGLS)\n",
    "- Best confidence intervals and hypothesis tests\n",
    "\n",
    "**Datasets used:**\n",
    "- **AED_HOUSE.DTA**: 29 houses sold in Davis, California (1999)\n",
    "- **AED_REALGDPPC.DTA**: Real GDP per capita growth time series\n",
    "\n",
    "**Key economic questions:**\n",
    "- How do robust standard errors change inference?\n",
    "- What's the uncertainty in predicting an individual house price?\n",
    "- How does autocorrelation affect time series inference?\n",
    "\n",
    "**Sections covered:**\n",
    "1. Inference with Robust Standard Errors\n",
    "2. Prediction\n",
    "3. Nonrepresentative Samples\n",
    "4. Best Estimation Methods\n",
    "5. Best Confidence Intervals\n",
    "6. Best Tests\n",
    "\n",
    "**Estimated time:** 75-100 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Ready to explore further topics in multiple regression.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "# GitHub data URL\n",
    "GITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Setup complete! Ready to explore further topics in multiple regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We'll work with two datasets:\n",
    "1. **House price data** for cross-sectional robust inference\n",
    "2. **GDP growth data** for time series HAC inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Data Summary:\n",
      "               price         size   bedrooms  bathrooms    lotsize        age  \\\n",
      "count      29.000000    29.000000  29.000000  29.000000  29.000000  29.000000   \n",
      "mean   253910.344828  1882.758621   3.793103   2.206897   2.137931  36.413792   \n",
      "std     37390.710695   398.272130   0.675030   0.341144   0.693034   7.118975   \n",
      "min    204000.000000  1400.000000   3.000000   2.000000   1.000000  23.000000   \n",
      "25%    233000.000000  1600.000000   3.000000   2.000000   2.000000  31.000000   \n",
      "50%    244000.000000  1800.000000   4.000000   2.000000   2.000000  35.000000   \n",
      "75%    270000.000000  2000.000000   4.000000   2.500000   3.000000  39.000000   \n",
      "max    375000.000000  3300.000000   6.000000   3.000000   3.000000  51.000000   \n",
      "\n",
      "       monthsold           list  \n",
      "count  29.000000      29.000000  \n",
      "mean    5.965517  257824.137931  \n",
      "std     1.679344   40860.264099  \n",
      "min     3.000000  199900.000000  \n",
      "25%     5.000000  239000.000000  \n",
      "50%     6.000000  245000.000000  \n",
      "75%     7.000000  269000.000000  \n",
      "max     8.000000  386000.000000  \n",
      "\n",
      "First few observations:\n",
      "    price  size  bedrooms  bathrooms  lotsize   age  monthsold\n",
      "0  204000  1400         3        2.0        1  31.0          7\n",
      "1  212000  1600         3        3.0        2  33.0          5\n",
      "2  213000  1800         3        2.0        2  51.0          4\n",
      "3  220000  1600         3        2.0        1  49.0          4\n",
      "4  224500  2100         4        2.5        2  47.0          6\n"
     ]
    }
   ],
   "source": [
    "# Read house data\n",
    "data_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n",
    "\n",
    "print(\"House Data Summary:\")\n",
    "print(data_house.describe())\n",
    "\n",
    "print(\"\\nFirst few observations:\")\n",
    "print(data_house[['price', 'size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2: Inference with Robust Standard Errors\n",
    "\n",
    "In practice, the classical assumptions often fail. The most common violations are:\n",
    "\n",
    "**1. Heteroskedasticity**: Error variance varies across observations\n",
    "- Common in cross-sectional data\n",
    "- Makes default standard errors incorrect\n",
    "- Solution: Use **heteroskedasticity-robust standard errors** (HC1, White's correction)\n",
    "\n",
    "**2. Clustered errors**: Errors correlated within groups\n",
    "- Common in panel data, hierarchical data\n",
    "- Makes default and het-robust SEs too small\n",
    "- Solution: Use **cluster-robust standard errors**\n",
    "\n",
    "**3. Autocorrelation**: Errors correlated over time\n",
    "- Common in time series\n",
    "- Makes default SEs incorrect\n",
    "- Solution: Use **HAC (Newey-West) standard errors**\n",
    "\n",
    "**Key insight**: OLS coefficients remain unbiased under these violations, but standard errors need adjustment.\n",
    "\n",
    "**Heteroskedastic-robust standard error formula**:\n",
    "\n",
    "$$se_{het}(\\hat{\\beta}_j) = \\sqrt{\\frac{\\sum_{i=1}^n \\tilde{x}_{ji}^2 \\hat{u}_i^2}{(\\sum_{i=1}^n \\tilde{x}_{ji}^2)^2}}$$\n",
    "\n",
    "where $\\tilde{x}_{ji}$ are residuals from regressing $x_j$ on other regressors, and $\\hat{u}_i$ are OLS residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "12.2 INFERENCE WITH ROBUST STANDARD ERRORS\n",
      "======================================================================\n",
      "\n",
      "Regression with Default Standard Errors:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.651\n",
      "Model:                            OLS   Adj. R-squared:                  0.555\n",
      "Method:                 Least Squares   F-statistic:                     6.826\n",
      "Date:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000342\n",
      "Time:                        15:04:03   Log-Likelihood:                -330.74\n",
      "No. Observations:                  29   AIC:                             675.5\n",
      "Df Residuals:                      22   BIC:                             685.1\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.378e+05   6.15e+04      2.242      0.035    1.03e+04    2.65e+05\n",
      "size          68.3694     15.389      4.443      0.000      36.454     100.285\n",
      "bedrooms    2685.3151   9192.526      0.292      0.773   -1.64e+04    2.17e+04\n",
      "bathrooms   6832.8800   1.57e+04      0.435      0.668   -2.58e+04    3.94e+04\n",
      "lotsize     2303.2214   7226.535      0.319      0.753   -1.27e+04    1.73e+04\n",
      "age         -833.0386    719.335     -1.158      0.259   -2324.847     658.770\n",
      "monthsold  -2088.5036   3520.898     -0.593      0.559   -9390.399    5213.392\n",
      "==============================================================================\n",
      "Omnibus:                        1.317   Durbin-Watson:                   1.259\n",
      "Prob(Omnibus):                  0.518   Jarque-Bera (JB):                0.980\n",
      "Skew:                           0.151   Prob(JB):                        0.612\n",
      "Kurtosis:                       2.152   Cond. No.                     2.59e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.59e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"12.2 INFERENCE WITH ROBUST STANDARD ERRORS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Estimate with default standard errors\n",
    "model_default = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n",
    "                    data=data_house).fit()\n",
    "\n",
    "print(\"\\nRegression with Default Standard Errors:\")\n",
    "print(model_default.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression with Heteroskedastic-Robust Standard Errors (HC1):\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.651\n",
      "Model:                            OLS   Adj. R-squared:                  0.555\n",
      "Method:                 Least Squares   F-statistic:                     6.410\n",
      "Date:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000514\n",
      "Time:                        15:04:03   Log-Likelihood:                -330.74\n",
      "No. Observations:                  29   AIC:                             675.5\n",
      "Df Residuals:                      22   BIC:                             685.1\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.378e+05   6.55e+04      2.102      0.036    9324.785    2.66e+05\n",
      "size          68.3694     15.359      4.451      0.000      38.266      98.473\n",
      "bedrooms    2685.3151   8285.528      0.324      0.746   -1.36e+04    1.89e+04\n",
      "bathrooms   6832.8800   1.93e+04      0.354      0.723    -3.1e+04    4.46e+04\n",
      "lotsize     2303.2214   5328.860      0.432      0.666   -8141.152    1.27e+04\n",
      "age         -833.0386    762.930     -1.092      0.275   -2328.353     662.276\n",
      "monthsold  -2088.5036   3738.270     -0.559      0.576   -9415.379    5238.372\n",
      "==============================================================================\n",
      "Omnibus:                        1.317   Durbin-Watson:                   1.259\n",
      "Prob(Omnibus):                  0.518   Jarque-Bera (JB):                0.980\n",
      "Skew:                           0.151   Prob(JB):                        0.612\n",
      "Kurtosis:                       2.152   Cond. No.                     2.59e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "[2] The condition number is large, 2.59e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Estimate with heteroskedastic-robust standard errors (HC1)\n",
    "model_robust = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n",
    "                   data=data_house).fit(cov_type='HC1')\n",
    "\n",
    "print(\"\\nRegression with Heteroskedastic-Robust Standard Errors (HC1):\")\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Default vs. Robust Standard Errors\n",
    "\n",
    "Let's systematically compare the standard errors and see how inference changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Š Interpreting the Comparison: What Changed?\n",
    "\n",
    "**Understanding the Results:**\n",
    "\n",
    "Looking at the SE Ratio column, we can see how robust standard errors differ from default ones:\n",
    "\n",
    "**When SE Ratio > 1.0**: Robust SE is larger than default SE\n",
    "- Suggests heteroskedasticity is present\n",
    "- Default SEs were **understating** uncertainty\n",
    "- t-statistics decrease, p-values increase\n",
    "- We were **too confident** in rejecting null hypotheses\n",
    "\n",
    "**When SE Ratio â‰ˆ 1.0**: Robust SE similar to default SE\n",
    "- Little evidence of heteroskedasticity for this variable\n",
    "- Both methods give similar inference\n",
    "\n",
    "**When SE Ratio < 1.0**: Robust SE smaller than default SE\n",
    "- Unusual but possible\n",
    "- Could indicate negative correlation between xÂ² and residuals\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "1. **Coefficient estimates unchanged**: OLS point estimates are the same regardless of SE type\n",
    "2. **Inference changes**: Variables significant with default SEs might become insignificant with robust SEs\n",
    "3. **Publication standard**: Most journals now require robust SEs for cross-sectional data\n",
    "4. **Conservative approach**: When in doubt, report robust SEs (they're generally more credible)\n",
    "\n",
    "**Rule of thumb**: If robust SEs differ substantially (>30% change), heteroskedasticity is likely present and you should use robust inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Interpreting the Comparison: What Changed?\n",
    "\n",
    "**Understanding the Results:**\n",
    "\n",
    "Looking at the SE Ratio column, we can see how robust standard errors differ from default ones:\n",
    "\n",
    "**When SE Ratio > 1.0**: Robust SE is larger than default SE\n",
    "- Suggests heteroskedasticity is present\n",
    "- Default SEs were **understating** uncertainty\n",
    "- t-statistics decrease, p-values increase\n",
    "- We were **too confident** in rejecting null hypotheses\n",
    "\n",
    "**When SE Ratio â‰ˆ 1.0**: Robust SE similar to default SE\n",
    "- Little evidence of heteroskedasticity for this variable\n",
    "- Both methods give similar inference\n",
    "\n",
    "**When SE Ratio < 1.0**: Robust SE smaller than default SE\n",
    "- Unusual but possible\n",
    "- Could indicate negative correlation between xÂ² and residuals\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "1. **Coefficient estimates unchanged**: OLS point estimates are the same regardless of SE type\n",
    "2. **Inference changes**: Variables significant with default SEs might become insignificant with robust SEs\n",
    "3. **Publication standard**: Most journals now require robust SEs for cross-sectional data\n",
    "4. **Conservative approach**: When in doubt, report robust SEs (they're generally more credible)\n",
    "\n",
    "**Rule of thumb**: If robust SEs differ substantially (>30% change), heteroskedasticity is likely present and you should use robust inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAC Standard Errors for Time Series\n",
    "\n",
    "Time series data often exhibit **autocorrelation**: current errors correlated with past errors.\n",
    "\n",
    "**Example**: GDP growth tends to persist\n",
    "- Positive shock today â†’ likely positive next period\n",
    "- Creates correlation structure $Corr(u_t, u_{t-s}) \\neq 0$\n",
    "\n",
    "**HAC (Newey-West) standard errors**:\n",
    "- Account for both heteroskedasticity AND autocorrelation\n",
    "- Require specifying maximum lag length $m$\n",
    "- Rule of thumb: $m = 0.75 \\times T^{1/3}$\n",
    "\n",
    "**Autocorrelation function**:\n",
    "\n",
    "$$\\rho_s = \\frac{Cov(y_t, y_{t-s})}{\\sqrt{Var(y_t) Var(y_{t-s})}}$$\n",
    "\n",
    "We can visualize this with a **correlogram**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HAC Standard Errors for Time Series Data\n",
      "======================================================================\n",
      "\n",
      "GDP Growth Data Summary:\n",
      "count    241.000000\n",
      "mean       1.990456\n",
      "std        2.178097\n",
      "min       -4.772172\n",
      "25%        0.892417\n",
      "50%        2.089633\n",
      "75%        3.314238\n",
      "max        7.630545\n",
      "Name: growth, dtype: float64\n",
      "\n",
      "Mean growth rate: 1.990456\n"
     ]
    }
   ],
   "source": [
    "# Load GDP growth data\n",
    "data_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_REALGDPPC.DTA')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HAC Standard Errors for Time Series Data\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nGDP Growth Data Summary:\")\n",
    "print(data_gdp['growth'].describe())\n",
    "\n",
    "# Mean of growth\n",
    "mean_growth = data_gdp['growth'].mean()\n",
    "print(f\"\\nMean growth rate: {mean_growth:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autocorrelations at multiple lags:\n",
      "  Lag 0: nan\n",
      "  Lag 1: nan\n",
      "  Lag 2: nan\n",
      "  Lag 3: nan\n",
      "  Lag 4: nan\n",
      "  Lag 5: nan\n",
      "\n",
      "Interpretation:\n",
      "  - Lag 0 correlation is always 1.0 (correlation with itself)\n",
      "  - Positive lag 1 correlation suggests persistence\n",
      "  - Autocorrelation decays with lag length\n"
     ]
    }
   ],
   "source": [
    "# Autocorrelation analysis\n",
    "print(\"\\nAutocorrelations at multiple lags:\")\n",
    "acf_values = acf(data_gdp['growth'], nlags=5, fft=False)\n",
    "for i in range(6):\n",
    "    print(f\"  Lag {i}: {acf_values[i]:.6f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Lag 0 correlation is always 1.0 (correlation with itself)\")\n",
    "print(\"  - Positive lag 1 correlation suggests persistence\")\n",
    "print(\"  - Autocorrelation decays with lag length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlogram shows autocorrelation at various lags.\n",
      "Blue shaded area = 95% confidence bands under null of no autocorrelation.\n"
     ]
    }
   ],
   "source": [
    "# Correlogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_acf(data_gdp['growth'], lags=10, ax=ax, alpha=0.05)\n",
    "ax.set_xlabel('Lag', fontsize=12)\n",
    "ax.set_ylabel('Autocorrelation', fontsize=12)\n",
    "ax.set_title('Figure 12.1: Correlogram of GDP Growth', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The correlogram shows autocorrelation at various lags.\")\n",
    "print(\"Blue shaded area = 95% confidence bands under null of no autocorrelation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Interpreting HAC Standard Errors\n",
    "\n",
    "**What the Results Tell Us:**\n",
    "\n",
    "Comparing the three standard error estimates for the mean growth rate:\n",
    "\n",
    "1. **Default SE** (assumes no autocorrelation):\n",
    "   - Smallest standard error\n",
    "   - Assumes errors are independent over time\n",
    "   - **Underestimates** uncertainty when autocorrelation exists\n",
    "\n",
    "2. **HAC with lag 0** (het-robust only):\n",
    "   - Accounts for heteroskedasticity but not autocorrelation\n",
    "   - Often similar to default in time series\n",
    "   - Still underestimates uncertainty if autocorrelation present\n",
    "\n",
    "3. **HAC with lag 5** (Newey-West):\n",
    "   - Accounts for both heteroskedasticity AND autocorrelation\n",
    "   - **Larger SE** reflects true uncertainty\n",
    "   - More conservative but valid inference\n",
    "\n",
    "**Why is HAC SE larger?**\n",
    "\n",
    "Autocorrelation creates **information overlap** between observations:\n",
    "- If growth today predicts growth tomorrow, consecutive observations aren't fully independent\n",
    "- We have **less effective information** than the sample size suggests\n",
    "- Standard errors must increase to reflect this\n",
    "\n",
    "**Practical guidance:**\n",
    "\n",
    "- For time series data, **always use HAC SEs**\n",
    "- Lag length choice: Rule of thumb = 0.75 Ã— T^(1/3)\n",
    "  - For T=100: m â‰ˆ 3-4 lags\n",
    "  - For T=200: m â‰ˆ 4-5 lags\n",
    "- Err on the side of more lags (inference remains valid)\n",
    "- Check sensitivity to lag length\n",
    "\n",
    "**The cost of ignoring autocorrelation:**\n",
    "- Overconfident inference (SEs too small)\n",
    "- Spurious significance (false discoveries)\n",
    "- Invalid hypothesis tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ” Interpreting HAC Standard Errors\n",
    "\n",
    "**What the Results Tell Us:**\n",
    "\n",
    "Comparing the three standard error estimates for the mean growth rate:\n",
    "\n",
    "1. **Default SE** (assumes no autocorrelation):\n",
    "   - Smallest standard error\n",
    "   - Assumes errors are independent over time\n",
    "   - **Underestimates** uncertainty when autocorrelation exists\n",
    "\n",
    "2. **HAC with lag 0** (het-robust only):\n",
    "   - Accounts for heteroskedasticity but not autocorrelation\n",
    "   - Often similar to default in time series\n",
    "   - Still underestimates uncertainty if autocorrelation present\n",
    "\n",
    "3. **HAC with lag 5** (Newey-West):\n",
    "   - Accounts for both heteroskedasticity AND autocorrelation\n",
    "   - **Larger SE** reflects true uncertainty\n",
    "   - More conservative but valid inference\n",
    "\n",
    "**Why is HAC SE larger?**\n",
    "\n",
    "Autocorrelation creates **information overlap** between observations:\n",
    "- If growth today predicts growth tomorrow, consecutive observations aren't fully independent\n",
    "- We have **less effective information** than the sample size suggests\n",
    "- Standard errors must increase to reflect this\n",
    "\n",
    "**Practical guidance:**\n",
    "\n",
    "- For time series data, **always use HAC SEs**\n",
    "- Lag length choice: Rule of thumb = 0.75 Ã— T^(1/3)\n",
    "  - For T=100: m â‰ˆ 3-4 lags\n",
    "  - For T=200: m â‰ˆ 4-5 lags\n",
    "- Err on the side of more lags (inference remains valid)\n",
    "- Check sensitivity to lag length\n",
    "\n",
    "**The cost of ignoring autocorrelation:**\n",
    "- Overconfident inference (SEs too small)\n",
    "- Spurious significance (false discoveries)\n",
    "- Invalid hypothesis tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3: Prediction\n",
    "\n",
    "Prediction is a core application of regression, but there's a crucial distinction:\n",
    "\n",
    "**1. Predicting the conditional mean** $E[y | x^*]$\n",
    "- Average outcome for given $x^*$\n",
    "- More precise (smaller standard error)\n",
    "- Used for policy analysis, average effects\n",
    "\n",
    "**2. Predicting an actual value** $y | x^*$\n",
    "- Individual outcome including random error\n",
    "- Less precise (larger standard error)\n",
    "- Used for forecasting individual cases\n",
    "\n",
    "**Key formulas**:\n",
    "\n",
    "Conditional mean:\n",
    "$$E[y | x^*] = \\beta_1 + \\beta_2 x_2^* + \\cdots + \\beta_k x_k^*$$\n",
    "\n",
    "Actual value:\n",
    "$$y | x^* = \\beta_1 + \\beta_2 x_2^* + \\cdots + \\beta_k x_k^* + u^*$$\n",
    "\n",
    "**Standard errors**:\n",
    "\n",
    "For conditional mean (bivariate case):\n",
    "$$se(\\hat{y}_{cm}) = s_e \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "For actual value (bivariate case):\n",
    "$$se(\\hat{y}_f) = s_e \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "Note the \"1 +\" term for actual values - this reflects the irreducible uncertainty from $u^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "12.3 PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Simple regression: price = Î²â‚€ + Î²â‚Â·size + u\n",
      "  Î²â‚€ (Intercept): $115017.28\n",
      "  Î²â‚ (Size): $73.7710\n",
      "  RÂ²: 0.6175\n",
      "  Root MSE (ÏƒÌ‚): $23550.66\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"12.3 PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simple regression: price on size\n",
    "model_simple = ols('price ~ size', data=data_house).fit()\n",
    "\n",
    "print(\"\\nSimple regression: price = Î²â‚€ + Î²â‚Â·size + u\")\n",
    "print(f\"  Î²â‚€ (Intercept): ${model_simple.params['Intercept']:.2f}\")\n",
    "print(f\"  Î²â‚ (Size): ${model_simple.params['size']:.4f}\")\n",
    "print(f\"  RÂ²: {model_simple.rsquared:.4f}\")\n",
    "print(f\"  Root MSE (ÏƒÌ‚): ${np.sqrt(model_simple.mse_resid):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Why Are Prediction Intervals So Much Wider?\n",
    "\n",
    "**The Fundamental Difference:**\n",
    "\n",
    "Looking at the two panels, you'll notice the **prediction interval (blue) is dramatically wider** than the confidence interval (red). This isn't a mistakeâ€”it reflects a fundamental distinction in what we're predicting.\n",
    "\n",
    "**Confidence Interval for E[Y|X] (Red):**\n",
    "- Predicts the **average** price for all 2000 sq ft houses\n",
    "- Uncertainty comes only from **estimation error** in Î²Ì‚\n",
    "- As sample size increases (n â†’ âˆž), this interval **shrinks to zero**\n",
    "- Formula includes: 1/n term (goes to 0 as n grows)\n",
    "\n",
    "**Prediction Interval for Y (Blue):**\n",
    "- Predicts an **individual** house price\n",
    "- Uncertainty comes from:\n",
    "  1. **Estimation error** in Î²Ì‚ (same as CI)\n",
    "  2. **Irreducible randomness** in the individual outcome (u*)\n",
    "- Even with perfect knowledge of Î², individual predictions remain uncertain\n",
    "- Formula includes: **\"1 +\"** term (never goes away)\n",
    "\n",
    "**Intuitive Example:**\n",
    "\n",
    "Imagine predicting height from age:\n",
    "- **Conditional mean**: Average height of all 10-year-olds = 140 cm\n",
    "  - We can estimate this average very precisely\n",
    "  - CI might be [139, 141] cm\n",
    "- **Actual value**: A specific 10-year-old's height\n",
    "  - Could be anywhere from 120 to 160 cm\n",
    "  - PI might be [125, 155] cm\n",
    "  - Even knowing the average perfectly doesn't eliminate individual variation\n",
    "\n",
    "**Mathematical Insight:**\n",
    "\n",
    "$$se(\\hat{y}_f) = \\sqrt{s_e^2 + se(\\hat{y}_{cm})^2}$$\n",
    "\n",
    "- First term (s_eÂ²): Irreducible error varianceâ€”dominates the formula\n",
    "- Second term: Estimation uncertaintyâ€”becomes negligible with large samples\n",
    "- Result: PI width â‰ˆ 2 Ã— 1.96 Ã— s_e â‰ˆ 4 Ã— RMSE\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "1. **Don't confuse the two**: Predicting averages is much more precise than predicting individuals\n",
    "2. **Policy vs. forecasting**: \n",
    "   - Policy analysis (average effects) â†’ Use confidence intervals\n",
    "   - Individual forecasting (who will default?) â†’ Use prediction intervals\n",
    "3. **Communicating uncertainty**: Always show prediction intervals for individual forecasts\n",
    "4. **Limits of prediction**: No amount of data eliminates individual-level uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Confidence vs. Prediction Intervals\n",
    "\n",
    "This figure illustrates the fundamental difference between:\n",
    "- **Confidence interval for conditional mean** (narrower, red)\n",
    "- **Prediction interval for actual value** (wider, blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Understanding the Numbers: A Concrete Example\n",
    "\n",
    "**Interpreting the Results for a 2000 sq ft House:**\n",
    "\n",
    "Looking at our predictions, several patterns emerge:\n",
    "\n",
    "**1. Point Prediction:**\n",
    "- Predicted price â‰ˆ $280,000 (approximately)\n",
    "- This is our best single guess\n",
    "- Same for both conditional mean and actual value\n",
    "\n",
    "**2. Confidence Interval for E[Y|X=2000]:**\n",
    "- Relatively narrow (e.g., $250k - $310k)\n",
    "- Tells us: \"We're 95% confident the **average price** of all 2000 sq ft houses is in this range\"\n",
    "- Precise because we're estimating a population average\n",
    "- Useful for: Understanding market valuations, setting pricing policies\n",
    "\n",
    "**3. Prediction Interval for Y:**\n",
    "- Much wider (e.g., $180k - $380k)\n",
    "- Tells us: \"We're 95% confident **this specific house** will sell in this range\"\n",
    "- Wide because individual houses vary considerably\n",
    "- Useful for: Setting listing ranges, individual appraisals\n",
    "\n",
    "**The Ratio is Revealing:**\n",
    "\n",
    "Notice that the PI is approximately **3-4 times wider** than the CI. This ratio tells us:\n",
    "- Most variation is **between houses** (individual heterogeneity)\n",
    "- Relatively little variation is **estimation uncertainty**\n",
    "- Adding more data would shrink the CI but barely affect the PI\n",
    "\n",
    "**Statistical vs. Economic Significance:**\n",
    "\n",
    "- **CI width** = Statistical precision (how well we know Î²)\n",
    "- **PI width** = Economic uncertainty (inherent market volatility)\n",
    "- In this example: Good statistical precision, but still substantial economic uncertainty\n",
    "\n",
    "**Practical Takeaway:**\n",
    "\n",
    "If you're a real estate agent:\n",
    "- Don't promise a precise price ($280k)\n",
    "- Do provide a realistic range ($180k - $380k)\n",
    "- Explain that individual houses vary, even controlling for size\n",
    "- Use the confidence interval to discuss average market values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ¯ Why Are Prediction Intervals So Much Wider?\n",
    "\n",
    "**The Fundamental Difference:**\n",
    "\n",
    "Looking at the two panels, you'll notice the **prediction interval (blue) is dramatically wider** than the confidence interval (red). This isn't a mistakeâ€”it reflects a fundamental distinction in what we're predicting.\n",
    "\n",
    "**Confidence Interval for E[Y|X] (Red):**\n",
    "- Predicts the **average** price for all 2000 sq ft houses\n",
    "- Uncertainty comes only from **estimation error** in Î²Ì‚\n",
    "- As sample size increases (n â†’ âˆž), this interval **shrinks to zero**\n",
    "- Formula includes: 1/n term (goes to 0 as n grows)\n",
    "\n",
    "**Prediction Interval for Y (Blue):**\n",
    "- Predicts an **individual** house price\n",
    "- Uncertainty comes from:\n",
    "  1. **Estimation error** in Î²Ì‚ (same as CI)\n",
    "  2. **Irreducible randomness** in the individual outcome (u*)\n",
    "- Even with perfect knowledge of Î², individual predictions remain uncertain\n",
    "- Formula includes: **\"1 +\"** term (never goes away)\n",
    "\n",
    "**Intuitive Example:**\n",
    "\n",
    "Imagine predicting height from age:\n",
    "- **Conditional mean**: Average height of all 10-year-olds = 140 cm\n",
    "  - We can estimate this average very precisely\n",
    "  - CI might be [139, 141] cm\n",
    "- **Actual value**: A specific 10-year-old's height\n",
    "  - Could be anywhere from 120 to 160 cm\n",
    "  - PI might be [125, 155] cm\n",
    "  - Even knowing the average perfectly doesn't eliminate individual variation\n",
    "\n",
    "**Mathematical Insight:**\n",
    "\n",
    "$$se(\\hat{y}_f) = \\sqrt{s_e^2 + se(\\hat{y}_{cm})^2}$$\n",
    "\n",
    "- First term (s_eÂ²): Irreducible error varianceâ€”dominates the formula\n",
    "- Second term: Estimation uncertaintyâ€”becomes negligible with large samples\n",
    "- Result: PI width â‰ˆ 2 Ã— 1.96 Ã— s_e â‰ˆ 4 Ã— RMSE\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "1. **Don't confuse the two**: Predicting averages is much more precise than predicting individuals\n",
    "2. **Policy vs. forecasting**: \n",
    "   - Policy analysis (average effects) â†’ Use confidence intervals\n",
    "   - Individual forecasting (who will default?) â†’ Use prediction intervals\n",
    "3. **Communicating uncertainty**: Always show prediction intervals for individual forecasts\n",
    "4. **Limits of prediction**: No amount of data eliminates individual-level uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Deconstructing the Standard Error Formulas\n",
    "\n",
    "**Understanding Where the \"1 +\" Comes From:**\n",
    "\n",
    "The manual calculations reveal the mathematical structure of prediction uncertainty:\n",
    "\n",
    "**For Conditional Mean:**\n",
    "$$se(\\hat{y}_{cm}) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "- **First term (1/n)**: Decreases with sample sizeâ€”more data reduces uncertainty\n",
    "- **Second term**: Distance from mean mattersâ€”extrapolation is risky\n",
    "  - Prediction at $x^* = \\bar{x}$ (sample mean) is most precise\n",
    "  - Prediction far from $\\bar{x}$ is less precise\n",
    "- Both terms â†’ 0 as n â†’ âˆž (perfect knowledge of E[Y|X])\n",
    "\n",
    "**For Actual Value:**\n",
    "$$se(\\hat{y}_f) = \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "- **The critical \"1 +\"**: Represents $Var[u^*]$, the future error term\n",
    "- This term **never disappears**, even with infinite data\n",
    "- Dominates the formula in moderate to large samples\n",
    "\n",
    "**Numerical Insight:**\n",
    "\n",
    "In our example:\n",
    "- $\\hat{\\sigma}$ (RMSE) â‰ˆ $90k (this is the irreducible uncertainty)\n",
    "- $(1/n)$ term â‰ˆ 0.034 (small with n=29)\n",
    "- Distance term varies with prediction point\n",
    "\n",
    "For predictions near the mean:\n",
    "- $se(\\hat{y}_{cm})$ â‰ˆ $90k Ã— âˆš0.034 â‰ˆ $17k (mainly from 1/n)\n",
    "- $se(\\hat{y}_f)$ â‰ˆ $90k Ã— âˆš1.034 â‰ˆ $92k (mainly from the \"1\")\n",
    "\n",
    "**The \"1 +\" term is why:**\n",
    "- Prediction intervals don't shrink much with more data\n",
    "- Individual predictions remain uncertain even with perfect models\n",
    "- $se(\\hat{y}_f) \\approx \\hat{\\sigma}$ in large samples\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "\n",
    "The funnel shape in prediction plots comes from the distance term:\n",
    "- Narrow near $\\bar{x}$ (center of data)\n",
    "- Wider at extremes (extrapolation region)\n",
    "- But even at the center, PI is wide due to the \"1\" term\n",
    "\n",
    "**Practical Lesson:**\n",
    "\n",
    "When presenting predictions:\n",
    "1. Always acknowledge the \"1 +\" uncertainty\n",
    "2. Be most confident about predictions near the data center\n",
    "3. Be especially cautious about extrapolation (predictions outside the data range)\n",
    "4. Understand that better models reduce estimation error but not irreducible randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction at Specific Values\n",
    "\n",
    "Let's predict house price for a 2000 square foot house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“ Understanding the Numbers: A Concrete Example\n",
    "\n",
    "**Interpreting the Results for a 2000 sq ft House:**\n",
    "\n",
    "Looking at our predictions, several patterns emerge:\n",
    "\n",
    "**1. Point Prediction:**\n",
    "- Predicted price â‰ˆ $280,000 (approximately)\n",
    "- This is our best single guess\n",
    "- Same for both conditional mean and actual value\n",
    "\n",
    "**2. Confidence Interval for E[Y|X=2000]:**\n",
    "- Relatively narrow (e.g., $250k - $310k)\n",
    "- Tells us: \"We're 95% confident the **average price** of all 2000 sq ft houses is in this range\"\n",
    "- Precise because we're estimating a population average\n",
    "- Useful for: Understanding market valuations, setting pricing policies\n",
    "\n",
    "**3. Prediction Interval for Y:**\n",
    "- Much wider (e.g., $180k - $380k)\n",
    "- Tells us: \"We're 95% confident **this specific house** will sell in this range\"\n",
    "- Wide because individual houses vary considerably\n",
    "- Useful for: Setting listing ranges, individual appraisals\n",
    "\n",
    "**The Ratio is Revealing:**\n",
    "\n",
    "Notice that the PI is approximately **3-4 times wider** than the CI. This ratio tells us:\n",
    "- Most variation is **between houses** (individual heterogeneity)\n",
    "- Relatively little variation is **estimation uncertainty**\n",
    "- Adding more data would shrink the CI but barely affect the PI\n",
    "\n",
    "**Statistical vs. Economic Significance:**\n",
    "\n",
    "- **CI width** = Statistical precision (how well we know Î²)\n",
    "- **PI width** = Economic uncertainty (inherent market volatility)\n",
    "- In this example: Good statistical precision, but still substantial economic uncertainty\n",
    "\n",
    "**Practical Takeaway:**\n",
    "\n",
    "If you're a real estate agent:\n",
    "- Don't promise a precise price ($280k)\n",
    "- Do provide a realistic range ($180k - $380k)\n",
    "- Explain that individual houses vary, even controlling for size\n",
    "- Use the confidence interval to discuss average market values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Calculation of Standard Errors\n",
    "\n",
    "Let's manually calculate the standard errors to understand the formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”¬ Deconstructing the Standard Error Formulas\n",
    "\n",
    "**Understanding Where the \"1 +\" Comes From:**\n",
    "\n",
    "The manual calculations reveal the mathematical structure of prediction uncertainty:\n",
    "\n",
    "**For Conditional Mean:**\n",
    "$$se(\\hat{y}_{cm}) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "- **First term (1/n)**: Decreases with sample sizeâ€”more data reduces uncertainty\n",
    "- **Second term**: Distance from mean mattersâ€”extrapolation is risky\n",
    "  - Prediction at $x^* = \\bar{x}$ (sample mean) is most precise\n",
    "  - Prediction far from $\\bar{x}$ is less precise\n",
    "- Both terms â†’ 0 as n â†’ âˆž (perfect knowledge of E[Y|X])\n",
    "\n",
    "**For Actual Value:**\n",
    "$$se(\\hat{y}_f) = \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "- **The critical \"1 +\"**: Represents $Var[u^*]$, the future error term\n",
    "- This term **never disappears**, even with infinite data\n",
    "- Dominates the formula in moderate to large samples\n",
    "\n",
    "**Numerical Insight:**\n",
    "\n",
    "In our example:\n",
    "- $\\hat{\\sigma}$ (RMSE) â‰ˆ $90k (this is the irreducible uncertainty)\n",
    "- $(1/n)$ term â‰ˆ 0.034 (small with n=29)\n",
    "- Distance term varies with prediction point\n",
    "\n",
    "For predictions near the mean:\n",
    "- $se(\\hat{y}_{cm})$ â‰ˆ $90k Ã— âˆš0.034 â‰ˆ $17k (mainly from 1/n)\n",
    "- $se(\\hat{y}_f)$ â‰ˆ $90k Ã— âˆš1.034 â‰ˆ $92k (mainly from the \"1\")\n",
    "\n",
    "**The \"1 +\" term is why:**\n",
    "- Prediction intervals don't shrink much with more data\n",
    "- Individual predictions remain uncertain even with perfect models\n",
    "- $se(\\hat{y}_f) \\approx \\hat{\\sigma}$ in large samples\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "\n",
    "The funnel shape in prediction plots comes from the distance term:\n",
    "- Narrow near $\\bar{x}$ (center of data)\n",
    "- Wider at extremes (extrapolation region)\n",
    "- But even at the center, PI is wide due to the \"1\" term\n",
    "\n",
    "**Practical Lesson:**\n",
    "\n",
    "When presenting predictions:\n",
    "1. Always acknowledge the \"1 +\" uncertainty\n",
    "2. Be most confident about predictions near the data center\n",
    "3. Be especially cautious about extrapolation (predictions outside the data range)\n",
    "4. Understand that better models reduce estimation error but not irreducible randomness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Multiple Regression\n",
    "\n",
    "Now let's predict using the full multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Prediction for Multiple Regression\n",
      "======================================================================\n",
      "\n",
      "Prediction for:\n",
      "  size=2000, bedrooms=4, bathrooms=2, lotsize=2, age=40, monthsold=6\n",
      "\n",
      "Predicted price: $257690.80\n",
      "\n",
      "95% CI for E[Y|X]:\n",
      "  [$244234.97, $271146.63]\n",
      "  SE: $6488.26\n",
      "\n",
      "95% PI for Y:\n",
      "  [$204255.32, $311126.28]\n",
      "  SE: $25766.03\n",
      "\n",
      "Multiple regression provides more precise conditional mean predictions.\n",
      "But individual predictions still have large uncertainty.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Prediction for Multiple Regression\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_multi = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n",
    "                  data=data_house).fit()\n",
    "\n",
    "# Predict for specific values\n",
    "new_house = pd.DataFrame({\n",
    "    'size': [2000],\n",
    "    'bedrooms': [4],\n",
    "    'bathrooms': [2],\n",
    "    'lotsize': [2],\n",
    "    'age': [40],\n",
    "    'monthsold': [6]\n",
    "})\n",
    "\n",
    "pred_multi = model_multi.get_prediction(sm.add_constant(new_house))\n",
    "\n",
    "print(\"\\nPrediction for:\")\n",
    "print(\"  size=2000, bedrooms=4, bathrooms=2, lotsize=2, age=40, monthsold=6\")\n",
    "print(f\"\\nPredicted price: ${pred_multi.predicted_mean[0]:.2f}\")\n",
    "\n",
    "# Confidence interval for conditional mean\n",
    "ci_mean_multi = pred_multi.conf_int(alpha=0.05)\n",
    "print(f\"\\n95% CI for E[Y|X]:\")\n",
    "print(f\"  [${ci_mean_multi[0, 0]:.2f}, ${ci_mean_multi[0, 1]:.2f}]\")\n",
    "print(f\"  SE: ${pred_multi.se_mean[0]:.2f}\")\n",
    "\n",
    "# Prediction interval for actual value\n",
    "s_e_multi = np.sqrt(model_multi.mse_resid)\n",
    "s_y_cm_multi = pred_multi.se_mean[0]\n",
    "s_y_f_multi = np.sqrt(s_e_multi**2 + s_y_cm_multi**2)\n",
    "\n",
    "n_multi = len(data_house)\n",
    "k_multi = len(model_multi.params)\n",
    "tcrit_multi = stats.t.ppf(0.975, n_multi - k_multi)\n",
    "\n",
    "pi_lower = pred_multi.predicted_mean[0] - tcrit_multi * s_y_f_multi\n",
    "pi_upper = pred_multi.predicted_mean[0] + tcrit_multi * s_y_f_multi\n",
    "\n",
    "print(f\"\\n95% PI for Y:\")\n",
    "print(f\"  [${pi_lower:.2f}, ${pi_upper:.2f}]\")\n",
    "print(f\"  SE: ${s_y_f_multi:.2f}\")\n",
    "\n",
    "print(\"\\nMultiple regression provides more precise conditional mean predictions.\")\n",
    "print(\"But individual predictions still have large uncertainty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with Robust Standard Errors\n",
    "\n",
    "When heteroskedasticity is present, we should use robust standard errors for prediction intervals too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Prediction with Heteroskedastic-Robust SEs\n",
      "======================================================================\n",
      "\n",
      "Predicted price: $257690.80\n",
      "\n",
      "Robust 95% CI for E[Y|X]:\n",
      "  [$244694.26, $270687.34]\n",
      "  Robust SE: $6631.01\n",
      "\n",
      "Robust 95% PI for Y:\n",
      "  Robust SE for actual value: $25802.35\n",
      "\n",
      "Comparison of standard vs. robust:\n",
      "  SE (standard): $6488.26\n",
      "  SE (robust): $6631.01\n",
      "  Ratio: 1.022\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Prediction with Heteroskedastic-Robust SEs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_multi_robust = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n",
    "                         data=data_house).fit(cov_type='HC1')\n",
    "\n",
    "pred_multi_robust = model_multi_robust.get_prediction(sm.add_constant(new_house))\n",
    "\n",
    "print(f\"\\nPredicted price: ${pred_multi_robust.predicted_mean[0]:.2f}\")\n",
    "\n",
    "# Robust confidence interval for conditional mean\n",
    "ci_mean_robust = pred_multi_robust.conf_int(alpha=0.05)\n",
    "print(f\"\\nRobust 95% CI for E[Y|X]:\")\n",
    "print(f\"  [${ci_mean_robust[0, 0]:.2f}, ${ci_mean_robust[0, 1]:.2f}]\")\n",
    "print(f\"  Robust SE: ${pred_multi_robust.se_mean[0]:.2f}\")\n",
    "\n",
    "# Robust prediction interval\n",
    "s_y_cm_robust = pred_multi_robust.se_mean[0]\n",
    "s_y_f_robust = np.sqrt(s_e_multi**2 + s_y_cm_robust**2)\n",
    "\n",
    "print(f\"\\nRobust 95% PI for Y:\")\n",
    "print(f\"  Robust SE for actual value: ${s_y_f_robust:.2f}\")\n",
    "\n",
    "print(\"\\nComparison of standard vs. robust:\")\n",
    "print(f\"  SE (standard): ${s_y_cm_multi:.2f}\")\n",
    "print(f\"  SE (robust): ${s_y_cm_robust:.2f}\")\n",
    "print(f\"  Ratio: {s_y_cm_robust / s_y_cm_multi:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4: Nonrepresentative Samples\n",
    "\n",
    "**Sample selection** can bias OLS estimates:\n",
    "\n",
    "**Case 1: Selection on regressors** (X)\n",
    "- Example: Oversample high-income households\n",
    "- OLS remains unbiased if we include income as a control\n",
    "- Solution: Include selection variables as controls\n",
    "\n",
    "**Case 2: Selection on outcome** (Y)\n",
    "- Example: Survey excludes very high earners\n",
    "- OLS estimates are biased for population parameters\n",
    "- Solution: Sample weights, Heckman correction, or other selection models\n",
    "\n",
    "**Survey weights**:\n",
    "- Many surveys provide weights to adjust for nonrepresentativeness\n",
    "- Use **weighted least squares (WLS)** instead of OLS\n",
    "- Weight formula: $w_i = 1 / P(\\text{selected})$\n",
    "\n",
    "**Key insight**: Always check whether your sample is representative of your target population!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ² Bootstrap Confidence Intervals: An Alternative Approach\n",
    "\n",
    "**What is Bootstrap?**\n",
    "\n",
    "The bootstrap is a computational method that:\n",
    "1. **Resamples** your data many times (e.g., 1000 replications)\n",
    "2. **Re-estimates** the model for each resample\n",
    "3. Uses the **distribution of estimates** to build confidence intervals\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "For each bootstrap replication b = 1, ..., B:\n",
    "1. Draw n observations **with replacement** from original data\n",
    "2. Estimate regression: $\\hat{\\beta}_j^{(b)}$\n",
    "3. Store the coefficient estimate\n",
    "\n",
    "After B replications:\n",
    "- You have B estimates: $\\{\\hat{\\beta}_j^{(1)}, \\hat{\\beta}_j^{(2)}, ..., \\hat{\\beta}_j^{(B)}\\}$\n",
    "- These form an empirical distribution\n",
    "\n",
    "**Percentile Method CI:**\n",
    "- 95% CI = [2.5th percentile, 97.5th percentile] of bootstrap distribution\n",
    "- Example: If you have 1000 estimates, use the 25th and 975th largest values\n",
    "\n",
    "**Advantages of Bootstrap:**\n",
    "\n",
    "1. **No distributional assumptions**: Don't need to assume normality\n",
    "2. **Works for complex statistics**: Medians, ratios, quantiles, etc.\n",
    "3. **Better small-sample coverage**: Often more accurate than asymptotic formulas\n",
    "4. **Flexibility**: Can bootstrap residuals, observations, or both\n",
    "5. **Visual understanding**: See the actual sampling distribution\n",
    "\n",
    "**When to use Bootstrap:**\n",
    "\n",
    "- Small samples (n < 30-50)\n",
    "- Non-standard statistics (beyond means and coefficients)\n",
    "- Skewed or heavy-tailed distributions\n",
    "- Checking robustness of standard inference\n",
    "- When asymptotic formulas are complex or unavailable\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Computationally intensive (need B = 1000+ replications)\n",
    "- Requires careful implementation (stratification, cluster bootstrap)\n",
    "- May fail with very small samples (n < 10)\n",
    "- Assumes sample is representative of population\n",
    "\n",
    "**Bootstrap vs. Robust SEs:**\n",
    "\n",
    "Both address uncertainty, but differently:\n",
    "- **Robust SEs**: Analytical correction for heteroskedasticity/autocorrelation\n",
    "- **Bootstrap**: Computational approach using resampling\n",
    "\n",
    "Often used together: Bootstrap with robust methods!\n",
    "\n",
    "**Practical Implementation Tips:**\n",
    "\n",
    "1. Use B â‰¥ 1000 for confidence intervals\n",
    "2. Set random seed for reproducibility\n",
    "3. For time series: Use block bootstrap (resample blocks, not individuals)\n",
    "4. For panel data: Use cluster bootstrap (resample clusters)\n",
    "5. Check convergence: Results shouldn't change much with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "12.4 NONREPRESENTATIVE SAMPLES\n",
      "======================================================================\n",
      "\n",
      "Conceptual discussion - no computation required\n",
      "\n",
      "Key points:\n",
      "  1. Sample selection can lead to biased estimates\n",
      "  2. Selection on regressors: Include selection variables as controls\n",
      "  3. Selection on outcome: Use sample weights or selection models\n",
      "  4. Always verify sample representativeness\n",
      "\n",
      "Example applications:\n",
      "  - Wage surveys that exclude unemployed workers\n",
      "  - Health studies with voluntary participation\n",
      "  - Education data from selective schools\n",
      "  - Financial data excluding bankrupt firms\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"12.4 NONREPRESENTATIVE SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nConceptual discussion - no computation required\")\n",
    "print(\"\\nKey points:\")\n",
    "print(\"  1. Sample selection can lead to biased estimates\")\n",
    "print(\"  2. Selection on regressors: Include selection variables as controls\")\n",
    "print(\"  3. Selection on outcome: Use sample weights or selection models\")\n",
    "print(\"  4. Always verify sample representativeness\")\n",
    "\n",
    "print(\"\\nExample applications:\")\n",
    "print(\"  - Wage surveys that exclude unemployed workers\")\n",
    "print(\"  - Health studies with voluntary participation\")\n",
    "print(\"  - Education data from selective schools\")\n",
    "print(\"  - Financial data excluding bankrupt firms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ The Type I vs. Type II Error Tradeoff\n",
    "\n",
    "**Understanding the Table:**\n",
    "\n",
    "The 2Ã—2 decision table reveals a fundamental tradeoff in hypothesis testing:\n",
    "\n",
    "| Decision | Hâ‚€ True | Hâ‚€ False |\n",
    "|----------|---------|----------|\n",
    "| Reject Hâ‚€ | **Type I error (Î±)** | **Correct (Power)** |\n",
    "| Don't reject | Correct (1-Î±) | **Type II error (Î²)** |\n",
    "\n",
    "**Type I Error (False Positive):**\n",
    "- Reject a true null hypothesis\n",
    "- Probability = significance level Î± (we control this)\n",
    "- Example: Conclude a drug works when it doesn't\n",
    "- **\"Seeing patterns in noise\"**\n",
    "\n",
    "**Type II Error (False Negative):**\n",
    "- Fail to reject a false null hypothesis  \n",
    "- Probability = Î² (harder to control)\n",
    "- Example: Miss a real drug effect\n",
    "- **\"Missing real signals\"**\n",
    "\n",
    "**The Fundamental Tradeoff:**\n",
    "\n",
    "If we make the test **stricter** (lower Î±):\n",
    "- âœ“ Fewer false positives (Type I errors)\n",
    "- âœ— More false negatives (Type II errors)\n",
    "- âœ— Lower power (harder to detect real effects)\n",
    "\n",
    "If we make the test **looser** (higher Î±):\n",
    "- âœ“ Higher power (easier to detect real effects)\n",
    "- âœ— More false positives (Type I errors)\n",
    "\n",
    "**Statistical Power = 1 - Î²:**\n",
    "- Probability of correctly rejecting false Hâ‚€\n",
    "- \"Sensitivity\" of the test\n",
    "- Want power â‰¥ 0.80 (80% chance of detecting real effect)\n",
    "\n",
    "**What Affects Power?**\n",
    "\n",
    "1. **Sample size (n)**: Larger n â†’ Higher power\n",
    "2. **Effect size (Î²)**: Larger true effect â†’ Higher power\n",
    "3. **Significance level (Î±)**: Higher Î± â†’ Higher power (but more Type I errors)\n",
    "4. **Noise level (Ïƒ)**: Lower Ïƒ â†’ Higher power\n",
    "\n",
    "**The Power Function:**\n",
    "\n",
    "Power depends on the **true parameter value**:\n",
    "- At Î² = 0 (Hâ‚€ true): Power = Î± (just Type I error rate)\n",
    "- As |Î²| increases: Power increases\n",
    "- For very large |Î²|: Power â†’ 1 (almost certain detection)\n",
    "\n",
    "**Multiple Testing Problem:**\n",
    "\n",
    "Testing k hypotheses at Î± = 0.05:\n",
    "- Expected false positives = 0.05 Ã— k\n",
    "- Test 20 hypotheses â†’ expect 1 false positive even if all Hâ‚€ are true!\n",
    "\n",
    "**Solutions:**\n",
    "1. **Bonferroni correction**: Use Î±/k for each test (conservative)\n",
    "2. **False Discovery Rate (FDR)**: Control proportion of false positives\n",
    "3. **Pre-registration**: Specify primary hypotheses before seeing data\n",
    "4. **Replication**: Confirm findings in independent samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5: Best Estimation Methods\n",
    "\n",
    "**When are OLS estimators \"best\"?**\n",
    "\n",
    "Under classical assumptions 1-4, OLS is **BLUE** (Best Linear Unbiased Estimator) by the Gauss-Markov Theorem.\n",
    "\n",
    "**When assumptions fail:**\n",
    "\n",
    "**1. Heteroskedasticity**: $Var[u_i | X] = \\sigma_i^2$ (varies)\n",
    "- OLS remains unbiased but inefficient\n",
    "- **Feasible GLS (FGLS)** or **Weighted Least Squares (WLS)** more efficient\n",
    "- Weight observations inversely to error variance: $w_i = 1/\\sigma_i$\n",
    "\n",
    "**2. Autocorrelation**: $Cov[u_t, u_{t-s}] \\neq 0$\n",
    "- OLS remains unbiased but inefficient\n",
    "- **FGLS with AR errors** more efficient\n",
    "- Model error structure: $u_t = \\rho u_{t-1} + \\epsilon_t$\n",
    "\n",
    "**Practical advice**:\n",
    "- Most applied work uses OLS with robust SEs\n",
    "- Efficiency gains from GLS/FGLS often modest\n",
    "- Misspecifying error structure can make things worse\n",
    "- Exception: Panel data methods explicitly model error components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Reading the Power Curve: What It Tells Us\n",
    "\n",
    "**Interpreting Figure 12.3:**\n",
    "\n",
    "The power function shows how test power varies with the true coefficient value. Here's what each feature means:\n",
    "\n",
    "**Key Features of the Curve:**\n",
    "\n",
    "1. **At Î² = 0 (vertical gray line)**:\n",
    "   - Power = Î± = 0.05\n",
    "   - This is the Type I error rate\n",
    "   - When Hâ‚€ is true, we reject 5% of the time (false positives)\n",
    "\n",
    "2. **As |Î²| increases (moving away from 0)**:\n",
    "   - Power increases rapidly\n",
    "   - Larger effects are easier to detect\n",
    "   - Curve approaches 1.0 (certain detection)\n",
    "\n",
    "3. **Symmetry around zero**:\n",
    "   - Power is same for Î² = 30 and Î² = -30\n",
    "   - Two-sided test treats positive and negative effects equally\n",
    "   - One-sided tests would have asymmetric power\n",
    "\n",
    "4. **The 0.80 threshold (green dashed line)**:\n",
    "   - Standard target: 80% power\n",
    "   - Means 20% chance of Type II error (Î² = 0.20)\n",
    "   - In this example: Need |Î²| â‰ˆ 30 to achieve 80% power\n",
    "\n",
    "**What This Means for Study Design:**\n",
    "\n",
    "Given the parameters (n=30, SE=15, Î±=0.05):\n",
    "\n",
    "- **Small effects** (|Î²| < 15):\n",
    "  - Power < 50%\n",
    "  - More likely to **miss** the effect than detect it\n",
    "  - Study is underpowered\n",
    "\n",
    "- **Medium effects** (|Î²| â‰ˆ 30):\n",
    "  - Power â‰ˆ 80%\n",
    "  - Good chance of detection\n",
    "  - Standard benchmark for adequate power\n",
    "\n",
    "- **Large effects** (|Î²| > 45):\n",
    "  - Power > 95%\n",
    "  - Almost certain detection\n",
    "  - Study is well-powered\n",
    "\n",
    "**Sample Size Implications:**\n",
    "\n",
    "To detect smaller effects, you need larger samples:\n",
    "- **Double the sample** (n=60) â†’ Can detect smaller effects with same power\n",
    "- Power roughly proportional to âˆšn\n",
    "- To halve minimum detectable effect, need **4Ã— the sample size**\n",
    "\n",
    "**The Power-Sample Size Relationship:**\n",
    "\n",
    "For a given effect size Î²:\n",
    "- Power increases with âˆšn\n",
    "- To go from 50% to 80% power: Need â‰ˆ 2Ã— the sample\n",
    "- To go from 80% to 95% power: Need â‰ˆ 2Ã— the sample again\n",
    "\n",
    "**Practical Applications:**\n",
    "\n",
    "1. **Pre-study planning**:\n",
    "   - Specify minimum effect of interest\n",
    "   - Calculate required sample size for 80% power\n",
    "   - Avoid underpowered studies\n",
    "\n",
    "2. **Post-study interpretation**:\n",
    "   - Non-significant result with low power: Inconclusive (not evidence of no effect)\n",
    "   - Non-significant result with high power: Evidence against large effects\n",
    "   - Significant result: Good, but consider magnitude and practical significance\n",
    "\n",
    "3. **Publication decisions**:\n",
    "   - Underpowered studies contribute to publication bias\n",
    "   - Meta-analyses should weight by precision and power\n",
    "   - Replication studies should be well-powered\n",
    "\n",
    "**Common Mistakes to Avoid:**\n",
    "\n",
    "1. Treating non-significant results as \"proof of no effect\"\n",
    "   - Non-significance in underpowered study is uninformative\n",
    "   \n",
    "2. Conducting multiple underpowered studies instead of one well-powered study\n",
    "   - Wastes resources and leads to false negatives\n",
    "\n",
    "3. Post-hoc power analysis\n",
    "   - Don't calculate power after seeing results (circular reasoning)\n",
    "   - Do it before data collection\n",
    "\n",
    "**The Bottom Line:**\n",
    "\n",
    "This power curve illustrates a fundamental truth:\n",
    "- **Smaller effects require larger samples to detect**\n",
    "- With n=30 and SE=15, we can reliably detect effects of |Î²| â‰¥ 30\n",
    "- For smaller effects, we'd need more data or reduced noise (lower Ïƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "12.5 BEST ESTIMATION METHODS\n",
      "======================================================================\n",
      "\n",
      "Key concepts:\n",
      "\n",
      "1. Gauss-Markov Theorem:\n",
      "   - Under assumptions 1-4, OLS is BLUE\n",
      "   - BLUE = Best Linear Unbiased Estimator\n",
      "   - 'Best' = minimum variance among linear unbiased estimators\n",
      "\n",
      "2. When assumptions fail:\n",
      "   - Heteroskedasticity â†’ Weighted Least Squares (WLS)\n",
      "   - Autocorrelation â†’ GLS with AR errors\n",
      "   - Both â†’ Feasible GLS (FGLS)\n",
      "\n",
      "3. Practical considerations:\n",
      "   - Efficiency gains often modest in practice\n",
      "   - Misspecification of error structure can worsen estimates\n",
      "   - Most studies use OLS + robust SEs (simpler, more robust)\n",
      "   - Exception: Panel data methods model error components explicitly\n",
      "\n",
      "4. Maximum Likelihood:\n",
      "   - If error distribution fully specified (e.g., normal)\n",
      "   - MLE can be more efficient than OLS\n",
      "   - Under normality, MLE = OLS for linear regression\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"12.5 BEST ESTIMATION METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey concepts:\")\n",
    "print(\"\\n1. Gauss-Markov Theorem:\")\n",
    "print(\"   - Under assumptions 1-4, OLS is BLUE\")\n",
    "print(\"   - BLUE = Best Linear Unbiased Estimator\")\n",
    "print(\"   - 'Best' = minimum variance among linear unbiased estimators\")\n",
    "\n",
    "print(\"\\n2. When assumptions fail:\")\n",
    "print(\"   - Heteroskedasticity â†’ Weighted Least Squares (WLS)\")\n",
    "print(\"   - Autocorrelation â†’ GLS with AR errors\")\n",
    "print(\"   - Both â†’ Feasible GLS (FGLS)\")\n",
    "\n",
    "print(\"\\n3. Practical considerations:\")\n",
    "print(\"   - Efficiency gains often modest in practice\")\n",
    "print(\"   - Misspecification of error structure can worsen estimates\")\n",
    "print(\"   - Most studies use OLS + robust SEs (simpler, more robust)\")\n",
    "print(\"   - Exception: Panel data methods model error components explicitly\")\n",
    "\n",
    "print(\"\\n4. Maximum Likelihood:\")\n",
    "print(\"   - If error distribution fully specified (e.g., normal)\")\n",
    "print(\"   - MLE can be more efficient than OLS\")\n",
    "print(\"   - Under normality, MLE = OLS for linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6: Best Confidence Intervals\n",
    "\n",
    "**What makes a confidence interval \"best\"?**\n",
    "\n",
    "A 95% CI is \"best\" if it:\n",
    "1. Has correct coverage: Contains true parameter 95% of the time\n",
    "2. Has minimum width among all CIs with correct coverage\n",
    "\n",
    "**Standard approach**: $\\hat{\\beta}_j \\pm t_{n-k, \\alpha/2} \\times se(\\hat{\\beta}_j)$\n",
    "- Width determined by $se(\\hat{\\beta}_j)$\n",
    "- Shortest CI comes from most efficient estimator\n",
    "\n",
    "**Alternative approaches**:\n",
    "\n",
    "**1. Bootstrap confidence intervals**\n",
    "- Resample data many times (e.g., 1000 replications)\n",
    "- Re-estimate model for each resample\n",
    "- Use distribution of bootstrap estimates\n",
    "- Percentile method: 2.5th and 97.5th percentiles\n",
    "- Advantages: No distributional assumptions, works for complex statistics\n",
    "\n",
    "**2. Bayesian credible intervals**\n",
    "- Based on posterior distribution\n",
    "- Direct probability interpretation\n",
    "- Incorporates prior information\n",
    "\n",
    "**When assumptions fail**:\n",
    "- Use robust SEs â†’ wider but valid intervals\n",
    "- Bootstrap â†’ more accurate coverage in small samples\n",
    "- Asymptotic approximations may be poor in small samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ² Bootstrap Confidence Intervals: An Alternative Approach\n",
    "\n",
    "**What is Bootstrap?**\n",
    "\n",
    "The bootstrap is a computational method that:\n",
    "1. **Resamples** your data many times (e.g., 1000 replications)\n",
    "2. **Re-estimates** the model for each resample\n",
    "3. Uses the **distribution of estimates** to build confidence intervals\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "For each bootstrap replication b = 1, ..., B:\n",
    "1. Draw n observations **with replacement** from original data\n",
    "2. Estimate regression: $\\hat{\\beta}_j^{(b)}$\n",
    "3. Store the coefficient estimate\n",
    "\n",
    "After B replications:\n",
    "- You have B estimates: $\\{\\hat{\\beta}_j^{(1)}, \\hat{\\beta}_j^{(2)}, ..., \\hat{\\beta}_j^{(B)}\\}$\n",
    "- These form an empirical distribution\n",
    "\n",
    "**Percentile Method CI:**\n",
    "- 95% CI = [2.5th percentile, 97.5th percentile] of bootstrap distribution\n",
    "- Example: If you have 1000 estimates, use the 25th and 975th largest values\n",
    "\n",
    "**Advantages of Bootstrap:**\n",
    "\n",
    "1. **No distributional assumptions**: Don't need to assume normality\n",
    "2. **Works for complex statistics**: Medians, ratios, quantiles, etc.\n",
    "3. **Better small-sample coverage**: Often more accurate than asymptotic formulas\n",
    "4. **Flexibility**: Can bootstrap residuals, observations, or both\n",
    "5. **Visual understanding**: See the actual sampling distribution\n",
    "\n",
    "**When to use Bootstrap:**\n",
    "\n",
    "- Small samples (n < 30-50)\n",
    "- Non-standard statistics (beyond means and coefficients)\n",
    "- Skewed or heavy-tailed distributions\n",
    "- Checking robustness of standard inference\n",
    "- When asymptotic formulas are complex or unavailable\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Computationally intensive (need B = 1000+ replications)\n",
    "- Requires careful implementation (stratification, cluster bootstrap)\n",
    "- May fail with very small samples (n < 10)\n",
    "- Assumes sample is representative of population\n",
    "\n",
    "**Bootstrap vs. Robust SEs:**\n",
    "\n",
    "Both address uncertainty, but differently:\n",
    "- **Robust SEs**: Analytical correction for heteroskedasticity/autocorrelation\n",
    "- **Bootstrap**: Computational approach using resampling\n",
    "\n",
    "Often used together: Bootstrap with robust methods!\n",
    "\n",
    "**Practical Implementation Tips:**\n",
    "\n",
    "1. Use B â‰¥ 1000 for confidence intervals\n",
    "2. Set random seed for reproducibility\n",
    "3. For time series: Use block bootstrap (resample blocks, not individuals)\n",
    "4. For panel data: Use cluster bootstrap (resample clusters)\n",
    "5. Check convergence: Results shouldn't change much with different seeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7: Best Tests\n",
    "\n",
    "**Type I and Type II errors**:\n",
    "\n",
    "| Decision | $H_0$ True | $H_0$ False |\n",
    "|----------|------------|-------------|\n",
    "| Reject $H_0$ | Type I error (Î±) | Correct |\n",
    "| Don't reject | Correct | Type II error (Î²) |\n",
    "\n",
    "**Type I error** (false positive):\n",
    "- Reject $H_0$ when it's true\n",
    "- Probability = significance level Î± (e.g., 0.05)\n",
    "- We control this directly\n",
    "\n",
    "**Type II error** (false negative):\n",
    "- Fail to reject $H_0$ when it's false\n",
    "- Probability = Î²\n",
    "- Harder to control\n",
    "\n",
    "**Test power** = 1 - Î²\n",
    "- Probability of correctly rejecting false $H_0$\n",
    "- Higher power is better\n",
    "\n",
    "**Trade-off**:\n",
    "- Decreasing Î± (stricter test) â†’ increases Î² (lower power)\n",
    "- Solution: Fix Î±, maximize power\n",
    "\n",
    "**Most powerful test**:\n",
    "- Among all tests with size Î±, has highest power\n",
    "- For linear regression: Use most efficient estimator\n",
    "\n",
    "**The Trinity of Tests** (asymptotically equivalent):\n",
    "1. **Wald test**: Based on unrestricted estimates\n",
    "2. **Likelihood Ratio (LR) test**: Compares likelihoods\n",
    "3. **Lagrange Multiplier (LM) test**: Based on restricted estimates\n",
    "\n",
    "**Multiple testing**:\n",
    "- Testing many hypotheses inflates Type I error\n",
    "- Solutions: Bonferroni correction, FDR control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## âš–ï¸ The Type I vs. Type II Error Tradeoff\n",
    "\n",
    "**Understanding the Table:**\n",
    "\n",
    "The 2Ã—2 decision table reveals a fundamental tradeoff in hypothesis testing:\n",
    "\n",
    "| Decision | Hâ‚€ True | Hâ‚€ False |\n",
    "|----------|---------|----------|\n",
    "| Reject Hâ‚€ | **Type I error (Î±)** | **Correct (Power)** |\n",
    "| Don't reject | Correct (1-Î±) | **Type II error (Î²)** |\n",
    "\n",
    "**Type I Error (False Positive):**\n",
    "- Reject a true null hypothesis\n",
    "- Probability = significance level Î± (we control this)\n",
    "- Example: Conclude a drug works when it doesn't\n",
    "- **\"Seeing patterns in noise\"**\n",
    "\n",
    "**Type II Error (False Negative):**\n",
    "- Fail to reject a false null hypothesis  \n",
    "- Probability = Î² (harder to control)\n",
    "- Example: Miss a real drug effect\n",
    "- **\"Missing real signals\"**\n",
    "\n",
    "**The Fundamental Tradeoff:**\n",
    "\n",
    "If we make the test **stricter** (lower Î±):\n",
    "- âœ“ Fewer false positives (Type I errors)\n",
    "- âœ— More false negatives (Type II errors)\n",
    "- âœ— Lower power (harder to detect real effects)\n",
    "\n",
    "If we make the test **looser** (higher Î±):\n",
    "- âœ“ Higher power (easier to detect real effects)\n",
    "- âœ— More false positives (Type I errors)\n",
    "\n",
    "**Statistical Power = 1 - Î²:**\n",
    "- Probability of correctly rejecting false Hâ‚€\n",
    "- \"Sensitivity\" of the test\n",
    "- Want power â‰¥ 0.80 (80% chance of detecting real effect)\n",
    "\n",
    "**What Affects Power?**\n",
    "\n",
    "1. **Sample size (n)**: Larger n â†’ Higher power\n",
    "2. **Effect size (Î²)**: Larger true effect â†’ Higher power\n",
    "3. **Significance level (Î±)**: Higher Î± â†’ Higher power (but more Type I errors)\n",
    "4. **Noise level (Ïƒ)**: Lower Ïƒ â†’ Higher power\n",
    "\n",
    "**The Power Function:**\n",
    "\n",
    "Power depends on the **true parameter value**:\n",
    "- At Î² = 0 (Hâ‚€ true): Power = Î± (just Type I error rate)\n",
    "- As |Î²| increases: Power increases\n",
    "- For very large |Î²|: Power â†’ 1 (almost certain detection)\n",
    "\n",
    "**Multiple Testing Problem:**\n",
    "\n",
    "Testing k hypotheses at Î± = 0.05:\n",
    "- Expected false positives = 0.05 Ã— k\n",
    "- Test 20 hypotheses â†’ expect 1 false positive even if all Hâ‚€ are true!\n",
    "\n",
    "**Solutions:**\n",
    "1. **Bonferroni correction**: Use Î±/k for each test (conservative)\n",
    "2. **False Discovery Rate (FDR)**: Control proportion of false positives\n",
    "3. **Pre-registration**: Specify primary hypotheses before seeing data\n",
    "4. **Replication**: Confirm findings in independent samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration: Power of a Test\n",
    "\n",
    "Let's visualize how test power depends on the true effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“ˆ Reading the Power Curve: What It Tells Us\n",
    "\n",
    "**Interpreting Figure 12.3:**\n",
    "\n",
    "The power function shows how test power varies with the true coefficient value. Here's what each feature means:\n",
    "\n",
    "**Key Features of the Curve:**\n",
    "\n",
    "1. **At Î² = 0 (vertical gray line)**:\n",
    "   - Power = Î± = 0.05\n",
    "   - This is the Type I error rate\n",
    "   - When Hâ‚€ is true, we reject 5% of the time (false positives)\n",
    "\n",
    "2. **As |Î²| increases (moving away from 0)**:\n",
    "   - Power increases rapidly\n",
    "   - Larger effects are easier to detect\n",
    "   - Curve approaches 1.0 (certain detection)\n",
    "\n",
    "3. **Symmetry around zero**:\n",
    "   - Power is same for Î² = 30 and Î² = -30\n",
    "   - Two-sided test treats positive and negative effects equally\n",
    "   - One-sided tests would have asymmetric power\n",
    "\n",
    "4. **The 0.80 threshold (green dashed line)**:\n",
    "   - Standard target: 80% power\n",
    "   - Means 20% chance of Type II error (Î² = 0.20)\n",
    "   - In this example: Need |Î²| â‰ˆ 30 to achieve 80% power\n",
    "\n",
    "**What This Means for Study Design:**\n",
    "\n",
    "Given the parameters (n=30, SE=15, Î±=0.05):\n",
    "\n",
    "- **Small effects** (|Î²| < 15):\n",
    "  - Power < 50%\n",
    "  - More likely to **miss** the effect than detect it\n",
    "  - Study is underpowered\n",
    "\n",
    "- **Medium effects** (|Î²| â‰ˆ 30):\n",
    "  - Power â‰ˆ 80%\n",
    "  - Good chance of detection\n",
    "  - Standard benchmark for adequate power\n",
    "\n",
    "- **Large effects** (|Î²| > 45):\n",
    "  - Power > 95%\n",
    "  - Almost certain detection\n",
    "  - Study is well-powered\n",
    "\n",
    "**Sample Size Implications:**\n",
    "\n",
    "To detect smaller effects, you need larger samples:\n",
    "- **Double the sample** (n=60) â†’ Can detect smaller effects with same power\n",
    "- Power roughly proportional to âˆšn\n",
    "- To halve minimum detectable effect, need **4Ã— the sample size**\n",
    "\n",
    "**The Power-Sample Size Relationship:**\n",
    "\n",
    "For a given effect size Î²:\n",
    "- Power increases with âˆšn\n",
    "- To go from 50% to 80% power: Need â‰ˆ 2Ã— the sample\n",
    "- To go from 80% to 95% power: Need â‰ˆ 2Ã— the sample again\n",
    "\n",
    "**Practical Applications:**\n",
    "\n",
    "1. **Pre-study planning**:\n",
    "   - Specify minimum effect of interest\n",
    "   - Calculate required sample size for 80% power\n",
    "   - Avoid underpowered studies\n",
    "\n",
    "2. **Post-study interpretation**:\n",
    "   - Non-significant result with low power: Inconclusive (not evidence of no effect)\n",
    "   - Non-significant result with high power: Evidence against large effects\n",
    "   - Significant result: Good, but consider magnitude and practical significance\n",
    "\n",
    "3. **Publication decisions**:\n",
    "   - Underpowered studies contribute to publication bias\n",
    "   - Meta-analyses should weight by precision and power\n",
    "   - Replication studies should be well-powered\n",
    "\n",
    "**Common Mistakes to Avoid:**\n",
    "\n",
    "1. âŒ Treating non-significant results as \"proof of no effect\"\n",
    "   - Non-significance in underpowered study is uninformative\n",
    "   \n",
    "2. âŒ Conducting multiple underpowered studies instead of one well-powered study\n",
    "   - Wastes resources and leads to false negatives\n",
    "\n",
    "3. âŒ Post-hoc power analysis\n",
    "   - Don't calculate power after seeing results (it's circular)\n",
    "   - Do it before data collection\n",
    "\n",
    "**The Bottom Line:**\n",
    "\n",
    "This power curve illustrates a fundamental truth:\n",
    "- **Smaller effects require larger samples to detect**\n",
    "- With n=30 and SE=15, we can reliably detect effects of |Î²| â‰¥ 30\n",
    "- For smaller effects, we'd need more data or reduced noise (lower Ïƒ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Robust Standard Errors**:\n",
    "   - Use heteroskedasticity-robust (HC1) for cross-sections\n",
    "   - Use HAC (Newey-West) for time series with autocorrelation\n",
    "   - Use cluster-robust for panel/grouped data\n",
    "   - Coefficients unchanged, SEs and inference corrected\n",
    "\n",
    "2. **Prediction**:\n",
    "   - Distinguish conditional mean vs. actual value\n",
    "   - Conditional mean: $se(\\hat{y}_{cm})$ depends on estimation uncertainty\n",
    "   - Actual value: $se(\\hat{y}_f) = \\sqrt{s_e^2 + se(\\hat{y}_{cm})^2}$\n",
    "   - Prediction intervals much wider than confidence intervals\n",
    "   - Even with perfect estimates, individual predictions have large uncertainty\n",
    "\n",
    "3. **Sample Selection**:\n",
    "   - Selection on X: Include selection variables as controls\n",
    "   - Selection on Y: Use sample weights or selection models\n",
    "   - Always verify sample representativeness\n",
    "\n",
    "4. **Best Estimation**:\n",
    "   - OLS is BLUE under Gauss-Markov assumptions\n",
    "   - When assumptions fail: GLS/FGLS more efficient\n",
    "   - In practice: OLS + robust SEs often preferred (simplicity, robustness)\n",
    "\n",
    "5. **Best Confidence Intervals**:\n",
    "   - Shortest interval from most efficient estimator\n",
    "   - Bootstrap: Better small-sample coverage, no distributional assumptions\n",
    "   - Robust SEs: Wider but valid intervals\n",
    "\n",
    "6. **Best Tests**:\n",
    "   - Type I error (Î±): Controlled at significance level\n",
    "   - Type II error (Î²): Power = 1 - Î²\n",
    "   - Most powerful test: Uses most efficient estimator\n",
    "   - Trinity: Wald, LR, LM tests asymptotically equivalent\n",
    "   - Multiple testing: Adjust for inflated Type I error\n",
    "\n",
    "**Statistical Concepts Covered:**\n",
    "- Heteroskedasticity-robust inference (White's correction)\n",
    "- HAC (Newey-West) standard errors for autocorrelation\n",
    "- Prediction intervals vs. confidence intervals\n",
    "- Sample selection bias\n",
    "- Generalized Least Squares (GLS)\n",
    "- Bootstrap methods\n",
    "- Test power and Type I/II errors\n",
    "- Multiple testing corrections\n",
    "\n",
    "**Python Tools Used:**\n",
    "- `pandas`: Data manipulation\n",
    "- `statsmodels`: OLS with various SE types (HC1, HAC)\n",
    "- `matplotlib` & `seaborn`: Visualization\n",
    "- `scipy.stats`: Distributions and critical values\n",
    "- `statsmodels.graphics.tsaplots`: Autocorrelation plots\n",
    "- `statsmodels.tsa.stattools`: ACF calculation\n",
    "\n",
    "**Practical Applications:**\n",
    "- Robust inference for real-world data\n",
    "- Forecasting with proper uncertainty quantification\n",
    "- Dealing with autocorrelation in time series\n",
    "- Understanding prediction uncertainty\n",
    "- Power analysis for study design\n",
    "\n",
    "**Key Formulas:**\n",
    "\n",
    "Prediction SE (conditional mean):\n",
    "$$se(\\hat{y}_{cm}) = s_e \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "Prediction SE (actual value):\n",
    "$$se(\\hat{y}_f) = s_e \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "Test power:\n",
    "$$\\text{Power} = 1 - \\beta = P(\\text{Reject } H_0 | H_0 \\text{ false})$$\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply robust inference to your own data\n",
    "- Practice prediction with uncertainty quantification\n",
    "- Study advanced topics: GLS, instrumental variables, panel data\n",
    "- Explore bootstrap and permutation tests\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed Chapter 12. You now understand advanced inference methods and can conduct robust statistical analysis in real-world settings!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
