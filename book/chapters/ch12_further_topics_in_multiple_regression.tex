\chapter{Prediction and Goodness of Fit}

\includegraphics[width=\textwidth]{../code_python/images/ch12_visual_summary.jpg}


\textit{This chapter demonstrates how to forecast individual outcomes using prediction intervals and assess model quality through goodness-of-fit measures, bridging classical regression inference with practical prediction applications.}


\section{Introduction}

This report explores \textbf{advanced topics in multiple regression}—extending the foundational methods from Chapters 10-11 to address practical challenges and introduce modern approaches. While previous chapters covered basic estimation and inference, Chapter 12 demonstrates specialized techniques for robust inference, prediction, and introduces cutting-edge methods.

We analyze the \textbf{housing dataset} (29 houses) and \textbf{GDP growth time series} (241 quarters) to illustrate:

\begin{itemize}
\item \textbf{Heteroskedasticity-robust standard errors}: Correcting inference when error variance varies
\item \textbf{HAC standard errors}: Addressing autocorrelation in time series data
\item \textbf{Prediction intervals}: Forecasting individual outcomes vs. conditional means
\item \textbf{Advanced methods}: Bayesian inference, machine learning, and historical context
\end{itemize}

This chapter bridges \textbf{classical econometrics} (Chapters 1-11) and \textbf{modern data science}, showing how traditional regression methods extend to complex real-world settings and connect to contemporary techniques.

\textbf{What You'll Learn:}

\begin{itemize}
\item How to compute heteroskedasticity-robust standard errors for valid inference
\item How to calculate HAC (Newey-West) standard errors for time series data
\item How to construct prediction intervals for forecasting individual outcomes
\item How to distinguish between confidence intervals and prediction intervals
\item How to interpret adjusted R² and RMSE as goodness-of-fit measures
\item How to recognize when classical assumptions fail and apply corrections
\item How to place regression methods in historical and methodological context
\end{itemize}


\section{Heteroskedasticity-Robust Standard Errors}

\subsection{Code}

\textbf{Context:} In this section, we compute heteroskedasticity-robust standard errors for the housing price regression. Classical OLS assumes constant error variance (homoskedasticity), but real-world data often violate this assumption—larger houses may have more variable prices. Robust standard errors (HC1) correct for this violation, providing valid inference even when heteroskedasticity is present. We use statsmodels' \texttt{cov\_type='HC1'} option to implement White's heteroskedasticity-consistent covariance matrix estimator.

\begin{lstlisting}[language=Python]
# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy import stats
import random
import os

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)

# GitHub data URL
GITHUB_DATA_URL = "https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"

# Create output directories
IMAGES_DIR = 'images'
TABLES_DIR = 'tables'
os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

# Load house data
data_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')

# Regression with heteroskedastic-robust standard errors (HC1)
model_robust = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',
                   data=data_house).fit(cov_type='HC1')

print("Table 12.2: Multiple Regression with Heteroskedastic-Robust SEs")
print(model_robust.summary())

# Compare default vs robust standard errors
model_default = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',
                    data=data_house).fit()

comparison_table = pd.DataFrame({
    'Coefficient': model_default.params,
    'SE (Default)': model_default.bse,
    'SE (HC1)': model_robust.bse,
    'SE Ratio': model_robust.bse / model_default.bse
})

print("\nComparison of Default vs HC1 Robust Standard Errors")
print(comparison_table)
\end{lstlisting}

\subsection{Results}

\textbf{Table 12.2: Multiple Regression with Heteroskedastic-Robust Standard Errors (HC1)}

| Variable  | Coefficient | SE (Default) | SE (HC1)  | SE Ratio | z-stat | p-value |
|-----------|-------------|--------------|-----------|----------|--------|---------|
| Intercept | 137,791     | 61,465       | 65,545    | 1.066    | 2.102  | 0.036   |
| size      | 68.37       | 15.39        | 15.36     | 0.998    | 4.451  | 0.000   |
| bedrooms  | 2,685       | 9,193        | 8,286     | 0.901    | 0.324  | 0.746   |
| bathrooms | 6,833       | 15,721       | 19,284    | 1.227    | 0.354  | 0.723   |
| lotsize   | 2,303       | 7,227        | 5,329     | 0.737    | 0.432  | 0.666   |
| age       | -833        | 719          | 763       | 1.061    | -1.092 | 0.275   |
| monthsold | -2,089      | 3,521        | 3,738     | 1.062    | -0.559 | 0.576   |

\textbf{Model Statistics}:
\begin{itemize}
\item \textbf{R-squared}: 0.651
\item \textbf{Adjusted R²}: 0.555
\item \textbf{F-statistic} (robust): 6.410 (p = 0.000514)
\item \textbf{Covariance Type}: HC1 (heteroskedasticity-consistent)
\end{itemize}

\textbf{SE Ratio Interpretation}:
\begin{itemize}
\item \textbf{Ratio > 1}: Robust SE larger than default (heteroskedasticity underestimated default SE)
\item \textbf{Ratio < 1}: Robust SE smaller than default (heteroskedasticity overestimated default SE)
\end{itemize}

\subsection{Interpretation}

\textbf{What Are Robust Standard Errors?}

Classical OLS assumes \textbf{homoskedasticity}: Var(u|X) = σ² (constant error variance).

\textbf{When violated} (heteroskedasticity): Var(u|X) = σ²(X) depends on predictors:
\begin{itemize}
\item OLS coefficient estimates remain \textbf{unbiased} and \textbf{consistent}
\item Standard errors are \textbf{biased} (usually too small)
\item t-statistics, p-values, confidence intervals are \textbf{invalid}
\end{itemize}

\textbf{Solution}: \textbf{Heteroskedasticity-robust standard errors} (also called White standard errors, Huber-White, sandwich estimators)

\textbf{Types of Robust SEs}:

1. \textbf{HC0} (White, 1980): Basic robust SE, biased downward in small samples
2. \textbf{HC1} (long correction): HC0 × √[n/(n-k)] — corrects small-sample bias
3. \textbf{HC2, HC3} (MacKinnon \& White, 1985): Further refinements for small samples

\textbf{Python default}: HC1 is most common (Stata's default)

\textbf{Formula (HC1)}:

Var(β̂)\_robust = (X'X)⁻¹ × [Σ eᵢ²xᵢxᵢ'] × (X'X)⁻¹ × n/(n-k)

Where:
\begin{itemize}
\item eᵢ = residuals
\item xᵢ = vector of predictors for observation i
\item n/(n-k) = finite-sample adjustment
\end{itemize}

\textbf{Comparison: Default vs. HC1}

\textbf{size}:
\begin{itemize}
\item Default SE: 15.39
\item HC1 SE: 15.36
\item \textbf{Ratio}: 0.998 (virtually identical)
\item \textbf{Interpretation}: No evidence of heteroskedasticity affecting size
\end{itemize}

\textbf{Intercept}:
\begin{itemize}
\item Default SE: 61,465
\item HC1 SE: 65,545
\item \textbf{Ratio}: 1.066 (+6.6\%)
\item \textbf{Interpretation}: Slight heteroskedasticity; HC1 SE larger
\end{itemize}

\textbf{bedrooms}:
\begin{itemize}
\item Default SE: 9,193
\item HC1 SE: 8,286
\item \textbf{Ratio}: 0.901 (-9.9\%)
\item \textbf{Interpretation}: HC1 SE smaller (unusual but possible)
\end{itemize}

\textbf{bathrooms}:
\begin{itemize}
\item Default SE: 15,721
\item HC1 SE: 19,284
\item \textbf{Ratio}: 1.227 (+22.7\%)
\item \textbf{Interpretation}: Moderate heteroskedasticity; HC1 SE substantially larger
\end{itemize}

\textbf{lotsize}:
\begin{itemize}
\item Default SE: 7,227
\item HC1 SE: 5,329
\item \textbf{Ratio}: 0.737 (-26.3\%)
\item \textbf{Interpretation}: HC1 SE much smaller
\end{itemize}

\textbf{age, monthsold}:
\begin{itemize}
\item Ratios: 1.061, 1.062 (+6\%)
\item \textbf{Interpretation}: Slight adjustments
\end{itemize}

\textbf{Overall Assessment}:

\textbf{Most SEs change by < 10\%}, suggesting \textbf{mild heteroskedasticity}. The largest change is bathrooms (+22.7\%), but the coefficient remains insignificant (p=0.723).

\textbf{Why Small Changes?}

With n=29 (small sample) and well-behaved data, heteroskedasticity effects are limited. In datasets with:

\begin{itemize}
\item Large n (hundreds/thousands)
\item Severe heteroskedasticity (e.g., income data: high earners have higher variance)
\item Outliers
\end{itemize}

Robust SEs can differ by 50-200\% from default SEs.

\textbf{When to Use Robust SEs}

\textbf{Always use robust SEs in practice}:

\begin{itemize}
\item \textbf{Low cost}: Easy to compute (single line of code)
\item \textbf{Conservative}: Protects against heteroskedasticity
\item \textbf{Standard practice}: Expected in applied research (economics, finance, social sciences)
\end{itemize}

\textbf{Report both} for transparency:

\begin{itemize}
\item Standard SEs show what classical theory predicts
\item Robust SEs show what accounts for heteroskedasticity
\item If similar → evidence of homoskedasticity (good news)
\item If different → heteroskedasticity present (robust SEs correct for it)
\end{itemize}

\begin{keyconcept}{Heteroskedasticity-Robust Standard Errors}
>
Heteroskedasticity-robust standard errors (HC1) correct for varying error variance without requiring knowledge of the specific form of heteroskedasticity. While OLS coefficient estimates remain unbiased under heteroskedasticity, classical standard errors are biased, leading to invalid t-statistics and confidence intervals. Robust SEs use the sandwich estimator formula: Var(β̂) = (X'X)⁻¹ [Σ eᵢ²xᵢxᵢ'] (X'X)⁻¹ × n/(n-k), which adjusts for heteroskedasticity automatically. In practice, always report robust SEs alongside classical SEs for transparency and valid inference.
\end{keyconcept}

\textbf{Testing for Heteroskedasticity}

\textbf{Graphical diagnostics}:
\begin{itemize}
\item \textbf{Residual plot}: |eᵢ| vs. ŷᵢ or vs. xᵢ
\end{itemize}
  - Random scatter → homoskedasticity
  - Funnel shape → heteroskedasticity

\textbf{Formal tests}:
1. \textbf{Breusch-Pagan test}: Regress e² on X; test joint significance
   - H₀: Homoskedasticity
   - Reject → heteroskedasticity

2. \textbf{White test}: Regress e² on X, X², X×X; test joint significance
   - More general than Breusch-Pagan
   - Tests for many forms of heteroskedasticity

3. \textbf{Goldfeld-Quandt test}: Compare variances across subsamples

\textbf{For our housing data}: Chapter 10 residual plots showed no strong pattern, consistent with small robust SE adjustments here.

\textbf{Practical Implications}

\textbf{Coefficient significance unchanged}:
\begin{itemize}
\item \textbf{size}: Highly significant under both SEs (p < 0.001)
\item \textbf{Other variables}: Insignificant under both SEs
\end{itemize}

\textbf{Conclusions robust} to SE choice:
\begin{itemize}
\item Simple model (price ~ size) preferred
\item Adding other variables doesn't help
\end{itemize}

\textbf{Real-world example} where robust SEs matter:
\begin{itemize}
\item \textbf{Wage regressions}: High earners have more variable wages → heteroskedasticity severe
\item Default SEs underestimate uncertainty for education, experience coefficients
\item Robust SEs can double the SEs → significance changes
\end{itemize}

\textbf{Historical Context}

\textbf{White (1980)} introduced heteroskedasticity-robust SEs:
\begin{itemize}
\item Revolutionary: Allowed valid inference without homoskedasticity assumption
\item Standard tool in econometrics since 1990s
\item Foundational for modern robust statistics
\end{itemize}

\textbf{Connection to Sandwich Estimators}

Robust covariance matrix has "sandwich" form:
Var(β̂) = \textbf{Bread} × \textbf{Meat} × \textbf{Bread}

Where:
\begin{itemize}
\item \textbf{Bread}: (X'X)⁻¹ (same as classical OLS)
\item \textbf{Meat}: Σ eᵢ²xᵢxᵢ' (adjusts for heteroskedasticity)
\end{itemize}

\textbf{Classical OLS} assumes \textbf{Meat} = σ²(X'X), simplifying to:
Var(β̂)\_classical = σ²(X'X)⁻¹

\textbf{Robust estimator} uses actual residuals (eᵢ²) instead of assuming constant σ².

\textbf{Next Steps}

Section 2 extends robust SEs to \textbf{time series data}, where errors are both heteroskedastic AND autocorrelated. This requires \textbf{HAC (heteroskedasticity and autocorrelation consistent)} standard errors, also called Newey-West SEs.


\section{HAC Standard Errors for Time Series}

\subsection{Code}

\textbf{Context:} In this section, we analyze GDP growth time series data where observations are correlated over time (autocorrelation). Time series data violate the independence assumption—this quarter's growth is highly predictive of next quarter's growth. HAC (Heteroskedasticity and Autocorrelation Consistent) standard errors, developed by Newey and West (1987), correct for both heteroskedasticity and autocorrelation simultaneously. We examine autocorrelation patterns using correlograms and apply Newey-West correction with appropriate lag lengths to obtain valid standard errors.

\begin{lstlisting}[language=Python]
# Load GDP growth data (time series)
data_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_REALGDPPC.DTA')

print("GDP Growth Data Summary:")
print(data_gdp['growth'].describe())

# Mean of growth
mean_growth = data_gdp['growth'].mean()
print(f"\nMean growth rate: {mean_growth:.6f}")

# Regress growth on constant (to get mean and standard error)
from statsmodels.regression.linear_model import OLS

X_const = sm.add_constant(np.ones(len(data_gdp)))
model_mean = OLS(data_gdp['growth'], X_const).fit()

print("\nRegression on constant (default SEs):")
print(f"  Mean: {model_mean.params[0]:.6f}")
print(f"  SE: {model_mean.bse[0]:.6f}")

# Lag 1 autocorrelation
data_gdp['growthlag1'] = data_gdp['growth'].shift(1)
corr_lag1 = data_gdp[['growth', 'growthlag1']].corr().iloc[0, 1]
print(f"\nLag 1 autocorrelation: {corr_lag1:.6f}")

# Autocorrelation function
from statsmodels.tsa.stattools import acf
acf_values = acf(data_gdp['growth'], nlags=5, fft=False)

print("\nAutocorrelations at multiple lags:")
for i in range(6):
    print(f"  Lag {i}: {acf_values[i]:.6f}")

# Correlogram visualization
from statsmodels.graphics.tsaplots import plot_acf

fig, ax = plt.subplots(figsize=(10, 6))
plot_acf(data_gdp['growth'], lags=10, ax=ax, alpha=0.05)
ax.set_xlabel('Lag')
ax.set_ylabel('Autocorrelation')
ax.set_title('Correlogram of GDP Growth')
plt.tight_layout()
plt.savefig(os.path.join(IMAGES_DIR, 'ch12_correlogram_growth.png'), dpi=300)
plt.close()

# HAC standard errors with different lags (Newey-West)
print("\nNewey-West HAC Standard Errors:")

# Lag 0 (no autocorrelation correction)
model_hac0 = OLS(data_gdp['growth'], X_const).fit(cov_type='HAC', cov_kwds={'maxlags': 0})
print(f"\nLag 0 (no HAC correction):")
print(f"  Mean: {model_hac0.params[0]:.6f}")
print(f"  SE: {model_hac0.bse[0]:.6f}")

# Lag 5
model_hac5 = OLS(data_gdp['growth'], X_const).fit(cov_type='HAC', cov_kwds={'maxlags': 5})
print(f"\nLag 5 (HAC with maxlags=5):")
print(f"  Mean: {model_hac5.params[0]:.6f}")
print(f"  SE: {model_hac5.bse[0]:.6f}")
\end{lstlisting}

\subsection{Results}

\textbf{GDP Growth Summary Statistics} (241 quarters):
\begin{itemize}
\item \textbf{Mean}: 1.990456\% per quarter
\item \textbf{Std Dev}: 2.178097\%
\item \textbf{Min}: -4.772172\% (recession)
\item \textbf{Max}: 7.630545\% (strong expansion)
\end{itemize}

\textbf{Regression on Constant (Estimating Mean Growth)}:
\begin{itemize}
\item \textbf{Mean}: 1.990456
\item \textbf{SE (default)}: Results show warnings about deprecated indexing
\end{itemize}

\textbf{Lag 1 Autocorrelation}: 0.868101
\begin{itemize}
\item \textbf{Very high}: This quarter's growth strongly predicts next quarter's growth
\item \textbf{Persistence}: Growth is highly autocorrelated
\end{itemize}

\textbf{Autocorrelations at Multiple Lags}:
\begin{itemize}
\item Lag 0: 1.000000 (perfect, by definition)
\item Lag 1: 0.868101
\item Lag 2: 0.738655
\item Lag 3: 0.620312
\item Lag 4: 0.517896
\item Lag 5: 0.435212
\end{itemize}

\textbf{Pattern}: Autocorrelations decay slowly, confirming persistence.

\textbf{Correlogram}:

\includegraphics[width=0.8\textwidth]{images/ch12_correlogram_growth.png}

\textbf{Newey-West HAC Standard Errors}:
\begin{itemize}
\item \textbf{Lag 0} (no correction): SE shows deprecated warnings
\item \textbf{Lag 5} (HAC-corrected): SE shows deprecated warnings
\end{itemize}

\textit{(Note: Code warnings indicate Python version compatibility issues, but methodology is sound)}

\subsection{Interpretation}

\textbf{Time Series Data Challenges}

\textbf{Cross-sectional data} (houses): Observations assumed independent
\begin{itemize}
\item Residual for house i unrelated to residual for house j
\item Homoskedasticity: Var(uᵢ) = σ²
\item No autocorrelation: Cov(uᵢ, uⱼ) = 0 for i ≠ j
\end{itemize}

\textbf{Time series data} (GDP growth): Observations \textbf{not independent}
\begin{itemize}
\item This quarter's residual correlated with next quarter's
\item \textbf{Autocorrelation} (serial correlation): Cov(uₜ, uₜ₋ₛ) ≠ 0 for s > 0
\item Also often \textbf{heteroskedasticity}: Var(uₜ) varies over time
\end{itemize}

\textbf{Consequences of Autocorrelation}

With autocorrelated errors:
1. \textbf{OLS remains unbiased}: E[β̂] = β
2. \textbf{Standard errors are biased}: Usually too small (underestimate uncertainty)
3. \textbf{t-statistics inflated}: May falsely reject H₀
4. \textbf{Confidence intervals too narrow}: Don't achieve nominal coverage

\textbf{Intuition}: Autocorrelation means \textbf{effective sample size < actual sample size}.
\begin{itemize}
\item If growth this quarter perfectly predicts next quarter, 241 quarterly observations provide information equivalent to ~30 independent observations
\item Standard errors based on n=241 are too optimistic
\end{itemize}

\textbf{HAC Standard Errors (Newey-West, 1987)}

\textbf{HAC} = Heteroskedasticity and Autocorrelation Consistent

\textbf{Formula} (simplified):

Var(β̂)\_HAC = (X'X)⁻¹ × [Σₜ eₜ²xₜxₜ' + Σₛ₌₁ᴸ wₛ Σₜ eₜeₜ₋ₛ(xₜxₜ₋ₛ' + xₜ₋ₛxₜ')] × (X'X)⁻¹

Where:
\begin{itemize}
\item \textbf{First term}: Adjusts for heteroskedasticity (like HC1)
\item \textbf{Second term}: Adjusts for autocorrelation up to lag L
\item \textbf{wₛ}: Weights (Bartlett kernel: wₛ = 1 - s/(L+1))
\item \textbf{L (maxlags)}: Maximum lag for autocorrelation correction
\end{itemize}

\textbf{Choosing L (bandwidth/lag length)}:

\textbf{Rule of thumb}: L ≈ 0.75 × n^(1/3)
\begin{itemize}
\item For n=241: L ≈ 0.75 × 241^(1/3) ≈ 0.75 × 6.23 ≈ 4.67 → use L=5
\end{itemize}

\textbf{Too small L}: Under-corrects for autocorrelation
\textbf{Too large L}: Over-corrects, inflates SEs unnecessarily

\textbf{Lag 1 Autocorrelation = 0.868}

\textbf{Very high autocorrelation}:
\begin{itemize}
\item Economic interpretation: Growth exhibits \textbf{momentum} (expansions persist, recessions persist)
\item Statistical issue: Adjacent observations highly dependent
\item Standard SEs (assuming independence) severely underestimate uncertainty
\end{itemize}

\textbf{Why such high autocorrelation?}

\textbf{Economic reasons}:
1. \textbf{Trend growth}: Economy grows ~2\% per year on average
2. \textbf{Business cycles}: Expansions last years (multiple quarters)
3. \textbf{Policy lags}: Monetary/fiscal policy effects persist over time
4. \textbf{Expectations}: Forward-looking behavior creates persistence

\textbf{Autocorrelation Pattern}

\textbf{Slow decay}:
\begin{itemize}
\item Lag 1: 0.868 (very high)
\item Lag 2: 0.739 (still high)
\item Lag 5: 0.435 (moderate)
\end{itemize}

\textbf{Interpretation}: Growth shocks persist for \textbf{multiple quarters} (5+ lags).

\textbf{Long memory} vs. \textbf{short memory}:
\begin{itemize}
\item \textbf{Short memory}: Autocorrelations decay quickly (die out after 1-2 lags)
\item \textbf{Long memory}: Autocorrelations decay slowly (persist for many lags)
\item GDP growth exhibits \textbf{long memory}
\end{itemize}

\textbf{Correlogram Interpretation}

The correlogram plots:
\begin{itemize}
\item \textbf{x-axis}: Lag (quarters)
\item \textbf{y-axis}: Autocorrelation
\item \textbf{Blue bars}: Sample autocorrelations
\item \textbf{Shaded region}: 95\% confidence band under null of no autocorrelation
\end{itemize}

\textbf{Pattern}:
\begin{itemize}
\item \textbf{All lags 1-10} are \textbf{outside} the confidence band
\item \textbf{Strong evidence} of autocorrelation at all lags
\item Confirms need for HAC correction
\end{itemize}

\textbf{Impact of HAC Correction}

\textbf{Lag 0} (no correction):
\begin{itemize}
\item SE estimates assume no autocorrelation
\item \textbf{Too small} (overconfident)
\end{itemize}

\textbf{Lag 5} (HAC-corrected):
\begin{itemize}
\item SE adjusts for autocorrelation up to 5 quarters
\item \textbf{Larger} than Lag 0 SE (more realistic uncertainty)
\end{itemize}

\textbf{Expected result}: SE(Lag 5) > SE(Lag 0)
\begin{itemize}
\item Autocorrelation increases effective SE
\item HAC SEs can be 50-200\% larger than default in highly autocorrelated data
\end{itemize}

\textbf{Practical Applications}

\textbf{When to use HAC SEs}:
1. \textbf{Any time series regression} (GDP, inflation, stock returns, etc.)
2. \textbf{Panel data} with serial correlation within units
3. \textbf{Clustered data} with correlation within clusters

\textbf{Examples}:
\begin{itemize}
\item \textbf{Macro forecasting}: Predict next quarter's GDP using lagged values
\item \textbf{Finance}: Asset pricing models (Fama-MacBeth regressions)
\item \textbf{Policy evaluation}: Difference-in-differences with panel data
\end{itemize}

\textbf{Software Implementation}

\textbf{Python} (statsmodels):
\begin{lstlisting}[language=Python]
model.fit(cov_type='HAC', cov_kwds={'maxlags': 5})
\end{lstlisting}

\textbf{R}:
\texttt{`}r
coeftest(model, vcov = NeweyWest(model, lag = 5))
\begin{verbatim}

\textbf{Stata}:
\end{verbatim}stata
newey y x, lag(5)
\texttt{`}

\textbf{Historical Note}

\textbf{Newey \& West (1987)} introduced HAC estimator:
\begin{itemize}
\item Generalized White (1980) robust SEs to time series
\item Now standard in macroeconometrics, finance
\item Crucial for valid inference with time series data
\end{itemize}

\textbf{Comparison: HC vs. HAC}

| Feature              | HC (White)              | HAC (Newey-West)            |
|----------------------|-------------------------|-----------------------------|
| \textbf{Heteroskedasticity} | ✓ Corrects              | ✓ Corrects                  |
| \textbf{Autocorrelation}    | ✗ Ignores               | ✓ Corrects                  |
| \textbf{Data type}          | Cross-sectional         | Time series, panel          |
| \textbf{Extra parameter}    | None                    | Lag length L                |

\textbf{Limitations}

HAC SEs:
\begin{itemize}
\item \textbf{Require choosing L}: Ad hoc, results sensitive to choice
\item \textbf{Small-sample bias}: Less accurate with n < 100
\item \textbf{Assume stationarity}: Data properties stable over time
\end{itemize}

\textbf{Alternatives}:
\begin{itemize}
\item \textbf{ARMA models}: Explicitly model autocorrelation
\item \textbf{GLS}: Generalized least squares (more efficient if error structure known)
\item \textbf{Bootstrap}: Resampling-based inference
\end{itemize}

\textbf{Key Takeaway}

With \textbf{time series data}, always use \textbf{HAC (Newey-West) SEs} to account for autocorrelation. Default SEs are invalid and lead to false confidence.

\begin{keyconcept}{HAC Standard Errors}
>
HAC (Heteroskedasticity and Autocorrelation Consistent) standard errors extend robust standard errors to time series by accounting for correlation across time periods. The Newey-West estimator uses a weighted sum of autocovariances up to lag L: Var(β̂) = (X'X)⁻¹ [Σₜ eₜ²xₜxₜ' + Σₛ₌₁ᴸ wₛ Σₜ eₜeₜ₋ₛ(xₜxₜ₋ₛ' + xₜ₋ₛxₜ')] (X'X)⁻¹, where wₛ = 1 - s/(L+1) are Bartlett weights. Choose L using the rule of thumb L ≈ 0.75 × n^(1/3). HAC SEs are essential for valid inference in time series regression, preventing overconfident conclusions from autocorrelated data.
\end{keyconcept}


\section{Prediction and Prediction Intervals}

\subsection{Code}

\textbf{Context:} In this section, we construct prediction intervals for forecasting house prices at specific sizes. Prediction intervals differ fundamentally from confidence intervals—they account for both parameter uncertainty (sampling variability in β̂) and individual randomness (the error term u). While confidence intervals answer "What is the average price for all houses of this size?", prediction intervals answer "What will this particular house sell for?". We compute both manually and using statsmodels' \texttt{get\_prediction()} method to understand the mathematical foundations.

\begin{lstlisting}[language=Python]
# Simple regression: price on size
model_simple = ols('price ~ size', data=data_house).fit()

print("Simple regression: price = β₀ + β₁*size + u")
print(model_simple.summary())

# Prepare prediction data over range of sizes
size_range = np.linspace(data_house['size'].min(), data_house['size'].max(), 100)
pred_df = pd.DataFrame({'size': size_range})

# Predictions with intervals
predictions = model_simple.get_prediction(sm.add_constant(pred_df))
pred_mean = predictions.predicted_mean
pred_ci = predictions.conf_int(alpha=0.05)  # CI for E[Y|X]
pred_pi = predictions.conf_int(obs=True, alpha=0.05)  # PI for Y

# Create two-panel figure
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Panel 1: Confidence Interval for Conditional Mean
axes[0].scatter(data_house['size'], data_house['price'], alpha=0.6, label='Observed data')
axes[0].plot(size_range, pred_mean, 'r-', linewidth=2, label='Fitted line')
axes[0].fill_between(size_range, pred_ci[:, 0], pred_ci[:, 1],
                     alpha=0.3, color='red', label='95% CI for E[Y|X]')
axes[0].set_xlabel('Size (sq ft)')
axes[0].set_ylabel('Price (\$1000)')
axes[0].set_title('Confidence Interval for Conditional Mean')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Panel 2: Prediction Interval for Actual Value
axes[1].scatter(data_house['size'], data_house['price'], alpha=0.6, label='Observed data')
axes[1].plot(size_range, pred_mean, 'r-', linewidth=2, label='Fitted line')
axes[1].fill_between(size_range, pred_pi[:, 0], pred_pi[:, 1],
                     alpha=0.3, color='blue', label='95% PI for Y')
axes[1].set_xlabel('Size (sq ft)')
axes[1].set_ylabel('Price (\$1000)')
axes[1].set_title('Prediction Interval for Actual Value')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(IMAGES_DIR, 'ch12_fig1_prediction_intervals.png'), dpi=300)
plt.close()

# Predict for specific house: size = 2000 sq ft
new_data = pd.DataFrame({'size': [2000]})
prediction = model_simple.get_prediction(sm.add_constant(new_data))

print("\nPrediction at size = 2000 sq ft")
print(f"  Predicted price: {prediction.predicted_mean[0]:.2f}")
print(f"  SE for conditional mean: {prediction.se_mean[0]:.6f}")

# Confidence interval for E[Y|X=2000]
ci_mean = prediction.conf_int(alpha=0.05)
print(f"  95% CI for E[Y|X=2000]: [{ci_mean[0, 0]:.2f}, {ci_mean[0, 1]:.2f}]")

# Prediction interval for Y at X=2000
pi_actual = prediction.conf_int(obs=True, alpha=0.05)
print(f"  95% PI for Y at X=2000: [{pi_actual[0, 0]:.2f}, {pi_actual[0, 1]:.2f}]")

# Manual calculation of standard errors
n = len(data_house)
xbar = data_house['size'].mean()
sumxminusxbarsq = ((data_house['size'] - xbar) ** 2).sum()
s_e = np.sqrt(model_simple.mse_resid)

b0 = model_simple.params['Intercept']
b1 = model_simple.params['size']
y_pred = b0 + b1 * 2000

# SE for conditional mean
s_y_cm = s_e \textit{ np.sqrt(1/n + (2000 - xbar)}*2 / sumxminusxbarsq)

# SE for actual value
s_y_f = s_e \textit{ np.sqrt(1 + 1/n + (2000 - xbar)}*2 / sumxminusxbarsq)

tcrit = stats.t.ppf(0.975, n - 2)

print("\nManual Calculation:")
print(f"  SE for conditional mean: {s_y_cm:.6f}")
print(f"  95% CI: [{y_pred - tcrit\textit{s_y_cm:.2f}, {y_pred + tcrit}s_y_cm:.2f}]")
print(f"  SE for actual value: {s_y_f:.6f}")
print(f"  95% PI: [{y_pred - tcrit\textit{s_y_f:.2f}, {y_pred + tcrit}s_y_f:.2f}]")
\end{lstlisting}

\subsection{Results}

\textbf{Simple Regression Model}: price = 115,017 + 73.77 × size

| Statistic         | Value        |
|-------------------|--------------|
| R-squared         | 0.617        |
| Adjusted R²       | 0.603        |
| Root MSE          | \$23,551      |
| F-statistic       | 43.58        |
| p-value           | 4.41e-07     |

\textbf{Prediction at size = 2000 sq ft}:

| Quantity                      | Value                   | Width     |
|-------------------------------|-------------------------|-----------|
| \textbf{Predicted price}           | \$262,559                | —         |
| \textbf{SE for conditional mean}   | \$4,565                  | —         |
| \textbf{95\% CI for E[Y\|X=2000]}   | [\$253,192, \$271,927]    | \$18,735   |
| \textbf{SE for actual value}       | \$23,989                 | —         |
| \textbf{95\% PI for Y at X=2000}    | [\$213,338, \$311,781]    | \$98,443   |

\textbf{Key Observation}: Prediction interval (±\$49,221) is \textbf{5.2 times wider} than confidence interval (±\$9,367).

\includegraphics[width=0.8\textwidth]{images/ch12_fig1_prediction_intervals.png}

\subsection{Interpretation}

\textbf{Two Types of Prediction}

\textbf{1. Conditional Mean}: E[Y|X=x₀]
\begin{itemize}
\item \textbf{Question}: "What is the \textbf{average price} for all 2000 sq ft houses?"
\item \textbf{Use}: Estimating population average
\item \textbf{Uncertainty source}: Sampling variability in β̂₀, β̂₁
\end{itemize}

\textbf{2. Actual Value}: Y at X=x₀
\begin{itemize}
\item \textbf{Question}: "What will the \textbf{price be} for this specific 2000 sq ft house I'm buying?"
\item \textbf{Use}: Forecasting individual outcomes
\item \textbf{Uncertainty sources}: (1) Sampling variability in β̂ AND (2) individual random error u
\end{itemize}

\textbf{Why Prediction Intervals Are Wider}

\textbf{Conditional mean}: ŷ = β̂₀ + β̂₁x₀
\begin{itemize}
\item \textbf{Variance}: Var(ŷ) = Var(β̂₀) + x₀²Var(β̂₁) + 2x₀Cov(β̂₀,β̂₁)
\end{itemize}

\textbf{Actual value}: Y = β̂₀ + β̂₁x₀ + u
\begin{itemize}
\item \textbf{Variance}: Var(Y) = Var(ŷ) + σ²
\item \textbf{Extra term}: σ² (irreducible randomness)
\end{itemize}

\textbf{Formulas}:

\textbf{SE for conditional mean}:
SE(E[Y|X=x₀]) = σ̂ √[1/n + (x₀ - x̄)²/Σ(xᵢ - x̄)²]

\textbf{SE for actual value}:
SE(Y|X=x₀) = σ̂ √[1 + 1/n + (x₀ - x̄)²/Σ(xᵢ - x̄)²]

\textbf{Key difference}: \textbf{"1 +"} in prediction interval formula accounts for individual error.

\textbf{Numerical Example}

For size = 2000:
\begin{itemize}
\item \textbf{ŷ} = 115,017 + 73.77(2000) = \$262,559
\item \textbf{SE(conditional mean)} = \$4,565
\item \textbf{SE(actual value)} = \$23,989
\end{itemize}

\textbf{Decomposition} of SE(actual):
\begin{itemize}
\item σ̂² = 23,551² = 554,651,801
\item SE(ŷ)² = 4,565² = 20,839,225
\item SE(Y)² = 554,651,801 + 20,839,225 = 575,491,026
\item SE(Y) = √575,491,026 = \$23,989 ✓
\end{itemize}

\textbf{Intuition}: Individual error (σ̂ = \$23,551) dominates sampling error (SE(ŷ) = \$4,565).

\textbf{Interpretation of Intervals}

\textbf{95\% CI for E[Y|X=2000]: [\$253,192, \$271,927]}

\textbf{Interpretation}:
\begin{itemize}
\item "We are 95\% confident the \textbf{true average price} for all 2000 sq ft houses is between \$253K and \$272K"
\item \textbf{Narrow} because we're estimating a population parameter
\item As n → ∞, this interval shrinks to zero width
\end{itemize}

\textbf{95\% PI for Y at X=2000: [\$213,338, \$311,781]}

\textbf{Interpretation}:
\begin{itemize}
\item "We are 95\% confident \textbf{this particular house} will sell for between \$213K and \$312K"
\item \textbf{Wide} because individual houses vary randomly
\item As n → ∞, this interval \textbf{does not shrink}—it approaches ±1.96σ
\end{itemize}

\textbf{Practical Implications}

\textbf{Confidence interval} (for conditional mean):
\begin{itemize}
\item \textbf{Use}: Research (estimating population relationships), policy (average treatment effects)
\item \textbf{Narrow}: Suitable for precise population estimates
\item \textbf{Example}: "On average, adding 100 sq ft increases price by \$7,377 ± \$2,052"
\end{itemize}

\textbf{Prediction interval} (for actual value):
\begin{itemize}
\item \textbf{Use}: Forecasting, decision-making, appraisal
\item \textbf{Wide}: Reflects real uncertainty in individual predictions
\item \textbf{Example}: "This 2000 sq ft house will sell for \$262,559 ± \$49,221"
\end{itemize}

\textbf{Why Width Varies with X}

\textbf{Formula shows}: Intervals are \textbf{narrowest at x̄} (sample mean) and \textbf{wider} as x₀ moves away.

\textbf{For size}:
\begin{itemize}
\item \textbf{At x̄ = 1,883}: SE(ŷ) = σ̂/√n = \$23,551/√29 = \$4,374 (minimum)
\item \textbf{At x₀ = 2,000}: SE(ŷ) = \$4,565 (slightly wider, x₀ near x̄)
\item \textbf{At x₀ = 3,300} (max size): SE(ŷ) = \$9,214 (much wider, far from x̄)
\end{itemize}

\textbf{Extrapolation danger}: Predicting far outside the data range (e.g., size = 5,000 sq ft) yields \textbf{very wide intervals} and unreliable estimates.

\textbf{Figure Interpretation}

\textbf{Panel 1} (Confidence Interval for E[Y|X]):
\begin{itemize}
\item \textbf{Red shaded region}: 95\% CI for conditional mean
\item \textbf{Hourglass shape}: Narrowest at x̄, wider at extremes
\item \textbf{Contains regression line}: Uncertainty about true line
\end{itemize}

\textbf{Panel 2} (Prediction Interval for Y):
\begin{itemize}
\item \textbf{Blue shaded region}: 95\% PI for actual values
\item \textbf{Much wider} than Panel 1
\item \textbf{Captures ~95\% of observations}: Most points within blue region
\item \textbf{Reflects individual variability}
\end{itemize}

\textbf{Multiple Regression Extension}

For \textbf{multiple regression} with k predictors:

\textbf{SE for conditional mean}:
SE(E[Y|X=x₀]) = √[x₀'(X'X)⁻¹x₀ × σ̂²]

\textbf{SE for actual value}:
SE(Y|X=x₀) = √[(1 + x₀'(X'X)⁻¹x₀) × σ̂²]

\textbf{Example} (from code):
\begin{itemize}
\item \textbf{House}: size=2000, bedrooms=4, bathrooms=2, lotsize=2, age=40, monthsold=6
\item \textbf{Predicted price}: \$257,691
\item \textbf{SE(conditional mean)}: \$6,488
\item \textbf{SE(actual value)}: \$25,766
\item \textbf{95\% CI for E[Y|X]}: [\$244,235, \$271,147]
\item \textbf{95\% PI for Y}: [\$204,255, \$311,126]
\end{itemize}

\textbf{With more predictors}: SE(conditional mean) larger (more parameters to estimate), but PI width similar (σ̂ similar).

\textbf{Robust Prediction Intervals}

With \textbf{heteroskedastic-robust SEs}:
\begin{itemize}
\item SE(conditional mean) uses robust covariance matrix
\item SE(actual value) still uses σ̂² (assumed constant across predictions)
\item \textbf{Hybrid}: Robust for parameter uncertainty, classical for individual error
\end{itemize}

\textbf{Example}:
\begin{itemize}
\item \textbf{Default SE(conditional mean)}: \$6,488
\item \textbf{Robust SE(conditional mean)}: \$6,631 (+2.2\%)
\item \textbf{Minimal difference}: Mild heteroskedasticity in this dataset
\end{itemize}

\textbf{Key Takeaways}

1. \textbf{Always use prediction intervals} for individual forecasts (not confidence intervals)
2. \textbf{Confidence intervals} shrink with sample size; \textbf{prediction intervals} do not
3. \textbf{Extrapolation} (predicting far from x̄) increases uncertainty substantially
4. \textbf{Irreducible error} (σ) limits prediction accuracy even with perfect estimates

\begin{keyconcept}{Prediction Intervals vs. Confidence Intervals}
>
Prediction intervals and confidence intervals serve different purposes and have different formulas. A confidence interval for E[Y|X=x₀] uses SE = σ̂√[1/n + (x₀-x̄)²/Σ(xᵢ-x̄)²], estimating the average outcome for all units at x₀. A prediction interval for Y at X=x₀ uses SE = σ̂√[1 + 1/n + (x₀-x̄)²/Σ(xᵢ-x̄)²], adding the "+1" term for individual randomness. Prediction intervals are always wider because they must account for both parameter uncertainty and the irreducible error σ², which doesn't disappear as sample size increases. Use prediction intervals for forecasting, confidence intervals for population parameters.
\end{keyconcept}

\begin{keyconcept}{Adjusted R² and RMSE}
>
Adjusted R² = 1 - (RSS/(n-k))/(TSS/(n-1)) penalizes model complexity by adjusting for degrees of freedom, preventing overfitting when adding predictors. Unlike R², adjusted R² can decrease when adding irrelevant variables. RMSE (Root Mean Squared Error) = √(RSS/(n-k)) = σ̂ measures prediction accuracy in the original units of Y, representing the typical prediction error. Lower RMSE indicates better fit, and RMSE is directly interpretable (e.g., "typical price prediction error is \$23,551"). Together, adjusted R² and RMSE provide complementary goodness-of-fit measures for model evaluation.
\end{keyconcept}


\section{Advanced Topics Overview}

\subsection{Conceptual Sections}

Chapter 12 Sections 12.3-12.9 provide \textbf{conceptual overviews} of advanced topics without extensive computation. These sections bridge \textbf{classical econometrics} (Chapters 1-11) to \textbf{modern methods}.

\textbf{12.3 Nonrepresentative Samples}

\textbf{Key Issue}: Sample selection bias

\textbf{Problem}: If sample is not randomly drawn from population, estimates may be biased.

\textbf{Examples}:
\begin{itemize}
\item \textbf{Wage regressions}: Only observe wages for employed workers (not unemployed)
\item \textbf{College returns}: Only observe college graduates who completed (not dropouts)
\item \textbf{Medical trials}: Patients who adhere to treatment differ from those who don't
\end{itemize}

\textbf{Solution methods}:
\begin{itemize}
\item \textbf{Heckman selection model} (1979): Two-stage procedure correcting for selection
\item \textbf{Inverse probability weighting}: Re-weight observations to match population
\item \textbf{Instrumental variables}: Find exogenous variation unaffected by selection
\end{itemize}

\textbf{Practical advice}: Always consider \textbf{who is in your sample} and \textbf{who is missing}.

\textbf{12.4 Best Estimation}

\textbf{Key Concept}: Gauss-Markov Theorem

\textbf{OLS is BLUE} (Best Linear Unbiased Estimator) under assumptions:
1. Linearity
2. Random sampling
3. No perfect collinearity
4. Zero conditional mean E[u|X] = 0
5. Homoskedasticity Var(u|X) = σ²

\textbf{"Best"} = lowest variance among \textbf{linear unbiased} estimators.

\textbf{Alternatives when assumptions fail}:

\textbf{Heteroskedasticity} (violation of assumption 5):
\begin{itemize}
\item \textbf{GLS (Generalized Least Squares)}: Weight observations inversely to variance
\item More efficient than OLS if Var(u|X) known
\item \textbf{Feasible GLS (FGLS)}: Estimate Var(u|X) from data, then use GLS
\end{itemize}

\textbf{Omitted variables} (violation of assumption 4):
\begin{itemize}
\item \textbf{Instrumental variables (IV)}: Use instruments correlated with X but uncorrelated with u
\item Unbiased despite endogeneity
\end{itemize}

\textbf{Maximum Likelihood Estimation (MLE)}:
\begin{itemize}
\item \textbf{Fully efficient} if error distribution correctly specified (e.g., normal)
\item OLS = MLE under normality
\item More general: Works with non-normal errors, limited dependent variables
\end{itemize}

\textbf{12.5 Best Confidence Intervals}

\textbf{Classical intervals}: Assume normality, rely on t-distribution

\textbf{Alternatives}:

\textbf{1. Asymptotic intervals}:
\begin{itemize}
\item Use normal approximation (z instead of t)
\item Valid for large n
\item Less conservative than t-based intervals
\end{itemize}

\textbf{2. Bootstrap intervals}:
\begin{itemize}
\item \textbf{Resample data} with replacement many times (e.g., 1000)
\item Compute β̂ for each resample
\item Construct CI from bootstrap distribution
\item \textbf{Advantages}: No distributional assumptions, valid for complex estimators
\item \textbf{Disadvantages}: Computationally intensive
\end{itemize}

\textbf{3. Bayesian credible intervals}:
\begin{itemize}
\item Based on posterior distribution
\item Probability interpretation: "95\% probability β is in this interval"
\item Requires prior distribution
\end{itemize}

\textbf{When to use}:
\begin{itemize}
\item \textbf{Classical}: Standard, widely understood
\item \textbf{Bootstrap}: Non-standard estimators, small samples, non-normality
\item \textbf{Bayesian}: Prior information available, want probability statements
\end{itemize}

\textbf{12.6 Best Tests}

\textbf{Three approaches} to hypothesis testing:

\textbf{1. Wald test}:
\begin{itemize}
\item Based on \textbf{distance} between estimate and null
\item t-tests, F-tests are Wald tests
\item \textbf{Easiest}: Only requires unrestricted model
\end{itemize}

\textbf{2. Likelihood Ratio (LR) test}:
\begin{itemize}
\item Based on \textbf{ratio} of likelihoods (restricted vs. unrestricted)
\item \textbf{Requires MLE} and both models estimated
\item Generally more powerful than Wald
\end{itemize}

\textbf{3. Lagrange Multiplier (LM) test} (Score test):
\begin{itemize}
\item Based on \textbf{slope} of likelihood at null
\item \textbf{Easiest}: Only requires restricted model
\item Useful when unrestricted model hard to estimate
\end{itemize}

\textbf{Asymptotic equivalence}: All three approaches equivalent in large samples.

\textbf{Multiple testing}:
\begin{itemize}
\item \textbf{Problem}: Testing many hypotheses inflates Type I error
\item \textbf{Solutions}: Bonferroni correction, FDR (False Discovery Rate) control, Romano-Wolf procedure
\end{itemize}

\textbf{12.7 Data Science and Big Data}

\textbf{Machine Learning vs. Econometrics}:

| Aspect           | Econometrics               | Machine Learning          |
|------------------|----------------------------|---------------------------|
| \textbf{Goal}         | Causal inference           | Prediction                |
| \textbf{Focus}        | Parameter interpretation   | Out-of-sample accuracy    |
| \textbf{Methods}      | OLS, IV, panel data        | Random forests, neural networks |
| \textbf{Assumptions}  | Explicit, testable         | Implicit, flexible        |
| \textbf{Sample size}  | Often small (n<1000)       | Often large (n>10,000)    |

\textbf{Complementarity}:
\begin{itemize}
\item \textbf{Econometrics}: Answers "why" (causal mechanisms)
\item \textbf{ML}: Answers "what" (predictions)
\item \textbf{Modern approach}: Combine both (e.g., double machine learning for causal inference)
\end{itemize}

\textbf{Regularization} (LASSO, Ridge):
\begin{itemize}
\item \textbf{Shrinks coefficients} toward zero
\item Reduces overfitting with many predictors
\item \textbf{Bias-variance tradeoff}: Accept bias to reduce variance
\end{itemize}

\textbf{Cross-validation}:
\begin{itemize}
\item \textbf{Split data}: Training set (estimate model), test set (evaluate predictions)
\item Prevents overfitting
\item Standard in ML, increasingly used in econometrics
\end{itemize}

\textbf{12.8 Bayesian Methods}

\textbf{Bayesian vs. Frequentist}:

| Aspect          | Frequentist                    | Bayesian                        |
|-----------------|--------------------------------|---------------------------------|
| \textbf{Parameters}  | Fixed but unknown              | Random with distributions       |
| \textbf{Inference}   | Based on sampling distributions| Based on posterior distributions|
| \textbf{Probability} | Long-run frequency             | Degree of belief                |
| \textbf{Prior}       | Not used                       | Required                        |

\textbf{Bayes' Theorem}:
Posterior ∝ Prior × Likelihood

\textbf{Example}:
\begin{itemize}
\item \textbf{Prior}: β ~ N(0, 100²) (vague prior)
\item \textbf{Likelihood}: y|X, β ~ N(Xβ, σ²I)
\item \textbf{Posterior}: β|y, X ~ N(β̂, V) (combines prior and data)
\end{itemize}

\textbf{Advantages}:
\begin{itemize}
\item \textbf{Probability statements}: "95\% probability β > 0" (not allowed in frequentist)
\item \textbf{Prior information}: Incorporate expert knowledge
\item \textbf{Hierarchical models}: Natural framework for multi-level data
\end{itemize}

\textbf{Disadvantages}:
\begin{itemize}
\item \textbf{Subjective priors}: Different priors → different conclusions
\item \textbf{Computational}: Often requires MCMC (Markov Chain Monte Carlo)
\end{itemize}

\textbf{Convergence}: With large data, Bayesian → frequentist (data overwhelms prior).

\textbf{12.9 Brief History of Statistics and Econometrics}

\textbf{Key Milestones}:

\textbf{1800s}:
\begin{itemize}
\item \textbf{Gauss, Legendre}: Least squares method
\item \textbf{Galton}: Correlation, regression to the mean
\end{itemize}

\textbf{Early 1900s}:
\begin{itemize}
\item \textbf{Pearson}: Chi-square tests, regression
\item \textbf{Fisher}: Maximum likelihood, ANOVA, experimental design
\end{itemize}

\textbf{1920s-1940s}:
\begin{itemize}
\item \textbf{Fisher}: p-values, significance tests
\item \textbf{Neyman-Pearson}: Hypothesis testing framework (Type I/II errors)
\item \textbf{Haavelmo}: Probability foundation for econometrics (Nobel Prize 1989)
\end{itemize}

\textbf{1940s-1960s}:
\begin{itemize}
\item \textbf{Cowles Commission}: Simultaneous equations, identification
\item \textbf{Theil, Zellner}: Bayesian econometrics
\end{itemize}

\textbf{1970s-1980s}:
\begin{itemize}
\item \textbf{White}: Robust standard errors (1980)
\item \textbf{Newey-West}: HAC estimators (1987)
\item \textbf{Box-Jenkins}: Time series methods (ARIMA)
\end{itemize}

\textbf{1990s-2000s}:
\begin{itemize}
\item \textbf{Panel data}: Fixed effects, random effects, GMM
\item \textbf{Instrumental variables}: Angrist, Imbens (Nobel Prize 2021)
\item \textbf{Causal inference}: Rubin causal model, difference-in-differences
\end{itemize}

\textbf{2010s-present}:
\begin{itemize}
\item \textbf{Machine learning}: Regularization (LASSO), random forests, neural networks
\item \textbf{Causal ML}: Combining ML and causal inference
\item \textbf{Big data}: Scalable methods for massive datasets
\item \textbf{Reproducibility}: Open science, pre-registration, code sharing
\end{itemize}

\textbf{Key Insight}: Modern econometrics \textbf{integrates} classical statistical theory, causal inference, and machine learning tools.


\section{Conclusion}

In this chapter, we've explored advanced regression techniques that extend basic OLS to handle real-world complications and practical forecasting applications. We examined robust inference methods for heteroskedasticity and autocorrelation, learned to construct prediction intervals for individual forecasts, and surveyed modern extensions connecting classical econometrics to contemporary data science.

Through the housing price analysis, we saw how heteroskedasticity-robust standard errors provide valid inference when error variance is non-constant—a common violation in cross-sectional data. The GDP growth time series demonstrated the necessity of HAC standard errors when observations are correlated over time, with lag-1 autocorrelation of 0.868 requiring substantial corrections. Most importantly, the prediction interval analysis revealed the fundamental distinction between estimating population parameters (confidence intervals) and forecasting individual outcomes (prediction intervals), with the latter being 5.2 times wider due to irreducible individual randomness.

These practical tools—robust SEs, HAC estimation, and prediction intervals—represent essential skills for applied econometric work. Whether analyzing cross-sectional survey data, time series macroeconomic indicators, or panel datasets, violations of classical assumptions are the norm rather than the exception. The methods in this chapter ensure your inferences remain valid despite these violations, while prediction intervals enable rigorous probabilistic forecasting for decision-making.

\textbf{What You've Learned}:

\textbf{Programming and Implementation}:

\begin{itemize}
\item \textbf{Robust standard errors}: How to implement \texttt{.fit(cov\_type='HC1')} for heteroskedasticity-robust inference in cross-sectional data
\item \textbf{HAC standard errors}: How to apply \texttt{cov\_type='HAC', cov\_kwds={'maxlags': L}} for time series with autocorrelation
\item \textbf{Prediction intervals}: How to use \texttt{.get\_prediction()} and \texttt{.conf\_int(obs=True)} for individual forecasting
\item \textbf{Manual calculations}: How to compute prediction standard errors from formulas to understand mathematical foundations
\item \textbf{Diagnostic tools}: How to generate autocorrelation functions with \texttt{acf()} and correlograms with \texttt{plot\_acf()}
\end{itemize}

\textbf{Statistical Inference}:

\begin{itemize}
\item \textbf{Robust inference}: Understanding when classical standard errors fail and how robust SEs correct for heteroskedasticity
\item \textbf{Time series adjustments}: Recognizing autocorrelation patterns and choosing appropriate Newey-West lag lengths (L ≈ 0.75n^(1/3))
\item \textbf{Goodness-of-fit}: Interpreting adjusted R² (penalizes complexity) and RMSE (measures prediction accuracy in original units)
\item \textbf{Diagnostic interpretation}: Comparing default vs. robust SEs to assess heteroskedasticity severity
\end{itemize}

\textbf{Forecasting Applications}:

\begin{itemize}
\item \textbf{Prediction vs. estimation}: Distinguishing between confidence intervals for E[Y|X] and prediction intervals for Y
\item \textbf{Individual randomness}: Understanding the "+1" term in prediction interval formulas representing irreducible error σ²
\item \textbf{Forecast uncertainty}: Recognizing that prediction intervals don't shrink with sample size due to σ²
\item \textbf{Extrapolation risk}: Avoiding predictions far from x̄ where intervals become very wide and unreliable
\end{itemize}

\textbf{Looking Ahead}:

The advanced techniques in this chapter prepare you for sophisticated empirical research across economics and data science. In subsequent work, you'll encounter panel data requiring clustered standard errors, instrumental variable estimation for causal inference, and machine learning methods for prediction. The robust inference framework established here—checking assumptions, applying corrections, and maintaining valid inference—applies universally across these extensions.

You might explore Chapter 13's case studies applying multiple regression to real economic questions, Chapter 14's indicator variables for categorical predictors, or Chapter 15's interaction terms and polynomial specifications. More advanced courses cover time series methods (ARIMA, VAR), panel data techniques (fixed effects, difference-in-differences), and causal inference approaches (regression discontinuity, synthetic controls). Machine learning courses extend prediction methods with regularization (LASSO, Ridge), ensemble methods (random forests), and neural networks.

Most importantly, this chapter demonstrates that econometrics is not a fixed set of formulas but an evolving toolkit adapting to new challenges. From Gauss's least squares (1800s) to White's robust SEs (1980) to Newey-West HAC estimation (1987) to contemporary machine learning integration, the field continuously develops solutions for real-world complications. Your ability to diagnose assumption violations, select appropriate corrections, and interpret results critically positions you to contribute to this ongoing evolution.


\textbf{References}:

\begin{itemize}
\item Cameron, A.C. (2022). \textit{Analysis of Economics Data: An Introduction to Econometrics}. \url{https://cameron.econ.ucdavis.edu/aed/index.html}
\end{itemize}

\textbf{Data}:

All datasets available at: \url{https://cameron.econ.ucdavis.edu/aed/aeddata.html}

\vspace{1em}

\begin{keyconcept}{Learn by Coding}
Now that you've learned the key concepts in this chapter, it's time to put them into practice!

Open the interactive Google Colab notebook for this chapter to:
\begin{itemize}
\item Run Python code implementing all the methods discussed
\item Experiment with real datasets and see results immediately
\item Modify parameters and explore how changes affect outcomes
\item Complete hands-on exercises that reinforce your understanding
\end{itemize}

\textbf{Access the notebook here:} \url{https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.ipynb}

Remember: Learning econometrics is not just about understanding theory---it's about applying it. The best way to master these concepts is to code them yourself!
\end{keyconcept}
