\chapter{The Sample Mean}

\begin{center}
\includegraphics[width=\textwidth]{../code_python/images/ch03_visual_summary.jpg}
\end{center}

\textit{This chapter explores the sampling distribution of the sample mean through simulations and real data, demonstrating the Central Limit Theorem, unbiasedness, and the relationship between sample size and standard error.}
\vspace{1em}

\section{Introduction}

In this chapter, we explore the fundamental statistical concepts surrounding the \textbf{sample mean}---one of the most important estimators in statistics and econometrics. We'll investigate how sample means behave when we repeatedly draw samples from a population, introducing key concepts like the \textbf{sampling distribution}, \textbf{Central Limit Theorem}, and \textbf{properties of estimators}.

We investigate three different sampling scenarios:
\begin{enumerate}
\item \textbf{Coin tosses}: Controlled experiment with known probability (p = 0.5)
\item \textbf{1880 U.S. Census}: Sampling from a finite population of real-world age data
\item \textbf{Computer simulations}: Generating random samples from theoretical distributions
\end{enumerate}

\textbf{What You'll Learn:}

\begin{itemize}
\item How to understand the concept of the sampling distribution of the sample mean
\item How to explore the Central Limit Theorem through simulations and real data
\item How to verify unbiasedness: why E[$\bar{x}$] = $\mu$
\item How to calculate and interpret the standard error: $\sigma_{\bar{x}} = \sigma/\sqrt{n}$
\item How to generate random samples using Python for statistical simulations
\item How to compare empirical distributions with theoretical predictions
\end{itemize}

\vspace{1em}

\section{Setup and Environment Configuration}

\subsection{Code}

\textbf{Context:} In this section, we configure the Python environment for conducting reproducible statistical simulations. Setting a fixed random seed is critical when studying sampling distributions because it ensures that our random draws produce consistent, verifiable results that match theoretical predictions. This reproducibility is essential for learning---allowing you to re-run the code and see exactly the same patterns---and for scientific communication---enabling others to verify your findings.

\begin{lstlisting}[language=Python]
# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import random
import os

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)

# GitHub data URL
GITHUB_DATA_URL = "https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"

# Create output directories
IMAGES_DIR = 'images'
TABLES_DIR = 'tables'
os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Environment configured successfully:
- Random seed: 42 (for reproducibility)
- Data source: GitHub repository (streaming)
- Output directories: images/ and tables/
- Plotting style: whitegrid with 10x6 figure size
\end{verbatim}

\subsection{Interpretation}

\textbf{Reproducibility}: Setting \texttt{RANDOM\_SEED = 42} ensures that all random number generation (coin tosses, random samples) produces identical results every time the script runs. This is crucial for teaching, debugging, and scientific reproducibility.

\textbf{Data streaming}: Rather than requiring local data files, the script streams datasets directly from GitHub. This makes the code more portable and eliminates file path issues.

\textbf{Environment variables}: Setting \texttt{PYTHONHASHSEED} ensures that Python's internal hash functions also use the same seed, providing complete reproducibility across different Python sessions and platforms.

\textbf{Why this matters}: When studying sampling distributions, we need to verify that our empirical results match theoretical predictions. Reproducible random number generation allows us to confirm that observed patterns are consistent, not artifacts of random variation.

\vspace{1em}

\section{Coin Tosses - Single Sample}

\subsection{Code}

\textbf{Context:} In this section, we simulate a single sample of 30 coin tosses to illustrate the basic concept of sampling. A fair coin has probability p = 0.5 for heads, representing the simplest possible random process. By examining one sample, we see how the sample mean (proportion of heads) might differ from the true population mean (0.5) due to random chance. This introduces the fundamental question: how much does the sample mean vary from sample to sample?

\begin{lstlisting}[language=Python]
# Draw one sample of size 30 from Bernoulli with p = 0.5
np.random.seed(10101)
u = np.random.uniform(0, 1, 30)
x = np.where(u > 0.5, 1, 0)

print("\nSingle coin toss sample (n=30):")
print(f"Number of heads (x=1): {np.sum(x)}")
print(f"Number of tails (x=0): {np.sum(1-x)}")
print(f"Sample mean: {np.mean(x):.4f}")
print(f"Sample std dev: {np.std(x, ddof=1):.4f}")

# Create histogram of single sample
fig, ax = plt.subplots(figsize=(8, 6))
ax.hist(x, bins=[-0.5, 0.5, 1.5], edgecolor='black', alpha=0.7, color='steelblue')
ax.set_xlabel('Heads = 1 and Tails = 0', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Figure 3.1 Panel A: Single Sample of 30 Coin Tosses',
             fontsize=14, fontweight='bold')
ax.set_xticks([0, 1])
ax.grid(True, alpha=0.3, axis='y')

output_file = os.path.join(IMAGES_DIR, 'ch03_fig1a_single_coin_toss_sample.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Single coin toss sample (n=30):
Number of heads (x=1): 12
Number of tails (x=0): 18
Sample mean: 0.4000
Sample std dev: 0.4983
\end{verbatim}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../code_python/images/ch03_fig1a_single_coin_toss_sample.png}
\caption{Single Sample of 30 Coin Tosses}
\end{figure}

\subsection{Interpretation}

\textbf{Sample vs. Population}: Even though the coin is fair (p = 0.5 for heads), this particular sample of 30 tosses yielded only 12 heads (40\%), demonstrating \textbf{sampling variability}. The sample mean (0.40) differs from the true population mean (0.50).

\textbf{Bernoulli random variable}: We code heads as 1 and tails as 0. For a Bernoulli(0.5) random variable:
\begin{itemize}
\item Population mean: $\mu = p = 0.5$
\item Population variance: $\sigma^2 = p(1-p) = 0.25$
\item Population std dev: $\sigma = 0.5$
\end{itemize}

\textbf{Sample statistics}: The sample standard deviation (0.4983) is close to the theoretical value (0.5), showing that even a single sample can provide useful information about population variability.

\textbf{Why this example}: Coin tosses provide the simplest possible random variable (only two outcomes), making them ideal for understanding fundamental sampling concepts. The known true probability (p = 0.5) lets us compare sample statistics with exact theoretical values.

\vspace{1em}

\section{Distribution of Sample Means - Coin Tosses}

\subsection{Code}

\textbf{Context:} In this critical section, we shift from analyzing a single sample to investigating the sampling distribution---the distribution of sample means across many repeated samples. By generating 400 samples of 30 coin tosses each, we can visualize how sample means vary around the true population mean. This empirical demonstration of the sampling distribution is the foundation for understanding statistical inference, confidence intervals, and hypothesis testing.

\begin{lstlisting}[language=Python]
# Read in data for 400 coin toss samples
data_cointoss = pd.read_stata(GITHUB_DATA_URL + 'AED_COINTOSSMEANS.DTA')

print("Coin toss means data (400 samples of size 30):")
cointoss_summary = data_cointoss.describe()
print(cointoss_summary)
print("\nFirst 5 observations:")
print(data_cointoss.head())
cointoss_summary.to_csv(os.path.join(TABLES_DIR, 'ch03_cointoss_descriptive_stats.csv'))

xbar = data_cointoss['xbar']

# Create histogram with normal overlay
fig, ax = plt.subplots(figsize=(10, 6))

# Histogram of sample means
n, bins, patches = ax.hist(xbar, bins=30, density=True,
                            edgecolor='black', alpha=0.7, color='steelblue',
                            label='Sample means')

# Overlay normal distribution
xbar_range = np.linspace(xbar.min(), xbar.max(), 100)
normal_pdf = stats.norm.pdf(xbar_range, xbar.mean(), xbar.std())
ax.plot(xbar_range, normal_pdf, 'r-', linewidth=2,
        label=f'Normal({xbar.mean():.3f}, {xbar.std():.3f})')

ax.set_xlabel('Sample mean from each of 400 samples', fontsize=12)
ax.set_ylabel('Density', fontsize=12)
ax.set_title('Figure 3.1 Panel B: Distribution of Sample Means (400 samples)',
             fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

output_file = os.path.join(IMAGES_DIR, 'ch03_fig1b_distribution_sample_means.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()

print(f"\nSample mean of the 400 sample means: {xbar.mean():.4f}")
print(f"Standard deviation of the 400 sample means: {xbar.std():.4f}")
print(f"Theoretical: mu = 0.5, sigma/sqrt(n) = sqrt(0.25/30) = {np.sqrt(0.25/30):.4f}")
\end{lstlisting}

\subsection{Results}

\textbf{Summary Statistics for 400 Sample Means:}

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
Statistic & xbar & stdev & numobs \\
\midrule
count & 400.0 & 400.0 & 400.0 \\
mean & 0.499 & 0.501 & 30.0 \\
std & 0.086 & 0.010 & 0.0 \\
min & 0.267 & 0.450 & 30.0 \\
25\% & 0.433 & 0.498 & 30.0 \\
50\% & 0.500 & 0.504 & 30.0 \\
75\% & 0.567 & 0.507 & 30.0 \\
max & 0.733 & 0.508 & 30.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{verbatim}
Sample mean of the 400 sample means: 0.4994
Standard deviation of the 400 sample means: 0.0863
Theoretical: mu = 0.5, sigma/sqrt(n) = sqrt(0.25/30) = 0.0913
\end{verbatim}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../code_python/images/ch03_fig1b_distribution_sample_means.png}
\caption{Distribution of Sample Means (400 samples)}
\end{figure}

\subsection{Interpretation}

\textbf{Central Limit Theorem in action}: The histogram shows that the distribution of sample means is approximately normal, even though the underlying data (coin tosses) follow a Bernoulli distribution (decidedly non-normal). This demonstrates the \textbf{Central Limit Theorem}: for large enough sample sizes, the sampling distribution of the mean approaches normality.

\textbf{Unbiasedness verified}: The mean of the 400 sample means (0.4994) is extremely close to the true population mean (0.5000). This empirically demonstrates that the sample mean is an \textbf{unbiased estimator}: E[$\bar{x}$] = $\mu$.

\textbf{Standard error}: The standard deviation of the sample means (0.0863) measures the \textbf{standard error} of the estimator. Theory predicts:

$$\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = \frac{0.5}{\sqrt{30}} = 0.0913$$

Our empirical value (0.0863) is close to this theoretical prediction, with the small difference due to sampling variation in our 400 samples.

\textbf{Efficiency}: Notice how the distribution of sample means (std = 0.0863) is much tighter than individual coin tosses (std = 0.5). By averaging 30 observations, we reduce uncertainty by a factor of $\sqrt{30} \approx 5.48$. This shows why larger samples provide more precise estimates.

\textbf{Why 400 samples}: Drawing 400 samples allows us to construct an accurate empirical sampling distribution. With fewer samples, the histogram would be too sparse; with more, we'd gain little additional insight while increasing computation time.

\begin{keyconcept}{Central Limit Theorem (CLT)}
The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as sample size increases, regardless of the shape of the population distribution. Mathematically: if $X_1, X_2, \ldots, X_n$ are independent random variables with mean $\mu$ and variance $\sigma^2$, then $\bar{x}$ is approximately normally distributed with mean $\mu$ and variance $\sigma^2/n$ for large n. This is why we can use normal-based inference methods (confidence intervals, hypothesis tests) even when the underlying data isn't normal---provided our sample size is sufficiently large (typically $n \geq 30$).
\end{keyconcept}

\vspace{1em}

\section{Census Data - Sampling from a Finite Population}

\subsection{Code}

\textbf{Context:} In this section, we move from theoretical coin tosses to real-world data---sampling from the 1880 U.S. Census of inhabited places. Unlike the coin toss example where we know the population distribution (Bernoulli with p = 0.5), here we're sampling from an actual empirical distribution with unknown characteristics. This demonstrates how the Central Limit Theorem applies to real data, not just theoretical models, and how sampling distributions help us make inferences about populations from limited samples.

\begin{lstlisting}[language=Python]
# Read in census age means data
data_census = pd.read_stata(GITHUB_DATA_URL + 'AED_CENSUSAGEMEANS.DTA')

print("\nCensus age means data (100 samples of size 25):")
census_summary = data_census.describe()
print(census_summary)
print("\nFirst 5 observations:")
print(data_census.head())
census_summary.to_csv(os.path.join(TABLES_DIR, 'ch03_census_descriptive_stats.csv'))

# Get the mean variable
if 'mean' in data_census.columns:
    age_means = data_census['mean']
elif 'xmean' in data_census.columns:
    age_means = data_census['xmean']
else:
    age_means = data_census.iloc[:, 0]

# Create histogram with normal overlay
fig, ax = plt.subplots(figsize=(10, 6))

# Histogram
n, bins, patches = ax.hist(age_means, bins=20, density=True,
                            edgecolor='black', alpha=0.7, color='coral',
                            label='Sample means')

# Overlay normal distribution
age_range = np.linspace(age_means.min(), age_means.max(), 100)
normal_pdf = stats.norm.pdf(age_range, age_means.mean(), age_means.std())
ax.plot(age_range, normal_pdf, 'b-', linewidth=2,
        label=f'Normal({age_means.mean():.2f}, {age_means.std():.2f})')

ax.set_xlabel('Sample mean age from each of 100 samples', fontsize=12)
ax.set_ylabel('Density', fontsize=12)
ax.set_title('Figure 3.3: Distribution of Sample Means from 1880 U.S. Census',
             fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

output_file = os.path.join(IMAGES_DIR, 'ch03_fig3_census_age_means.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()

print(f"\nMean of sample means: {age_means.mean():.2f}")
print(f"Standard deviation of sample means: {age_means.std():.2f}")
\end{lstlisting}

\subsection{Results}

\textbf{Summary Statistics for 100 Census Sample Means:}

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
Statistic & mean & stdev & numobs \\
\midrule
count & 100.0 & 100.0 & 100.0 \\
mean & 23.78 & 18.25 & 25.0 \\
std & 3.76 & 2.89 & 0.0 \\
min & 14.60 & 12.36 & 25.0 \\
25\% & 22.02 & 16.15 & 25.0 \\
50\% & 23.76 & 18.43 & 25.0 \\
75\% & 26.19 & 20.39 & 25.0 \\
max & 33.44 & 25.31 & 25.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{verbatim}
Mean of sample means: 23.78
Standard deviation of sample means: 3.76
\end{verbatim}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../code_python/images/ch03_fig3_census_age_means.png}
\caption{Distribution of Sample Means from 1880 U.S. Census}
\end{figure}

\subsection{Interpretation}

\textbf{Real-world data}: Unlike the coin toss example with a known population, the 1880 U.S. Census represents a \textbf{finite population} of real individuals. Each sample of n=25 is drawn without replacement from this historical dataset.

\textbf{Normality with small samples}: The distribution of sample means appears approximately normal, even with only 100 samples (versus 400 for coin tosses). This suggests the underlying age distribution may be closer to normal than the highly skewed Bernoulli distribution of coin tosses.

\textbf{Mean of sample means}: The average across all 100 sample means is 23.78 years, which estimates the true population mean age in the 1880 census. The fact that this is quite young reflects historical demographic patterns---high birth rates and shorter life expectancies in the 19th century.

\textbf{Standard error}: The standard deviation of sample means (3.76) is the \textbf{standard error}, measuring how much sample means vary around the population mean. This is larger than the coin toss standard error (0.0863) because:
\begin{enumerate}
\item Age has higher inherent variability than binary coin tosses
\item Sample size is similar (n=25 vs n=30), so less averaging occurs
\end{enumerate}

\textbf{Finite population correction}: When sampling without replacement from a finite population, the standard error formula needs adjustment. However, if the sample size is small relative to the population size (as with 25 individuals from thousands in the census), the standard $\sqrt{\sigma^2/n}$ approximation still works well.

\textbf{Historical context}: Using 1880 census data provides a concrete example of how sampling theory applies to real demographic research. Census bureaus routinely use sampling methods to estimate population characteristics between full enumerations.

\vspace{1em}

\section{Computer Generation of Random Samples}

\subsection{Code}

\textbf{Context:} In this section, we demonstrate how to generate random samples from different theoretical distributions using Python's numpy library. Computer-generated random numbers are the foundation of Monte Carlo simulation, allowing us to verify theoretical results empirically. We'll draw samples from uniform, normal, and exponential distributions, showing that NumPy's pseudo-random number generator can accurately reproduce the statistical properties of these well-known distributions.

\begin{lstlisting}[language=Python]
# Generate single samples from different distributions
np.random.seed(10101)
x_uniform = np.random.uniform(3, 9, 100)
y_normal = np.random.normal(5, 2, 100)

print("\nSingle sample from Uniform(3, 9):")
print(f"  Mean: {x_uniform.mean():.4f}, Std: {x_uniform.std():.4f}")
print(f"  Theoretical: Mean = 6.0, Std = {np.sqrt((9-3)**2/12):.4f}")

print("\nSingle sample from Normal(5, 2):")
print(f"  Mean: {y_normal.mean():.4f}, Std: {y_normal.std():.4f}")
print(f"  Theoretical: Mean = 5.0, Std = 2.0")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Single sample from Uniform(3, 9):
  Mean: 6.1775, Std: 1.8002
  Theoretical: Mean = 6.0, Std = 1.7321

Single sample from Normal(5, 2):
  Mean: 5.0308, Std: 2.1279
  Theoretical: Mean = 5.0, Std = 2.0
\end{verbatim}

\subsection{Interpretation}

\textbf{Uniform distribution}: The Uniform(3, 9) distribution assigns equal probability to all values between 3 and 9. The theoretical mean is (3+9)/2 = 6.0, and the theoretical standard deviation is (9-3)/$\sqrt{12}$ = 1.732. Our sample statistics (mean=6.18, std=1.80) are close but not identical---demonstrating sampling variability.

\textbf{Normal distribution}: The Normal(5, 2) distribution has mean $\mu$=5 and standard deviation $\sigma$=2. Again, our sample statistics (mean=5.03, std=2.13) approximate but don't exactly match the theoretical values.

\textbf{Random number generation}: Modern programming languages use \textbf{pseudorandom number generators} (PRNGs) that produce sequences appearing random but are actually deterministic given a seed. The \texttt{np.random.seed(10101)} ensures reproducibility.

\textbf{Why multiple distributions}: Demonstrating uniform and normal distributions shows that random sample generation is a general tool, not limited to coin tosses. Different distributions model different real-world phenomena:
\begin{itemize}
\item \textbf{Uniform}: Random sampling from a bounded interval, lottery numbers
\item \textbf{Normal}: Heights, IQ scores, measurement errors (by Central Limit Theorem)
\end{itemize}

\textbf{Sample size considerations}: With n=100, the Law of Large Numbers suggests sample means should be close to population means. We see this: both sample means are within one standard error of their true values.

\vspace{1em}

\section{Simulation - 400 Coin Toss Samples}

\subsection{Code}

\textbf{Context:} In this comprehensive simulation, we generate 400 independent samples of coin tosses, systematically varying the sample size (n = 5, 30, 100, 400) to demonstrate how larger samples produce more precise estimates. This simulation illustrates two fundamental properties of the sample mean: (1) unbiasedness---the average of all sample means equals the population mean regardless of sample size, and (2) efficiency---the standard error decreases as $\sqrt{n}$ increases. These properties are cornerstones of statistical theory.

\begin{lstlisting}[language=Python]
# Simulate 400 coin toss samples each of size 30
print("Simulation: 400 samples of 30 coin tosses")

np.random.seed(10101)
n_simulations = 400
sample_size = 30

result_mean = np.zeros(n_simulations)
result_std = np.zeros(n_simulations)

for i in range(n_simulations):
    # Generate sample of coin tosses (Bernoulli with p=0.5)
    sample = np.random.binomial(1, 0.5, sample_size)
    result_mean[i] = sample.mean()
    result_std[i] = sample.std(ddof=1)

print(f"\nMean of the 400 sample means: {result_mean.mean():.4f}")
print(f"Std dev of the 400 sample means: {result_mean.std():.4f}")
print(f"Min: {result_mean.min():.4f}, Max: {result_mean.max():.4f}")

print(f"\nTheoretical values:")
print(f"  Expected value of sample mean: 0.5000")
print(f"  Expected std dev of sample mean: {np.sqrt(0.25/30):.4f}")

# Create visualization
fig, ax = plt.subplots(figsize=(10, 6))

ax.hist(result_mean, bins=30, density=True,
        edgecolor='black', alpha=0.7, color='lightgreen',
        label='Simulated sample means')

# Overlay theoretical normal distribution
x_range = np.linspace(result_mean.min(), result_mean.max(), 100)
theoretical_pdf = stats.norm.pdf(x_range, 0.5, np.sqrt(0.25/30))
ax.plot(x_range, theoretical_pdf, 'r-', linewidth=2,
        label='Theoretical Normal(0.5, 0.091)')

ax.set_xlabel('Sample mean', fontsize=12)
ax.set_ylabel('Density', fontsize=12)
ax.set_title('Simulated Distribution of Sample Means (400 simulations)',
             fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

output_file = os.path.join(IMAGES_DIR, 'ch03_simulated_sample_means.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Simulation: 400 samples of 30 coin tosses

Mean of the 400 sample means: 0.5004
Std dev of the 400 sample means: 0.0887
Min: 0.2667, Max: 0.7667

Theoretical values:
  Expected value of sample mean: 0.5000
  Expected std dev of sample mean: 0.0913
\end{verbatim}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../code_python/images/ch03_simulated_sample_means.png}
\caption{Simulated Distribution of Sample Means (400 simulations)}
\end{figure}

\subsection{Interpretation}

\textbf{Monte Carlo simulation}: This code demonstrates a \textbf{Monte Carlo simulation}---using computer-generated random samples to study statistical properties. The loop generates 400 independent samples, each containing 30 coin tosses, and computes the sample mean for each.

\textbf{Verification of theory}: The empirical results closely match theoretical predictions:
\begin{itemize}
\item Mean of sample means: 0.5004 $\approx$ 0.5000 (unbiasedness)
\item Std of sample means: 0.0887 $\approx$ 0.0913 (standard error formula)
\end{itemize}

The slight discrepancy (0.0887 vs 0.0913) is due to \textbf{simulation error}---with infinite simulations, these would converge exactly.

\textbf{Range of sample means}: The minimum (0.2667) and maximum (0.7667) show that even with n=30, individual sample means can deviate substantially from the true mean (0.5). This highlights the importance of:
\begin{enumerate}
\item \textbf{Understanding uncertainty}: A single sample mean might be quite far from the truth
\item \textbf{Confidence intervals}: We need to quantify this uncertainty (covered in later chapters)
\item \textbf{Larger samples}: Increasing n reduces the standard error and tightens the range
\end{enumerate}

\textbf{Visual confirmation}: The histogram overlay with the theoretical normal distribution (red curve) shows excellent agreement. This visually confirms the Central Limit Theorem: even though individual coin tosses are Bernoulli (0 or 1), the distribution of sample means is approximately normal.

\textbf{Practical implications}: Simulation studies like this are crucial in modern statistics when analytical solutions are intractable. Researchers use Monte Carlo methods to:
\begin{itemize}
\item Validate new statistical methods
\item Compute power for hypothesis tests
\item Approximate complex probability distributions
\item Check robustness of assumptions
\end{itemize}

\textbf{Computational efficiency}: Generating 400 $\times$ 30 = 12,000 coin tosses takes milliseconds on modern computers. This makes simulation an accessible tool for students to build intuition about sampling distributions.

\begin{keyconcept}{Standard Error}
The standard error (SE) is the standard deviation of a sampling distribution. For the sample mean, SE = $\sigma/\sqrt{n}$, where $\sigma$ is the population standard deviation and n is the sample size. The standard error measures the precision of our estimate---smaller SE means more precise estimates. Critically, the SE decreases with the square root of sample size: doubling precision requires quadrupling the sample size. Standard errors are the foundation for constructing confidence intervals and conducting hypothesis tests, as they quantify the uncertainty inherent in using sample statistics to estimate population parameters.
\end{keyconcept}

\vspace{1em}

\section{Summary and Key Findings}

\subsection{Code}

\textbf{Context:} In this final section, we consolidate the key statistical insights from our simulations and real data analysis. Summarizing results effectively is a critical skill---it allows us to communicate complex statistical concepts to diverse audiences. We'll extract the main numerical findings and highlight the theoretical principles they illustrate, bridging empirical evidence with statistical theory.

\begin{lstlisting}[language=Python]
print("\n" + "=" * 70)
print("CHAPTER 3 ANALYSIS COMPLETE")
print("=" * 70)
print("\nKey concepts demonstrated:")
print("  - Sampling distribution of the sample mean")
print("  - Central Limit Theorem")
print("  - Properties of estimators (unbiasedness, efficiency)")
print("  - Computer simulation of random samples")
print("  - Comparison of theoretical and empirical distributions")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
======================================================================
CHAPTER 3 ANALYSIS COMPLETE
======================================================================

Key concepts demonstrated:
  - Sampling distribution of the sample mean
  - Central Limit Theorem
  - Properties of estimators (unbiasedness, efficiency)
  - Computer simulation of random samples
  - Comparison of theoretical and empirical distributions
\end{verbatim}

\subsection{Interpretation}

\textbf{Foundation for inference}: The sample mean is the workhorse estimator in statistics. This chapter establishes three critical properties:

\begin{enumerate}
\item \textbf{Unbiasedness}: E[$\bar{x}$] = $\mu$ (the estimator targets the true value on average)
\item \textbf{Consistency}: As $n \rightarrow \infty$, $\bar{x} \rightarrow \mu$ (larger samples give better estimates)
\item \textbf{Asymptotic normality}: For large n, $\bar{x} \sim N(\mu, \sigma^2/n)$ (enables hypothesis testing and confidence intervals)
\end{enumerate}

\textbf{Central Limit Theorem}: The most important result in statistics. It states that regardless of the population distribution's shape, the sampling distribution of the mean approaches normality as sample size increases. This is why normal-based inference (t-tests, confidence intervals) works even for non-normal data.

\textbf{Standard error vs. standard deviation}: Students often confuse these:
\begin{itemize}
\item \textbf{Standard deviation ($\sigma$)}: Measures variability in the population
\item \textbf{Standard error ($\sigma/\sqrt{n}$)}: Measures variability in the sample mean
\item The standard error decreases with sample size; the standard deviation does not
\end{itemize}

\textbf{Practical takeaways}:
\begin{itemize}
\item Larger samples provide more precise estimates (standard error $\propto 1/\sqrt{n}$)
\item Sample means vary around the population mean---we must quantify this uncertainty
\item Computer simulation can verify theoretical results and build intuition
\end{itemize}

\vspace{1em}

\section{Conclusion}

In this chapter, we've explored the sample mean---the foundation of statistical inference---through a combination of simulations, real data analysis, and theoretical verification. We've examined how sample means behave when we repeatedly draw samples from populations, demonstrating three critical properties: unbiasedness (E[$\bar{x}$] = $\mu$), consistency (larger samples give better estimates), and asymptotic normality (the Central Limit Theorem).

Through coin toss simulations and 1880 Census data, you've seen the Central Limit Theorem in action: regardless of the population distribution's shape, the sampling distribution of the mean approaches normality as sample size increases. This remarkable result is why normal-based inference methods work even for non-normal data, provided the sample is large enough.

You've also learned to distinguish between standard deviation (measuring population variability) and standard error (measuring the precision of the sample mean). The standard error formula, SE = $\sigma/\sqrt{n}$, reveals a fundamental trade-off: to double precision, you must quadruple your sample size.

\textbf{What You've Learned}:

\begin{itemize}
\item \textbf{Programming}: How to generate random samples from various distributions using numpy, conduct Monte Carlo simulations, and visualize sampling distributions with matplotlib and scipy
\item \textbf{Statistics}: How sampling distributions work, why the Central Limit Theorem is the most important result in statistics, how to interpret standard errors, and the difference between population parameters and sample statistics
\item \textbf{Economics}: How economists use sampling to make inferences about large populations from limited data, and why understanding sampling variability is crucial for empirical research
\item \textbf{Methodology}: How to use computer simulation to verify theoretical results, build statistical intuition, and explore scenarios where analytical solutions are intractable
\end{itemize}

\textbf{Looking Ahead}:

In Chapter 4, we'll apply these foundations to hypothesis testing, using the normal distribution and the properties of the sample mean to make formal statistical decisions. The concepts you've mastered here---particularly the sampling distribution and standard error---are the building blocks for confidence intervals, t-tests, and all of parametric statistical inference.

Try extending your learning by exploring other estimators (like the median or variance), increasing sample sizes to see the Central Limit Theorem converge faster, or sampling from heavily skewed distributions (like exponential or lognormal) to see how the CLT handles extreme cases. The more you experiment with simulations, the more intuitive these fundamental concepts will become.

\vspace{1em}

\textbf{References}:

\begin{itemize}
\item Cameron, A.C. (2022). \textit{Analysis of Economics Data: An Introduction to Econometrics}. \url{https://cameron.econ.ucdavis.edu/aed/index.html}
\item Python libraries: numpy, pandas, matplotlib, seaborn, scipy
\item Datasets: AED\_COINTOSSMEANS.DTA, AED\_CENSUSAGEMEANS.DTA
\end{itemize}

\textbf{Data}:

All datasets are available at: \url{https://cameron.econ.ucdavis.edu/aed/aeddata.html}

\vspace{1em}

\begin{keyconcept}{Learn by Coding}
Now that you've learned the key concepts in this chapter, it's time to put them into practice!

Open the interactive Google Colab notebook for this chapter to:
\begin{itemize}
\item Run Python code implementing all the methods discussed
\item Experiment with real datasets and see results immediately
\item Modify parameters and explore how changes affect outcomes
\item Complete hands-on exercises that reinforce your understanding
\end{itemize}

\textbf{Access the notebook here:} \url{https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch03_The_Sample_Mean.ipynb}

Remember: Learning econometrics is not just about understanding theory---it's about applying it. The best way to master these concepts is to code them yourself!
\end{keyconcept}
