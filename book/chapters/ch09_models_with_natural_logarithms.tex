\chapter{Models with Natural Logarithms}

\begin{center}
\includegraphics[width=\textwidth]{../code_python/images/ch09_visual_summary.jpg}
\end{center}

\textit{This chapter demonstrates how logarithmic transformations unlock powerful interpretations in regression models, enabling us to estimate elasticities, growth rates, and percentage effects that are fundamental to economic analysis.}

\section{Introduction}

This report introduces \textbf{logarithmic transformations in regression models}---a powerful tool for addressing nonlinearity, interpreting effects as percentages, and modeling growth processes. While previous chapters focused on linear models (y = $\beta_0$ + $\beta_1$x), Chapter 9 shows that many economic relationships are better represented using \textbf{natural logarithms}.

Logarithmic transformations enable us to:
\begin{itemize}
\item \textbf{Interpret coefficients as elasticities or semi-elasticities} (percentage changes)
\item \textbf{Model multiplicative relationships} (earnings increase by a percentage, not a fixed dollar amount)
\item \textbf{Estimate exponential growth rates} (GDP, stock prices grow at constant percentage rates)
\item \textbf{Reduce skewness} in distributions (earnings, income are right-skewed)
\item \textbf{Stabilize variance} (heteroskedasticity often disappears in logs)
\end{itemize}

This chapter covers four model specifications:
\begin{enumerate}
\item \textbf{Linear}: y = $\beta_0$ + $\beta_1$x (unit change interpretation)
\item \textbf{Log-linear}: ln(y) = $\beta_0$ + $\beta_1$x (semi-elasticity interpretation)
\item \textbf{Log-log}: ln(y) = $\beta_0$ + $\beta_1$ln(x) (elasticity interpretation)
\item \textbf{Linear-log}: y = $\beta_0$ + $\beta_1$ln(x) (diminishing returns interpretation)
\end{enumerate}

\textbf{What You'll Learn:}

\begin{itemize}
\item How to apply logarithmic transformations to economic variables
\item How to interpret coefficients as elasticities and semi-elasticities
\item How to distinguish between four model specifications (linear, log-linear, log-log, linear-log)
\item How to estimate exponential growth rates from time series data
\item How to choose appropriate model specifications based on economic theory
\item How to apply retransformation bias correction for accurate predictions
\item How to recognize when log transformations improve model fit and interpretability
\end{itemize}

\section{Setup and Natural Logarithm Properties}

\subsection{Code}

\textbf{Context:} In this section, we establish the computational environment and explore the fundamental properties of natural logarithms. Understanding these mathematical properties is essential because they determine how we interpret regression coefficients in log-transformed models. By demonstrating key logarithm rules (product, quotient, and power rules), we build the foundation for understanding why log transformations convert multiplicative relationships into additive ones---a crucial insight for economic modeling.

\begin{lstlisting}[language=Python]
# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy import stats
import random
import os

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)

# GitHub data URL
GITHUB_DATA_URL = "https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"

# Create output directories
IMAGES_DIR = 'images'
TABLES_DIR = 'tables'
os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

# Table 9.1 - Demonstration of logarithm properties
x_values = np.array([0.5, 1, 2, 5, 10, 20, 100])
ln_values = np.log(x_values)

log_table = pd.DataFrame({
    'x': x_values,
    'ln(x)': ln_values,
    'exp(ln(x))': np.exp(ln_values)
})
print(log_table)

print("\nKey properties:")
print(f"  ln(1) = {np.log(1):.4f}")
print(f"  ln(e) = {np.log(np.e):.4f}")
print(f"  ln(2*5) = ln(2) + ln(5): {np.log(2*5):.4f} = {np.log(2) + np.log(5):.4f}")
print(f"  ln(10/2) = ln(10) - ln(2): {np.log(10/2):.4f} = {np.log(10) - np.log(2):.4f}")
\end{lstlisting}

\subsection{Results}

\textbf{Table 9.1: Properties of Natural Logarithm}

\begin{verbatim}
       x     ln(x)  exp(ln(x))
0    0.5 -0.693147         0.5
1    1.0  0.000000         1.0
2    2.0  0.693147         2.0
3    5.0  1.609438         5.0
4   10.0  2.302585        10.0
5   20.0  2.995732        20.0
6  100.0  4.605170       100.0

Key properties:
  ln(1) = 0.0000
  ln(e) = 1.0000
  ln(2*5) = ln(2) + ln(5): 2.3026 = 2.3026
  ln(10/2) = ln(10) - ln(2): 1.6094 = 1.6094
\end{verbatim}

\subsection{Interpretation}

\textbf{What is the natural logarithm?}

The \textbf{natural logarithm} (ln or $\log_e$) is the inverse of the exponential function:
\begin{itemize}
\item If y = $e^x$, then x = ln(y)
\item e $\approx$ 2.71828 (Euler's number, base of natural logarithm)
\end{itemize}

\textbf{Key properties}:

\textbf{1. ln(1) = 0}
\begin{itemize}
\item The log of 1 is always 0 (any base)
\item This is because $e^0$ = 1
\end{itemize}

\textbf{2. ln(e) = 1}
\begin{itemize}
\item The log of the base equals 1
\item This is because $e^1$ = e
\end{itemize}

\textbf{3. ln(ab) = ln(a) + ln(b) (product rule)}
\begin{itemize}
\item Example: ln(2 $\times$ 5) = ln(2) + ln(5) = 0.693 + 1.609 = 2.303
\item Verification: ln(10) = 2.303 \checkmark
\item \textbf{Economic interpretation}: Percentage changes add up (multiplication $\to$ addition in logs)
\end{itemize}

\textbf{4. ln(a/b) = ln(a) - ln(b) (quotient rule)}
\begin{itemize}
\item Example: ln(10/2) = ln(10) - ln(2) = 2.303 - 0.693 = 1.609
\item Verification: ln(5) = 1.609 \checkmark
\item \textbf{Economic interpretation}: Percentage differences subtract
\end{itemize}

\textbf{5. ln($a^b$) = b $\times$ ln(a) (power rule)}
\begin{itemize}
\item Example: ln($2^3$) = 3 $\times$ ln(2) = 3 $\times$ 0.693 = 2.079
\item Verification: ln(8) = 2.079 \checkmark
\item \textbf{Economic interpretation}: Elasticities multiply
\end{itemize}

\textbf{6. exp(ln(x)) = x and ln(exp(x)) = x (inverse functions)}
\begin{itemize}
\item The exponential function undoes the logarithm
\item The logarithm undoes the exponential function
\item This is crucial for \textbf{retransformation} (converting predictions back to original scale)
\end{itemize}

\textbf{Why natural logarithm (base e) instead of $\log_{10}$?}

\begin{enumerate}
\item \textbf{Calculus}: d/dx[ln(x)] = 1/x (simple derivative)
\item \textbf{Economics}: Percentage changes are naturally expressed using base e
\item \textbf{Growth models}: Continuous compounding uses e
\item \textbf{Statistical theory}: Maximum likelihood for many distributions uses ln
\end{enumerate}

\textbf{Visual intuition}:

\textbf{Table interpretation}:
\begin{itemize}
\item \textbf{x < 1}: ln(x) is negative (ln(0.5) = -0.693)
\item \textbf{x = 1}: ln(x) = 0 (reference point)
\item \textbf{x > 1}: ln(x) is positive (ln(100) = 4.605)
\end{itemize}

\textbf{Curvature}:
\begin{itemize}
\item Logarithm \textbf{grows slowly} as x increases (concave function)
\item From x = 1 to x = 2: ln increases by 0.693
\item From x = 10 to x = 20: ln increases by 0.693 (same change!)
\item This reflects \textbf{diminishing marginal returns} to percentage increases
\end{itemize}

\textbf{Approximate percentage change formula}:

For small changes, $\Delta$ln(x) $\approx$ $\Delta$x/x (percentage change)

Example: x increases from 10 to 11 (10\% increase)
\begin{itemize}
\item Exact: ln(11) - ln(10) = 2.398 - 2.303 = 0.095
\item Approximation: (11 - 10) / 10 = 0.10
\item Close, especially for small changes!
\end{itemize}

This approximation is the \textbf{foundation} for interpreting log-linear regression coefficients as percentages.

\textbf{Economic applications}:

\textbf{1. Earnings}: A \$10,000 raise means more to someone earning \$30,000 than to someone earning \$300,000
\begin{itemize}
\item ln(earnings) accounts for this (percentage change matters, not absolute change)
\end{itemize}

\textbf{2. GDP growth}: Countries care about growth rates (\%) not absolute increases
\begin{itemize}
\item ln(GDP) converts exponential growth to linear trend
\end{itemize}

\textbf{3. Elasticities}: Economists measure responsiveness as percentage changes
\begin{itemize}
\item ln(y) and ln(x) yield elasticity directly
\end{itemize}

\textbf{4. Skewness reduction}: Income, wealth, firm size are right-skewed
\begin{itemize}
\item ln(income) is more symmetric (closer to normal distribution)
\end{itemize}

\textbf{Python implementation}:

\begin{itemize}
\item \textbf{np.log()}: Natural logarithm (base e)
\item \textbf{np.log10()}: Common logarithm (base 10)
\item \textbf{np.exp()}: Exponential function ($e^x$)
\end{itemize}

\textbf{Caution}: ln(x) is undefined for x $\leq$ 0. Must ensure all values are positive before taking logs.

\begin{keyconcept}{Natural Logarithm Properties}
The natural logarithm converts multiplicative relationships into additive ones through three key properties: ln(ab) = ln(a) + ln(b) (products become sums), ln(a/b) = ln(a) - ln(b) (quotients become differences), and ln($a^b$) = b $\times$ ln(a) (powers become multiplications). This transformation is fundamental to econometrics because it allows percentage changes---which multiply---to be analyzed as linear relationships that add. These properties explain why log-transformed models can interpret coefficients as elasticities and growth rates.
\end{keyconcept}

\section{Semi-Elasticities and Elasticities}

\subsection{Code}

\textbf{Context:} In this section, we systematically compare four model specifications to understand how the choice of transformations affects coefficient interpretation. Each specification---linear, log-linear, log-log, and linear-log---answers a different economic question. By examining these interpretations side-by-side, we learn to match model specifications to research questions and recognize when percentage changes are more economically meaningful than absolute changes.

\begin{lstlisting}[language=Python]
print("Model interpretations:")
print("  Linear model: y = beta_0 + beta_1*x")
print("    Interpretation: Delta_y = beta_1*Delta_x")
print("\n  Log-linear model: ln(y) = beta_0 + beta_1*x")
print("    Interpretation: %Delta_y ~= 100*beta_1*Delta_x (semi-elasticity)")
print("\n  Log-log model: ln(y) = beta_0 + beta_1*ln(x)")
print("    Interpretation: %Delta_y ~= beta_1*%Delta_x (elasticity)")
print("\n  Linear-log model: y = beta_0 + beta_1*ln(x)")
print("    Interpretation: Delta_y ~= beta_1*(%Delta_x/100)")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Model interpretations:
  Linear model: y = beta_0 + beta_1*x
    Interpretation: Delta_y = beta_1*Delta_x

  Log-linear model: ln(y) = beta_0 + beta_1*x
    Interpretation: %Delta_y ~= 100*beta_1*Delta_x (semi-elasticity)

  Log-log model: ln(y) = beta_0 + beta_1*ln(x)
    Interpretation: %Delta_y ~= beta_1*%Delta_x (elasticity)

  Linear-log model: y = beta_0 + beta_1*ln(x)
    Interpretation: Delta_y ~= beta_1*(%Delta_x/100)
\end{verbatim}

\subsection{Interpretation}

\textbf{Four model specifications and their interpretations}:

The choice of whether to log-transform y and/or x determines how we interpret the slope coefficient $\beta_1$.

\textbf{Model 1: Linear (y = $\beta_0$ + $\beta_1$x)}

\textbf{Interpretation}: $\beta_1$ is the \textbf{absolute change} in y for a 1-unit increase in x.

\textbf{Example}: earnings = 30,000 + 5,000 $\times$ education
\begin{itemize}
\item One more year of education $\to$ \$5,000 increase in earnings
\item This is the same \$5,000 whether starting from 8 years (high school) or 20 years (PhD)
\end{itemize}

\textbf{When to use}:
\begin{itemize}
\item Both variables measured in absolute units (dollars, years, etc.)
\item Effect is constant across range of x (no diminishing returns)
\item Residuals are homoskedastic and normally distributed
\end{itemize}

\textbf{Model 2: Log-linear (ln(y) = $\beta_0$ + $\beta_1$x)}

\textbf{Interpretation}: $\beta_1$ is the \textbf{proportional change} in y for a 1-unit increase in x.

\textbf{Derivation}:
\begin{itemize}
\item $\Delta$ln(y) = $\beta_1\Delta$x
\item $\Delta$ln(y) $\approx$ $\Delta$y/y (percentage change approximation)
\item Therefore: $\Delta$y/y $\approx$ $\beta_1\Delta$x
\item Or: \%$\Delta$y $\approx$ 100$\beta_1\Delta$x
\end{itemize}

\textbf{Example}: ln(earnings) = 8.56 + 0.13 $\times$ education
\begin{itemize}
\item One more year of education $\to$ 13\% increase in earnings
\item If currently earning \$40,000: increase = 0.13 $\times$ \$40,000 = \$5,200
\item If currently earning \$80,000: increase = 0.13 $\times$ \$80,000 = \$10,400
\end{itemize}

\textbf{When to use}:
\begin{itemize}
\item y is measured in dollars/quantities that grow proportionally
\item Effect is multiplicative (same percentage change, not same absolute change)
\item y is right-skewed (earnings, income, prices)
\item Elasticity with respect to x is constant
\end{itemize}

\textbf{Note}: This is called \textbf{semi-elasticity} because only y is logged (half-elastic).

\textbf{Model 3: Log-log (ln(y) = $\beta_0$ + $\beta_1$ln(x))}

\textbf{Interpretation}: $\beta_1$ is the \textbf{elasticity} of y with respect to x.

\textbf{Derivation}:
\begin{itemize}
\item $\Delta$ln(y) = $\beta_1\Delta$ln(x)
\item $\Delta$ln(y) $\approx$ $\Delta$y/y and $\Delta$ln(x) $\approx$ $\Delta$x/x
\item Therefore: $\Delta$y/y $\approx$ $\beta_1$($\Delta$x/x)
\item Or: \%$\Delta$y $\approx$ $\beta_1$\%$\Delta$x
\end{itemize}

\textbf{Example}: ln(earnings) = 6.55 + 1.48 $\times$ ln(education)
\begin{itemize}
\item 1\% increase in education $\to$ 1.48\% increase in earnings
\item 10\% increase in education $\to$ 14.8\% increase in earnings
\end{itemize}

\textbf{When to use}:
\begin{itemize}
\item Both x and y measured in dollars/quantities
\item Relationship is multiplicative for both variables
\item Both variables are right-skewed
\item Elasticity is constant ($\beta_1$ doesn't depend on x or y levels)
\end{itemize}

\textbf{Economic interpretation}:
\begin{itemize}
\item $\beta_1$ > 1: Elastic (y responds strongly to x)
\item $\beta_1$ = 1: Unit elastic (y changes proportionally to x)
\item $\beta_1$ < 1: Inelastic (y responds weakly to x)
\end{itemize}

\textbf{Model 4: Linear-log (y = $\beta_0$ + $\beta_1$ln(x))}

\textbf{Interpretation}: $\beta_1$/100 is the \textbf{absolute change} in y for a 1\% increase in x.

\textbf{Derivation}:
\begin{itemize}
\item $\Delta$y = $\beta_1\Delta$ln(x)
\item $\Delta$ln(x) $\approx$ $\Delta$x/x (percentage change)
\item Therefore: $\Delta$y $\approx$ $\beta_1$($\Delta$x/x)
\item For 1\% increase: $\Delta$y $\approx$ $\beta_1$/100
\end{itemize}

\textbf{Example}: earnings = -102,700 + 54,433 $\times$ ln(education)
\begin{itemize}
\item 1\% increase in education $\to$ \$544 increase in earnings
\item 10\% increase in education $\to$ \$5,443 increase in earnings
\end{itemize}

\textbf{When to use}:
\begin{itemize}
\item y is measured in levels (dollars)
\item x is measured in levels but has diminishing returns
\item Want to capture decreasing marginal effects (concave relationship)
\item x is right-skewed but y is not
\end{itemize}

\textbf{Comparison table}:

\begin{center}
\begin{tabular}{llll}
\hline
Model & Specification & $\beta_1$ Interpretation & Example \\
\hline
Linear & y $\sim$ x & $\Delta$y for 1-unit $\Delta$x & \$5,000 per year of education \\
Log-linear & ln(y) $\sim$ x & \%$\Delta$y for 1-unit $\Delta$x & 13\% per year of education \\
Log-log & ln(y) $\sim$ ln(x) & \%$\Delta$y for 1\% $\Delta$x & Elasticity = 1.48 \\
Linear-log & y $\sim$ ln(x) & $\Delta$y for 1\% $\Delta$x & \$544 per 1\% education increase \\
\hline
\end{tabular}
\end{center}

\textbf{Choosing the right model}:

\textbf{Decision tree}:
\begin{enumerate}
\item \textbf{Is y right-skewed (earnings, prices)?} $\to$ Consider log(y)
\item \textbf{Is x right-skewed (firm size, wealth)?} $\to$ Consider log(x)
\item \textbf{Do effects vary by level (proportional vs. absolute)?} $\to$ Consider log transformation
\item \textbf{Does economic theory suggest elasticity?} $\to$ Use log-log
\item \textbf{Are residuals heteroskedastic?} $\to$ Log transformation often helps
\item \textbf{Compare $R^2$ across models} $\to$ Higher $R^2$ suggests better fit
\end{enumerate}

\textbf{Common pitfall}: Don't blindly maximize $R^2$---choose model based on economic interpretation and theory.

\begin{keyconcept}{Semi-Elasticities and Elasticities}
The choice of which variables to log-transform determines how we interpret regression coefficients. In a log-linear model (ln(y) $\sim$ x), the coefficient is a semi-elasticity measuring the percentage change in y for a one-unit change in x. In a log-log model (ln(y) $\sim$ ln(x)), the coefficient is an elasticity measuring the percentage change in y for a one percent change in x. Elasticities are unitless and scale-free, making them ideal for comparing effects across different contexts, which is why they're ubiquitous in economics.
\end{keyconcept}

\textbf{Exact vs. approximate percentage changes}:

For log-linear model: ln(y) = $\beta_0$ + $\beta_1$x

\textbf{Approximate}: \%$\Delta$y $\approx$ 100$\beta_1\Delta$x

\textbf{Exact}: \%$\Delta$y = 100 $\times$ [exp($\beta_1\Delta$x) - 1]

\textbf{Example}: $\beta_1$ = 0.13, $\Delta$x = 1
\begin{itemize}
\item Approximate: 100 $\times$ 0.13 = 13\%
\item Exact: 100 $\times$ [exp(0.13) - 1] = 100 $\times$ [1.139 - 1] = 13.9\%
\end{itemize}

\textbf{When does it matter?}
\begin{itemize}
\item For small $\beta_1$ (< 0.15): approximation is accurate
\item For large $\beta_1$ (> 0.15): use exact formula
\item In practice, most coefficients are small enough for approximation
\end{itemize}

\section{Earnings and Education: Four Model Specifications}

\subsection{Code}

\textbf{Context:} In this section, we estimate the returns to education using four different model specifications applied to the same dataset. This comparison is crucial for understanding how model choice affects economic interpretation---what appears as a fixed dollar return in one specification becomes a percentage return in another. By fitting all four models and comparing their $R^2$ values and interpretations, we learn evidence-based criteria for selecting the most appropriate specification for earnings-education relationships.

\begin{lstlisting}[language=Python]
# Read in the earnings data
data_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS.DTA')

print("Data summary:")
print(data_earnings.describe())
print("\nFirst few observations:")
print(data_earnings.head())

# Create log variables
data_earnings['lnearn'] = np.log(data_earnings['earnings'])
data_earnings['lneduc'] = np.log(data_earnings['education'])

# Model 1: Linear model
model_linear = ols('earnings ~ education', data=data_earnings).fit()
print(model_linear.summary())
print(f"\nInterpretation: One additional year of education is associated with")
print(f"${model_linear.params['education']:.2f} increase in annual earnings")

# Model 2: Log-linear model
model_loglin = ols('lnearn ~ education', data=data_earnings).fit()
print(model_loglin.summary())
print(f"\nInterpretation: One additional year of education is associated with")
print(f"{100*model_loglin.params['education']:.2f}% increase in earnings")

# Model 3: Log-log model
model_loglog = ols('lnearn ~ lneduc', data=data_earnings).fit()
print(model_loglog.summary())
print(f"\nInterpretation: A 1% increase in education is associated with")
print(f"{model_loglog.params['lneduc']:.4f}% increase in earnings (elasticity)")

# Model 4: Linear-log model
model_linlog = ols('earnings ~ lneduc', data=data_earnings).fit()
print(model_linlog.summary())
print(f"\nInterpretation: A 1% increase in education is associated with")
print(f"${model_linlog.params['lneduc']/100:.2f} increase in annual earnings")
\end{lstlisting}

\subsection{Results}

\textbf{Data Summary (n = 171 individuals, age 30):}

\begin{verbatim}
            earnings   education    age  gender
count     171.000000  171.000000  171.0   171.0
mean    41412.690058   14.432749   30.0     0.0
std     25527.053396    2.735364    0.0     0.0
min      1050.000000    3.000000   30.0     0.0
25%     25000.000000   12.000000   30.0     0.0
50%     36000.000000   14.000000   30.0     0.0
75%     49000.000000   16.000000   30.0     0.0
max    172000.000000   20.000000   30.0     0.0
\end{verbatim}

\textbf{Model 1: Linear (earnings = -31,060 + 5,021 $\times$ education)}

\begin{verbatim}
                            OLS Regression Results
==============================================================================
Dep. Variable:               earnings   R-squared:                       0.289
Model:                            OLS   Adj. R-squared:                  0.285
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept  -3.106e+04   8887.835     -3.494      0.001   -4.86e+04   -1.35e+04
education   5021.1229    605.101      8.298      0.000    3826.593    6215.653
==============================================================================

Interpretation: One additional year of education is associated with
$5021.12 increase in annual earnings
\end{verbatim}

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch09_fig1a_linear_model.png}
\end{center}

\textbf{Model 2: Log-linear (ln(earnings) = 8.56 + 0.131 $\times$ education)}

\begin{verbatim}
                            OLS Regression Results
==============================================================================
Dep. Variable:                 lnearn   R-squared:                       0.334
Model:                            OLS   Adj. R-squared:                  0.330
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      8.5608      0.210     40.825      0.000       8.147       8.975
education      0.1314      0.014      9.206      0.000       0.103       0.160
==============================================================================

Interpretation: One additional year of education is associated with
13.14% increase in earnings
\end{verbatim}

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch09_fig1b_loglinear_model.png}
\end{center}

\textbf{Model 3: Log-log (ln(earnings) = 6.55 + 1.478 $\times$ ln(education))}

\begin{verbatim}
                            OLS Regression Results
==============================================================================
Dep. Variable:                 lnearn   R-squared:                       0.286
Model:                            OLS   Adj. R-squared:                  0.282
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      6.5454      0.477     13.725      0.000       5.604       7.487
lneduc         1.4775      0.179      8.233      0.000       1.123       1.832
==============================================================================

Interpretation: A 1% increase in education is associated with
1.4775% increase in earnings (elasticity)
\end{verbatim}

\textbf{Model 4: Linear-log (earnings = -102,700 + 54,433 $\times$ ln(education))}

\begin{verbatim}
                            OLS Regression Results
==============================================================================
Dep. Variable:               earnings   R-squared:                       0.231
Model:                            OLS   Adj. R-squared:                  0.226
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept  -1.027e+05   2.03e+04     -5.056      0.000   -1.43e+05   -6.26e+04
lneduc      5.443e+04   7645.805      7.119      0.000    3.93e+04    6.95e+04
==============================================================================

Interpretation: A 1% increase in education is associated with
$544.33 increase in annual earnings
\end{verbatim}

\textbf{Model Comparison Summary:}

\begin{verbatim}
     Model Specification       R-squared   Slope Coef
    Linear         y ~ x 0.289488  5021.122947
Log-linear     ln(y) ~ x 0.333972     0.131424
   Log-log ln(y) ~ ln(x) 0.286248     1.477503
Linear-log     y ~ ln(x) 0.230719 54433.415866
\end{verbatim}

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch09_four_models_comparison.png}
\end{center}

\subsection{Interpretation}

\textbf{Dataset}: AED\_EARNINGS.DTA contains earnings and education data for 171 individuals, all aged 30.

\textbf{Variables}:
\begin{itemize}
\item \textbf{earnings}: Annual earnings in dollars (dependent variable)
\item \textbf{education}: Years of completed schooling (independent variable)
\item \textbf{age}: All observations are 30 years old (controlled by design)
\item \textbf{gender}: All observations are male (controlled by design)
\end{itemize}

\textbf{Research question}: How does education affect earnings?

This is a classic question in labor economics, testing the \textbf{human capital theory}: education increases productivity, which increases earnings.

\textbf{Data characteristics}:

\textbf{Earnings}:
\begin{itemize}
\item Mean: \$41,413
\item Median: \$36,000 (below mean, indicating right skew)
\item Range: \$1,050 to \$172,000 (very wide spread)
\item Coefficient of variation: 62\% (high dispersion)
\item \textbf{Right-skewed}: A few high earners pull the mean above median
\end{itemize}

\textbf{Education}:
\begin{itemize}
\item Mean: 14.4 years (some college)
\item Median: 14 years (2 years of college)
\item Range: 3 to 20 years (dropout to PhD)
\item Coefficient of variation: 19\% (moderate dispersion)
\item Less skewed than earnings
\end{itemize}

\textbf{Why all age 30?}: Controlling for age isolates the effect of education from age/experience effects. In reality, earnings increase with both education and experience.

\textbf{Model 1: Linear Model}

\textbf{Specification}: earnings = -31,060 + 5,021 $\times$ education

\textbf{Interpretation}: Each additional year of education is associated with a \textbf{\$5,021 increase} in annual earnings.

\textbf{Examples}:
\begin{itemize}
\item High school graduate (12 years): Predicted earnings = -31,060 + 5,021(12) = \$29,192
\item Bachelor's degree (16 years): Predicted earnings = -31,060 + 5,021(16) = \$29,276
\item Difference: 4 years $\to$ \$20,084 increase (4 $\times$ \$5,021)
\end{itemize}

\textbf{Intercept}: -\$31,060 is nonsensical (negative earnings for zero education). This is extrapolation---the model only applies within the data range (3-20 years).

\textbf{$R^2$ = 0.289}: Education explains 28.9\% of earnings variation. The other 71.1\% is due to:
\begin{itemize}
\item Ability (IQ, motivation)
\item Field of study (engineering vs. humanities)
\item Occupation (teacher vs. software engineer)
\item Location (New York vs. rural area)
\item Random factors (luck, connections)
\end{itemize}

\textbf{Statistical significance}:
\begin{itemize}
\item t-statistic: 8.30
\item p-value: < 0.0001
\item 95\% CI: [\$3,827, \$6,216]
\end{itemize}

\textbf{Conclusion}: Very strong evidence that education increases earnings.

\textbf{Limitations of linear model}:
\begin{enumerate}
\item \textbf{Constant returns}: Assumes each year of education has the same effect (\$5,021) --- In reality, returns may be higher for college than high school
\item \textbf{Heteroskedasticity}: Variance of earnings likely increases with education
\item \textbf{Skewness}: Earnings are right-skewed, violating normality assumption
\item \textbf{Negative predictions}: For education < 6.2 years, model predicts negative earnings (impossible)
\end{enumerate}

\textbf{Model 2: Log-linear Model}

\textbf{Specification}: ln(earnings) = 8.56 + 0.131 $\times$ education

\textbf{Interpretation}: Each additional year of education is associated with a \textbf{13.1\% increase} in earnings.

\textbf{Examples}:
\begin{itemize}
\item High school graduate (12 years): Predicted ln(earnings) = 8.56 + 0.131(12) = 10.13
  \begin{itemize}
  \item Predicted earnings = exp(10.13) = \$25,013
  \end{itemize}
\item Bachelor's degree (16 years): Predicted ln(earnings) = 8.56 + 0.131(16) = 10.66
  \begin{itemize}
  \item Predicted earnings = exp(10.66) = \$42,885
  \end{itemize}
\item \textbf{Percentage increase}: (42,885 - 25,013) / 25,013 = 71.5\%
\item \textbf{Annual percentage}: 71.5\% / 4 years $\approx$ 13.1\% per year \checkmark
\end{itemize}

\textbf{Why this makes sense}:
\begin{itemize}
\item A \$5,000 raise means more to someone earning \$25,000 (20\% increase) than to someone earning \$100,000 (5\% increase)
\item Percentage changes are more economically meaningful than absolute changes
\end{itemize}

\textbf{$R^2$ = 0.334}: \textbf{Higher than linear model} (0.289), suggesting log-linear fits better.

\textbf{Why higher $R^2$?}
\begin{itemize}
\item Log transformation reduces skewness in earnings
\item Variance is more constant in log(earnings) (less heteroskedasticity)
\item Relationship is more linear in log space
\end{itemize}

\textbf{Statistical significance}:
\begin{itemize}
\item t-statistic: 9.21
\item p-value: < 0.0001
\item \textbf{Stronger evidence} than linear model (higher t-stat)
\end{itemize}

\textbf{Practical application}:
\begin{itemize}
\item High school graduate earning \$30,000 gets 4 more years of education (bachelor's)
\item Predicted increase: 4 $\times$ 13.1\% = 52.4\%
\item New earnings: \$30,000 $\times$ 1.524 = \$45,720
\end{itemize}

\textbf{Comparison to linear model}:
\begin{itemize}
\item Linear predicts: \$30,000 + 4(\$5,021) = \$50,084
\item Log-linear predicts: \$30,000 $\times$ 1.524 = \$45,720
\item Difference: \$4,364 (linear overestimates for low earners)
\end{itemize}

\textbf{Model 3: Log-log Model}

\textbf{Specification}: ln(earnings) = 6.55 + 1.478 $\times$ ln(education)

\textbf{Interpretation}: A 1\% increase in education is associated with a \textbf{1.478\% increase} in earnings (elasticity).

\textbf{Elasticity = 1.478}: This is \textbf{greater than 1} (elastic), meaning earnings respond strongly to education.

\textbf{Examples}:
\begin{itemize}
\item 10\% increase in education (14 $\to$ 15.4 years): Predicted earnings increase = 1.478 $\times$ 10\% = 14.78\%
\item If currently earning \$40,000: Increase = 0.1478 $\times$ \$40,000 = \$5,912
\end{itemize}

\textbf{Why elasticity interpretation is awkward here}:
\begin{itemize}
\item ``1\% increase in education'' = 0.01 $\times$ 14 years = 0.14 years $\approx$ 7 weeks of school
\item This is not a natural unit for education
\item Log-linear (one additional year) is more interpretable
\end{itemize}

\textbf{$R^2$ = 0.286}: \textbf{Lower than log-linear} (0.334), suggesting log-linear fits better for this relationship.

\textbf{Why lower $R^2$?}
\begin{itemize}
\item Education is not very skewed (CV = 19\%)
\item No strong reason to log-transform x
\item The relationship is not multiplicative in education
\end{itemize}

\textbf{When log-log makes sense}:
\begin{itemize}
\item If x were ``years of experience'' (highly skewed)
\item If x were ``firm size'' (extremely right-skewed)
\item If economic theory predicts constant elasticity
\end{itemize}

\textbf{Model 4: Linear-log Model}

\textbf{Specification}: earnings = -102,700 + 54,433 $\times$ ln(education)

\textbf{Interpretation}: A 1\% increase in education is associated with a \textbf{\$544 increase} in annual earnings.

\textbf{Diminishing returns}:
\begin{itemize}
\item This model captures \textbf{decreasing marginal returns} to education
\item Additional years of education have smaller absolute effects at higher levels
\item The log(education) term ``compresses'' high education values
\end{itemize}

\textbf{Examples}:
\begin{itemize}
\item From 12 to 13 years: $\Delta$ln(educ) = ln(13) - ln(12) = 0.080 $\to$ $\Delta$earnings = 54,433 $\times$ 0.080 = \$4,355
\item From 16 to 17 years: $\Delta$ln(educ) = ln(17) - ln(16) = 0.061 $\to$ $\Delta$earnings = 54,433 $\times$ 0.061 = \$3,320
\item \textbf{Diminishing effect}: Same 1-year increase has smaller dollar impact at higher education
\end{itemize}

\textbf{$R^2$ = 0.231}: \textbf{Lowest of all four models}, suggesting this specification fits worst.

\textbf{Why lowest $R^2$?}
\begin{itemize}
\item Earnings (y) is right-skewed, so levels are not ideal
\item Education (x) is not extremely skewed, so logging it doesn't help much
\item The model tries to have it both ways (log x but not log y) and does worst
\end{itemize}

\textbf{Intercept}: -\$102,700 is even more nonsensical than linear model. Ignore it.

\textbf{Model Selection}:

Based on $R^2$, statistical significance, and economic interpretation:

\textbf{Winner: Log-linear model (ln(earnings) = $\beta_0$ + $\beta_1$ $\times$ education)}

\textbf{Reasons}:
\begin{enumerate}
\item \textbf{Highest $R^2$} (0.334): Best fit
\item \textbf{Most interpretable}: ``13.1\% increase per year'' makes intuitive sense
\item \textbf{Addresses skewness}: ln(earnings) is more symmetric
\item \textbf{Likely homoskedastic}: Variance more constant in log space
\item \textbf{Economic theory}: Returns to education are proportional, not absolute
\end{enumerate}

\textbf{Runner-up: Linear model (earnings = $\beta_0$ + $\beta_1$ $\times$ education)}

\textbf{Reasons}:
\begin{enumerate}
\item \textbf{Simple}: Dollar interpretation is clear (\$5,021 per year)
\item \textbf{Decent fit}: $R^2$ = 0.289 is respectable
\item \textbf{Familiar}: No log transformations to explain
\end{enumerate}

\textbf{Not recommended: Log-log or linear-log}

\textbf{Reasons}:
\begin{enumerate}
\item \textbf{Lower $R^2$}: Fit is worse
\item \textbf{Awkward interpretation}: ``1\% increase in education'' is unnatural
\item \textbf{No theoretical justification}: Education is not skewed enough to require logging
\end{enumerate}

\textbf{Visual comparison}:

The combined figure shows all four models:
\begin{itemize}
\item \textbf{Linear}: Straight line in (x, y) space
\item \textbf{Log-linear}: Straight line in (x, ln(y)) space, exponential in (x, y) space
\item \textbf{Log-log}: Straight line in (ln(x), ln(y)) space, power function in (x, y) space
\item \textbf{Linear-log}: Straight line in (ln(x), y) space, logarithmic in (x, y) space
\end{itemize}

\textbf{Policy implications}:

Using the log-linear model:
\begin{itemize}
\item \textbf{13.1\% return per year} of education
\item Over a 40-year career, this compounds: \$30,000 $\times$ $(1.131)^4$ $\approx$ \$48,000 (bachelor's premium)
\item Justifies public investment in education (positive externalities)
\item Explains wage inequality (college grads earn >50\% more)
\end{itemize}

\begin{keyconcept}{Model Selection Criteria}
Choosing between linear and log-transformed specifications requires balancing statistical fit ($R^2$), economic interpretability, and theoretical appropriateness. For earnings-education relationships, the log-linear model typically performs best because: (1) it achieves higher $R^2$ by reducing skewness, (2) percentage returns are more economically meaningful than fixed dollar amounts, (3) it addresses heteroskedasticity naturally, and (4) human capital theory predicts proportional rather than absolute returns. Model selection should be driven by economic theory, not just statistical fit.
\end{keyconcept}

\section{Exponential Growth: S\&P 500 Stock Index}

\subsection{Code}

\textbf{Context:} In this section, we apply logarithmic transformation to estimate the long-run growth rate of the U.S. stock market from 1927 to 2019. Time series data on prices, GDP, and other economic aggregates typically exhibit exponential growth---values increase by a constant percentage rather than a constant amount. By regressing ln(price) on time, we convert this exponential trend into a linear relationship where the slope directly measures the compound annual growth rate, providing a powerful tool for analyzing economic trends.

\begin{lstlisting}[language=Python]
# Read in the S&P 500 data
data_sp500 = pd.read_stata(GITHUB_DATA_URL + 'AED_SP500INDEX.DTA')

print("S&P 500 Index data summary:")
print(data_sp500.describe())
print("\nFirst few observations:")
print(data_sp500.head())

# Regression in logs to estimate exponential growth
model_logs = ols('lnsp500 ~ year', data=data_sp500).fit()
print(model_logs.summary())

print(f"\nInterpretation:")
print(f"  Growth rate: {100*model_logs.params['year']:.4f}% per year")

# Retransformation bias correction
n = len(data_sp500)
k = 2  # intercept + slope
ResSSQ = np.sum(model_logs.resid**2)
MSE = ResSSQ / (n - k)
rmse = np.sqrt(MSE)

print(f"\nRetransformation bias correction:")
print(f"  RMSE: {rmse:.6f}")
print(f"  MSE: {MSE:.6f}")
print(f"  Correction factor: exp(MSE/2) = {np.exp(MSE/2):.6f}")

# Predictions in levels with bias correction
plnsp500 = model_logs.fittedvalues
psp500 = np.exp(plnsp500) * np.exp(MSE/2)
\end{lstlisting}

\subsection{Results}

\textbf{S\&P 500 Index data summary (1927-2019, n = 93):}

\begin{verbatim}
             year        sp500    lnsp500
count    93.00000    93.000000  93.000000
mean   1973.00000   473.664307   4.817428
std      26.99074   710.751831   1.801842
min    1927.00000     6.920000   1.934416
25%    1950.00000    23.770000   3.168424
50%    1973.00000    96.470001   4.569232
75%    1996.00000   740.739990   6.607650
max    2019.00000  3230.780029   8.080479
\end{verbatim}

\textbf{Exponential Growth Model: ln(sp500) = -124.09 + 0.0653 $\times$ year}

\begin{verbatim}
                            OLS Regression Results
==============================================================================
Dep. Variable:                lnsp500   R-squared:                       0.958
Model:                            OLS   Adj. R-squared:                  0.957
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept   -124.0933      2.833    -43.798      0.000    -129.721    -118.465
year           0.0653      0.001     45.503      0.000       0.062       0.068
==============================================================================

Interpretation:
  Growth rate: 6.5337% per year

Retransformation bias correction:
  RMSE: 0.371733
  MSE: 0.138185
  Correction factor: exp(MSE/2) = 1.071535
\end{verbatim}

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch09_fig2a_sp500_levels.png}
\end{center}

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch09_fig2b_sp500_logs.png}
\end{center}

\subsection{Interpretation}

\textbf{Research question}: What is the long-run growth rate of the U.S. stock market?

\textbf{Dataset}: S\&P 500 Index from 1927 to 2019 (93 years)
\begin{itemize}
\item \textbf{Historical}: Includes Great Depression, WWII, 1970s stagflation, 2008 financial crisis
\item \textbf{Long time series}: Averages out short-term volatility
\item \textbf{Real returns}: Adjust for inflation to get real growth
\end{itemize}

\textbf{Why log transformation?}

\textbf{Exponential growth}: Stock prices grow by a constant \textbf{percentage} each year, not a constant dollar amount.

\textbf{Compounding}: \$100 growing at 7\% per year becomes:
\begin{itemize}
\item Year 1: \$107 (+\$7)
\item Year 2: \$114.49 (+\$7.49, not +\$7)
\item Year 10: \$196.72
\item Year 50: \$2,945.70
\end{itemize}

This is \textbf{exponential growth}: y(t) = y(0) $\times$ $e^{rt}$, where r is the growth rate.

\textbf{Taking logs}:
\begin{itemize}
\item ln[y(t)] = ln[y(0)] + rt
\item ln(sp500) = $\beta_0$ + $\beta_1$ $\times$ year
\end{itemize}

This converts exponential growth to \textbf{linear growth} in log space.

\textbf{Regression Results}

\textbf{Specification}: ln(sp500) = -124.09 + 0.0653 $\times$ year

\textbf{Growth rate ($\beta_1$ = 0.0653)}:

\textbf{Interpretation}: The S\&P 500 grows at an average rate of \textbf{6.53\% per year} (compound annual growth rate, CAGR).

\textbf{Historical context}:
\begin{itemize}
\item 6.53\% is the \textbf{nominal} growth rate (not adjusted for inflation)
\item Real growth $\approx$ 6.53\% - 3\% inflation $\approx$ 3.5\% per year
\item This is consistent with long-run stock market returns
\end{itemize}

\textbf{Verification}:
\begin{itemize}
\item 1927: Predicted ln(sp500) = -124.09 + 0.0653(1927) = 1.80 $\to$ sp500 = exp(1.80) = 6.05
\item 2019: Predicted ln(sp500) = -124.09 + 0.0653(2019) = 7.81 $\to$ sp500 = exp(7.81) = 2,463
\item Ratio: 2,463 / 6.05 = 407 (index multiplied by 407 over 92 years)
\item Annual growth: $(407)^{(1/92)}$ = 1.0674 $\to$ 6.74\% per year $\approx$ 6.53\% \checkmark
\end{itemize}

\textbf{$R^2$ = 0.958}: Year explains 95.8\% of variation in ln(sp500).

\textbf{Remarkable fit}: Despite major events (crashes, booms), long-run growth is \textbf{remarkably stable}.

\textbf{What explains the other 4.2\%?}
\begin{itemize}
\item Business cycles (recessions, expansions)
\item Stock market crashes (1929, 1987, 2000, 2008)
\item Bull markets (1990s tech boom)
\item Random shocks
\end{itemize}

\textbf{Statistical significance}:
\begin{itemize}
\item t-statistic: 45.50
\item p-value: < 0.0001
\item \textbf{Overwhelming evidence} of positive long-run growth
\end{itemize}

\textbf{Intercept ($\beta_0$ = -124.09)}:

\textbf{Interpretation}: The predicted ln(sp500) when year = 0 is -124.09.

\textbf{Nonsensical}: Year 0 is outside the data range (1927-2019). Ignore the intercept.

\textbf{Better approach}: Report growth rate ($\beta_1$) without interpreting intercept.

\textbf{Retransformation Bias Correction}

\textbf{Problem}: When predicting in levels (original scale), naively using $\hat{y}$ = exp(predicted ln(y)) produces \textbf{biased} predictions.

\textbf{Why?} Jensen's inequality: E[exp(X)] $\neq$ exp(E[X]) for random variable X.

\textbf{Correction}: Multiply predictions by exp(MSE/2), where MSE is the mean squared error from the log regression.

\textbf{Formula}: $\hat{y}$ = exp($\hat{\beta}_0$ + $\hat{\beta}_1$x) $\times$ exp(MSE/2)

\textbf{Our results}:
\begin{itemize}
\item MSE = 0.1382 (from log regression)
\item Correction factor = exp(0.1382/2) = exp(0.0691) = 1.0715 (7.15\% upward adjustment)
\end{itemize}

\textbf{Interpretation}: Without correction, predictions would be 7.15\% too low on average.

\textbf{Example}:
\begin{itemize}
\item Year 2019: ln(sp500) predicted = 7.81
\item Naive prediction: exp(7.81) = 2,463
\item \textbf{Corrected prediction}: 2,463 $\times$ 1.0715 = 2,639
\item \textbf{Actual}: 3,231 (still underestimates, but less biased)
\end{itemize}

\textbf{Why does this matter?}
\begin{itemize}
\item For forecasting future stock prices
\item For retirement planning (estimating portfolio growth)
\item For comparing models (corrected predictions are more accurate)
\end{itemize}

\textbf{Visual interpretation}:

\textbf{Panel A (Levels)}:
\begin{itemize}
\item S\&P 500 shows clear \textbf{exponential growth} (upward curving trajectory)
\item Linear trend in levels would fit poorly (miss the curvature)
\item Fitted line (blue) tracks actual reasonably well, capturing long-run trend
\item Deviations (residuals) are relatively small given the scale
\end{itemize}

\textbf{Panel B (Logarithms)}:
\begin{itemize}
\item ln(S\&P 500) shows clear \textbf{linear trend} (straight line)
\item Slope = 0.0653 (growth rate)
\item Scatter around line represents short-term volatility
\item Major crashes visible as dips below trend (1929, 2008)
\end{itemize}

\textbf{Economic interpretation}:

\textbf{Compound growth}: Starting with \$1,000 in 1927:
\begin{itemize}
\item Growth at 6.53\% per year for 92 years
\item Ending value: \$1,000 $\times$ $(1.0653)^{92}$ = \$407,000 (not inflation-adjusted)
\end{itemize}

\textbf{Doubling time}: Rule of 72: Years to double $\approx$ 72 / 6.53 $\approx$ 11 years
\begin{itemize}
\item Verification: $(1.0653)^{11}$ = 2.01 \checkmark
\end{itemize}

\textbf{Historical comparison}:
\begin{itemize}
\item S\&P 500 growth (6.53\%) > GDP growth ($\sim$3\%)
\item S\&P 500 growth (6.53\%) > inflation ($\sim$3\%)
\item Real returns $\approx$ 3.5\% per year (after inflation)
\end{itemize}

\textbf{Investment implications}:

\textbf{Long-run perspective}:
\begin{itemize}
\item Despite crashes, stocks grow reliably over long periods
\item Short-term volatility (4.2\% unexplained) but long-run trend (95.8\% explained)
\item ``Time in the market beats timing the market''
\end{itemize}

\textbf{Forecasting}:
\begin{itemize}
\item Predicted 2030: ln(sp500) = -124.09 + 0.0653(2030) = 8.52
\item Corrected: exp(8.52) $\times$ 1.0715 = 5,354 (from 3,231 in 2019, 7.7\% annual growth)
\end{itemize}

\textbf{Limitations}:

\begin{itemize}
\item \textbf{Past $\neq$ future}: Historical returns don't guarantee future returns
\item \textbf{Survivorship bias}: S\&P 500 excludes failed companies (selected for success)
\item \textbf{Structural changes}: Economy, technology, regulation differ from 1927
\item \textbf{Model risk}: Assumes constant growth rate (may vary over time)
\item \textbf{No causality}: Regression describes trend, doesn't explain drivers (earnings growth, productivity)
\end{itemize}

\textbf{Extensions}:

\textbf{Time-varying growth}: Allow $\beta_1$ to change over time (e.g., pre-/post-1980)

\textbf{Volatility modeling}: GARCH models for time-varying variance

\textbf{Multiple variables}: Add earnings, dividends, interest rates as predictors

\textbf{Bottom line}: Logarithmic regression provides a simple, powerful tool for estimating \textbf{exponential growth rates} in time series data. The 6.53\% annual growth rate summarizes a century of stock market history in a single number.

\section{Conclusion}

In this chapter, we've explored how logarithmic transformations fundamentally change the way we interpret regression relationships, enabling us to measure effects in percentage terms rather than absolute units. We examined the mathematical properties of natural logarithms, compared four model specifications for the earnings-education relationship, and estimated long-run stock market growth rates. Through these applications, you've seen how the choice of transformation---whether to log y, x, both, or neither---determines whether coefficients measure elasticities, semi-elasticities, or absolute effects.

The power of logarithmic models lies in their economic interpretability. While a linear model tells us that an extra year of education increases earnings by \$5,021, the log-linear model reveals a more nuanced story: earnings increase by 13.1\% per year of education, meaning the dollar benefit scales with current income level. This percentage interpretation aligns with how we think about economic returns and naturally addresses issues like right-skewness and heteroskedasticity that plague linear models with monetary outcomes.

Through the S\&P 500 analysis, we saw logarithms unlock time series applications as well. Converting exponential growth to linear trends by logging prices revealed a remarkably stable 6.53\% annual growth rate spanning nearly a century---a single parameter summarizing complex market dynamics. This demonstrates how logarithmic transformations don't just improve statistical fit; they reveal fundamental economic patterns obscured in level specifications.

\textbf{What You've Learned}:

\begin{itemize}
\item \textbf{Transformation Skills}: How to apply np.log() and np.exp() transformations correctly, create log-transformed variables for regression analysis, and recognize when positive-only variables require logarithmic treatment
\item \textbf{Coefficient Interpretation}: How to interpret semi-elasticities in log-linear models (percentage change in y per unit change in x), interpret elasticities in log-log models (percentage change in y per percentage change in x), and distinguish these from absolute effects in linear specifications
\item \textbf{Model Comparison}: How to fit multiple specifications (linear, log-linear, log-log, linear-log) to the same data, compare $R^2$ values while considering economic interpretability, and select models based on theory, not just statistical fit
\item \textbf{Practical Applications}: How to estimate returns to education as percentage gains, calculate compound annual growth rates from time series, and apply retransformation bias correction for accurate level predictions
\end{itemize}

\textbf{Looking Ahead}:

In Chapter 10, we'll extend these bivariate techniques to multiple regression, where you'll estimate partial effects while controlling for confounding variables. The logarithmic transformations you've mastered here apply equally in multiple regression---you'll estimate wage equations controlling for experience and demographics, production functions with multiple inputs, and demand equations with price and income elasticities. Subsequent chapters will introduce interaction terms (how returns to education vary by gender), polynomial terms (non-linear relationships), and indicator variables (categorical predictors).

The interpretive framework you've developed---thinking in terms of elasticities and percentage changes rather than absolute units---will prove invaluable as models grow more complex. You might also explore Box-Cox transformations (which estimate the optimal power transformation) or inverse hyperbolic sine transformations (which handle zeros that logarithms cannot). These advanced techniques build on the logarithmic foundation established here.

Most importantly, you've learned that model specification isn't just a technical choice---it's an economic one. The question isn't ``which model has the highest $R^2$?'' but rather ``which specification best captures the economic relationship and provides the most meaningful interpretation?'' This economic thinking, combined with your growing statistical toolkit, positions you to tackle sophisticated empirical questions across labor economics, finance, industrial organization, and macroeconomics.

\textbf{References}:

\begin{itemize}
\item Cameron, A.C. (2022). \textit{Analysis of Economics Data: An Introduction to Econometrics}. \url{https://cameron.econ.ucdavis.edu/aed/index.html}
\item Python libraries: numpy, pandas, matplotlib, seaborn, statsmodels, scipy
\end{itemize}

\textbf{Data}:

All datasets are available at: \url{https://cameron.econ.ucdavis.edu/aed/aeddata.html}

\vspace{1em}

\begin{keyconcept}{Learn by Coding}
Now that you've learned the key concepts in this chapter, it's time to put them into practice!

Open the interactive Google Colab notebook for this chapter to:
\begin{itemize}
\item Run Python code implementing all the methods discussed
\item Experiment with real datasets and see results immediately
\item Modify parameters and explore how changes affect outcomes
\item Complete hands-on exercises that reinforce your understanding
\end{itemize}

\textbf{Access the notebook here:} \url{https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch09_Models_with_Natural_Logarithms.ipynb}

Remember: Learning econometrics is not just about understanding theory---it's about applying it. The best way to master these concepts is to code them yourself!
\end{keyconcept}
