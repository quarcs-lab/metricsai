\chapter{Statistical Inference for the Mean}

\begin{center}
\includegraphics[width=\textwidth]{../code_python/images/ch04_visual_summary.jpg}
\end{center}

\textit{This chapter introduces statistical inference---using sample data to draw conclusions about population parameters---through confidence intervals and hypothesis tests for means and proportions.}

\section{Introduction}

In this chapter, we explore fundamental techniques for \textbf{statistical inference}---the process of drawing conclusions about population parameters from sample data. Building on the sampling distribution concepts from Chapter 3, we introduce two core tools of statistical inference: \textbf{confidence intervals} and \textbf{hypothesis tests}.

We explore these concepts using multiple real-world datasets:
\begin{enumerate}
\item \textbf{Earnings data}: Women aged 30, annual earnings (n=171)
\item \textbf{Gasoline prices}: U.S. gasoline prices (n=32)
\item \textbf{Male earnings}: Full-time male workers (n=191)
\item \textbf{GDP growth}: U.S. quarterly GDP growth rates (n=245)
\item \textbf{Proportions}: Binary outcome data (n=921)
\end{enumerate}

\textbf{What You'll Learn:}

\begin{itemize}
\item How to understand the t-distribution and when to use it instead of the normal distribution
\item How to construct and interpret confidence intervals for population means
\item How to conduct two-sided hypothesis tests using t-statistics and p-values
\item How to perform one-sided (directional) hypothesis tests
\item How to apply inference methods to proportions data
\item How to distinguish between statistical significance and practical significance
\item How to make decisions using both p-values and critical value approaches
\end{itemize}

\section{Setup and Data Loading}

\subsection{Code}

In this section, we configure the Python environment and load earnings data for 171 women aged 30. This dataset provides the foundation for demonstrating statistical inference---we'll use this sample to make conclusions about the broader population of all working women of this age. The earnings data is ideal for learning inference because income questions are central to economic policy, and policymakers must routinely estimate population means from limited samples.

\begin{lstlisting}[language=Python]
# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import random
import os

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)

# GitHub data URL
GITHUB_DATA_URL = "https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"

# Create output directories
IMAGES_DIR = 'images'
TABLES_DIR = 'tables'
os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

# Read in earnings data
data_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS.DTA')
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Environment configured successfully
Data loaded: AED_EARNINGS.DTA (171 observations)
\end{verbatim}

\subsection{Interpretation}

\textbf{Reproducibility}: Setting \texttt{RANDOM\_SEED = 42} ensures consistent results across runs, though this chapter focuses on real data analysis rather than simulations.

\textbf{Data source}: The earnings dataset contains information on 171 women aged 30 working full-time in the U.S. in 2010. This is the same dataset used in previous chapters, allowing us to build on earlier descriptive analyses.

\textbf{Why earnings data}: Income data provides an ideal context for statistical inference because:
\begin{enumerate}
\item \textbf{Policy relevance}: Governments need to estimate average earnings to set minimum wages, tax policies, and social benefits
\item \textbf{Uncertainty}: We observe a sample, not the entire population, requiring inference to estimate true population mean
\item \textbf{Skewness}: Earnings distributions are often right-skewed, testing the robustness of inference methods
\item \textbf{Practical stakes}: Confidence intervals help policymakers understand precision of estimates
\end{enumerate}

\textbf{Statistical framework}: This chapter transitions from describing samples (Chapters 1-2) and understanding sampling distributions (Chapter 3) to \textbf{making inferences}---using sample statistics to draw conclusions about unknown population parameters.

\section{Sample Statistics and Initial Inference}

\subsection{Code}

In this section, we calculate basic sample statistics---the sample mean, standard deviation, and sample size---that form the building blocks for statistical inference. From these summary statistics, we construct our first confidence interval for the population mean using the t-distribution. This interval quantifies the uncertainty in our estimate and provides a range of plausible values for the true population mean earnings.

\begin{lstlisting}[language=Python]
# Get earnings variable
earnings = data_earnings['earnings']

# Summary statistics
mean_earnings = earnings.mean()
std_earnings = earnings.std(ddof=1)
n = len(earnings)
se_earnings = std_earnings / np.sqrt(n)

print(f"Sample Statistics:")
print(f"Sample size (n):        {n}")
print(f"Mean:                   ${mean_earnings:,.2f}")
print(f"Standard deviation:     ${std_earnings:,.2f}")
print(f"Standard error:         ${se_earnings:,.2f}")

# 95% Confidence interval
conf_level = 0.95
alpha = 1 - conf_level
t_crit = stats.t.ppf(1 - alpha/2, n - 1)
ci_lower = mean_earnings - t_crit * se_earnings
ci_upper = mean_earnings + t_crit * se_earnings

print(f"\n95% Confidence Interval:")
print(f"  [{ci_lower:,.2f}, {ci_upper:,.2f}]")

# Hypothesis test: H0: mu = 40000
mu0 = 40000
t_stat = (mean_earnings - mu0) / se_earnings
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 1))

print(f"\nHypothesis Test: H0: mu = ${mu0:,}")
print(f"  t-statistic:  {t_stat:.4f}")
print(f"  p-value:      {p_value:.4f}")
print(f"  Decision:     {'Reject H0' if p_value < 0.05 else 'Fail to reject H0'} at alpha=0.05")
\end{lstlisting}

\subsection{Results}

\textbf{Descriptive Statistics:}

\begin{tabular}{lrrrr}
\hline
Statistic & earnings & education & age & gender \\
\hline
count & 171.0 & 171.0 & 171.0 & 171.0 \\
mean & 41,412.69 & 14.43 & 30.0 & 0.0 \\
std & 25,527.05 & 2.74 & 0.0 & 0.0 \\
min & 1,050.0 & 3.0 & 30.0 & 0.0 \\
25\% & 25,000.0 & 12.0 & 30.0 & 0.0 \\
50\% & 36,000.0 & 14.0 & 30.0 & 0.0 \\
75\% & 49,000.0 & 16.0 & 30.0 & 0.0 \\
max & 172,000.0 & 20.0 & 30.0 & 0.0 \\
\hline
\end{tabular}

\textbf{Sample Statistics:}
\begin{verbatim}
Sample size (n):        171
Mean:                   $41,412.69
Standard deviation:     $25,527.05
Standard error:         $1,952.10

95% Confidence Interval:
  [37,559.21, 45,266.17]

Hypothesis Test: H0: mu = $40,000
  t-statistic:  0.7237
  p-value:      0.4703
  Decision:     Fail to reject H0 at alpha=0.05
\end{verbatim}

\subsection{Interpretation}

\textbf{Sample mean}: The average earnings in our sample is \$41,412.69. This is our \textbf{point estimate} of the population mean ($\mu$).

\textbf{Standard deviation vs. standard error}:
\begin{itemize}
\item \textbf{Standard deviation (\$25,527.05)}: Measures variability in individual earnings. The large value indicates substantial inequality---some women earn much more than others.
\item \textbf{Standard error (\$1,952.10)}: Measures uncertainty in the sample mean. This is much smaller than the standard deviation because averaging reduces variability by a factor of $\sqrt{n} = \sqrt{171} \approx 13.08$.
\end{itemize}

\textbf{95\% Confidence interval}: [\$37,559.21, \$45,266.17]

This interval means: ``We are 95\% confident that the true population mean earnings lies between \$37,559 and \$45,266.'' More precisely, if we repeated this sampling process many times, approximately 95\% of the resulting confidence intervals would contain the true population mean.

\textbf{Width of CI}: The interval spans \$7,707---about 19\% of the point estimate. This reflects substantial uncertainty, driven by:
\begin{enumerate}
\item \textbf{High variability}: Large standard deviation in earnings
\item \textbf{Moderate sample size}: n=171 is reasonable but not huge
\item \textbf{Confidence level}: 95\% requires wider intervals than, say, 90\%
\end{enumerate}

\textbf{Hypothesis test results}:
\begin{itemize}
\item \textbf{Null hypothesis}: $H_0$: $\mu = \$40,000$ (true mean equals \$40,000)
\item \textbf{Alternative hypothesis}: $H_a$: $\mu \neq \$40,000$ (true mean differs from \$40,000)
\item \textbf{t-statistic}: 0.7237 (small, indicating sample mean is close to hypothesized value)
\item \textbf{p-value}: 0.4703 (47\% probability of observing data this extreme if $H_0$ is true)
\item \textbf{Decision}: Fail to reject $H_0$ because p-value (0.47) > $\alpha$ (0.05)
\end{itemize}

\textbf{Economic interpretation}: The data do not provide sufficient evidence to conclude that the population mean earnings differ from \$40,000. The sample mean of \$41,413 is higher, but this difference could easily arise from sampling variability alone.

\begin{keyconcept}{Confidence Intervals}
A confidence interval provides a range of plausible values for an unknown population parameter. A 95\% confidence interval means that if we repeated our sampling procedure many times and constructed an interval each time, about 95\% of those intervals would contain the true population parameter. Critically, it does NOT mean there's a 95\% probability that the true parameter lies in our specific interval---the parameter is fixed, the interval is random. Confidence intervals quantify the precision of our estimate: wider intervals indicate more uncertainty, while narrower intervals suggest more precise estimates.
\end{keyconcept}

\section{The t-Distribution vs. Normal Distribution}

\subsection{Code}

In this section, we compare the t-distribution with the normal distribution to understand when and why we use each one. When the population standard deviation $\sigma$ is unknown (the typical case), we must estimate it with the sample standard deviation s, introducing additional uncertainty. The t-distribution accounts for this extra uncertainty by having heavier tails than the normal distribution, especially for small sample sizes. As sample size increases, the t-distribution converges to the normal distribution.

\begin{lstlisting}[language=Python]
# Figure 4.1: Comparison of t and normal distributions
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

x = np.linspace(-4, 4, 200)

# Panel A: t(4) vs standard normal
axes[0].plot(x, stats.norm.pdf(x), 'b--', linewidth=2, label='Standard Normal')
axes[0].plot(x, stats.t.pdf(x, df=4), 'r-', linewidth=2, label='t(4)')
axes[0].set_xlabel('x value', fontsize=12)
axes[0].set_ylabel('Density', fontsize=12)
axes[0].set_title('Panel A: t(4) and Standard Normal', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Panel B: t(30) vs standard normal
axes[1].plot(x, stats.norm.pdf(x), 'b--', linewidth=2, label='Standard Normal')
axes[1].plot(x, stats.t.pdf(x, df=30), 'r-', linewidth=2, label='t(30)')
axes[1].set_xlabel('x value', fontsize=12)
axes[1].set_ylabel('Density', fontsize=12)
axes[1].set_title('Panel B: t(30) and Standard Normal', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.suptitle('Figure 4.1: t Distribution vs Standard Normal',
             fontsize=14, fontweight='bold', y=1.0)
output_file = os.path.join(IMAGES_DIR, 'ch04_fig1_t_vs_normal_distributions.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()
\end{lstlisting}

\subsection{Results}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../code_python/images/ch04_fig1_t_vs_normal_distributions.png}
\caption{Comparison of t-Distribution and Standard Normal Distribution for Different Degrees of Freedom}
\label{fig:ch04:t-vs-normal}
\end{figure}

\subsection{Interpretation}

\textbf{Why the t-distribution?} When the population standard deviation ($\sigma$) is unknown---which is almost always the case---we must estimate it from the sample using s. This introduces additional uncertainty. The t-distribution accounts for this extra uncertainty by having \textbf{heavier tails} than the normal distribution.

\textbf{Degrees of freedom}: The t-distribution is characterized by degrees of freedom (df = n - 1). The degrees of freedom reflect how much information we have to estimate the standard deviation.

\textbf{Panel A (df = 4)}: With very few degrees of freedom:
\begin{itemize}
\item The t-distribution has \textbf{much heavier tails} than the normal
\item This means extreme values are more likely
\item Critical values for hypothesis tests are larger (e.g., $t_{0.025,4} = 2.776$ vs. $z_{0.025} = 1.96$)
\item This reflects high uncertainty when estimating $\sigma$ with only n=5 observations
\end{itemize}

\textbf{Panel B (df = 30)}: With more degrees of freedom:
\begin{itemize}
\item The t-distribution is \textbf{nearly identical} to the normal
\item Heavier tails are barely visible
\item Critical values converge toward normal (e.g., $t_{0.025,30} = 2.042$ vs. $z_{0.025} = 1.96$)
\end{itemize}

\textbf{Practical implication}:
\begin{itemize}
\item \textbf{Small samples (n < 30)}: Use t-distribution; the difference from normal matters
\item \textbf{Large samples (n > 100)}: t and normal give nearly identical results
\item \textbf{Our earnings data (n = 171)}: t-distribution appropriate, but results would be similar with normal
\end{itemize}

\textbf{Historical note}: Before computers, statisticians used normal distribution because t-tables were limited. Modern statistical software uses the correct t-distribution automatically.

\begin{keyconcept}{The t-Distribution}
The t-distribution is used for inference about means when the population standard deviation $\sigma$ is unknown and must be estimated from the sample. Developed by William Sealy Gosset (publishing under the pseudonym ``Student''), the t-distribution has heavier tails than the normal distribution, reflecting the additional uncertainty from estimating $\sigma$ with s. The t-distribution is characterized by degrees of freedom (df = n - 1): smaller samples have heavier tails, while as df $\to \infty$, the t-distribution converges to the standard normal. For most applications with n > 30, the t and normal distributions yield nearly identical results, but using t is always correct when $\sigma$ is unknown.
\end{keyconcept}

\section{Confidence Intervals at Different Levels}

\subsection{Code}

In this section, we construct confidence intervals at different confidence levels (90\%, 95\%, and 99\%) to understand the trade-off between precision and confidence. A higher confidence level requires a wider interval---to be more certain that we've captured the true parameter, we must accept less precision. This trade-off is fundamental to statistical inference, and choosing the appropriate confidence level depends on the consequences of being wrong in your specific application.

\begin{lstlisting}[language=Python]
# Different confidence levels
conf_levels = [0.90, 0.95, 0.99]

print(f"Confidence Intervals for Mean Earnings:")
print(f"{'Level':<10} {'Lower Bound':>15} {'Upper Bound':>15} {'Width':>15}")
print("-" * 60)

for conf in conf_levels:
    alpha = 1 - conf
    t_crit = stats.t.ppf(1 - alpha/2, n - 1)
    ci_lower = mean_earnings - t_crit * se_earnings
    ci_upper = mean_earnings + t_crit * se_earnings
    width = ci_upper - ci_lower
    print(f"{conf*100:.0f}%{ci_lower:>18,.2f}{ci_upper:>18,.2f}{width:>18,.2f}")

# Manual calculation for 95% CI (demonstration)
print(f"\nManual Calculation (95% CI):")
print(f"  Mean:              ${mean_earnings:,.2f}")
print(f"  Std Error:         ${se_earnings:,.2f}")
print(f"  Critical value:     {t_crit:.4f}")
print(f"  Margin of error:   ${t_crit * se_earnings:,.2f}")
print(f"  CI:                [${ci_lower:,.2f}, ${ci_upper:,.2f}]")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Confidence Intervals for Mean Earnings:
Level          Lower Bound     Upper Bound           Width
------------------------------------------------------------
90%         38,184.17         44,641.21          6,457.03
95%         37,559.21         45,266.17          7,706.97
99%         36,327.35         46,498.03         10,170.68

Manual Calculation (95% CI):
  Mean:              $41,412.69
  Std Error:         $1,952.10
  Critical value:     2.6051
  Margin of error:   $5,085.34
  CI:                [$36,327.35, $46,498.03]
\end{verbatim}

\subsection{Interpretation}

\textbf{Confidence interval formula}:

CI = $\bar{x} \pm t_{\alpha/2, n-1} \times$ SE

Where:
\begin{itemize}
\item $\bar{x}$ = sample mean (\$41,412.69)
\item $t_{\alpha/2, n-1}$ = critical value from t-distribution
\item SE = standard error (\$1,952.10)
\end{itemize}

\textbf{Trade-off between confidence and precision}:

\begin{enumerate}
\item \textbf{90\% CI}: [\$38,184, \$44,641] --- Width: \$6,457
   \begin{itemize}
   \item Narrowest interval
   \item Lower confidence (only 90\% certain it contains $\mu$)
   \item Critical value: $t_{0.05,170} = 1.654$
   \end{itemize}

\item \textbf{95\% CI}: [\$37,559, \$45,266] --- Width: \$7,707
   \begin{itemize}
   \item Standard choice in most research
   \item Good balance between confidence and precision
   \item Critical value: $t_{0.025,170} = 1.974$
   \end{itemize}

\item \textbf{99\% CI}: [\$36,327, \$46,498] --- Width: \$10,171
   \begin{itemize}
   \item Widest interval
   \item Highest confidence (99\% certain it contains $\mu$)
   \item Critical value: $t_{0.005,170} = 2.605$
   \end{itemize}
\end{enumerate}

\textbf{Key insight}: You cannot have both high confidence and high precision simultaneously. To be more confident the interval contains $\mu$, you must accept a wider (less precise) interval.

\textbf{Margin of error}: For the 95\% CI, the margin of error is $t \times$ SE = 1.974 $\times$ \$1,952.10 = \$3,853.48. This represents the ``$\pm$'' part of the interval.

\textbf{Practical interpretation}: A policymaker could say: ``We estimate mean earnings for 30-year-old women to be \$41,413, with a margin of error of $\pm$\$3,853 at 95\% confidence.'' This communicates both the estimate and its uncertainty.

\textbf{Why not always use 99\% CI?} Higher confidence requires wider intervals, making estimates less informative. The 99\% CI spans \$10,171---nearly 25\% of the point estimate. Most fields use 95\% as the conventional standard.

\section{Two-Sided Hypothesis Tests}

\subsection{Code}

In this section, we introduce hypothesis testing---a formal framework for making decisions about population parameters. Unlike confidence intervals which estimate a parameter, hypothesis tests evaluate specific claims. We test whether the population mean earnings equal \$40,000 (the null hypothesis) against the alternative that they differ from \$40,000. Using the t-statistic and p-value, we quantify the evidence against the null hypothesis and make a decision based on our chosen significance level.

\begin{lstlisting}[language=Python]
# Test H0: mu = 40000 vs HA: mu != 40000
mu0 = 40000
t_stat = (mean_earnings - mu0) / se_earnings
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 1))
t_crit_95 = stats.t.ppf(0.975, n - 1)

print(f"Two-Sided Test: H0: mu = ${mu0:,} vs HA: mu != ${mu0:,}")
print(f"  Sample mean:      ${mean_earnings:,.2f}")
print(f"  t-statistic:       {t_stat:.4f}")
print(f"  p-value:           {p_value:.4f}")
print(f"  Critical value:    +-{t_crit_95:.4f}")
print(f"  Decision:          {'Reject H0' if abs(t_stat) > t_crit_95 else 'Fail to reject H0'}")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Two-Sided Test: H0: mu = $40,000 vs HA: mu != $40,000
  Sample mean:      $41,412.69
  t-statistic:       0.7237
  p-value:           0.4703
  Critical value:    +-1.9740
  Decision:          Fail to reject H0
\end{verbatim}

\subsection{Interpretation}

\textbf{Hypothesis testing framework}:

\begin{enumerate}
\item \textbf{Null hypothesis ($H_0$)}: $\mu = \$40,000$ (the population mean equals \$40,000)
\item \textbf{Alternative hypothesis ($H_a$)}: $\mu \neq \$40,000$ (the population mean differs from \$40,000)
\item \textbf{Significance level ($\alpha$)}: 0.05 (5\% chance of Type I error)
\end{enumerate}

\textbf{Test statistic}:

$t = \frac{\bar{x} - \mu_0}{\text{SE}} = \frac{41,412.69 - 40,000}{1,952.10} = 0.7237$

This measures how many standard errors the sample mean is from the hypothesized value. A t-statistic of 0.72 means the sample mean is only 0.72 standard errors above \$40,000---quite close.

\textbf{p-value}: 0.4703 (47.03\%)

The p-value answers: ``If $H_0$ were true ($\mu = \$40,000$), what is the probability of observing a sample mean at least as extreme as \$41,413?''

A p-value of 0.47 means there is a 47\% chance of seeing such a result purely by random sampling variation---very likely! This provides \textbf{no evidence} against $H_0$.

\textbf{Critical value approach}:

With $\alpha = 0.05$ (two-sided), the critical values are $\pm 1.974$. We reject $H_0$ only if $|t| > 1.974$.

Our t-statistic (0.7237) falls well within the acceptance region (-1.974, 1.974), so we \textbf{fail to reject $H_0$}.

\textbf{Decision rule (equivalent approaches)}:
\begin{enumerate}
\item \textbf{p-value}: If p-value < $\alpha$, reject $H_0$ $\to$ 0.47 > 0.05, fail to reject
\item \textbf{Critical value}: If $|t| > t_{\text{critical}}$, reject $H_0$ $\to$ 0.72 < 1.974, fail to reject
\end{enumerate}

Both approaches reach the same conclusion.

\textbf{Economic interpretation}: The data are consistent with the hypothesis that mean earnings equal \$40,000. While the sample mean (\$41,413) is higher, the difference is not statistically significant---it could easily arise from random sampling variation.

\textbf{Important distinction}:
\begin{itemize}
\item ``Fail to reject $H_0$'' $\neq$ ``Accept $H_0$''
\item We haven't proven $H_0$ is true; we simply lack evidence against it
\item Absence of evidence is not evidence of absence
\end{itemize}

\textbf{Connection to confidence intervals}: Note that \$40,000 lies within the 95\% CI [\$37,559, \$45,266]. This is no coincidence---values inside the 95\% CI would not be rejected at $\alpha = 0.05$.

\begin{keyconcept}{P-Values and Hypothesis Testing}
A p-value is the probability of observing data as extreme as (or more extreme than) what we actually observed, assuming the null hypothesis is true. Small p-values (typically < 0.05) suggest the data are unlikely under $H_0$, leading us to reject the null hypothesis in favor of the alternative. However, p-values do NOT tell us the probability that $H_0$ is true, the size of an effect, or the practical importance of a finding. Common misinterpretations abound: ``p < 0.05'' means ``statistically significant'' but not necessarily ``important,'' and ``p > 0.05'' means ``fail to reject $H_0$'' but not ``prove $H_0$ is true.'' The significance level $\alpha$ (often 0.05) is the threshold for rejection---chosen before seeing the data to control Type I error (false positive) rate.
\end{keyconcept}

\section{Two-Sided Hypothesis Test Examples}

\subsection{Code}

In this section, we apply the hypothesis testing framework to multiple real-world datasets---gasoline prices, male earnings, and GDP growth---to demonstrate how the same principles apply across different economic contexts. Each example illustrates different outcomes: sometimes we reject the null hypothesis, sometimes we fail to reject it. By working through diverse examples, you'll develop intuition for interpreting t-statistics, p-values, and making appropriate statistical decisions.

\begin{lstlisting}[language=Python]
# Example 1: Gasoline prices
data_gasprice = pd.read_stata(GITHUB_DATA_URL + 'AED_GASPRICE.DTA')
price = data_gasprice['price']

mean_price = price.mean()
std_price = price.std(ddof=1)
n_price = len(price)
se_price = std_price / np.sqrt(n_price)

mu0_price = 3.81
t_stat_price = (mean_price - mu0_price) / se_price
p_value_price = 2 * (1 - stats.t.cdf(abs(t_stat_price), n_price - 1))
t_crit_price = stats.t.ppf(0.975, n_price - 1)

print("Example 1: Gasoline Prices")
print(f"H0: mu = ${mu0_price:.2f}")
print(f"  Sample size:       {n_price}")
print(f"  Sample mean:      ${mean_price:.4f}")
print(f"  Std error:        ${se_price:.4f}")
print(f"  t-statistic:       {t_stat_price:.4f}")
print(f"  p-value:           {p_value_price:.4f}")
print(f"  Critical value:    +-{t_crit_price:.4f}")

# Example 2: Male earnings
data_male = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGSMALE.DTA')
earnings_male = data_male['earnings']

mean_male = earnings_male.mean()
std_male = earnings_male.std(ddof=1)
n_male = len(earnings_male)
se_male = std_male / np.sqrt(n_male)

mu0_male = 50000
t_stat_male = (mean_male - mu0_male) / se_male
p_value_male = 2 * (1 - stats.t.cdf(abs(t_stat_male), n_male - 1))
t_crit_male = stats.t.ppf(0.975, n_male - 1)

print("\nExample 2: Male Earnings")
print(f"H0: mu = ${mu0_male:,}")
print(f"  Sample size:       {n_male}")
print(f"  Sample mean:      ${mean_male:,.2f}")
print(f"  Std error:        ${se_male:,.2f}")
print(f"  t-statistic:       {t_stat_male:.4f}")
print(f"  p-value:           {p_value_male:.4f}")
print(f"  Critical value:    +-{t_crit_male:.4f}")

# Example 3: GDP growth
data_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_REALGDPPC.DTA')
growth = data_gdp['growth']

mean_growth = growth.mean()
std_growth = growth.std(ddof=1)
n_growth = len(growth)
se_growth = std_growth / np.sqrt(n_growth)

mu0_growth = 2.0
t_stat_growth = (mean_growth - mu0_growth) / se_growth
p_value_growth = 2 * (1 - stats.t.cdf(abs(t_stat_growth), n_growth - 1))
t_crit_growth = stats.t.ppf(0.975, n_growth - 1)

print("\nExample 3: Real GDP per Capita Growth")
print(f"H0: mu = {mu0_growth:.1f}%")
print(f"  Sample size:       {n_growth}")
print(f"  Sample mean:       {mean_growth:.4f}%")
print(f"  Std error:         {se_growth:.4f}%")
print(f"  t-statistic:       {t_stat_growth:.4f}")
print(f"  p-value:           {p_value_growth:.4f}")
print(f"  Critical value:    +-{t_crit_growth:.4f}")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Example 1: Gasoline Prices
H0: mu = $3.81
  Sample size:       32
  Sample mean:      $3.6697
  Std error:        $0.0267
  t-statistic:       -5.2577
  p-value:           0.0000
  Critical value:    +-2.0395

Example 2: Male Earnings
H0: mu = $50,000
  Sample size:       191
  Sample mean:      $52,353.93
  Std error:        $4,705.75
  t-statistic:       0.5002
  p-value:           0.6175
  Critical value:    +-1.9725

Example 3: Real GDP per Capita Growth
H0: mu = 2.0%
  Sample size:       245
  Sample mean:       1.9905%
  Std error:         0.1392%
  t-statistic:       -0.0686
  p-value:           0.9454
  Critical value:    +-1.9697
\end{verbatim}

\subsection{Interpretation}

\textbf{Example 1: Gasoline Prices} (REJECT $H_0$)
\begin{itemize}
\item \textbf{Context}: Testing whether mean gas price equals \$3.81
\item \textbf{Result}: t = -5.26, p < 0.0001 (highly significant)
\item \textbf{Decision}: \textbf{Reject $H_0$} --- Strong evidence that mean price differs from \$3.81
\item \textbf{Economic interpretation}: Sample mean (\$3.67) is significantly lower than hypothesized value. The t-statistic of -5.26 means the sample mean is more than 5 standard errors below \$3.81---extremely unlikely if $H_0$ were true.
\item \textbf{Why significant?}: Small standard error (\$0.027) due to low variability in gas prices. Even a modest difference (\$0.14) becomes statistically significant with such precision.
\end{itemize}

\textbf{Example 2: Male Earnings} (FAIL TO REJECT $H_0$)
\begin{itemize}
\item \textbf{Context}: Testing whether mean male earnings equal \$50,000
\item \textbf{Result}: t = 0.50, p = 0.62
\item \textbf{Decision}: \textbf{Fail to reject $H_0$} --- No evidence that mean differs from \$50,000
\item \textbf{Economic interpretation}: Sample mean (\$52,354) is higher than \$50,000, but the difference is not statistically significant. With such high variability in earnings (large SE = \$4,706), observing \$52,354 when $\mu = \$50,000$ is quite plausible.
\item \textbf{Gender comparison}: Male mean (\$52,354) exceeds female mean (\$41,413) by \$10,941---a descriptive finding worthy of further investigation (e.g., comparing means using two-sample tests in later chapters).
\end{itemize}

\textbf{Example 3: Real GDP per Capita Growth} (FAIL TO REJECT $H_0$)
\begin{itemize}
\item \textbf{Context}: Testing whether mean growth rate equals 2.0\%
\item \textbf{Result}: t = -0.07, p = 0.95 (very high p-value)
\item \textbf{Decision}: \textbf{Fail to reject $H_0$} --- Data strongly consistent with 2\% growth hypothesis
\item \textbf{Economic interpretation}: Sample mean (1.99\%) is nearly identical to hypothesized value (2.00\%). The tiny t-statistic (-0.07) indicates virtually no difference.
\item \textbf{Large sample}: With n=245, the standard error is small (0.14\%), yet we still can't reject $H_0$ because the sample mean is so close to 2.0\%.
\item \textbf{Policy implication}: If long-run growth assumptions for fiscal planning use 2\%, the data support this assumption.
\end{itemize}

\textbf{Patterns across examples}:

\begin{enumerate}
\item \textbf{Sample size effects}:
   \begin{itemize}
   \item Small n (32 for gas) $\to$ larger critical values ($\pm 2.04$)
   \item Large n (245 for GDP) $\to$ smaller critical values ($\pm 1.97$), approaching z-value ($\pm 1.96$)
   \end{itemize}

\item \textbf{Standard error matters}:
   \begin{itemize}
   \item Gas prices: Small SE (\$0.027) $\to$ small difference (\$0.14) is significant
   \item Male earnings: Large SE (\$4,706) $\to$ large difference (\$2,354) is not significant
   \end{itemize}

\item \textbf{Statistical vs. practical significance}:
   \begin{itemize}
   \item Gas: Statistically significant, practically small (\$0.14 difference)
   \item Male earnings: Not statistically significant, but \$2,354 difference is economically meaningful
   \end{itemize}
\end{enumerate}

This illustrates why we must consider both statistical evidence (p-values) and economic/practical magnitude when interpreting results.

\section{One-Sided (Directional) Hypothesis Tests}

\subsection{Code}

In this section, we explore one-sided hypothesis tests where the alternative hypothesis specifies a direction (e.g., $\mu > 40,000$ or $\mu < 40,000$ rather than $\mu \neq 40,000$). One-sided tests are appropriate when you have a directional research question or when you only care about deviations in one direction. However, they should be specified before seeing the data to avoid data snooping bias. The p-values for one-sided tests are exactly half those of two-sided tests when the data support the specified direction.

\begin{lstlisting}[language=Python]
# Test H0: mu >= 40000 vs HA: mu < 40000 (Lower-tailed)
mu0 = 40000
t_stat = (mean_earnings - mu0) / se_earnings
p_value_one_sided = stats.t.cdf(t_stat, n - 1)
t_crit_one_sided = stats.t.ppf(0.05, n - 1)

print(f"One-Sided Test (Lower-tailed): H0: mu >= ${mu0:,} vs HA: mu < ${mu0:,}")
print(f"  t-statistic:       {t_stat:.4f}")
print(f"  p-value:           {p_value_one_sided:.4f}")
print(f"  Critical value:    {t_crit_one_sided:.4f}")
print(f"  Decision:          {'Reject H0' if t_stat < t_crit_one_sided else 'Fail to reject H0'}")

# Test H0: mu <= 40000 vs HA: mu > 40000 (Upper-tailed)
p_value_upper = 1 - stats.t.cdf(t_stat, n - 1)
t_crit_upper = stats.t.ppf(0.95, n - 1)

print(f"\nOne-Sided Test (Upper-tailed): H0: mu <= ${mu0:,} vs HA: mu > ${mu0:,}")
print(f"  t-statistic:       {t_stat:.4f}")
print(f"  p-value:           {p_value_upper:.4f}")
print(f"  Critical value:    {t_crit_upper:.4f}")
print(f"  Decision:          {'Reject H0' if t_stat > t_crit_upper else 'Fail to reject H0'}")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
One-Sided Test (Lower-tailed): H0: mu >= $40,000 vs HA: mu < $40,000
  t-statistic:       0.7237
  p-value:           0.7649
  Critical value:    -1.6539
  Decision:          Fail to reject H0

One-Sided Test (Upper-tailed): H0: mu <= $40,000 vs HA: mu > $40,000
  t-statistic:       0.7237
  p-value:           0.2351
  Critical value:    1.6539
  Decision:          Fail to reject H0
\end{verbatim}

\subsection{Interpretation}

\textbf{When to use one-sided tests}: Directional tests are appropriate when theory or policy dictates that we only care about deviations in one direction.

\textbf{Lower-tailed test} ($H_0$: $\mu \geq \$40,000$ vs $H_a$: $\mu < \$40,000$):
\begin{itemize}
\item \textbf{Use case}: Testing if earnings are \textit{below} a minimum threshold (e.g., poverty line, minimum wage compliance)
\item \textbf{Rejection region}: Left tail (t < -1.654)
\item \textbf{Results}: t = 0.72, p = 0.76
\item \textbf{Decision}: Fail to reject $H_0$ --- No evidence that mean is below \$40,000
\item \textbf{Interpretation}: The sample mean (\$41,413) is \textit{above} \$40,000, giving a p-value of 0.76 (76\% probability of seeing such data if $\mu \geq \$40,000$). There's no reason to think earnings are below the threshold.
\end{itemize}

\textbf{Upper-tailed test} ($H_0$: $\mu \leq \$40,000$ vs $H_a$: $\mu > \$40,000$):
\begin{itemize}
\item \textbf{Use case}: Testing if earnings \textit{exceed} a policy target or benchmark
\item \textbf{Rejection region}: Right tail (t > 1.654)
\item \textbf{Results}: t = 0.72, p = 0.24
\item \textbf{Decision}: Fail to reject $H_0$ --- No strong evidence that mean exceeds \$40,000
\item \textbf{Interpretation}: The sample mean is above \$40,000, and the p-value (0.24) is smaller than in the lower-tailed test, but still not below 0.05. We can't confidently conclude $\mu > \$40,000$.
\end{itemize}

\textbf{Comparison with two-sided test}:
\begin{itemize}
\item \textbf{Two-sided}: $H_0$: $\mu = \$40,000$, p = 0.47, critical values $\pm 1.974$
\item \textbf{One-sided (upper)}: $H_0$: $\mu \leq \$40,000$, p = 0.24, critical value 1.654
\item \textbf{One-sided (lower)}: $H_0$: $\mu \geq \$40,000$, p = 0.76, critical value -1.654
\end{itemize}

\textbf{Key relationships}:
\begin{enumerate}
\item One-sided p-value = (Two-sided p-value) / 2 (in the relevant direction)
   \begin{itemize}
   \item Upper: 0.47 / 2 = 0.235 $\approx$ 0.24
   \end{itemize}
\item One-sided critical values ($\pm 1.654$) are smaller in absolute value than two-sided ($\pm 1.974$)
   \begin{itemize}
   \item Easier to reject $H_0$ with one-sided tests (if evidence points in hypothesized direction)
   \end{itemize}
\end{enumerate}

\textbf{When should you use one-sided tests?}
\begin{itemize}
\item \textbf{Appropriate}: When direction is pre-specified by theory or policy (e.g., testing if a drug is \textit{better} than placebo, not just \textit{different})
\item \textbf{Inappropriate}: When you might reject in either direction (post-hoc directional tests are controversial)
\item \textbf{Conservative practice}: Most researchers use two-sided tests unless strong justification exists
\end{itemize}

\textbf{Power consideration}: One-sided tests have more \textbf{power} (ability to detect effects) in the specified direction because they concentrate all the rejection region in one tail. But this comes at the cost of being unable to detect effects in the other direction.

\section{Inference for Proportions}

\subsection{Code}

In this section, we extend inference methods to proportions---situations where the variable of interest is binary (0/1, yes/no, success/failure). Proportions are ubiquitous in economics and social sciences: employment rates, market shares, voter preferences, default rates, and treatment effects. The inference framework is similar to means, but we use the binomial distribution's properties: for proportions, the standard error is $\sqrt{p(1-p)/n}$, where p is the sample proportion.

\begin{lstlisting}[language=Python]
# Example: proportion data
n_total = 921
n_success = 480
p_hat = n_success / n_total
se_prop = np.sqrt(p_hat * (1 - p_hat) / n_total)

# Confidence interval
z_crit = 1.96  # For 95% CI
ci_lower_prop = p_hat - z_crit * se_prop
ci_upper_prop = p_hat + z_crit * se_prop

print(f"Proportion Analysis:")
print(f"  Sample size:       {n_total}")
print(f"  Number of successes: {n_success}")
print(f"  Sample proportion: {p_hat:.4f}")
print(f"  Standard error:    {se_prop:.4f}")
print(f"  95% CI:            [{ci_lower_prop:.4f}, {ci_upper_prop:.4f}]")

# Test H0: p = 0.50
p0 = 0.50
se_under_h0 = np.sqrt(p0 * (1 - p0) / n_total)
z_stat = (p_hat - p0) / se_under_h0
p_value_prop = 2 * (1 - stats.norm.cdf(abs(z_stat)))

print(f"\nHypothesis Test: H0: p = {p0:.2f}")
print(f"  z-statistic:       {z_stat:.4f}")
print(f"  p-value:           {p_value_prop:.4f}")
print(f"  Decision:          {'Reject H0' if abs(z_stat) > 1.96 else 'Fail to reject H0'}")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Proportion Analysis:
  Sample size:       921
  Number of successes: 480
  Sample proportion: 0.5212
  Standard error:    0.0165
  95% CI:            [0.4889, 0.5534]

Hypothesis Test: H0: p = 0.50
  z-statistic:       1.2851
  p-value:           0.1988
  Decision:          Fail to reject H0
\end{verbatim}

\subsection{Interpretation}

\textbf{Binary data}: Proportions arise when the outcome is binary (yes/no, success/failure, vote/abstain). Examples:
\begin{itemize}
\item \textbf{Political polling}: Proportion supporting a candidate
\item \textbf{Quality control}: Proportion of defective items
\item \textbf{Medical trials}: Proportion of patients who recover
\item \textbf{Employment}: Proportion employed vs. unemployed
\end{itemize}

\textbf{Sample proportion}: $\hat{p} = 480/921 = 0.5212$ (52.12\%)

This estimates the population proportion (p). In our sample, about 52\% of observations are ``successes.''

\textbf{Standard error for proportions}:

SE = $\sqrt{\hat{p}(1 - \hat{p}) / n} = \sqrt{0.5212 \times 0.4788 / 921} = 0.0165$

For proportions, the standard error depends on:
\begin{enumerate}
\item \textbf{Sample proportion} (maximum at $\hat{p} = 0.5$, decreases toward 0 or 1)
\item \textbf{Sample size} (decreases as n increases)
\end{enumerate}

\textbf{95\% Confidence interval}: [0.489, 0.553]

We are 95\% confident the true population proportion lies between 48.9\% and 55.3\%. The interval spans about 6.4 percentage points, reflecting moderate precision.

\textbf{Hypothesis test}: $H_0$: p = 0.50 vs $H_a$: p $\neq$ 0.50
\begin{itemize}
\item \textbf{Null hypothesis}: Population proportion equals 50\% (e.g., a fair coin, evenly split electorate)
\item \textbf{z-statistic}: 1.29 (note we use \textbf{z}, not t, for large-sample proportion tests)
\item \textbf{p-value}: 0.20 (20\% probability of observing $\hat{p} = 0.52$ if p = 0.50)
\item \textbf{Decision}: Fail to reject $H_0$ --- Data are consistent with p = 0.50
\end{itemize}

\textbf{Why z-statistic instead of t?} For proportions with large samples, the Central Limit Theorem implies:

$\hat{p} \sim N(p, p(1-p)/n)$

We use the normal (z) distribution rather than t because:
\begin{enumerate}
\item Sample size is large (n = 921)
\item The standard error formula for proportions is different from means
\item Historically, proportion tests used z; modern practice with large n gives nearly identical results
\end{enumerate}

\textbf{Interpretation}: Although the sample proportion (52.12\%) is slightly above 50\%, the difference is not statistically significant. If the true proportion were 50\%, observing 52.12\% in a sample of 921 would occur about 20\% of the time by random chance---quite plausible.

\textbf{Special case: SE under $H_0$}: Notice we computed SE two ways:
\begin{enumerate}
\item \textbf{For CI}: SE = $\sqrt{\hat{p}(1 - \hat{p}) / n} = 0.0165$ (uses sample proportion)
\item \textbf{For hypothesis test}: SE = $\sqrt{p_0(1 - p_0) / n} = \sqrt{0.5 \times 0.5 / 921} = 0.0165$ (uses hypothesized proportion)
\end{enumerate}

In this case they're almost identical because $\hat{p} \approx p_0$. But in general, hypothesis tests for proportions use the hypothesized value in the SE calculation.

\textbf{Sample size and precision}: With n=921, the SE is quite small (0.0165 = 1.65\%). Doubling the sample size would reduce SE by $\sqrt{2}$, to about 1.17\%. This square-root relationship means diminishing returns---quadrupling sample size only halves the SE.

\section{Hypothesis Testing Visualization}

\subsection{Code}

In this final section, we create comprehensive visualizations of the hypothesis testing framework---showing the null distribution, the observed test statistic, critical regions, and p-values graphically. Visual representations help build intuition about what p-values mean, how critical values define rejection regions, and why more extreme test statistics lead to smaller p-values. These visualizations are essential for communicating hypothesis test results to non-technical audiences and for developing a geometric understanding of statistical inference.

\begin{lstlisting}[language=Python]
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Panel A: Two-sided test
x = np.linspace(-4, 4, 500)
y = stats.t.pdf(x, n - 1)

axes[0].plot(x, y, 'b-', linewidth=2, label=f't({n-1}) distribution')
axes[0].axvline(x=t_stat, color='red', linewidth=2, linestyle='--',
                label=f't-statistic = {t_stat:.2f}')
axes[0].axvline(x=t_crit_95, color='green', linewidth=1.5, linestyle=':',
                label=f'Critical value = +-{t_crit_95:.2f}')
axes[0].axvline(x=-t_crit_95, color='green', linewidth=1.5, linestyle=':')

# Shade rejection regions
x_reject_lower = x[x < -t_crit_95]
x_reject_upper = x[x > t_crit_95]
axes[0].fill_between(x_reject_lower, 0, stats.t.pdf(x_reject_lower, n-1),
                      alpha=0.3, color='red', label='Rejection region')
axes[0].fill_between(x_reject_upper, 0, stats.t.pdf(x_reject_upper, n-1),
                      alpha=0.3, color='red')

axes[0].set_xlabel('t-statistic', fontsize=12)
axes[0].set_ylabel('Density', fontsize=12)
axes[0].set_title('Panel A: Two-Sided Hypothesis Test', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=9)
axes[0].grid(True, alpha=0.3)

# Panel B: One-sided test
axes[1].plot(x, y, 'b-', linewidth=2, label=f't({n-1}) distribution')
axes[1].axvline(x=t_stat, color='red', linewidth=2, linestyle='--',
                label=f't-statistic = {t_stat:.2f}')
axes[1].axvline(x=t_crit_upper, color='green', linewidth=1.5, linestyle=':',
                label=f'Critical value = {t_crit_upper:.2f}')

# Shade rejection region (upper tail)
x_reject = x[x > t_crit_upper]
axes[1].fill_between(x_reject, 0, stats.t.pdf(x_reject, n-1),
                      alpha=0.3, color='red', label='Rejection region')

axes[1].set_xlabel('t-statistic', fontsize=12)
axes[1].set_ylabel('Density', fontsize=12)
axes[1].set_title('Panel B: One-Sided Hypothesis Test (Upper-tailed)',
                  fontsize=13, fontweight='bold')
axes[1].legend(fontsize=9)
axes[1].grid(True, alpha=0.3)

plt.suptitle('Hypothesis Testing Visualization', fontsize=14, fontweight='bold', y=1.0)
output_file = os.path.join(IMAGES_DIR, 'ch04_hypothesis_testing_visualization.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()
\end{lstlisting}

\subsection{Results}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../code_python/images/ch04_hypothesis_testing_visualization.png}
\caption{Hypothesis Test Visualization Showing Critical Regions and Test Statistic}
\label{fig:ch04:hypothesis-test-viz}
\end{figure}

\subsection{Interpretation}

\textbf{Panel A: Two-Sided Test}

This visualization shows the geometry of hypothesis testing:

\begin{enumerate}
\item \textbf{Blue curve}: The t-distribution with df=170 under $H_0$ ($\mu = \$40,000$)
\item \textbf{Red dashed line}: Our observed t-statistic (0.72), far from the rejection regions
\item \textbf{Green dotted lines}: Critical values ($\pm 1.974$) defining the rejection regions
\item \textbf{Red shaded areas}: Rejection regions (5\% total area, 2.5\% in each tail)
\end{enumerate}

\textbf{Key insights}:
\begin{itemize}
\item The t-statistic (0.72) falls well within the ``non-rejection region'' (between -1.974 and +1.974)
\item If t had fallen in the red shaded areas, we would reject $H_0$
\item The distance from t=0.72 to the critical value (1.974) shows how far we are from rejecting
\item The non-rejection region contains 95\% of the probability under $H_0$
\end{itemize}

\textbf{Panel B: One-Sided Test (Upper-Tailed)}

For the upper-tailed test ($H_a$: $\mu > \$40,000$):

\begin{enumerate}
\item \textbf{Same blue curve and red dashed line} (distribution and t-statistic unchanged)
\item \textbf{Single green dotted line}: Critical value (1.654) is smaller than for two-sided test
\item \textbf{Red shaded area}: Rejection region only in upper tail (5\% area)
\end{enumerate}

\textbf{Key differences from two-sided}:
\begin{itemize}
\item Critical value is smaller (1.654 vs. 1.974) because all 5\% $\alpha$ is in one tail
\item Easier to reject $H_0$ if evidence points in the correct direction
\item Our t-statistic (0.72) is still far from the rejection region
\end{itemize}

\textbf{Visual interpretation of p-values}:
\begin{itemize}
\item \textbf{Two-sided}: The p-value (0.47) is the area in both tails beyond $\pm 0.72$
\item \textbf{One-sided (upper)}: The p-value (0.24) is the area in the upper tail beyond 0.72
\end{itemize}

\textbf{Practical decision-making}:
\begin{itemize}
\item If t-statistic lands in red zone $\to$ Reject $H_0$ (result is ``statistically significant'')
\item If t-statistic lands in white zone $\to$ Fail to reject $H_0$ (result is ``not statistically significant'')
\end{itemize}

\textbf{Why visualizations matter}: Seeing the geometry helps students understand:
\begin{itemize}
\item What ``5\% significance level'' means (area in tails)
\item How extreme the data must be to reject $H_0$
\item Why larger $|t|$ values provide stronger evidence against $H_0$
\item The relationship between critical values, rejection regions, and p-values
\end{itemize}

This visualization transforms abstract statistical concepts into concrete geometric intuition.

\section{Conclusion}

In this chapter, we've explored the fundamental tools of statistical inference---confidence intervals and hypothesis tests---using real earnings, price, and GDP data. You've learned how to quantify uncertainty in your estimates, test specific claims about population parameters, and make principled decisions based on sample data.

We've examined the t-distribution and understood why it's necessary when estimating population standard deviations from samples. Through multiple examples, you've seen how to construct confidence intervals at different levels (trading off precision for confidence) and conduct both two-sided and one-sided hypothesis tests. You've also extended these methods to proportions, a critical skill for analyzing binary outcomes in economics and social sciences.

Perhaps most importantly, you've learned to interpret p-values correctly---understanding that they measure the probability of your data given the null hypothesis, not the probability that the null is true. You've also seen the crucial distinction between statistical significance (is the effect detectably different from zero?) and practical significance (is the effect large enough to matter?).

\textbf{What You've Learned}:

\begin{itemize}
\item \textbf{Programming}: How to use scipy.stats for t-distribution calculations, construct confidence intervals programmatically, compute test statistics and p-values, and create visualizations that illuminate hypothesis testing concepts
\item \textbf{Statistics}: How the t-distribution differs from the normal and when to use each, how to construct and interpret confidence intervals, how to conduct hypothesis tests using both p-values and critical values, and how to avoid common misinterpretations of statistical results
\item \textbf{Economics}: How economists quantify uncertainty in estimates of means (earnings, prices, growth rates), how policymakers use hypothesis tests to evaluate claims about the economy, and why distinguishing statistical from practical significance matters for real-world decisions
\item \textbf{Methodology}: How to specify null and alternative hypotheses before seeing data, choose appropriate significance levels based on the costs of errors, and communicate statistical findings to both technical and non-technical audiences
\end{itemize}

\textbf{Looking Ahead}:

In Chapter 5, we'll extend these inference tools to relationships between variables through simple linear regression. The concepts you've mastered here---sampling distributions, standard errors, confidence intervals, and hypothesis tests---will transfer directly to testing whether regression coefficients differ from zero. Understanding inference for means is the foundation for understanding inference about relationships.

Try extending your learning by conducting power analyses (how likely are you to detect a true effect of a given size?), exploring two-sample tests (comparing means between groups), or investigating what happens to inference when assumptions are violated. The more you practice with real data, the more confident you'll become in applying these essential statistical tools.

\textbf{References}:

\begin{itemize}
\item Cameron, A.C. (2022). \textit{Analysis of Economics Data: An Introduction to Econometrics}. \url{https://cameron.econ.ucdavis.edu/aed/index.html}
\item Python libraries: numpy, pandas, matplotlib, seaborn, scipy.stats
\item Datasets: AED\_EARNINGS.DTA, AED\_GASPRICE.DTA, AED\_EARNINGSMALE.DTA, AED\_REALGDPPC.DTA
\end{itemize}

\textbf{Data}:

All datasets are available at: \url{https://cameron.econ.ucdavis.edu/aed/aeddata.html}

\vspace{1em}

\begin{keyconcept}{Learn by Coding}
Now that you've learned the key concepts in this chapter, it's time to put them into practice!

Open the interactive Google Colab notebook for this chapter to:
\begin{itemize}
\item Run Python code implementing all the methods discussed
\item Experiment with real datasets and see results immediately
\item Modify parameters and explore how changes affect outcomes
\item Complete hands-on exercises that reinforce your understanding
\end{itemize}

\textbf{Access the notebook here:} \url{https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch04_Statistical_Inference_for_the_Mean.ipynb}

Remember: Learning econometrics is not just about understanding theory---it's about applying it. The best way to master these concepts is to code them yourself!
\end{keyconcept}
