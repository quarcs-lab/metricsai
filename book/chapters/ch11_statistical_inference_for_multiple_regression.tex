\chapter{Multiple Regression Inference}

\includegraphics[width=\textwidth]{../code_python/images/ch11_visual_summary.jpg}

\textit{This chapter demonstrates how to quantify uncertainty and test hypotheses in multiple regression, transforming point estimates into rigorous statistical evidence for policy and research decisions.}

\section{Introduction}

This chapter explores \textbf{statistical inference in multiple regression}—the methods for testing hypotheses about regression coefficients and constructing confidence intervals. While Chapter 10 focused on estimation (computing coefficient values), Chapter 11 addresses inference: \textbf{How certain are we about these estimates? Can we reject null hypotheses? Which variables significantly affect the outcome?}

We continue analyzing the \textbf{housing dataset} (29 houses with price and six characteristics) to demonstrate fundamental inference techniques:

\begin{itemize}
    \item \textbf{Confidence intervals}: Quantifying uncertainty in coefficient estimates
    \item \textbf{t-tests}: Testing hypotheses about individual coefficients
    \item \textbf{F-tests}: Testing joint hypotheses about multiple coefficients
    \item \textbf{Model comparison}: Formally comparing nested specifications
    \item \textbf{Robust standard errors}: Addressing heteroskedasticity
\end{itemize}

Statistical inference provides the \textbf{evidentiary basis} for scientific claims. Rather than simply reporting "size coefficient = 68.37," we report "size coefficient = 68.37 with 95\% CI [36.45, 100.28], significantly different from zero (p < 0.001)." This quantifies uncertainty and enables hypothesis testing.

\textbf{What You'll Learn:}

\begin{itemize}
    \item How to construct and interpret confidence intervals for regression coefficients
    \item How to conduct t-tests for individual coefficients and interpret p-values correctly
    \item How to perform F-tests for overall model significance and joint hypothesis tests
    \item How to compare nested models using subset F-tests
    \item How to compute and interpret heteroskedasticity-robust standard errors
    \item How to distinguish statistical significance from economic significance
    \item How to present regression results in professional tables
\end{itemize}

\section{Setup and OLS Properties}

\subsection{Code}

In this section, we establish the Python environment and review the fundamental assumptions underlying statistical inference in multiple regression. Understanding these assumptions is crucial because they determine whether our hypothesis tests and confidence intervals are valid. We demonstrate the Classical Linear Model (CLM) assumptions that enable us to conduct t-tests and F-tests, distinguishing between properties needed for unbiased estimation versus those required for valid inference.

\begin{lstlisting}[language=Python]
# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy import stats
from statsmodels.stats.anova import anova_lm
import random
import os

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)

# GitHub data URL
GITHUB_DATA_URL = "https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"

# Create output directories
IMAGES_DIR = 'images'
TABLES_DIR = 'tables'
os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

# Read data
data_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')

# Display OLS assumptions and properties
print("Under assumptions 1-4:")
print("  1. Linearity: y = beta_0 + beta_1*x_1 + ... + beta_k*x_k + u")
print("  2. Random sampling from population")
print("  3. No perfect collinearity")
print("  4. Zero conditional mean: E[u|X] = 0")
print("\nThe OLS estimator is:")
print("  - Unbiased: E[beta_hat] = beta")
print("  - Consistent: plim(beta_hat) = beta")
print("  - Efficient (BLUE under Gauss-Markov theorem)")
\end{lstlisting}

\subsection{Results}

\textbf{Classical Linear Model (CLM) Assumptions:}

\begin{enumerate}
    \item \textbf{Linearity}: $y = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k + u$
    \item \textbf{Random Sampling}: $\{(x_i, y_i): i=1,...,n\}$ is a random sample from the population
    \item \textbf{No Perfect Collinearity}: No predictor is an exact linear combination of others
    \item \textbf{Zero Conditional Mean}: $E[u|X] = 0$ (errors uncorrelated with predictors)
    \item \textbf{Homoskedasticity} (for inference): $Var(u|X) = \sigma^2$ (constant error variance)
    \item \textbf{Normality} (for finite-sample inference): $u|X \sim N(0, \sigma^2)$
\end{enumerate}

\textbf{OLS Properties Under Assumptions 1-4:}
\begin{itemize}
    \item \textbf{Unbiasedness}: $E[\hat{\beta}] = \beta$ (on average, estimates equal true parameters)
    \item \textbf{Consistency}: $\text{plim}(\hat{\beta}) = \beta$ as $n \to \infty$ (large samples give accurate estimates)
    \item \textbf{Efficiency (Gauss-Markov)}: OLS has the smallest variance among linear unbiased estimators (BLUE)
\end{itemize}

\textbf{Additional Properties with Assumptions 5-6:}
\begin{itemize}
    \item \textbf{Normality of estimators}: $\hat{\beta} \sim N(\beta, Var(\hat{\beta}))$ (enables exact t and F tests)
    \item \textbf{Valid inference}: Confidence intervals and hypothesis tests have correct coverage/size
\end{itemize}

\subsection{Interpretation}

\textbf{Why Assumptions Matter}

Statistical inference \textbf{requires assumptions}. Without them, we cannot derive sampling distributions, compute standard errors, or test hypotheses. Each assumption serves a purpose:

\textbf{Assumption 1 (Linearity)}: Specifies the functional form
\begin{itemize}
    \item \textbf{Why needed}: OLS estimates linear relationships; if true relationship is nonlinear, estimates are biased
    \item \textbf{In practice}: Transform variables (log, quadratic) or use nonlinear models if needed
    \item \textbf{For housing data}: Assumes price is linear in size, bedrooms, etc. (might be wrong—could be log-linear)
\end{itemize}

\textbf{Assumption 2 (Random Sampling)}: Ensures representativeness
\begin{itemize}
    \item \textbf{Why needed}: Allows generalization from sample to population
    \item \textbf{Violations}: Convenience samples, selection bias, survivor bias
    \item \textbf{For housing data}: 29 houses from one neighborhood—not a random sample of all houses! Inference applies only to this neighborhood
\end{itemize}

\begin{keyconcept}{Classical Linear Model (CLM) Assumptions}
Valid statistical inference in OLS regression requires six assumptions: (1) linearity in parameters, (2) random sampling, (3) no perfect collinearity, (4) zero conditional mean $E[u|X]=0$, (5) homoskedasticity $Var(u|X)=\sigma^2$, and (6) normality of errors. Assumptions 1-4 ensure unbiasedness; adding assumption 5 enables efficient standard errors; assumption 6 permits exact t and F tests in finite samples. Large samples (CLT) make normality less critical, and robust standard errors relax homoskedasticity, but the first four assumptions remain essential for causal interpretation.
\end{keyconcept}

\section{Regression Estimation and Standard Errors}

\subsection{Code}

Here we estimate the full multiple regression model with all six predictors and examine the standard errors associated with each coefficient. Standard errors measure estimation uncertainty—they tell us how much our coefficient estimates would vary if we drew different samples from the same population.

\begin{lstlisting}[language=Python]
# Full multiple regression model
model_full = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',
                 data=data_house).fit()

print("Table 11.2: Multiple Regression Results")
print(model_full.summary())

# Extract key statistics
n = len(data_house)
k = len(model_full.params)
df = n - k

print(f"\nModel diagnostics:")
print(f"  Sample size: {n}")
print(f"  Number of parameters: {k}")
print(f"  Degrees of freedom: {df}")
print(f"  Root MSE (sigma_hat): {np.sqrt(model_full.mse_resid):.6f}")
\end{lstlisting}

\subsection{Results}

\begin{table}[h]
\centering
\caption{Multiple Regression Results with Standard Errors and Confidence Intervals}
\label{tab:ch11:multiple-regression-results}
\begin{tabular}{lrrrrrr}
\hline
Variable & Coefficient & Std. Error & t-statistic & p-value & \multicolumn{2}{c}{95\% Conf. Interval} \\
\hline
Intercept & 137,791 & 61,465 & 2.242 & 0.035 & 10,321 & 265,262 \\
size & 68.37 & 15.39 & 4.443 & 0.000 & 36.45 & 100.29 \\
bedrooms & 2,685 & 9,193 & 0.292 & 0.773 & -16,379 & 21,749 \\
bathrooms & 6,833 & 15,721 & 0.435 & 0.668 & -25,771 & 39,437 \\
lotsize & 2,303 & 7,227 & 0.319 & 0.753 & -12,684 & 17,290 \\
age & -833 & 719 & -1.158 & 0.259 & -2,325 & 659 \\
monthsold & -2,089 & 3,521 & -0.593 & 0.559 & -9,390 & 5,213 \\
\hline
\end{tabular}
\end{table}

\textbf{Model Statistics}:
\begin{itemize}
    \item \textbf{R-squared}: 0.651
    \item \textbf{Adjusted R$^2$}: 0.555
    \item \textbf{F-statistic}: 6.826 (p = 0.000342)
    \item \textbf{Sample size}: n = 29
    \item \textbf{Parameters}: k = 7 (includes intercept)
    \item \textbf{Degrees of freedom}: 22
    \item \textbf{Root MSE}: \$24,935.73
\end{itemize}

\subsection{Interpretation}

\textbf{Coefficient Estimates and Standard Errors}

Each coefficient estimate $\hat{\beta}_j$ comes with a \textbf{standard error SE$(\hat{\beta}_j)$}, measuring estimation uncertainty.

\textbf{size: $\hat{\beta}$ = 68.37, SE = 15.39}
\begin{itemize}
    \item \textbf{Point estimate}: Each additional square foot increases price by \$68.37
    \item \textbf{Standard error}: The typical sampling error is \$15.39
    \item \textbf{Interpretation}: If we drew many samples, the standard deviation of $\hat{\beta}_{size}$ estimates would be $\sim$\$15.39
    \item \textbf{t-statistic}: 68.37 / 15.39 = 4.443 (coefficient is 4.4 SEs above zero)
    \item \textbf{p-value}: < 0.001 (highly significant)
\end{itemize}

\begin{keyconcept}{Variance Inflation Factor (VIF)}
The Variance Inflation Factor measures how much multicollinearity increases the variance of coefficient estimates. Defined as $VIF_j = 1/(1-R^2_j)$, where $R^2_j$ is from regressing predictor j on all other predictors, VIF quantifies the inflation in $SE(\hat{\beta}_j)$ due to correlation among predictors. VIF=1 indicates no correlation (ideal), VIF=5-10 suggests moderate multicollinearity, and VIF>10 signals severe multicollinearity that may make individual coefficients imprecise or unstable even though the overall model fit remains valid.
\end{keyconcept}

\section{Confidence Intervals}

\subsection{Code}

This section constructs 95\% confidence intervals for each regression coefficient, providing ranges of plausible values for the true parameters.

\begin{lstlisting}[language=Python]
# 95% confidence intervals
conf_int = model_full.conf_int(alpha=0.05)
print("95% Confidence Intervals:")
print(conf_int)

# Detailed calculation for 'size' coefficient
coef_size = model_full.params['size']
se_size = model_full.bse['size']
t_crit = stats.t.ppf(0.975, df)

ci_lower = coef_size - t_crit * se_size
ci_upper = coef_size + t_crit * se_size

print(f"\nManual calculation for 'size' coefficient:")
print(f"  Coefficient: {coef_size:.6f}")
print(f"  Standard error: {se_size:.6f}")
print(f"  Degrees of freedom: {df}")
print(f"  Critical t-value (alpha=0.05): {t_crit:.4f}")
print(f"  95% CI: [{ci_lower:.6f}, {ci_upper:.6f}]")
\end{lstlisting}

\subsection{Results}

\textbf{95\% Confidence Intervals:}

\begin{table}[h]
\centering
\caption{95\% Confidence Intervals for Multiple Regression Coefficients}
\label{tab:ch11:confidence-intervals}
\begin{tabular}{lrrrr}
\hline
Variable & Coefficient & 95\% CI Lower & 95\% CI Upper & CI Width \\
\hline
Intercept & 137,791 & 10,321 & 265,262 & 254,941 \\
size & 68.37 & 36.45 & 100.29 & 63.84 \\
bedrooms & 2,685 & -16,379 & 21,749 & 38,128 \\
bathrooms & 6,833 & -25,771 & 39,437 & 65,208 \\
lotsize & 2,303 & -12,684 & 17,290 & 29,974 \\
age & -833 & -2,325 & 659 & 2,984 \\
monthsold & -2,089 & -9,390 & 5,213 & 14,603 \\
\hline
\end{tabular}
\end{table}

\textbf{Manual Calculation for size}:
\begin{itemize}
    \item Coefficient: 68.369419
    \item Standard error: 15.389472
    \item Degrees of freedom: 22
    \item Critical t-value ($\alpha$=0.05): 2.0739
    \item 95\% CI: [36.453608, 100.285230]
\end{itemize}

\includegraphics[width=0.8\textwidth]{images/ch11_fig1_confidence_intervals.png}

\subsection{Interpretation}

\textbf{What is a Confidence Interval?}

A 95\% confidence interval provides a range of plausible values for the true parameter $\beta_j$.

\textbf{Formula}: $[\hat{\beta}_j - t_{crit} \times SE(\hat{\beta}_j), \hat{\beta}_j + t_{crit} \times SE(\hat{\beta}_j)]$

Where:
\begin{itemize}
    \item $t_{crit} = t_{0.025,df} = 2.074$ (for df=22, $\alpha$=0.05)
    \item Two-tailed: 2.5\% in each tail
\end{itemize}

\textbf{Interpretation (frequentist)}:
\begin{itemize}
    \item "If we repeated sampling many times and constructed 95\% CIs each time, 95\% of intervals would contain the true $\beta_j$"
    \item NOT: "There's a 95\% probability $\beta$ is in this interval" ($\beta$ is fixed, not random)
\end{itemize}

\textbf{size: [36.45, 100.29]}

\textbf{What we learn}:
\begin{itemize}
    \item We're 95\% confident the true effect of size is between \$36.45 and \$100.29 per sq ft
    \item \textbf{Does not include zero} $\to$ statistically significant at 5\% level
    \item \textbf{Wide interval} (width = \$63.84) due to small sample, but still informative
\end{itemize}

\section{Conclusion}

In this chapter, we've moved beyond point estimation to explore the full power of statistical inference in multiple regression. Using our housing dataset of 29 properties in Central Davis, we've learned not just to estimate coefficients, but to quantify our uncertainty about them, test hypotheses rigorously, and make principled decisions about model specification.

The journey began with understanding that every regression coefficient comes with uncertainty. A point estimate like "\$68.37 per square foot" is useful, but without knowing its precision, we can't assess whether it's a reliable finding or merely statistical noise. By constructing confidence intervals—\$68.37 with 95\% CI [36.45, 100.29]—we've learned to communicate the range of plausible values and demonstrate that the effect is statistically distinguishable from zero.

\textbf{What You've Learned}:

Through this chapter's analyses, you've gained essential skills in statistical inference:

\begin{itemize}
    \item \textbf{Quantifying uncertainty}: You can construct confidence intervals that communicate not just a single estimate but the full range of plausible values, enabling more honest and complete reporting of empirical findings
    \item \textbf{Testing hypotheses}: You understand how to formulate null and alternative hypotheses, compute test statistics, interpret p-values correctly, and make decisions while recognizing that "failing to reject" doesn't mean "accepting" the null hypothesis
    \item \textbf{Joint significance testing}: You can use F-tests to evaluate whether groups of variables contribute to model fit, distinguishing between overall model significance and the added value of specific variable subsets
    \item \textbf{Model comparison}: You've learned to compare nested models systematically using subset F-tests, ANOVA tables, and information criteria, balancing fit against parsimony
    \item \textbf{Robust inference}: You know how to compute heteroskedasticity-robust standard errors and when they matter for your conclusions
\end{itemize}

\textbf{References}:

\begin{itemize}
    \item Cameron, A.C. (2022). \textit{Analysis of Economics Data: An Introduction to Econometrics}. \url{https://cameron.econ.ucdavis.edu/aed/index.html}
    \item Python libraries: numpy, pandas, statsmodels, matplotlib, seaborn, scipy
\end{itemize}

\textbf{Data}:

All datasets available at: \url{https://cameron.econ.ucdavis.edu/aed/aeddata.html}

\vspace{1em}

\begin{keyconcept}{Learn by Coding}
Now that you've learned the key concepts in this chapter, it's time to put them into practice!

Open the interactive Google Colab notebook for this chapter to:
\begin{itemize}
\item Run Python code implementing all the methods discussed
\item Experiment with real datasets and see results immediately
\item Modify parameters and explore how changes affect outcomes
\item Complete hands-on exercises that reinforce your understanding
\end{itemize}

\textbf{Access the notebook here:} \url{https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.ipynb}

Remember: Learning econometrics is not just about understanding theory---it's about applying it. The best way to master these concepts is to code them yourself!
\end{keyconcept}
