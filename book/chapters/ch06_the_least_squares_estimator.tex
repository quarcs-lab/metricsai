\chapter{The Least Squares Estimator}

\begin{center}
\includegraphics[width=\textwidth]{../code_python/images/ch06_visual_summary.jpg}
\end{center}

\textit{This chapter uses Monte Carlo simulation to demonstrate the statistical properties of OLS estimators, showing why we can trust regression estimates to reveal population relationships despite working with sample data.}

\section{Introduction}

While Chapter 5 introduced regression mechanics—how to fit a line to data—Chapter 6 examines a deeper question: Why can we trust OLS estimates to tell us about the population? In this chapter, we use Monte Carlo simulation and carefully constructed data generating processes (DGPs) to demonstrate that OLS estimators possess desirable statistical properties. You'll see empirical evidence that OLS is unbiased (on average, estimates equal true parameters), consistent (larger samples produce more precise estimates), and approximately normally distributed (enabling hypothesis testing and confidence intervals).

We explore the crucial distinction between population and sample regressions, demonstrating that while any single sample produces imperfect estimates, the OLS estimator performs reliably when we consider its behavior across many samples. This shift from focusing on individual estimates to understanding estimator properties marks the transition from descriptive statistics to statistical inference.

\textbf{What You'll Learn:}

\begin{itemize}
\item How to distinguish between population parameters and sample estimates
\item How to understand data generating processes and simulation design
\item How to recognize that regression coefficients are random variables with distributions
\item How to demonstrate OLS unbiasedness through Monte Carlo simulation
\item How to visualize and interpret sampling distributions of regression coefficients
\item How to assess how sample size affects estimator precision
\item How to connect theoretical properties to empirical evidence through simulation
\end{itemize}

\section{Setup and Data Generating Process}

\subsection{Code}

\textbf{Context:} In this section, we establish the computational environment and load artificially generated data with a known underlying relationship. Unlike real-world analysis, simulation allows us to know the true parameters, enabling us to verify that OLS estimators recover these parameters correctly. By setting random seeds, we ensure complete reproducibility—anyone running this code will see identical results, which is essential for teaching statistical concepts.

\begin{lstlisting}[language=Python]
# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy import stats
import random
import os

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)

# GitHub data URL
GITHUB_DATA_URL = "https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"

# Create output directories
IMAGES_DIR = 'images'
TABLES_DIR = 'tables'
os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(TABLES_DIR, exist_ok=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

# Read in generated data
data_gen = pd.read_stata(GITHUB_DATA_URL + 'AED_GENERATEDDATA.DTA')
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Generated data loaded: AED_GENERATEDDATA.DTA (5 observations)
Variables: x, Eygivenx, u, y

Data structure:
- x: Regressor (values 1, 2, 3, 4, 5)
- Eygivenx: E[y|x] = 1 + 2x (population regression line)
- u: Random error term
- y: Observed outcome = Eygivenx + u
\end{verbatim}

\subsection{Interpretation}

\textbf{Data generating process (DGP)}: This dataset was artificially created to illustrate core regression concepts. The true (population) relationship is:

y = 1 + 2x + u

Where:
\begin{itemize}
\item \textbf{$\beta_0 = 1$}: True intercept (population parameter)
\item \textbf{$\beta_1 = 2$}: True slope (population parameter)
\item \textbf{u}: Random error term with E[u] = 0 and Var(u) = $\sigma^2$
\end{itemize}

\textbf{Key insight}: In real research, we never observe the true DGP. We only see one sample (the y values). But in simulation studies like this, we control the DGP, allowing us to:
\begin{enumerate}
\item Know the true parameters ($\beta_0 = 1$, $\beta_1 = 2$)
\item Compare OLS estimates to truth
\item Verify that OLS recovers the true parameters on average
\end{enumerate}

\textbf{Population vs. sample}:
\begin{itemize}
\item \textbf{Population regression}: E[y|x] = 1 + 2x (what we want to learn)
\item \textbf{Sample regression}: $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ (what we estimate from data)
\end{itemize}

The challenge: We only observe one sample (one realization of the random variables). How can we make inferences about the population?

\textbf{Why simulation?} By generating many samples from the same DGP, we can empirically demonstrate that:
\begin{itemize}
\item OLS is unbiased: $E[\hat{\beta}_1] = \beta_1$ (the average of many estimates equals the truth)
\item OLS is consistent: As $n \to \infty$, $\hat{\beta}_1 \to \beta_1$ (larger samples get closer to truth)
\item OLS estimates are normally distributed (enabling hypothesis testing and confidence intervals)
\end{itemize}

\textbf{Reproducibility}: Setting \texttt{RANDOM\_SEED = 42} ensures that ``random'' number generation produces identical results every time. This is crucial for teaching—students should see the same output when running the code.

\begin{keyconcept}{Data Generating Process (DGP)}
A data generating process is the true underlying mechanism that produces observed data. In simulation studies, we specify the DGP explicitly (e.g., y = 1 + 2x + u), allowing us to know the true parameters. In real research, the DGP is unknown—we only observe sample data and must infer the underlying relationship. Understanding DGPs is fundamental to econometrics because all statistical inference rests on assumptions about how data are generated.
\end{keyconcept}

\section{Population vs. Sample Regression}

\subsection{Code}

\textbf{Context:} In this section, we compare the population regression line (the true relationship without noise) to a sample regression line (estimated from data with random errors). This comparison illustrates a fundamental challenge in econometrics: we never observe the population relationship directly, only noisy sample realizations. By fitting both regressions, we can see how sampling error causes estimates to deviate from truth.

\begin{lstlisting}[language=Python]
# Figure 6.2: Panel A - Population regression line E[y|x] = 1 + 2x
model_population = ols('Eygivenx ~ x', data=data_gen).fit()

fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(data_gen['x'], data_gen['y'], alpha=0.6, s=50, color='black', label='Actual')
ax.plot(data_gen['x'], model_population.fittedvalues,
        color='blue', linewidth=2, label='Population line E[y|x]')
ax.set_xlabel('Regressor x', fontsize=12)
ax.set_ylabel('Dependent variable y', fontsize=12)
ax.set_title('Figure 6.2 Panel A: Population Line E[y|x] = 1 + 2x',
             fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

output_file = os.path.join(IMAGES_DIR, 'ch06_fig2a_population_line.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()

print("\nPopulation regression results:")
print(model_population.summary())

# Figure 6.2: Panel B - Sample regression line
model_sample = ols('y ~ x', data=data_gen).fit()

fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(data_gen['x'], data_gen['y'], alpha=0.6, s=50, color='black', label='Actual')
ax.plot(data_gen['x'], model_sample.fittedvalues,
        color='red', linewidth=2, label=f'Sample line y-hat = {model_sample.params[0]:.2f} + {model_sample.params[1]:.2f}x')
ax.set_xlabel('Regressor x', fontsize=12)
ax.set_ylabel('Dependent variable y', fontsize=12)
ax.set_title('Figure 6.2 Panel B: Sample Regression Line',
             fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

output_file = os.path.join(IMAGES_DIR, 'ch06_fig2b_sample_regression.png')
plt.tight_layout()
plt.savefig(output_file, dpi=300, bbox_inches='tight')
plt.close()

print("\nSample regression results:")
print(model_sample.summary())
\end{lstlisting}

\subsection{Results}

\textbf{Population Regression (E[y|x] = 1 + 2x):}

\begin{center}
\begin{tabular}{lcccc}
\hline
Variable & Coefficient & Std Error & t-value & p-value \\
\hline
Intercept & 1.000 & $\sim$0 & Very large & 0.000 \\
x & 2.000 & $\sim$0 & Very large & 0.000 \\
\hline
\end{tabular}
\end{center}

R-squared: 1.000 (perfect fit)

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch06_fig2a_population_line.png}
\end{center}

\textbf{Sample Regression (y = $\hat{\beta}_0 + \hat{\beta}_1 x + \varepsilon$):}

\begin{center}
\begin{tabular}{lcccc}
\hline
Variable & Coefficient & Std Error & t-value & p-value \\
\hline
Intercept & 2.811 & 1.103 & 2.547 & 0.084 \\
x & 1.052 & 0.333 & 3.163 & 0.051 \\
\hline
\end{tabular}
\end{center}

R-squared: 0.769

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch06_fig2b_sample_regression.png}
\end{center}

\subsection{Interpretation}

\textbf{Panel A: Population Regression} (Perfect Fit)

When we regress \texttt{Eygivenx} on \texttt{x}, we get \textbf{exactly} the true parameters:
\begin{itemize}
\item Intercept: 1.000 (true value: 1.0)
\item Slope: 2.000 (true value: 2.0)
\item $R^2 = 1.000$ (perfect fit, no unexplained variation)
\end{itemize}

\textbf{Why perfect?} Because \texttt{Eygivenx = 1 + 2x} by construction—we regressing the conditional expectation on x, which is deterministic with no error term. The standard errors are essentially zero, and t-values are astronomically large.

\textbf{This is a fantasy scenario}: In real life, we never observe E[y|x]. We only observe y, which includes random error (u).

\textbf{Panel B: Sample Regression} (Imperfect Estimates)

When we regress the actual \texttt{y} on \texttt{x}, we get \textbf{different} estimates:
\begin{itemize}
\item Intercept: 2.811 (true value: 1.0) — \textbf{off by 1.811}
\item Slope: 1.052 (true value: 2.0) — \textbf{off by -0.948}
\item $R^2 = 0.769$ (some unexplained variation due to u)
\end{itemize}

\textbf{Why different?} Because y includes random error (u). With only n=5 observations, the errors happen to push the estimates away from the truth.

\textbf{Key insight}: The sample estimates ($\hat{\beta}_0 = 2.811$, $\hat{\beta}_1 = 1.052$) are \textbf{wrong} in this particular sample. But OLS is still unbiased because:
\begin{itemize}
\item If we drew many samples, the average $\hat{\beta}_1$ would equal 2.0
\item This particular sample happened to have unlucky errors
\item With more data (larger n), estimates would be closer to truth
\end{itemize}

\textbf{Sampling variability}: The difference between estimates and truth is \textbf{sampling error}. This is unavoidable—any finite sample will have some error. The goal of statistical inference is to quantify this uncertainty.

\textbf{Standard errors}:
\begin{itemize}
\item Intercept SE = 1.103 (large relative to estimate)
\item Slope SE = 0.333 (large relative to estimate)
\end{itemize}

These large standard errors reflect high uncertainty with only n=5 observations. The 95\% confidence interval for $\beta_1$ includes the true value (2.0), though barely.

\textbf{p-values}:
\begin{itemize}
\item Intercept: p = 0.084 (marginally significant at 10\%, not at 5\%)
\item Slope: p = 0.051 (barely not significant at 5\%, but close)
\end{itemize}

Despite being based on the true DGP, this small sample doesn't produce ``statistically significant'' results at conventional levels—another illustration of sampling variability.

\begin{keyconcept}{Population vs. Sample Regression}
The population regression E[y|x] = $\beta_0 + \beta_1 x$ represents the true relationship we want to learn about, but we never observe it directly. The sample regression $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ is what we estimate from data. The difference between them is sampling error—unavoidable variation due to working with finite samples. OLS is unbiased because $E[\hat{\beta}_1] = \beta_1$, meaning that while individual sample estimates vary, their average across many samples equals the truth.
\end{keyconcept}

\section{Three Samples from the Same DGP}

\subsection{Code}

\textbf{Context:} In this section, we generate three independent samples from the same data generating process to illustrate sampling variability. Each ``researcher'' collects their own data (n=30 observations) and estimates the same underlying relationship. This thought experiment demonstrates that different samples produce different estimates, yet all are valid realizations from the sampling distribution of the OLS estimator.

\begin{lstlisting}[language=Python]
# Generate three samples from the same data generating process
np.random.seed(12345)
n = 30

# Sample 1
x1 = np.random.normal(3, 1, n)
u1 = np.random.normal(0, 2, n)
y1 = 1 + 2*x1 + u1

# Sample 2
x2 = np.random.normal(3, 1, n)
u2 = np.random.normal(0, 2, n)
y2 = 1 + 2*x2 + u2

# Sample 3
x3 = np.random.normal(3, 1, n)
u3 = np.random.normal(0, 2, n)
y3 = 1 + 2*x3 + u3

# Fit regressions for each sample
df1 = pd.DataFrame({'x': x1, 'y': y1})
model1 = ols('y ~ x', data=df1).fit()

df2 = pd.DataFrame({'x': x2, 'y': y2})
model2 = ols('y ~ x', data=df2).fit()

df3 = pd.DataFrame({'x': x3, 'y': y3})
model3 = ols('y ~ x', data=df3).fit()

print("\nSample 1 - Regression coefficients:")
print(f"  Intercept: {model1.params[0]:.4f}, Slope: {model1.params[1]:.4f}")

print("\nSample 2 - Regression coefficients:")
print(f"  Intercept: {model2.params[0]:.4f}, Slope: {model2.params[1]:.4f}")

print("\nSample 3 - Regression coefficients:")
print(f"  Intercept: {model3.params[0]:.4f}, Slope: {model3.params[1]:.4f}")

print("\nTrue population parameters: Intercept = 1.0, Slope = 2.0")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Sample 1 - Regression coefficients:
  Intercept: 0.8195, Slope: 1.8054

Sample 2 - Regression coefficients:
  Intercept: 1.7496, Slope: 1.7857

Sample 3 - Regression coefficients:
  Intercept: 2.0128, Slope: 1.6697

True population parameters: Intercept = 1.0, Slope = 2.0
\end{verbatim}

\subsection{Interpretation}

\textbf{Thought experiment}: Imagine three different researchers independently collect data (n=30 each) from the same population. They're all studying the same DGP:

y = 1 + 2x + u

where x $\sim$ N(3, 1) and u $\sim$ N(0, 2).

\textbf{What would they find?}

\textbf{Researcher 1}:
\begin{itemize}
\item $\hat{\beta}_0 = 0.820$, $\hat{\beta}_1 = 1.805$
\item Slope estimate is 0.195 below truth (underestimate by 9.8\%)
\end{itemize}

\textbf{Researcher 2}:
\begin{itemize}
\item $\hat{\beta}_0 = 1.750$, $\hat{\beta}_1 = 1.786$
\item Slope estimate is 0.214 below truth (underestimate by 10.7\%)
\end{itemize}

\textbf{Researcher 3}:
\begin{itemize}
\item $\hat{\beta}_0 = 2.013$, $\hat{\beta}_1 = 1.670$
\item Slope estimate is 0.330 below truth (underestimate by 16.5\%)
\end{itemize}

\textbf{Key observations}:

\begin{enumerate}
\item \textbf{All three estimates differ}: No two researchers get the same answer
\item \textbf{All three are ``wrong''}: None exactly equal the true $\beta_1 = 2.0$
\item \textbf{All three underestimate}: By chance, all three samples have negative sampling error
\item \textbf{Variability is substantial}: Estimates range from 1.67 to 1.81 (14-point spread)
\end{enumerate}

\textbf{But OLS is still unbiased!} How can this be?

\textbf{Unbiasedness} means $E[\hat{\beta}_1] = \beta_1$, not that every individual estimate equals $\beta_1$. If we:
\begin{itemize}
\item Drew millions of samples (not just 3)
\item Computed $\hat{\beta}_1$ for each
\item Averaged all the estimates
\end{itemize}

The average would equal 2.0, even though most individual estimates are far from 2.0.

\textbf{Analogy}: Imagine a fair coin (true probability of heads = 0.5):
\begin{itemize}
\item Flip it 10 times $\to$ might get 7 heads ($\hat{p} = 0.7$)
\item Flip it 10 times again $\to$ might get 4 heads ($\hat{p} = 0.4$)
\item Flip it 10 times again $\to$ might get 6 heads ($\hat{p} = 0.6$)
\end{itemize}

No single sample gives exactly 0.5, but the estimator is still unbiased because the average of many samples equals 0.5.

\textbf{Practical implication}: When you read a paper reporting $\hat{\beta}_1 = 1.805$, remember:
\begin{itemize}
\item This is ONE realization from the sampling distribution
\item The true $\beta_1$ might be quite different
\item We use standard errors and confidence intervals to quantify this uncertainty
\item Replication studies (different samples) will get different estimates
\end{itemize}

\textbf{Why n=30 matters}: With only 30 observations, sampling variability is substantial. If we increased sample size to n=300 or n=3,000, the three estimates would be much closer to each other and to the truth. This is \textbf{consistency}.

\begin{keyconcept}{Unbiasedness}
An estimator is unbiased if its expected value equals the true parameter: $E[\hat{\beta}_1] = \beta_1$. This doesn't mean every individual estimate equals $\beta_1$—that's impossible with random sampling. Instead, it means that if we could repeat our sampling infinitely many times and average all the estimates, we'd get the true value. Unbiasedness is a desirable property because it means our estimator has ``no systematic error'' on average, even though individual estimates can be far from the truth.
\end{keyconcept}

\section{Monte Carlo Simulation: 1,000 Samples}

\subsection{Code}

\textbf{Context:} In this section, we conduct a Monte Carlo experiment with 1,000 independent samples to empirically verify OLS unbiasedness and normality. By generating many samples from a known DGP and estimating regression coefficients for each, we can construct the sampling distribution of the estimators. This simulation provides concrete evidence for theoretical properties that would be difficult to demonstrate with real data alone.

\begin{lstlisting}[language=Python]
# Simulate many regressions to demonstrate sampling distribution
np.random.seed(42)
n_simulations = 1000
sample_size = 30

beta0_estimates = np.zeros(n_simulations)
beta1_estimates = np.zeros(n_simulations)

for i in range(n_simulations):
    # Generate data from DGP: y = 1 + 2x + u
    x = np.random.normal(3, 1, sample_size)
    u = np.random.normal(0, 2, sample_size)
    y = 1 + 2*x + u

    # Fit OLS
    df = pd.DataFrame({'x': x, 'y': y})
    model = ols('y ~ x', data=df).fit()

    beta0_estimates[i] = model.params[0]
    beta1_estimates[i] = model.params[1]

print(f"\nSimulation results ({n_simulations} replications):")
print(f"\nIntercept beta-0:")
print(f"  Mean: {beta0_estimates.mean():.4f} (True value: 1.0)")
print(f"  Std dev: {beta0_estimates.std():.4f}")

print(f"\nSlope beta-1:")
print(f"  Mean: {beta1_estimates.mean():.4f} (True value: 2.0)")
print(f"  Std dev: {beta1_estimates.std():.4f}")
\end{lstlisting}

\subsection{Results}

\begin{verbatim}
Simulation results (1000 replications):

Intercept beta-0:
  Mean: 0.9960 (True value: 1.0)
  Std dev: 1.2069

Slope beta-1:
  Mean: 1.9944 (True value: 2.0)
  Std dev: 0.3836
\end{verbatim}

\begin{center}
\includegraphics[width=0.8\textwidth]{../code_python/images/ch06_simulation_ols_sampling_distributions.png}
\end{center}

\subsection{Interpretation}

\textbf{The Monte Carlo experiment}: We generated 1,000 independent samples, each with n=30 observations, from the same DGP (y = 1 + 2x + u). For each sample, we estimated $\hat{\beta}_0$ and $\hat{\beta}_1$ using OLS. This gives us 1,000 estimates of each parameter.

\textbf{Unbiasedness empirically verified}:

\textbf{Intercept ($\beta_0$)}:
\begin{itemize}
\item True value: 1.0
\item Mean of 1,000 estimates: 0.9960
\item Difference: -0.0040 (0.4\% error)
\end{itemize}

The average estimate is \textbf{extremely close} to the truth. The tiny difference (0.004) is due to simulation error—if we ran 10,000 or 100,000 simulations, it would be even closer to 1.0.

\textbf{Slope ($\beta_1$)}:
\begin{itemize}
\item True value: 2.0
\item Mean of 1,000 estimates: 1.9944
\item Difference: -0.0056 (0.28\% error)
\end{itemize}

Again, the average is \textbf{virtually equal} to the truth.

\textbf{This confirms OLS unbiasedness}: $E[\hat{\beta}_1] = \beta_1$. On average, OLS recovers the true parameter.

\textbf{Sampling variability}:

\textbf{Intercept standard deviation}: 1.2069
\begin{itemize}
\item Individual estimates range roughly from 1.0 $\pm$ 2(1.21) = [-1.42, 3.42]
\item High variability reflects the inherent randomness in small samples (n=30)
\end{itemize}

\textbf{Slope standard deviation}: 0.3836
\begin{itemize}
\item Individual estimates range roughly from 2.0 $\pm$ 2(0.38) = [1.24, 2.76]
\item Lower variability than intercept (slopes are generally estimated more precisely)
\end{itemize}

\textbf{Distribution shape}: The histograms show that both $\hat{\beta}_0$ and $\hat{\beta}_1$ follow approximately \textbf{normal distributions}:
\begin{itemize}
\item Centered at the true values (unbiasedness)
\item Bell-shaped and symmetric
\item Well-approximated by normal curves (red/blue lines)
\end{itemize}

This confirms the \textbf{Central Limit Theorem} applied to OLS: regression coefficients are approximately normally distributed, even when the errors (u) are not perfectly normal.

\textbf{Why normality matters}:
\begin{enumerate}
\item Enables hypothesis testing using t-statistics
\item Justifies confidence intervals based on t-distributions
\item Allows us to compute p-values
\item Provides theoretical foundation for inference
\end{enumerate}

\textbf{Interpretation of standard deviation (0.3836)}:

This is the \textbf{standard error} of $\hat{\beta}_1$. In practice, we don't know this value (we only have one sample, not 1,000), so we estimate it from the data. But this simulation shows what the standard error represents:

\begin{itemize}
\item \textbf{68\% of estimates} fall within 2.0 $\pm$ 0.38 = [1.62, 2.38]
\item \textbf{95\% of estimates} fall within 2.0 $\pm$ 1.96(0.38) = [1.25, 2.75]
\end{itemize}

When you see a regression table reporting $\hat{\beta}_1 = 1.85$ with SE = 0.40, you can interpret the SE as: ``If I repeated this study many times, 95\% of my slope estimates would fall within 1.85 $\pm$ 0.78.''

\textbf{Sample size effects}: The standard deviations (1.21 for $\beta_0$, 0.38 for $\beta_1$) are determined by:
\begin{itemize}
\item Sample size (n=30): Larger n $\to$ smaller standard errors
\item Error variance ($\sigma^2 = 4$): More noise $\to$ larger standard errors
\item Variance of x: More spread in x $\to$ smaller standard errors for $\beta_1$
\end{itemize}

\textbf{Practical implication}: When designing a study, you can:
\begin{itemize}
\item Increase sample size to reduce standard errors
\item Choose x values with more variation
\item Reduce measurement error to lower $\sigma^2$
\end{itemize}

\textbf{Histogram interpretation}:
\begin{itemize}
\item \textbf{Peaked at truth}: The highest bars are centered at 1.0 (intercept) and 2.0 (slope)
\item \textbf{Symmetric}: Equal numbers above and below the truth (no bias)
\item \textbf{Tails}: Some estimates far from truth (e.g., $\hat{\beta}_1$ as low as 1.2 or as high as 2.8)
\item \textbf{Outliers are rare but real}: In 1,000 simulations, a few estimates deviate substantially by chance
\end{itemize}

\textbf{Connection to t-tests}: When we test $H_0: \beta_1 = 0$, we're asking: ``Is 0 within the 95\% interval [1.25, 2.75]?'' No—so we reject $H_0$. The t-statistic measures how many standard errors the estimate is from the hypothesized value.

\begin{keyconcept}{Sampling Distribution and Standard Errors}
The sampling distribution of an estimator shows how that estimator varies across repeated samples from the same population. For OLS, $\hat{\beta}_1$ follows an approximately normal distribution centered at the true parameter $\beta_1$ with standard deviation equal to the standard error SE($\hat{\beta}_1$). This distribution is fundamental to inference—it tells us how much uncertainty exists in our estimate due to random sampling. The standard error quantifies this uncertainty: smaller standard errors mean more precise estimates.
\end{keyconcept}

\section{Conclusion}

In this chapter, we've moved beyond regression mechanics to explore the statistical foundations that justify using OLS estimates for inference. Through Monte Carlo simulation, we've seen direct empirical evidence that OLS estimators possess crucial properties: they're unbiased (averaging to the true parameters across many samples), approximately normally distributed (enabling hypothesis tests and confidence intervals), and consistent (becoming more precise with larger samples).

The key insight is recognizing the distinction between individual estimates and estimator properties. Any single sample produces imperfect estimates—we saw three researchers studying the same relationship get noticeably different results. Yet when we consider the behavior across 1,000 samples, a clear pattern emerges: the distribution of estimates centers precisely on the true parameters, forming a predictable bell-shaped curve. This is why we can trust OLS despite working with imperfect, finite samples.

Monte Carlo simulation proved invaluable for building intuition about abstract statistical concepts. By controlling the data generating process, we could directly verify theoretical properties that would be impossible to demonstrate with real data (where the true parameters are unknown). This computational approach to understanding statistical theory complements mathematical proofs and provides concrete evidence that OLS performs as economic theory predicts.

\textbf{What You've Learned}:

\begin{itemize}
\item \textbf{Programming}: How to design and implement Monte Carlo simulations, generate synthetic data from specified distributions, automate regression estimation across many samples, and create informative visualizations of sampling distributions
\item \textbf{Statistics}: How to distinguish population parameters from sample estimates, understand what unbiasedness means (and what it doesn't mean), interpret sampling distributions and standard errors, and recognize the role of sample size in estimation precision
\item \textbf{Simulation Methods}: How to use computational experiments to verify theoretical results, design informative simulation studies, and interpret simulation evidence for statistical properties
\end{itemize}

\textbf{Looking Ahead}:

In Chapter 7, we'll build on these foundations to develop formal inference procedures—hypothesis tests, confidence intervals, and prediction intervals—that allow us to quantify uncertainty and make probabilistic statements about population parameters. You'll see how the normality of OLS estimates (demonstrated here through simulation) enables us to construct t-statistics and test economic hypotheses. Subsequent chapters will extend these principles to multiple regression, where we'll estimate partial effects while controlling for confounding variables.

The simulation skills you've developed here extend far beyond OLS. Monte Carlo methods are widely used in econometrics, finance, and data science for tasks ranging from bootstrap inference to evaluating machine learning algorithms. You've learned a powerful computational tool for understanding and validating statistical methods that will serve you throughout your quantitative career.

\textbf{References}:

\begin{itemize}
\item Cameron, A.C. (2022). \textit{Analysis of Economics Data: An Introduction to Econometrics}. \url{https://cameron.econ.ucdavis.edu/aed/index.html}
\item Python libraries: numpy, pandas, matplotlib, seaborn, statsmodels, scipy
\end{itemize}

\textbf{Data}:

All datasets are available at: \url{https://cameron.econ.ucdavis.edu/aed/aeddata.html}

\vspace{1em}

\begin{keyconcept}{Learn by Coding}
Now that you've learned the key concepts in this chapter, it's time to put them into practice!

Open the interactive Google Colab notebook for this chapter to:
\begin{itemize}
\item Run Python code implementing all the methods discussed
\item Experiment with real datasets and see results immediately
\item Modify parameters and explore how changes affect outcomes
\item Complete hands-on exercises that reinforce your understanding
\end{itemize}

\textbf{Access the notebook here:} \url{https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch06_The_Least_Squares_Estimator.ipynb}

Remember: Learning econometrics is not just about understanding theory---it's about applying it. The best way to master these concepts is to code them yourself!
\end{keyconcept}
