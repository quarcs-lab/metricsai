[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics Powered by AI",
    "section": "",
    "text": "Welcome\n\n\n\n\n\nA modern introduction to econometrics that combines foundational concepts, cloud-based computational notebooks, and AI learning tools.\nA PDF version of this book is available for download at leanpub.com/econometrics-ai.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch00_Preface.html",
    "href": "../notebooks_colab/ch00_Preface.html",
    "title": "Preface: Econometrics Powered by AI",
    "section": "",
    "text": "Introduction\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nWelcome to a new approach to learning econometrics—one that embraces the power of modern computational tools and artificial intelligence while maintaining the rigor of traditional econometric theory.\nWelcome to Econometrics Powered by AI: An Introduction Using Cloud-based Python Notebooks. This book represents a new approach to learning econometrics—one that embraces the power of modern computational tools while maintaining the rigor of traditional econometric theory. In an era where artificial intelligence is transforming how we learn, work, and conduct research, this book seeks to bridge the gap between foundational statistical concepts and cutting-edge learning technologies.\nThe vision behind this project is simple yet ambitious: to make econometrics accessible, interactive, and engaging for a new generation of learners. By combining authoritative textbook content with cloud-based computational notebooks and AI-enhanced learning tools, I aim to modernize the often-daunting journey of learning econometrics into an more exciting AI-powered discovery of economic stories based real data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface: Econometrics Powered by AI</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch00_Preface.html#introduction",
    "href": "../notebooks_colab/ch00_Preface.html#introduction",
    "title": "Preface: Econometrics Powered by AI",
    "section": "",
    "text": "The Challenge of Learning Econometrics\nEconometrics has traditionally been taught through a combination of theoretical lectures, textbook readings, and problem sets. While this approach has served generations of students, it faces several inherent limitations in today’s learning environment. Traditional textbooks, no matter how well-written, remain fundamentally passive learning tools. Students read about regression analysis, hypothesis testing, and statistical inference, but the gap between reading about these concepts and actually implementing them can be substantial.\nTechnical barriers compound these challenges. Learning econometrics typically requires installing statistical software, navigating complex syntax, managing data files, and troubleshooting installation issues—all before a single regression can be estimated. For many students, these technical hurdles can be discouraging, diverting energy away from understanding core concepts and toward wrestling with software configuration.\nMoreover, there exists a persistent gap between theory and practical implementation. Students may understand the mathematical derivation of the ordinary least squares estimator but struggle to translate that knowledge into working code that analyzes real data. This disconnect between “knowing” and “doing” is still a challenge in econometrics education.\n\n\nThis Book’s Approach\nThis book takes a different approach. It serves as a companion to A. Colin Cameron’s textbook, Analysis of Economics Data: An Introduction to Econometrics (2022). Specifically, it brings its key lessons and examples into the interactive, computational world of Python programming and AI-enhanced learning.\nAt the heart of this approach is a three-pillar methodology that combines Foundational Concepts, Computational Notebooks, and AI-Powered Learning. These three pillars work together to create a comprehensive learning ecosystem that addresses the limitations of traditional econometrics education while leveraging the best of what modern technology offers.\nThe foundational concepts pillar ensures that students build their understanding on Cameron’s pedagogical framework, covering everything from basic statistical foundations to advanced topics in panel data and causation. The computational notebooks pillar provides zero-installation, browser-based access to Python implementations of every concept, allowing students to learn by coding from the very first chapter. The AI-powered learning pillar enhances this foundation with visual summaries, interactive slides, podcast discussions, quizzes, and an AI tutor—all designed to reinforce learning through multiple modalities.\nThis is not just a textbook with code examples—it’s a reimagining of how econometrics can be taught and learned in the age of cloud computing and artificial intelligence.\n\n\nWho This Book Is For\nThis book is designed for a diverse audience of learners:\nEconomics and social science students will find a comprehensive introduction to econometrics that emphasizes hands-on learning with real data. Whether you’re taking your first econometrics course or looking to deepen your quantitative skills, the combination of theory and practice provided here will serve you well.\nResearchers transitioning from Stata or R to Python will appreciate the parallel structure that follows Cameron’s familiar textbook while introducing Python’s powerful ecosystem of data science libraries. Each chapter demonstrates how classic econometric techniques can be implemented using modern Python tools like Pandas, Statsmodels, and Linearmodels.\nSelf-learners seeking interactive resources will benefit from the zero-installation requirement and comprehensive AI support. Simply open a notebook in your browser and start learning—no complex setup required. The multiple learning modalities (notebooks, podcasts, slides, quizzes) allow you to create a personalized learning path that fits your style and schedule.\nInstructors looking for modern teaching materials will find ready-made computational notebooks, AI-generated slides, and assessment tools that can supplement traditional lectures. The materials are designed to be flexible, allowing instructors to adopt the entire framework or selectively incorporate individual components into their existing courses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface: Econometrics Powered by AI</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch00_Preface.html#why-this-book-three-pillars-of-learning",
    "href": "../notebooks_colab/ch00_Preface.html#why-this-book-three-pillars-of-learning",
    "title": "Preface: Econometrics Powered by AI",
    "section": "Why This Book? Three Pillars of Learning",
    "text": "Why This Book? Three Pillars of Learning\n\nPillar 1: Foundational Concepts\n\nBuilt on Cameron’s Introductory Textbook\nThe foundation of this book rests on A. Colin Cameron’s Analysis of Economics Data: An Introduction to Econometrics (2022), an accessible introductory textbook that provides a clear exposition of econometric concepts and practical approach to data analysis. Cameron’s work provides comprehensive coverage of introductory econometric theory while maintaining an accessible writing style that resonates with students.\nBy building on this pedagogical framework, we ensure that the statistical and econometric foundations you learn are rigorous, complete, and aligned with how econometrics is actually practiced by researchers. The book features real-world datasets and examples drawn from economics and social sciences, demonstrating how econometric methods are applied to answer important research questions.\n\n\nCore Statistical Principles\nThe book covers the complete spectrum of econometric methods, from foundational statistical concepts through advanced techniques. You’ll begin with statistical foundations, learning about descriptive statistics, probability distributions, sampling theory, and statistical inference. These fundamentals provide the mathematical and statistical toolkit needed for all subsequent econometric analysis.\nFrom there, you’ll progress through bivariate regression analysis, learning how to model relationships between two variables, estimate linear relationships, and conduct hypothesis tests. Multiple regression analysis extends these techniques to multivariate settings, introducing concepts like omitted variable bias, multicollinearity, and model specification testing.\nFinally, advanced topics cover panel data methods, time series analysis, and approaches to establishing causation—techniques that are essential for modern empirical research in economics and social sciences. Throughout, theory is consistently grounded in practical applications, showing how abstract statistical concepts translate into tools for answering real research questions.\n\n\n17 Chapters, Four Parts\nThe book’s 17 chapters are organized into four coherent parts that build systematically from foundations to advanced applications:\nPart I: Statistical Foundations (Chapters 1-4) introduces you to data analysis, summary statistics, the sample mean, and statistical inference. These chapters establish the statistical toolkit you’ll use throughout the course.\nPart II: Bivariate Regression (Chapters 5-9) covers simple regression analysis, from data summarization through least squares estimation, statistical inference, case studies, and models with natural logarithms. These chapters develop your understanding of the fundamental regression model.\nPart III: Multiple Regression (Chapters 10-13) extends regression to multiple explanatory variables, covering data summary techniques, statistical inference, advanced topics, and extensive case studies that demonstrate how multiple regression is applied in practice.\nPart IV: Advanced Topics (Chapters 14-17) introduces indicator variables, variable transformations, model diagnostics, and concludes with panel data, time series methods, and causal inference—techniques at the frontier of applied econometric research.\n\n\n\nPillar 2: Computational Notebooks\n\nCloud-Based Python Implementation\nEvery one of the 17 chapters has a corresponding Google Colab notebook that brings the econometric concepts to life through interactive Python code. This cloud-based approach eliminates the single biggest barrier to learning computational econometrics: software installation and configuration.\nWith Google Colab, there’s zero installation required. You simply click an “Open in Colab” badge, and within seconds you’re running code in your browser. There’s no need to install Python, manage package dependencies, or troubleshoot compatibility issues. Google Colab provides free access to computing resources, including CPUs and GPUs, ensuring you have the computational power needed for data analysis.\nThis approach removes all technical barriers to getting started. Whether you’re using a Windows PC, a Mac, a Chromebook, or even a tablet, as long as you have internet access and a web browser, you can work through every chapter of this book.\n\n\nModern Python Stack\nThe notebooks leverage Python’s rich ecosystem of data science and statistical libraries, introducing you to the same tools used by professional data scientists and researchers worldwide.\nPandas serves as the foundation for data manipulation and analysis, providing powerful tools for loading, cleaning, transforming, and summarizing datasets. Statsmodels provides econometric modeling capabilities, including OLS regression, generalized linear models, and time series analysis. Linearmodels extends this toolkit with advanced regression techniques specifically designed for econometric applications.\nFor visualization, we use Matplotlib and Seaborn, which together provide publication-quality graphics for exploring data and presenting results. NumPy and SciPy handle the numerical computing that underlies all statistical analysis, from matrix operations to optimization algorithms.\nLearning these tools doesn’t just teach you econometrics—it provides you with a valuable skillset that transfers directly to careers in data science, quantitative research, policy analysis, and consulting.\n\n\nInteractive Learning by Coding\nGoogle Colab notebooks are fundamentally interactive documents that combine code, explanations, and results in a single, cohesive environment. Unlike static textbooks or lecture slides, notebooks allow you to see the code that generates each table and figure, modify that code, and immediately see the results of your changes.\nThis immediate feedback loop transforms learning from passive consumption to active experimentation. Wondering what happens if you change a parameter? Modify the code and re-run the cell. Curious about how a result changes with different data? Load a different dataset and see for yourself. Want to extend an analysis beyond what the textbook shows? Add your own code cells and explore.\nEach notebook provides step-by-step implementation of econometric concepts, starting from data loading and proceeding through analysis, visualization, and interpretation. Code is thoroughly commented and explained, ensuring you understand not just what each line does, but why it’s needed and how it fits into the broader analysis.\n\n\nAccessibility and Convenience\nThe cloud-based approach provides unprecedented accessibility. You can access your work from any device with an internet connection—start working on your desktop at home, continue on a laptop at a café, and review results on a tablet while commuting. There are no storage space requirements on your local machine; everything is saved in the cloud.\nYour notebooks are always up-to-date with the latest software dependencies, as Google Colab maintains the underlying Python environment. You never have to worry about package version conflicts or breaking changes—everything just works. The platform also includes collaborative features for group learning, allowing students to work together on assignments, share insights, and learn from each other’s approaches.\n\n\n\nPillar 3: AI-Powered Learning\n\nLeveraging Google’s NotebookLM and AI Tools\nThe third pillar of our approach harnesses the power of artificial intelligence to enhance learning. We’ve developed AI-enhanced study materials for all 17 chapters, leveraging cutting-edge tools like Google’s NotebookLM and Gemini PRO to create multiple learning modalities that accommodate diverse learning preferences.\nThese AI tools provide interactive learning assistance, offering explanations, answering questions, and helping you develop deeper understanding of complex concepts. The materials support multiple modalities—visual, auditory, textual, and interactive—ensuring that regardless of your preferred learning style, you’ll find resources that resonate with you.\n\n\nAI-Generated Visual Summaries\nEach chapter includes a visual summary that distills the key concepts into an intuitive, graphical format. These summaries present chapter content visually, highlighting the relationships between concepts, the flow of ideas, and the main takeaways.\nVisual summaries serve as both quick reference tools and review aids. Before diving into a chapter, you can preview the visual summary to understand what you’ll learn. After completing a chapter, the visual summary helps consolidate your understanding and serves as a memory aid for later review. Research consistently shows that visual learning enhances retention, and these AI-generated summaries leverage that insight.\n\n\nAI-Generated Video Overviews\nTo complement the static visual summaries, each chapter includes an AI-generated video overview that brings key concepts to life through motion, animation, and narration. These short videos, typically less than 10 minutes, provide visual explanations of the most important ideas in each chapter, presented in an intuitive and engaging format.\nThe videos are designed for first-time learning and quick reviews. Before diving into a chapter’s notebook, watch the video overview to build a mental framework for the concepts you’ll encounter. The combination of visual animation and clear narration helps you understand the “big picture” before engaging with code and mathematical details. After completing a chapter, the video serves as an efficient review tool, allowing you to quickly refresh your understanding of key concepts without re-reading the entire notebook. Like all resources in this learning ecosystem, videos are AI-generated using cutting-edge tools, ensuring consistent quality and pedagogical clarity across all the 17 chapters.\n\n\nInteractive AI Slides and Presentations\nFor each chapter, we’ve generated presentation materials using NotebookLM that complement the traditional slides created by Professor Cameron. These AI-generated slides are designed for self-paced learning, with clear explanations, progressive concept building, and visual aids that clarify complex ideas.\nThe slides are presentation-ready, making them useful not just for individual study but also for group discussions, study sessions, or classroom presentations. They provide an alternative way to engage with the material, breaking down complex topics into digestible chunks that can be reviewed at your own pace.\n\n\nPodcast Episodes for Audio Learning\nOne of the most innovative features of this learning ecosystem is the availability of AI-generated podcast discussions for all 17 chapters. These podcasts present chapter content through conversational dialogue, making complex econometric concepts accessible through natural language discussion.\nPodcasts provide a perfect learning modality for commuting, exercising, or any time when visual focus isn’t possible. The conversational format—where concepts are explained through dialogue rather than formal lecture—often makes difficult ideas more approachable. Complex topics are explained through back-and-forth discussion, reinforcing key concepts and providing alternative perspectives on the material.\nThese audio resources offer an alternative learning modality that complements the visual and interactive elements of notebooks and slides, ensuring you can continue learning even when you’re away from your computer.\n\n\nQuiz and AI Tutor Integration\nAssessment and feedback are critical components of effective learning. Each chapter includes interactive quizzes powered by EdCafe and NotebookLM that test your understanding of key concepts. These aren’t just multiple-choice questions—they’re interactive self-assessment tools that provide immediate feedback and personalized learning assistance based on your responses.\nWhen you struggle with a concept, the AI tutor is available to provide code explanations, clarify theoretical points, and guide you toward understanding. This immediate, personalized support ensures you don’t get stuck—help is always available when you need it. The AI tutor can explain why a particular approach works, suggest alternative methods, and help debug code when things don’t work as expected.\n\n\nResponsible Use of AI Tools\nWhile AI tools provide powerful learning support, it’s crucial to understand their proper role in your education. AI serves as an enhancement, not a replacement for critical thinking and genuine understanding. The foundation of your learning remains Cameron’s authoritative textbook and the verified Python code in the notebooks—AI tools supplement this foundation, they don’t replace it.\nIt’s important to cross-reference AI-generated content with authoritative sources. While tools like NotebookLM and Gemini PRO are sophisticated, they can occasionally make mistakes or oversimplify complex concepts. You bear responsibility for verifying information and developing true understanding rather than simply accepting AI-generated explanations at face value.\nAll Python code in the notebooks has been carefully verified and tested for accuracy. When AI tools provide code explanations or suggestions, compare them against the tested code in the notebooks to ensure accuracy. The goal is to use AI tools to develop multiple learning pathways and deeper understanding—transparency about these tools’ capabilities and limitations is essential to using them effectively.\n\nKey resources for learning\nThis book is designed to be used in conjunction with two essential companion resources:\nThe metricsAI Website (https://quarcs-lab.github.io/metricsai) provides access to: - Interactive Google Colab notebooks for all 17 chapters - AI-generated visual summaries, video overviews, and podcast episodes - Links to quizzes, AI tutors, and presentation slides - Quick summaries of foundational content\nCameron’s Original Textbook (Analysis of Economics Data: An Introduction to Econometrics, 2022) and materials (https://cameron.econ.ucdavis.edu/aed/index.html) provide: - Comprehensive explanations of econometric theory - Deeper mathematical derivations and proofs - Extended examples and applications - Additional exercises and practice problems - Original Stata, R, and Gretl code implementations - Comprehensive datasets and detailed slides\nTogether, these resources create a complete learning ecosystem: this book offers structured content and context and the website provides interactive computational tools with AI learning support.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface: Econometrics Powered by AI</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch00_Preface.html#how-to-use-this-book",
    "href": "../notebooks_colab/ch00_Preface.html#how-to-use-this-book",
    "title": "Preface: Econometrics Powered by AI",
    "section": "How to Use This Book",
    "text": "How to Use This Book\n\nGetting Started\nOne of the greatest advantages of this learning platform is that there’s no installation required—you can start learning immediately. The path from deciding to learn econometrics to running your first regression can be measured in seconds, not hours or days.\nTo begin, simply find the chapter you want to study and click the “Open in Colab” badge. Within moments, you’ll have a fully functional Python environment in your browser, complete with all necessary libraries, datasets, and code. You can run code cells, modify examples, and experiment with variations immediately.\nWe recommend following the four-part progression of the book, starting with Statistical Foundations and working through to Advanced Topics. Each chapter builds on previous material, so working sequentially ensures you have the necessary background for more complex concepts. However, the modular structure also allows you to jump to specific topics of interest if you’re already familiar with foundational material.\nAs you work through each chapter, make use of the supplementary AI materials as needed. Some learners will want to use every resource—notebook, visual summary, podcast, slides, and quiz. Others might focus primarily on the notebooks with occasional reference to other materials. The flexible design allows you to create a learning path that suits your needs and preferences.\n\n\nFor Each Chapter\nTo get the most out of each chapter, we recommend a multi-stage approach that combines different learning modalities:\nStart by reading foundational concepts from Cameron’s textbook. While this is optional (the notebooks are self-contained), reading the corresponding textbook chapter first provides valuable theoretical context and mathematical derivations that complement the computational focus of the notebooks. The textbook explains the “why” behind methods, while notebooks show the “how.”\nRun the Python notebook, executing code cells step-by-step. Don’t just run the cells passively—read the explanations, study the code, and make sure you understand what each section accomplishes. Experiment by changing parameters, trying different datasets, or extending analyses beyond what’s shown. This active engagement is where deep learning happens.\nReview the visual summary for a quick overview of key concepts. The visual summary helps consolidate what you’ve learned and provides a different perspective on the chapter’s main ideas. Visual representations often reveal connections between concepts that aren’t immediately obvious in text or code.\nWatch the video overview for an animated explanation of the chapter’s key concepts. Unlike the static visual summary, the video uses motion and narration to demonstrate dynamic relationships and step-by-step processes. This is particularly valuable for understanding how econometric techniques work in practice—you can see regression lines being fitted, distributions changing with different parameters, or hypothesis tests being conducted. The short format (under 10 minutes) makes videos ideal for focused learning sessions or quick reviews before exams.\nListen to the podcast for a conversational explanation of the chapter’s content. The podcast offers yet another way to engage with the material, particularly useful for reinforcement and review. Many learners find that hearing concepts explained conversationally helps cement understanding in ways that reading or coding alone doesn’t achieve.\nStudy the AI slides to see the material presented in presentation format. The slides break down complex topics into digestible pieces, making them ideal for review and for identifying areas where you might need additional study.\nReview Cameron’s original slides for the authoritative instructor perspective. These slides, created by Professor Cameron himself, provide the traditional academic presentation of the material and often include additional insights and examples not found elsewhere.\nTake the quiz to test your understanding. The EdCafe quizzes provide immediate feedback on whether you’ve truly grasped the key concepts. Don’t skip this step—self-assessment is crucial for identifying gaps in understanding before moving forward.\nConsult the AI tutor whenever you need help. Whether you’re stuck on a coding problem, confused about a statistical concept, or want a deeper explanation of a particular point, the NotebookLM and EdCafe AI tutors are available to provide personalized assistance.\n\n\nCustomize Your Learning!\nBy purchasing this book, you gain access to PDF versions of each chapter. These PDFs are designed to be used with a wide range of AI-powered learning tools, giving you substantial flexibility to custimize your learning path.\n\nNote: If you purchased the book on Leanpub, the PDFs are available directly from your Leanpub library.\n\nThe chapter PDFs are fully compatible with several AI tools that support adaptive and interactive learning. For example, NotebookLM (https://notebooklm.google.com/) can transform chapters into podcast-style audio discussions, generate personalized quizzes and flashcards, and produce interactive learning guides. EdCafe (https://www.edcafe.ai/) allows you to create custom lesson plans, interactive quizzes, and AI chatbots. It also provides personalized feedback and analytics to monitor your learning progress. AISheets (https://www.aisheets.study/) converts chapter PDFs into interactive worksheets, concept maps, and fill-in-the-blank exercises that can be edited, customized, and exported as PDFs for offline study. Importantly, the outputs generated by these tools can be adapted to multiple languages and learning styles, highlighting AI’s potential to promote inclusive learning.\nYou are encouraged to experiment with these PDFs and AI tools to identify the combination that best aligns with your learning preferences and objectives. Whether you prefer listening to AI-generated discussions during your commute, reinforcing understanding through interactive quizzes, or working systematically with structured worksheets, these tools offer multiple complementary pathways for mastering new concepts. They are most effective when used as supplements to—rather than substitutes for—active engagement with the Python notebooks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface: Econometrics Powered by AI</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch00_Preface.html#acknowledgments",
    "href": "../notebooks_colab/ch00_Preface.html#acknowledgments",
    "title": "Preface: Econometrics Powered by AI",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nA. Colin Cameron\nThis entire project would not exist without the foundational work of Professor A. Colin Cameron. His textbook, Analysis of Economics Data: An Introduction to Econometrics (2022), represents years of refinement in teaching econometrics with clarity, rigor, and practical relevance. Professor Cameron’s generous permission to use his textbook content and structure made this computational companion possible.\nBeyond the textbook itself, Professor Cameron has created and shared extensive teaching materials—original Stata, R, and Gretl code implementations, comprehensive datasets, and detailed PDF slides. These materials have served students and instructors, and they continue to serve as the authoritative reference for this Python implementation.\nMost fundamentally, Professor Cameron’s pioneering approach to making econometrics accessible and practical has been the inspiration for this entire project. His work demonstrates that rigorous econometric education need not be intimidating or inaccessible. This book attempts to extend that philosophy into the cloud computing and AI era, maintaining Cameron’s commitment to clarity while leveraging new technological capabilities.\n\n\nTechnology and Platform Partners\nThis project relies heavily on cutting-edge technology platforms that have made modern, accessible education possible:\nGoogle Colab provides the cloud computing infrastructure that makes zero-installation learning a reality. By offering free access to powerful computing resources and maintaining up-to-date Python environments, Colab has democratized access to data science education in ways that would have been unimaginable just a few years ago.\nGoogle’s NotebookLM powers the AI learning tools—podcasts, slides, and tutoring—that provide personalized learning support. This sophisticated AI technology transforms static educational content into interactive learning experiences tailored to individual needs and learning styles.\nGoogle’s Gemini PRO generates the visual summaries that help consolidate understanding and provide alternative perspectives on chapter content. The ability to automatically create meaningful visualizations of complex concepts represents a significant advance in educational technology.\nEdCafe provides the platform for interactive quizzes and AI tutoring, offering the assessment and feedback mechanisms that are essential for effective learning. Their tools help ensure that learning is not just exposure to content but genuine mastery of concepts.\n\n\nOpen Source Community\nThis book builds on the incredible work of the open source community that has created and maintains Python’s scientific computing ecosystem. The developers of Statsmodels, Pandas, NumPy, Matplotlib, and countless other libraries have created the tools that make sophisticated statistical analysis accessible to anyone with a computer and internet connection.\nSpecial thanks go to the creators of Linearmodels and other econometric software tools that extend Python’s capabilities specifically for econometric analysis. The documentation writers and maintainers who make these complex tools accessible through clear explanations and examples deserve particular recognition—their work is often invisible but absolutely essential.\nThe open source ethos—that knowledge and tools should be freely shared for the benefit of all—is fundamental to this project. We hope this book contributes back to that community by introducing new users to Python’s capabilities and demonstrating how these tools can be applied to econometric analysis.\n\n\nStudents, Reviewers, and Family\nFinally, thanks are due to the beta testers who worked through early versions of these notebooks and materials, providing invaluable feedback on what worked, what didn’t, and what needed clarification. Their suggestions have improved the accessibility and clarity of the final product immeasurably.\nEarly adopters who used these materials in courses and self-study helped identify gaps, correct errors, and refine explanations. The integration of AI tools in particular benefited from their feedback on what types of support were most valuable at different stages of learning.\nThis book is ultimately for students—those learning econometrics now and those who will learn in the future. The goal has been to create materials that make that learning journey more accessible, more engaging, and more successful. If this book helps you understand econometrics better, apply it more confidently, and appreciate its power for answering important questions, then the effort has been worthwhile.\nThis book is dedicated to my family, whose unwavering support and encouragement have made this journey possible. Their patience during long hours of writing, coding, and testing has been a source of strength and motivation. Thank you for believing in this vision and standing by me throughout this endeavor.\nNow, let’s begin the journey into econometrics, powered by AI and brought to life through code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface: Econometrics Powered by AI</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to regression analysis using Python. You can run all code directly in Google Colab without any local setup required. The data streams directly from GitHub, making this notebook fully self-contained.\nThis chapter introduces the fundamental concepts of econometrics and regression analysis. We’ll explore how economists use statistical methods to understand relationships in economic data, focusing on a practical example of house prices and house sizes.\nWhat you’ll learn:\nDataset used:\nChapter outline:",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#chapter-overview",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#chapter-overview",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "",
    "text": "What regression analysis is and why it’s the primary tool in econometrics\nHow to load and explore economic data using Python (pandas)\nHow to visualize relationships between variables using scatter plots\nHow to fit a simple linear regression model using Ordinary Least Squares (OLS)\nHow to interpret regression coefficients in economic terms\nHow to use Python’s statsmodels package for regression analysis\n\n\n\nAED_HOUSE.DTA: House sale prices for 29 houses in Central Davis, California (1999)\n\nVariables: price (sale price in dollars), size (house size in square feet), plus 7 other characteristics\n\n\n\n\n1.1 What is Regression Analysis?\n1.2 Load the Data\n1.3 Preview the Data\n1.4 Explore the Data\n1.5 Visualizing the Relationship\n1.6 Fitting a Regression Line\n1.7 Interpreting the Results\n1.8 Visualizing the Fitted Line\n1.9 Economic Interpretation and Examples\n1.10 Practice Exercises\n1.11 Case Studies",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#setup",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#setup",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "Setup",
    "text": "Setup\nRun this cell first to import all required packages and configure the environment. This sets up:\n\nData manipulation (pandas, numpy)\nStatistical modeling (statsmodels)\nVisualization (matplotlib)\nReproducibility (random seeds)\n\n\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL (data streams directly from here)\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Optional: Create directories for saving outputs locally\nIMAGES_DIR = 'images'\nTABLES_DIR = 'tables'\nos.makedirs(IMAGES_DIR, exist_ok=True)\nos.makedirs(TABLES_DIR, exist_ok=True)\n\nprint(\"✓ Setup complete! All packages imported successfully.\")\nprint(f\"✓ Random seed set to {RANDOM_SEED} for reproducibility.\")\nprint(f\"✓ Data will stream from: {GITHUB_DATA_URL}\")\n\n✓ Setup complete! All packages imported successfully.\n✓ Random seed set to 42 for reproducibility.\n✓ Data will stream from: https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#what-is-regression-analysis",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#what-is-regression-analysis",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.1 What is Regression Analysis?",
    "text": "1.1 What is Regression Analysis?\nRegression analysis is the primary tool economists use to understand relationships between variables. At its core, regression answers questions like: “How does Y change when X changes?”\nIn our example:\n\nY (dependent variable): House sale price (in dollars)\nX (independent variable): House size (in square feet)\n\nThe regression line is the “line of best fit” that minimizes the sum of squared distances between actual prices and predicted prices. The mathematical form is:\n\\[\\text{price} = \\beta_0 + \\beta_1 \\times \\text{size} + \\varepsilon\\]\nWhere:\n\n\\(\\beta_0\\) = intercept (predicted price when size = 0)\n\\(\\beta_1\\) = slope (change in price for each additional square foot)\n\\(\\varepsilon\\) = error term (random variation not explained by size)\n\nEconomic Interpretation:\nThe slope coefficient \\(\\beta_1\\) tells us: “On average, how much more expensive is a house that is 1 square foot larger?” This is a measure of association, not necessarily causation.\n\nKey Concept 1.1: Descriptive vs. Inferential Analysis\nDescriptive analysis summarizes data using statistics and visualizations, while statistical inference uses sample data to draw conclusions about the broader population. Most econometric analysis involves statistical inference.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#load-the-data",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#load-the-data",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.2 Load the Data",
    "text": "1.2 Load the Data\nLet’s load the house price dataset directly from GitHub. This dataset contains information on 29 house sales in Central Davis, California in 1999.\n\n# Load the Stata dataset from GitHub\ndata_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n\nprint(f\"✓ Data loaded successfully!\")\nprint(f\"  Shape: {data_house.shape[0]} observations, {data_house.shape[1]} variables\")\n\n✓ Data loaded successfully!\n  Shape: 29 observations, 8 variables",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#preview-the-data",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#preview-the-data",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.3 Preview the Data",
    "text": "1.3 Preview the Data\nLet’s look at the first few rows to understand what variables we have available.\n\n# Display first 5 rows\nprint(\"First 5 observations:\")\nprint(data_house.head())\n\nprint(\"\\nColumn names:\")\nprint(data_house.columns.tolist())\n\nFirst 5 observations:\n    price  size  bedrooms  bathrooms  lotsize   age  monthsold    list\n0  204000  1400         3        2.0        1  31.0          7  199900\n1  212000  1600         3        3.0        2  33.0          5  212000\n2  213000  1800         3        2.0        2  51.0          4  219900\n3  220000  1600         3        2.0        1  49.0          4  229000\n4  224500  2100         4        2.5        2  47.0          6  224500\n\nColumn names:\n['price', 'size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold', 'list']\n\n\nTransition: Before jumping into regression analysis, we need to understand our data. Descriptive statistics reveal the scale, variability, and range of our variables—essential for interpreting regression results.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#explore-the-data",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#explore-the-data",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.4 Explore the Data",
    "text": "1.4 Explore the Data\nBefore running any regression, it’s essential to understand the data through descriptive statistics. Let’s look at the key statistics for our variables of interest: price and size.\n\n# Summary statistics for all variables\nprint(\"=\" * 70)\nprint(\"DESCRIPTIVE STATISTICS\")\nprint(\"=\" * 70)\nprint(data_house.describe().round(2))\n\n# Focus on our key variables\nprint(\"\\n\" + \"=\" * 70)\nprint(\"KEY VARIABLES: PRICE AND SIZE\")\nprint(\"=\" * 70)\nprint(data_house[['price', 'size']].describe().round(2))\n\n======================================================================\nDESCRIPTIVE STATISTICS\n======================================================================\n           price     size  bedrooms  bathrooms  lotsize    age  monthsold  \\\ncount      29.00    29.00     29.00      29.00    29.00  29.00      29.00   \nmean   253910.34  1882.76      3.79       2.21     2.14  36.41       5.97   \nstd     37390.71   398.27      0.68       0.34     0.69   7.12       1.68   \nmin    204000.00  1400.00      3.00       2.00     1.00  23.00       3.00   \n25%    233000.00  1600.00      3.00       2.00     2.00  31.00       5.00   \n50%    244000.00  1800.00      4.00       2.00     2.00  35.00       6.00   \n75%    270000.00  2000.00      4.00       2.50     3.00  39.00       7.00   \nmax    375000.00  3300.00      6.00       3.00     3.00  51.00       8.00   \n\n            list  \ncount      29.00  \nmean   257824.14  \nstd     40860.26  \nmin    199900.00  \n25%    239000.00  \n50%    245000.00  \n75%    269000.00  \nmax    386000.00  \n\n======================================================================\nKEY VARIABLES: PRICE AND SIZE\n======================================================================\n           price     size\ncount      29.00    29.00\nmean   253910.34  1882.76\nstd     37390.71   398.27\nmin    204000.00  1400.00\n25%    233000.00  1600.00\n50%    244000.00  1800.00\n75%    270000.00  2000.00\nmax    375000.00  3300.00\n\n\nKey observations:\n\nMean house price: Around $253,910\nMean house size: Around 1,883 square feet\nPrice range: $204,000 to $375,000\nSize range: 1,400 to 3,300 square feet\n\nNotice the variation in both variables - this variation is what allows us to estimate a relationship!\n\nKey Concept 1.2: Observational Data in Economics\nEconomics primarily uses observational data where we observe behavior in uncontrolled settings. Unlike experimental data where conditions can be controlled, observational data requires careful methods to establish relationships and, when possible, causal effects.\n\nNow that we have explored the data numerically, let’s visualize the relationship between house size and price.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#visualizing-the-relationship",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#visualizing-the-relationship",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.5 Visualizing the Relationship",
    "text": "1.5 Visualizing the Relationship\nBefore running any regression, it’s good practice to visualize the relationship between X and Y. A scatter plot helps us:\n\nCheck if there appears to be a linear relationship\nIdentify any outliers or unusual observations\nGet an intuitive sense of the strength of the relationship\n\nLet’s create a scatter plot of house price vs. house size.\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the data points\nax.scatter(data_house['size'], data_house['price'],\n           color='navy', s=50, alpha=0.7)\n\n# Labels and formatting\nax.set_xlabel('House Size (square feet)', fontsize=12)\nax.set_ylabel('House Sale Price (dollars)', fontsize=12)\nax.set_title('House Price vs Size (29 observations)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.spines[['top', 'right']].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nWhat do you see?\")\nprint(\"- Positive relationship: Larger houses tend to have higher prices\")\nprint(\"- Roughly linear: The points follow an upward-sloping pattern\")\nprint(\"- Some scatter: Not all points lie exactly on a line (this is the 'error')\")\n\n\n\n\n\n\n\n\n\nWhat do you see?\n- Positive relationship: Larger houses tend to have higher prices\n- Roughly linear: The points follow an upward-sloping pattern\n- Some scatter: Not all points lie exactly on a line (this is the 'error')\n\n\nTransition: Having visualized a clear positive relationship between house size and price, we’re ready to quantify this relationship precisely using regression analysis.\n\nKey Concept 1.3: Visual Exploration Before Regression\nAlways plot your data before running a regression. Scatter plots reveal the direction, strength, and form of relationships between variables, and can expose outliers or nonlinearities that summary statistics alone would miss. Visual exploration is the essential first step in any empirical analysis.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#fitting-a-regression-line",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#fitting-a-regression-line",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.6 Fitting a Regression Line",
    "text": "1.6 Fitting a Regression Line\nNow we’ll fit an Ordinary Least Squares (OLS) regression line to these data. OLS chooses the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) that minimize the sum of squared residuals:\n\\[\\min_{\\beta_0, \\beta_1} \\sum_{i=1}^{n} (\\text{price}_i - \\beta_0 - \\beta_1 \\times \\text{size}_i)^2\\]\nIn other words, we’re finding the line that makes our prediction errors as small as possible (in a squared sense).\nWe’ll use Python’s statsmodels package, which provides regression output similar to Stata and R.\n\nKey Concept 1.4: Introduction to Regression Analysis\nRegression analysis quantifies the relationship between variables. In a bivariate regression, the slope coefficient tells us how much the outcome variable (\\(y\\)) changes when the explanatory variable (\\(x\\)) increases by one unit.\n\n\n# Fit OLS regression: price ~ size\n# The formula syntax is: 'dependent_variable ~ independent_variable'\nmodel = ols('price ~ size', data=data_house).fit()\n\n# Display the full regression output\nprint(\"=\" * 70)\nprint(\"OLS REGRESSION RESULTS: price ~ size\")\nprint(\"=\" * 70)\nprint(model.summary())\n\n======================================================================\nOLS REGRESSION RESULTS: price ~ size\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.617\nModel:                            OLS   Adj. R-squared:                  0.603\nMethod:                 Least Squares   F-statistic:                     43.58\nDate:                Sat, 31 Jan 2026   Prob (F-statistic):           4.41e-07\nTime:                        02:18:54   Log-Likelihood:                -332.05\nNo. Observations:                  29   AIC:                             668.1\nDf Residuals:                      27   BIC:                             670.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    1.15e+05   2.15e+04      5.352      0.000    7.09e+04    1.59e+05\nsize          73.7710     11.175      6.601      0.000      50.842      96.700\n==============================================================================\nOmnibus:                        0.576   Durbin-Watson:                   1.219\nProb(Omnibus):                  0.750   Jarque-Bera (JB):                0.638\nSkew:                          -0.078   Prob(JB):                        0.727\nKurtosis:                       2.290   Cond. No.                     9.45e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#interpreting-the-results",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#interpreting-the-results",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.7 Interpreting the Results",
    "text": "1.7 Interpreting the Results\nThe regression output contains a lot of information! Let’s break down the most important parts:\n\nKey Statistics to Focus On:\n\nCoefficients table (middle section):\n\nIntercept: The predicted price when size = 0 (often not economically meaningful)\nsize: The slope coefficient - our main interest!\nstd err: Standard error (measures precision of the estimate)\nt: t-statistic (coefficient / standard error)\nP&gt;|t|: p-value (tests if coefficient is significantly different from zero)\n\nR-squared (top right section):\n\nProportion of variation in Y explained by X\nRanges from 0 to 1 (higher = better fit)\n\nF-statistic (top right section):\n\nTests overall significance of the regression\nLow p-value (Prob F-statistic) means the model is statistically significant\n\n\nLet’s extract and interpret the key coefficients.\n\n# Extract key statistics\nintercept = model.params['Intercept']\nslope     = model.params['size']\nr_squared = model.rsquared\nn_obs     = int(model.nobs)\n\nprint(\"=\" * 70)\nprint(\"KEY REGRESSION COEFFICIENTS\")\nprint(\"=\" * 70)\nprint(f\"Intercept (β₀): ${intercept:,.2f}\")\nprint(f\"Slope (β₁):  ${slope:,.2f}\")\nprint(f\"R-squared: {r_squared:.4f} ({r_squared*100:.2f}%)\")\nprint(f\"Number of observations: {n_obs}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ECONOMIC INTERPRETATION\")\nprint(\"=\" * 70)\nprint(f\"📌 For every additional square foot of house size,\")\nprint(f\"   the sale price increases by approximately ${slope:,.2f}\")\nprint(f\"\\n📌 The model explains {r_squared*100:.2f}% of the variation in house prices\")\nprint(f\"\\n📌 The remaining {(1-r_squared)*100:.2f}% is due to other factors not included\")\nprint(f\"   (e.g., location, age, condition, neighborhood quality)\")\n\n======================================================================\nKEY REGRESSION COEFFICIENTS\n======================================================================\nIntercept (β₀): $115,017.28\nSlope (β₁):  $73.77\nR-squared: 0.6175 (61.75%)\nNumber of observations: 29\n\n======================================================================\nECONOMIC INTERPRETATION\n======================================================================\n📌 For every additional square foot of house size,\n   the sale price increases by approximately $73.77\n\n📌 The model explains 61.75% of the variation in house prices\n\n📌 The remaining 38.25% is due to other factors not included\n   (e.g., location, age, condition, neighborhood quality)\n\n\n\nKey Concept 1.5: Reading Regression Output\nThe key elements of regression output are: the coefficient estimate (magnitude and direction of the relationship), the standard error (precision of the estimate), the t-statistic and p-value (statistical significance), and R-squared (proportion of variation explained). Together, these tell us whether the relationship is economically meaningful and statistically reliable.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#visualizing-the-fitted-line",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#visualizing-the-fitted-line",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.8 Visualizing the Fitted Line",
    "text": "1.8 Visualizing the Fitted Line\nThe fitted regression line represents our model’s predictions. For any given house size, the line shows the predicted price according to our equation:\n\\[\\hat{\\text{price}} = \\beta_0 + \\beta_1 \\times \\text{size}\\]\nLet’s overlay this fitted line on our scatter plot to see how well it captures the relationship.\n\n# Create scatter plot with fitted regression line\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot actual data points\nax.scatter(data_house['size'], data_house['price'],\n           color='navy', s=50, label='Actual prices', alpha=0.7)\n\n# Plot fitted regression line\nax.plot(data_house['size'], model.fittedvalues,\n        color='purple', linewidth=2, label='Fitted regression line')\n\n# Add equation to plot\nequation_text = f'price = {intercept:,.0f} + {slope:.2f} × size'\nax.text(0.05, 0.95, equation_text,\n        transform=ax.transAxes, fontsize=11,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# Labels and formatting\nax.set_xlabel('House Size (square feet)', fontsize=12)\nax.set_ylabel('House Sale Price (dollars)', fontsize=12)\nax.set_title('House Price vs Size with Fitted Regression Line',\n             fontsize=14, fontweight='bold')\nax.legend(loc='lower right', fontsize=10)\nax.spines[['top', 'right']].set_visible(False)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 The purple line is our 'line of best fit'\")\nprint(\"   It minimizes the sum of squared vertical distances from each point\")\n\n\n\n\n\n\n\n\n\n📊 The purple line is our 'line of best fit'\n   It minimizes the sum of squared vertical distances from each point\n\n\nTransition: Statistical output is only meaningful when translated into economic insights. Let’s explore what our regression coefficients tell us about housing markets and the limitations of our analysis.\nHaving fitted and visualized our regression model, let’s now interpret what these results mean in economic terms.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#economic-interpretation-and-examples",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#economic-interpretation-and-examples",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.9 Economic Interpretation and Examples",
    "text": "1.9 Economic Interpretation and Examples\nNow that we’ve estimated the regression, let’s think about what it means in economic terms.\n\nPractical Implications:\nOur estimated slope of approximately $74 per square foot means:\n\nA house that’s 100 sq ft larger is predicted to sell for $74 × 100 = $7,400 more\nA house that’s 500 sq ft larger is predicted to sell for $74 × 500 = $37,000 more\n\n\n\nMaking Predictions:\nWe can use our regression equation to predict prices for houses of different sizes. For example, for a 2,000 sq ft house:\n\\[\\hat{\\text{price}} = 115,952 + 74.03 \\times 2000 = \\$264,012\\]\n\n\nImportant Caveats:\n\nThis is association, not causation: We can’t conclude that adding square footage to a house will increase its value by $74/sq ft. Other factors (like quality of construction) might be correlated with size.\nOmitted variables: Many other factors affect house prices (location, age, condition, amenities). Our simple model ignores these - we’ll learn how to include them in later chapters.\nSample-specific: These results are from 29 houses in Davis, CA in 1999. The relationship might differ in other locations or time periods.\nDon’t extrapolate too far: Our data ranges from 1,400 to 3,300 sq ft. Predictions far outside this range (e.g., for a 10,000 sq ft house) may not be reliable.\n\n\nKey Concept 1.6: Interpreting Regression Results\nRegression results must be interpreted with caution. Association does not imply causation, omitted variables can bias estimates, and predictions should not extrapolate beyond the range of the data.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#key-takeaways",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#key-takeaways",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nStatistical Methods and Data Types: - Econometrics uses two main approaches: descriptive analysis (summarizing data) and statistical inference (drawing population conclusions from samples) - Economic data are primarily continuous and numerical, though categorical and discrete data are also important - Economics relies mainly on observational data, making causal inference more challenging than with experimental data - The three data collection methods are cross-section (individuals at one time), time series (one individual over time), and panel data (individuals over time) - Each data type requires different considerations for statistical inference, particularly when computing standard errors - This textbook focuses on continuous numerical data and cross-section analysis as the foundation for more advanced methods\nRegression Analysis and Interpretation: - Regression analysis is the primary tool in econometrics, quantifying how outcome variables (y) vary with explanatory variables (x) - The simple linear regression model has the form: \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\), where \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope - The slope coefficient measures association: how much y changes when x increases by one unit - OLS (Ordinary Least Squares) finds the best-fitting line by minimizing the sum of squared prediction errors - R-squared measures the proportion of variation in y explained by x, ranging from 0 to 1 (higher = better fit) - Economic interpretation focuses on magnitude (size of effect), statistical significance, and practical importance\nPractical Application: - Our house price example: Each additional square foot is associated with a $73.77 increase in price (R² = 61.75%) - Visualization is essential: scatter plots reveal the nature of relationships before fitting regression models - Regression shows association, not causation—omitted variables and confounding factors require careful consideration - Predictions should not extrapolate beyond the range of observed data - Sample-specific results may not generalize to other locations, time periods, or populations\nPython Tools and Workflow: - pandas handles data loading, manipulation, and descriptive statistics - statsmodels.formula.api.ols() estimates OLS regression models with R-style formula syntax - matplotlib creates publication-quality scatter plots and visualizations - Standard workflow: load data → explore descriptively → visualize → model → interpret → validate - Random seeds ensure reproducibility of results across different runs\nPrerequisites and Mathematical Background: - Summation notation (Σ) expresses formulas concisely and appears throughout econometrics - Calculus concepts (derivatives, rates of change) help understand marginal effects but are not essential - Expected values (E[X]) define population parameters like means and variances - “Learning-by-doing” is the most effective approach: practice with real data and software is essential for mastery\n\nNext Steps: - Chapter 2: Univariate data summary (describing single variables) - Chapter 3: The sample mean and sampling distributions - Chapter 4: Statistical inference for the mean (confidence intervals, hypothesis tests) - Chapters 5-7: Deep dive into bivariate regression (extending what we learned here)\nYou have now mastered: Loading and exploring economic data in Python Creating scatter plots to visualize relationships Estimating simple linear regression models with OLS Interpreting regression coefficients economically Understanding the limitations of regression analysis\nThese foundational concepts are the building blocks for all of econometrics. Everything that follows builds on this introduction!",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#practice-exercises",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#practice-exercises",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of regression analysis with these exercises:\nExercise 1: Conceptual understanding\n\n\nWhat is the difference between descriptive analysis and statistical inference?\n\n\nWhy do economists primarily use observational data rather than experimental data?\n\n\nName the three main types of data collection methods and give an example of each.\n\n\nExercise 2: Data types\n\nClassify each of the following as continuous numerical, discrete numerical, or categorical:\n\n\nAnnual household income\n\n\nNumber of children in a family\n\n\nEmployment status (employed, unemployed, not in labor force)\n\n\nTemperature in degrees Celsius\n\n\n\nExercise 3: Regression interpretation\n\nSuppose you estimate: earnings = 20,000 + 5,000 × education\n\n\nInterpret the intercept coefficient\n\n\nInterpret the slope coefficient\n\n\nPredict earnings for someone with 16 years of education\n\n\nWhat is the predicted difference in earnings between someone with 12 vs. 16 years of education?\n\n\n\nExercise 4: Using our house price model\n\nUsing the regression equation: price = 115,017 + 73.77 × size\n\n\nPredict the price for a 1,800 sq ft house\n\n\nPredict the price for a 2,500 sq ft house\n\n\nWhat is the predicted price difference between these two houses?\n\n\nIs the intercept economically meaningful in this context? Why or why not?\n\n\n\nExercise 5: Critical thinking about causation\n\nOur regression shows larger houses have higher prices. Does this mean:\n\n\nAdding square footage to a house will increase its value by $73.77 per sq ft?\n\n\nWhat other factors might be correlated with both house size and price?\n\n\nHow would you design a study to establish a causal relationship?\n\n\n\nExercise 6: R-squared interpretation\n\nOur model has R² = 0.6175 (61.75%)\n\n\nWhat does this number tell us about our model?\n\n\nWhat factors might explain the remaining 38.25% of variation in prices?\n\n\nWould R² = 1.0 be realistic for real-world economic data? Why or why not?\n\n\n\nExercise 7: Summation notation\n\nCalculate: \\(\\sum_{i=1}^{4} (3 + 2i)\\)\nShow all steps in your calculation\n\nExercise 8: Python practice\n\nLoad the house dataset and:\n\n\nCalculate the correlation between price and bedrooms\n\n\nCreate a scatter plot of price vs. bedrooms\n\n\nEstimate the regression: price ~ bedrooms\n\n\nCompare the R² to our size regression. Which predictor is better?",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#case-studies",
    "href": "../notebooks_colab/ch01_Analysis_of_Economics_Data.html#case-studies",
    "title": "Chapter 1: Analysis of Economics Data",
    "section": "1.11 Case Studies",
    "text": "1.11 Case Studies\nNow let’s apply what you’ve learned to real economic research! In this section, you’ll explore data from actual published studies, using the same tools and techniques from this chapter.\nWhy case studies matter:\n\nSee how regression analysis is used in real research\nPractice applying Chapter 1 tools to authentic data\nDevelop intuition for economic relationships\nBridge the gap between textbook examples and research practice\n\n\nCase Study 1: Economic Convergence Clubs\nResearch Question: Do countries converge toward similar levels of economic development, or do they form distinct “convergence clubs” with different trajectories?\nBackground: Traditional economic theory suggests that poor countries should grow faster than rich countries, eventually “catching up” in terms of income and productivity. This is called the convergence hypothesis. However, empirical evidence shows a more complex picture: countries may form distinct groups (clubs) that converge toward different long-run equilibrium levels rather than a single global level.\nThis Research (Mendez, 2020): Uses modern econometric methods to identify convergence clubs in labor productivity across countries. The analysis examines whether countries follow one common development path or multiple distinct paths, and what factors (capital accumulation, technology, institutions) drive these patterns.\nThe Data: Panel dataset tracking multiple countries over several years, with 26 variables including:\n\nOutput measures: GDP, GDP per capita\nProductivity: Labor productivity, total factor productivity (TFP)\nCapital: Physical capital stock, capital per worker\nHuman capital: Years of schooling, human capital index\nClassifications: Country codes, regions, income groups\n\nYour Task: Use the descriptive analysis and regression tools from Chapter 1 to explore patterns in the convergence clubs data. You’ll investigate productivity gaps, visualize relationships, and begin to understand why some countries develop differently than others.\n\nKey Concept 1.7: Economic Convergence and Productivity Drivers\nBeta convergence refers to the hypothesis that poor countries will grow faster than rich countries, eventually “catching up” in terms of income and productivity. However, evidence suggests countries may form distinct convergence clubs—groups that converge toward different long-run equilibrium levels rather than a single global level.\nLabor productivity (output per worker) depends on capital accumulation (capital per worker) and aggregate efficiency (total factor productivity or TFP). The regression of productivity on capital captures this association, allowing us to quantify how much of cross-country productivity differences are explained by capital versus efficiency factors.\n\n\nLoad the Convergence Clubs Data\nLet’s load two datasets:\n\nMain dataset (dat.csv): Country-year panel data with economic variables\nData dictionary (dat-definitions.csv): Explains what each variable means\n\nThe data uses a multi-index structure with (country, year) pairs, allowing us to track each country over time.\n\n# Import data with sorted multi-index\ndf1 = pd.read_csv(\n    \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\",\n    index_col=[\"country\", \"year\"]\n).sort_index()\n\n# Import data dictionary\ndf2 = pd.read_csv(\n    \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat-definitions.csv\"\n)\n\n# Display basic information\nprint(\"=\" * 70)\nprint(\"CONVERGENCE CLUBS DATASET\")\nprint(\"=\" * 70)\nprint(f\"Dataset shape: {df1.shape[0]} observations (country-year pairs), {df1.shape[1]} variables\")\nprint(f\"\\nCountries: {len(df1.index.get_level_values('country').unique())} unique countries\")\nprint(f\"Years: {df1.index.get_level_values('year').min()} to {df1.index.get_level_values('year').max()}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FIRST 5 OBSERVATIONS\")\nprint(\"=\" * 70)\nprint(df1.head(5))\n\n======================================================================\nCONVERGENCE CLUBS DATASET\n======================================================================\nDataset shape: 2700 observations (country-year pairs), 27 variables\n\nCountries: 108 unique countries\nYears: 1990 to 2014\n\n======================================================================\nFIRST 5 OBSERVATIONS\n======================================================================\n              id          Y      K       pop         L         s  alpha_it  \\\ncountry year                                                                 \nAlbania 1990   1  12449.999  31217  3.281453  1.250096  8.497386       NaN   \n        1991   1  11310.000  30082  3.275438  1.243719  8.442703       NaN   \n        1992   1  10122.000  28968  3.240613  0.993492  8.388020       NaN   \n        1993   1  11636.000  29010  3.189623  0.935928  8.333337       NaN   \n        1994   1  13120.000  29634  3.140634  1.008687  8.278653       NaN   \n\n                  GDPpc          lp         h  ...  log_h_raw  log_tfp_raw  \\\ncountry year                                   ...                           \nAlbania 1990  3794.0508   9959.2344  3.165140  ...   1.152197     5.560511   \n        1991  3452.9731   9093.6943  3.150347  ...   1.147513     5.479933   \n        1992  3123.4832  10188.3060  3.135588  ...   1.142817     5.457089   \n        1993  3648.0798  12432.5870  3.120863  ...   1.138110     5.617254   \n        1994  4177.5005  13007.0080  3.106171  ...   1.133391     5.707116   \n\n              log_GDPpc    log_lp    log_ky     log_h   log_tfp  isocode  \\\ncountry year                                                               \nAlbania 1990   8.130327  9.209681  0.880770  1.125790  5.541879      ALB   \n        1991   8.170837  9.268303  0.869605  1.133772  5.574306      ALB   \n        1992   8.211625  9.326916  0.858536  1.141821  5.606780      ALB   \n        1993   8.252907  9.385130  0.847930  1.150036  5.639110      ALB   \n        1994   8.294488  9.442308  0.838639  1.158521  5.670734      ALB   \n\n              hi1990  region  \ncountry year                  \nAlbania 1990      no  Europe  \n        1991      no  Europe  \n        1992      no  Europe  \n        1993      no  Europe  \n        1994      no  Europe  \n\n[5 rows x 27 columns]\n\n\n\nprint(\"\\n\" + \"=\" * 75)\nprint(\"VARIABLE DEFINITIONS\")\nprint(\"=\" * 75)\nprint(df2)\n\n\n===========================================================================\nVARIABLE DEFINITIONS\n===========================================================================\n         var_name                                            var_def     type\n0         country               Standardized country name (from PWT)    cs_id\n1            year                                               Year    ts_id\n2               Y                                                GDP  numeric\n3               K                                   Physical Capital  numeric\n4             pop                                         Population  numeric\n5               L                                        Labor Force  numeric\n6               s                                 Years of Schooling  numeric\n7        alpha_it                             Variable Capital Share  numeric\n8           GDPpc                                     GDP per capita  numeric\n9              lp                                 Labor Productivity  numeric\n10              h                                Human Capital Index  numeric\n11             kl                                 Capital per Worker  numeric\n12             kp                               Capital Productivity  numeric\n13             ky                               Capital-Output Ratio  numeric\n14            TFP                               Aggregate Efficiency  numeric\n15  log_GDPpc_raw                              Log of GDP per capita  numeric\n16     log_lp_raw                          Log of Labor Productivity  numeric\n17     log_ky_raw                        Log of Capital-Output Ratio  numeric\n18      log_h_raw                               Log of Human Capital  numeric\n19    log_tfp_raw                   Log of Total Factor Productivity  numeric\n20      log_GDPpc             Trend (HP400) of log of GDP per capita  numeric\n21         log_lp         Trend (HP400) of log of labor productivity  numeric\n22         log_ky       Trend (HP400) of log of Capital-Output Ratio  numeric\n23          log_h              Trend (HP400) of log of Human Capital  numeric\n24        log_tfp       Trend (HP400) of log of Aggregate Efficiency  numeric\n25         region          Regional group (Classification of the UN)   factor\n26         hi1990  High income country (as of 1990, World Bank cl...   factor\n27        isocode                           ISO code from the PWT9.0   factor\n\n\n\n\nTask 1: Data Exploration (Guided)\nObjective: Understand the dataset structure and available variables.\nInstructions:\n\nExamine the output above to understand the multi-index (country, year) structure\nReview the data dictionary to identify key productivity variables\nCheck for missing values in key variables\nIdentify the variable names you’ll use for subsequent analyses\n\nKey variables to focus on (check exact names in the data dictionary):\n\nLabor productivity variables\nCapital per worker variables\nGDP per capita measures\nCountry classification variables (region, income group)\n\nRun the code above and study the output. What patterns do you notice? How many time periods does each country have?\n\n# Your code here: Explore the dataset structure\n#\n# Suggested explorations:\n# 1. Check column names: df1.columns.tolist()\n# 2. Check for missing values: df1.isnull().sum()\n# 3. Examine a specific country's data\n# 4. Count observations per country\n\n# Example: Examine USA's data\n# df1.loc['USA']\n\n\n\nTask 2: Descriptive Statistics (Semi-guided)\nObjective: Generate summary statistics for key productivity variables.\nInstructions:\n\nSelect 3-4 key variables related to productivity and capital\nGenerate descriptive statistics (mean, median, std, min, max)\nIdentify countries with highest and lowest productivity levels\nCalculate the productivity gap between top and bottom performers\n\nApply what you learned in sections 1.4: Use .describe() method like we did with the house price data.\nHint: You’ll need to identify the exact variable names from the data dictionary. Look for variables measuring labor productivity, GDP per capita, or capital per worker.\n\n# Your code here: Generate descriptive statistics\n#\n# Steps:\n# 1. Identify variable names from data dictionary (check df2)\n# 2. Select key variables from df1\n# 3. Generate summary statistics using .describe()\n# 4. Find countries with max/min values using .idxmax() and .idxmin()\n# 5. Calculate gaps between high and low performers\n\n# Example structure:\n# key_vars = ['variable1', 'variable2', 'variable3']  # Replace with actual names\n# df1[key_vars].describe()\n\n\nKey Concept 1.8: Panel Data Structure\nPanel data combines cross-section and time series dimensions, tracking multiple entities (countries) over multiple time periods (years). This structure allows us to study both differences between countries (cross-sectional variation) and changes within countries over time (time series variation). The data is indexed by (country, year) pairs.\n\n\n\nTask 3: Visualizing Productivity Patterns (Semi-guided)\nObjective: Create scatter plots to visualize productivity relationships.\nInstructions:\n\nCreate a scatter plot comparing two productivity-related variables\nAdd appropriate axis labels and a descriptive title\nOptionally: Color-code points by region or income group\nInterpret the pattern you observe\n\nApply what you learned in section 1.5: Use matplotlib to create scatter plots like the house price visualization.\nSuggested relationships to explore:\n\nGDP per capita vs. labor productivity\nCapital per worker vs. labor productivity\nHuman capital vs. GDP per capita\n\n\n# Your code here: Create scatter plot\n#\n# Steps:\n# 1. Prepare data (select variables, remove missing values)\n# 2. Create figure and axis: fig, ax = plt.subplots(figsize=(10, 6))\n# 3. Create scatter plot: ax.scatter(x, y, ...)\n# 4. Add labels, title, and formatting\n# 5. Display and interpret\n\n# Example structure:\n# plot_data = df1[['var_x', 'var_y']].dropna()\n# fig, ax = plt.subplots(figsize=(10, 6))\n# ax.scatter(plot_data['var_x'], plot_data['var_y'], alpha=0.6)\n# ax.set_xlabel('Variable X')\n# ax.set_ylabel('Variable Y')\n# plt.show()\n\n# What pattern do you observe? Positive or negative relationship?\n\n\n\nTask 4: Time Series Exploration (More Independent)\nObjective: Examine productivity trends over time for specific countries.\nInstructions:\n\nSelect 2-3 countries of interest (e.g., USA, China, India, Japan, or countries from your region)\nPlot labor productivity over time for each country\nCompare their trajectories: Which countries are growing faster?\nCalculate the average annual growth rate (optional: percentage change per year)\n\nHint: Remember that panel data is indexed by (country, year). Use .loc[country] to filter data for a specific country.\nQuestions to answer:\n\nAre productivity levels converging (getting closer) or diverging (spreading apart)?\nWhich country experienced the fastest productivity growth?\nDo you see evidence of convergence clubs (groups following similar paths)?\n\n\n# Your code here: Time series plots\n#\n# Steps:\n# 1. Select countries (e.g., countries = ['USA', 'CHN', 'JPN'])\n# 2. For each country, extract time series data\n# 3. Create line plot over time\n# 4. Compare trajectories\n\n# Example structure:\n# countries = ['USA', 'CHN', 'IND']  # Adjust based on actual country codes\n# fig, ax = plt.subplots(figsize=(10, 6))\n#\n# for country in countries:\n#     country_data = df1.loc[country]\n#     ax.plot(country_data.index, country_data['productivity_var'], label=country)\n#\n# ax.set_xlabel('Year')\n# ax.set_ylabel('Labor Productivity')\n# ax.legend()\n# plt.show()\n\n\n\nTask 5: Simple Regression Analysis (INDEPENDENT)\nObjective: Estimate the relationship between capital and productivity.\nResearch Question: Does higher capital per worker lead to higher labor productivity?\nInstructions: 1. Prepare regression data (select variables, remove missing values) 2. Estimate OLS regression: labor_productivity ~ capital_per_worker 3. Display the regression summary 4. Interpret the slope coefficient economically 5. Report the R-squared value 6. Critical thinking: Discuss whether this is association or causation\nApply what you learned in sections 1.6-1.7: Use ols() from statsmodels and interpret coefficients.\nImportant questions: - What does the slope coefficient tell us? - Could there be omitted variables affecting both capital and productivity? - Could reverse causality be a concern (does higher productivity lead to more capital accumulation)?\n\n# Your code here: Regression analysis\n#\n# Steps:\n# 1. Prepare data: reg_data = df1[['productivity_var', 'capital_var']].dropna()\n# 2. Reset index if needed: reg_data = reg_data.reset_index()\n# 3. Estimate regression: model = ols('productivity_var ~ capital_var', data=reg_data).fit()\n# 4. Display summary: print(model.summary())\n# 5. Extract and interpret coefficients\n\n# Example structure:\n# from statsmodels.formula.api import ols\n#\n# reg_data = df1[['var_y', 'var_x']].dropna().reset_index()\n# model = ols('var_y ~ var_x', data=reg_data).fit()\n# print(model.summary())\n#\n# Interpretation:\n# Slope = ___: For every unit increase in capital per worker, \n#               labor productivity increases by ___ units\n# R² = ___: Capital explains ___% of variation in productivity\n\n\n\nTask 6: Comparative Analysis (Independent)\nObjective: Compare productivity patterns between country groups.\nResearch Question: Does the capital-productivity relationship differ between high-income and developing countries?\nInstructions:\n\nIf the data has an income group variable, group countries by income level\nCalculate average productivity and capital for each group\nCreate comparative scatter plots (one color per group)\n(Advanced) Run separate regressions for each group\nCompare the slope coefficients: Is the relationship stronger in one group?\n\nThis extends Chapter 1 concepts: You’re using grouping and comparative analysis to see if relationships vary across subsamples.\nQuestions to explore:\n\nDo high-income countries have uniformly higher productivity?\nIs the capital-productivity relationship steeper in developing countries?\nWhat might explain differences between groups?\n\n\n# Your code here: Comparative analysis\n#\n# Steps:\n# 1. Identify grouping variable (income level, region, etc.)\n# 2. Group data: df1.groupby('group_var').mean()\n# 3. Create comparative visualizations\n# 4. Run regressions by group (optional)\n\n# Example structure for comparative scatter plot:\n# fig, ax = plt.subplots(figsize=(10, 6))\n#\n# for group in df1['group_var'].unique():\n#     group_data = df1[df1['group_var'] == group]\n#     ax.scatter(group_data['var_x'], group_data['var_y'], label=group, alpha=0.6)\n#\n# ax.set_xlabel('Capital per Worker')\n# ax.set_ylabel('Labor Productivity')\n# ax.legend()\n# plt.show()\n\n# Advanced: Separate regressions\n# for group in groups:\n#     group_data = df1[df1['group_var'] == group].dropna()\n#     model = ols('var_y ~ var_x', data=group_data).fit()\n#     print(f\"\\n{group}: Slope = {model.params['var_x']:.4f}, R² = {model.rsquared:.4f}\")\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this hands-on exploration of economic convergence data, you’ve applied all the core Chapter 1 tools:\n\nData loading and exploration: Worked with real panel data from research\nDescriptive statistics: Summarized productivity patterns across countries\nVisualization: Created scatter plots and time series to reveal relationships\nRegression analysis: Quantified the capital-productivity relationship\nCritical thinking: Distinguished association from causation\nComparative analysis: Explored differences between country groups\n\nConnection to the research: The patterns you’ve discovered—productivity gaps, the role of capital, differences between country groups—are the empirical motivation for the convergence clubs analysis in Mendez (2020). The full research uses advanced methods (covered in later chapters) to formally identify clubs and test convergence hypotheses.\nLooking ahead:\n\nChapter 2 will teach you more sophisticated descriptive analysis for univariate data\nChapter 3-4 cover statistical inference, allowing you to test hypotheses formally\nChapter 5-9 extend regression analysis to multiple predictors and transformations\nChapter 10-17 introduce advanced methods like panel data regression—perfect for convergence clubs!\n\n\nGreat work! You’ve completed Chapter 1 and applied your new skills to real economic research. Continue to Chapter 2 to learn more about data summary and distributions.\n\n\n\nCase Study 2: Can Satellites See Poverty? Predicting Local Development in Bolivia\nResearch Question: Can satellite data—nighttime lights and satellite image embeddings—predict local economic development across Bolivia’s municipalities?\nBackground: Monitoring progress toward the United Nations Sustainable Development Goals (SDGs) requires timely, granular data on economic conditions. However, many developing countries lack comprehensive municipality-level statistics. Recent advances in remote sensing and machine learning offer a promising alternative: using satellite data to predict local development outcomes.\nTwo types of satellite data have proven particularly useful:\n\nNighttime lights (NTL): Satellite images of Earth at night reveal the intensity of artificial lighting. Brighter areas typically correspond to greater economic activity, electrification, and urbanization. NTL data is available globally and annually, making it a powerful proxy for economic development in data-scarce regions (Henderson et al., 2012).\nSatellite image embeddings: Deep learning models trained on daytime satellite imagery (Sentinel-2, Landsat) can extract 64-dimensional feature vectors that capture visual patterns—road networks, building density, vegetation cover, agricultural activity—without requiring manual labeling. These abstract features often correlate strongly with socioeconomic outcomes (Jean et al., 2016).\n\nThis Research (DS4Bolivia Project): A comprehensive data science initiative that integrates satellite data with Bolivia’s Municipal SDG Atlas (Andersen et al., 2020) to study geospatial development patterns across all 339 municipalities. The project demonstrates how machine learning models can predict SDG indicators from satellite features, achieving meaningful predictive accuracy for poverty and energy access indicators.\nThe Data: Cross-sectional dataset covering 339 Bolivian municipalities with over 350 variables, including:\n\nDevelopment outcomes: Municipal Sustainable Development Index (IMDS, 0-100 composite), individual SDG indices (SDG 1-17)\nSatellite data: Log nighttime lights per capita (2012-2020), 64 satellite embedding dimensions (2017)\nDemographics: Population (2001-2020), municipality and department names\nSocioeconomic indicators: Unsatisfied basic needs, literacy rates, electricity coverage, health outcomes\n\nYour Task: Use the descriptive analysis and regression tools from Chapter 1 to explore the DS4Bolivia dataset. You’ll investigate whether nighttime lights predict municipal development, visualize satellite-development relationships, and begin to assess how useful remote sensing data is for SDG monitoring. This case study introduces a dataset that we will revisit throughout the textbook, applying increasingly sophisticated econometric methods in each chapter.\n\nKey Concept 1.9: Satellite Data as Economic Proxy\nNighttime lights (NTL) captured by satellites measure the intensity of artificial illumination on Earth’s surface. Because lighting requires electricity and economic activity, NTL intensity strongly correlates with GDP, income levels, and urbanization. The log of NTL per capita transforms the highly skewed raw luminosity into a more symmetric variable suitable for regression analysis.\nSatellite embeddings are 64-dimensional feature vectors extracted by deep learning models from daytime satellite imagery. Each dimension captures abstract visual patterns (building density, road networks, vegetation) that correlate with socioeconomic conditions. Together, NTL and embeddings provide complementary information: NTL captures nighttime economic activity while embeddings capture daytime physical infrastructure.\n\n\nLoad the DS4Bolivia Data\nLet’s load the comprehensive DS4Bolivia dataset directly from GitHub. This dataset integrates satellite data, SDG indicators, and demographic information for all 339 Bolivian municipalities.\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Display basic information\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA DATASET\")\nprint(\"=\" * 70)\nprint(f\"Dataset shape: {bol.shape[0]} municipalities, {bol.shape[1]} variables\")\nprint(f\"\\nDepartments: {bol['dep'].nunique()} unique departments\")\nprint(f\"Department names: {sorted(bol['dep'].unique())}\")\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017', 'pop2017',\n            'index_sdg1', 'index_sdg4', 'index_sdg8', 'sdg1_1_ubn']\nbol_key = bol[key_vars].copy()\n\nprint(f\"\\nKey variables selected: {len(key_vars)}\")\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FIRST 10 MUNICIPALITIES\")\nprint(\"=\" * 70)\nprint(bol_key.head(10))\n\n\n# Variable descriptions for this case study\nprint(\"=\" * 70)\nprint(\"KEY VARIABLE DESCRIPTIONS\")\nprint(\"=\" * 70)\ndescriptions = {\n    'mun': 'Municipality name',\n    'dep': 'Department (administrative region, 9 total)',\n    'imds': 'Municipal Sustainable Development Index (0-100, composite of all SDGs)',\n    'ln_NTLpc2017': 'Log of nighttime lights per capita (2017, satellite-based)',\n    'pop2017': 'Population in 2017',\n    'index_sdg1': 'SDG 1 Index: No Poverty (0-100)',\n    'index_sdg4': 'SDG 4 Index: Quality Education (0-100)',\n    'index_sdg8': 'SDG 8 Index: Decent Work and Economic Growth (0-100)',\n    'sdg1_1_ubn': 'Unsatisfied Basic Needs (% of population, 2012)',\n}\nfor var, desc in descriptions.items():\n    print(f\"  {var:20s} — {desc}\")\n\n\n\nTask 1: Data Exploration (Guided)\nObjective: Understand the DS4Bolivia dataset structure and key variables.\nInstructions:\n\nExamine the output above: How many municipalities? How many departments?\nCheck for missing values in the key variables\nIdentify the range of the IMDS index (development measure)\nExplore the distribution of departments (how many municipalities per department?)\n\nKey variables to focus on:\n\nimds: Overall development index (our main dependent variable)\nln_NTLpc2017: Log nighttime lights per capita (our main predictor)\ndep: Department (for regional comparisons)\npop2017: Population (for context)\n\nRun the code below to explore the data structure.\n\n# Your code here: Explore the DS4Bolivia dataset\n#\n# Suggested explorations:\n# 1. Check for missing values: bol_key.isnull().sum()\n# 2. Municipalities per department: bol_key['dep'].value_counts()\n# 3. Range of IMDS: bol_key['imds'].describe()\n# 4. Largest/smallest municipalities: bol_key.nlargest(5, 'pop2017')\n\n# Example: Check missing values\nprint(\"Missing values per variable:\")\nprint(bol_key.isnull().sum())\nprint(f\"\\nTotal municipalities: {len(bol_key)}\")\nprint(f\"Complete cases (no missing in key vars): {bol_key.dropna().shape[0]}\")\n\n\n\nTask 2: Descriptive Statistics (Guided)\nObjective: Generate summary statistics for key development and satellite variables.\nInstructions:\n\nCalculate descriptive statistics for imds, ln_NTLpc2017, and pop2017\nIdentify the municipality with the highest and lowest IMDS\nCompare average IMDS across departments\nDiscuss what the summary statistics reveal about inequality across municipalities\n\nApply what you learned in section 1.4: Use .describe() and .groupby() methods like we did with the house price data.\n\n# Your code here: Descriptive statistics for DS4Bolivia\n#\n# Steps:\n# 1. Summary statistics for key variables\n# 2. Identify top/bottom municipalities\n# 3. Compare departments\n\n# Example: Summary statistics\nprint(\"=\" * 70)\nprint(\"DESCRIPTIVE STATISTICS: KEY VARIABLES\")\nprint(\"=\" * 70)\nprint(bol_key[['imds', 'ln_NTLpc2017', 'pop2017', 'sdg1_1_ubn']].describe().round(2))\n\n# Top and bottom municipalities by IMDS\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TOP 5 MUNICIPALITIES BY DEVELOPMENT (IMDS)\")\nprint(\"=\" * 70)\nprint(bol_key.nlargest(5, 'imds')[['mun', 'dep', 'imds', 'ln_NTLpc2017']].to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"BOTTOM 5 MUNICIPALITIES BY DEVELOPMENT (IMDS)\")\nprint(\"=\" * 70)\nprint(bol_key.nsmallest(5, 'imds')[['mun', 'dep', 'imds', 'ln_NTLpc2017']].to_string(index=False))\n\n\nKey Concept 1.10: Subnational Development Analysis\nNational-level statistics can mask enormous variation in development outcomes within a country. Bolivia’s 339 municipalities span a wide range of development levels—from highly urbanized departmental capitals with strong infrastructure to remote rural communities with limited services. Municipality-level analysis reveals this within-country inequality and helps identify specific areas where SDG progress lags behind. Satellite data is particularly valuable for subnational analysis because it provides spatially granular measurements even where traditional surveys are scarce or infrequent.\n\n\n\nTask 3: Visualize the NTL-Development Relationship (Semi-guided)\nObjective: Create scatter plots to visualize the relationship between nighttime lights and development.\nInstructions:\n\nCreate a scatter plot of ln_NTLpc2017 (x-axis) vs imds (y-axis)\nAdd appropriate axis labels and title\nOptionally: Color-code points by department\nInterpret the pattern: Is there a positive relationship? How strong does it look?\n\nApply what you learned in section 1.5: Use matplotlib to create scatter plots like the house price visualization.\nHint: Drop missing values before plotting with .dropna()\n\n# Your code here: Scatter plot of NTL vs Development\n#\n# Steps:\n# 1. Prepare data (drop missing values)\n# 2. Create scatter plot\n# 3. Add labels and formatting\n# 4. Interpret the pattern\n\n# Example structure:\n# plot_data = bol_key[['ln_NTLpc2017', 'imds']].dropna()\n# fig, ax = plt.subplots(figsize=(10, 6))\n# ax.scatter(plot_data['ln_NTLpc2017'], plot_data['imds'], alpha=0.5, color='navy')\n# ax.set_xlabel('Log Nighttime Lights per Capita (2017)')\n# ax.set_ylabel('Municipal Development Index (IMDS)')\n# ax.set_title('Can Satellites See Development? NTL vs IMDS in Bolivia')\n# plt.show()\n\n\n\nTask 4: Simple Regression Analysis (Semi-guided)\nObjective: Estimate the relationship between nighttime lights and development using OLS.\nResearch Question: How much does nighttime light intensity predict municipal development levels?\nInstructions:\n\nPrepare regression data (drop missing values in key variables)\nEstimate OLS regression: imds ~ ln_NTLpc2017\nDisplay the regression summary\nInterpret the slope coefficient: What does a 1-unit increase in log NTL mean for IMDS?\nReport and interpret R-squared: How much variation in development does NTL explain?\n\nApply what you learned in sections 1.6-1.7: Use ols() from statsmodels.\n\n# Your code here: OLS regression of IMDS on NTL\n#\n# Steps:\n# 1. Prepare data\n# 2. Estimate regression\n# 3. Display and interpret results\n\n# Example structure:\n# reg_data = bol_key[['imds', 'ln_NTLpc2017']].dropna()\n# model_bol = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()\n# print(model_bol.summary())\n#\n# # Extract key statistics\n# print(f\"\\nSlope: {model_bol.params['ln_NTLpc2017']:.4f}\")\n# print(f\"R-squared: {model_bol.rsquared:.4f}\")\n# print(f\"\\nInterpretation: A 1-unit increase in log NTL per capita\")\n# print(f\"is associated with a {model_bol.params['ln_NTLpc2017']:.2f}-point increase in IMDS\")\n\n\n\nTask 5: Regional Comparison (Independent)\nObjective: Compare development and NTL patterns across Bolivia’s nine departments.\nResearch Question: Do satellite-development patterns vary across Bolivia’s regions?\nInstructions:\n\nCalculate mean IMDS and mean NTL by department\nCreate a bar chart or dot plot comparing department averages\nIdentify which departments are the most and least developed\nCreate scatter plots colored by department to see if the NTL-IMDS relationship differs by region\nDiscuss what might explain regional differences (geography, urbanization, economic structure)\n\nThis extends Chapter 1 concepts: You’re using grouping and comparative analysis to explore heterogeneity.\n\n# Your code here: Regional comparison\n#\n# Steps:\n# 1. Group by department: bol_key.groupby('dep')[['imds', 'ln_NTLpc2017']].mean()\n# 2. Create comparative bar chart\n# 3. Create scatter plot colored by department\n# 4. Identify top/bottom departments\n\n# Example structure:\n# dept_means = bol_key.groupby('dep')[['imds', 'ln_NTLpc2017']].mean().sort_values('imds')\n# print(dept_means.round(2))\n#\n# fig, ax = plt.subplots(figsize=(10, 6))\n# dept_means['imds'].plot(kind='barh', ax=ax, color='purple', alpha=0.7)\n# ax.set_xlabel('Mean IMDS')\n# ax.set_title('Average Municipal Development by Department')\n# plt.tight_layout()\n# plt.show()\n\n\n\nTask 6: Policy Brief on Satellite Data for SDG Monitoring (Independent)\nObjective: Write a 200-300 word policy brief summarizing your findings.\nYour brief should address:\n\nKey finding: What is the relationship between nighttime lights and municipal development in Bolivia?\nMagnitude: How strong is the association? What does the R-squared tell us about predictive power?\nRegional variation: Do some departments show higher development levels? Is there a geographic pattern?\nPolicy implications: How could satellite data be used for SDG monitoring in Bolivia?\nLimitations: What can satellite data not tell us about development? What other data sources are needed?\n\nConnection to Research: The DS4Bolivia project uses machine learning (Random Forest, XGBoost) to predict SDG indicators from satellite embeddings, achieving R² up to 0.57 for extreme energy poverty. Your simple OLS regression provides a baseline for understanding how much satellite data captures about development outcomes.\nLooking ahead: In subsequent chapters, we will revisit this dataset to:\n\nSummarize the distribution of development indicators (Chapter 2)\nTest whether development differences are statistically significant (Chapter 4)\nExplore bivariate relationships between NTL and specific SDG outcomes (Chapter 5)\nAdd multiple satellite features as predictors (Chapters 10-12)\nTest for regional structural differences (Chapter 14)\nCheck model assumptions and diagnostics (Chapter 16)\nAnalyze NTL panel data over time (Chapter 17)\n\n\n# Your code here: Additional analysis for the policy brief\n#\n# You might want to:\n# 1. Create a summary table of key results\n# 2. Generate a visualization that tells a compelling story\n# 3. Calculate specific statistics to cite in your brief\n\n# Example: Summary of key results\n# print(\"KEY RESULTS FOR POLICY BRIEF\")\n# print(f\"Sample: {len(reg_data)} municipalities\")\n# print(f\"NTL coefficient: {model_bol.params['ln_NTLpc2017']:.2f}\")\n# print(f\"R-squared: {model_bol.rsquared:.2%}\")\n# print(f\"Most developed department: {dept_means['imds'].idxmax()}\")\n# print(f\"Least developed department: {dept_means['imds'].idxmin()}\")\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this exploration of satellite data and municipal development in Bolivia, you’ve applied the Chapter 1 toolkit to a cutting-edge research application:\n\nData loading and exploration: Worked with a real geospatial dataset covering 339 municipalities\nDescriptive statistics: Summarized development indicators and identified high/low performers\nVisualization: Created scatter plots revealing the satellite-development relationship\nRegression analysis: Quantified how nighttime lights predict development outcomes\nRegional comparison: Explored how the relationship varies across Bolivia’s departments\nCritical thinking: Assessed the potential and limitations of satellite data for SDG monitoring\n\nConnection to the research: The DS4Bolivia project extends this simple analysis by incorporating 64-dimensional satellite embeddings and advanced machine learning methods. Your OLS baseline provides the foundation for understanding what these more complex models improve upon.\nThis dataset returns throughout the textbook: Each subsequent chapter applies its specific econometric tools to the DS4Bolivia data, building progressively from univariate summaries (Chapter 2) through panel data analysis (Chapter 17). By the end of the book, you’ll have a comprehensive econometric analysis of satellite-based development prediction.\n\nWell done! You’ve now explored two real-world datasets—cross-country convergence and Bolivian municipal development—using the fundamental tools of econometrics.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Analysis of Economics Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to univariate data analysis using Python. You’ll learn how to summarize and visualize single-variable datasets using summary statistics and various chart types.\nUnivariate data consists of observations on a single variable—for example, annual earnings, individual income, or GDP over time. This chapter teaches you how to summarize and visualize such data effectively.\nWhat you’ll learn:\nDatasets used:\nChapter outline:",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#chapter-overview",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#chapter-overview",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "",
    "text": "Calculate summary statistics (mean, median, standard deviation, quartiles, skewness, kurtosis)\nCreate visualizations for numerical data (box plots, histograms, kernel density estimates, line charts)\nVisualize categorical data (bar charts, pie charts)\nApply data transformations (logarithms, standardization)\nWork with time series transformations (moving averages, seasonal adjustment)\n\n\n\nAED_EARNINGS.DTA: Annual earnings for 171 full-time working women aged 30 in 2010\nAED_REALGDPPC.DTA: U.S. quarterly real GDP per capita from 1959 to 2020\nAED_HEALTHCATEGORIES.DTA: U.S. health expenditures by category in 2018\nAED_FISHING.DTA: Fishing site choices for 1,182 fishers\nAED_MONTHLYHOMESALES.DTA: Monthly U.S. home sales from 1999 to 2015\n\n\n\n2.1 Summary Statistics for Numerical Data\n2.2 Charts for Numerical Data\n2.3 Charts for Numerical Data by Category\n2.4 Charts for Categorical Data\n2.5 Data Transformation\n2.6 Data Transformations for Time Series Data\n2.7 Practice Exercises\n2.8 Case Studies",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#setup",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#setup",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "Setup",
    "text": "Setup\nRun this cell first to import all required packages and configure the environment.\n\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL (data streams directly from here)\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Optional: Create directories for saving outputs locally\nIMAGES_DIR = 'images'\nTABLES_DIR = 'tables'\nos.makedirs(IMAGES_DIR, exist_ok=True)\nos.makedirs(TABLES_DIR, exist_ok=True)\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"✓ Setup complete! All packages imported successfully.\")\nprint(f\"✓ Random seed set to {RANDOM_SEED} for reproducibility.\")\nprint(f\"✓ Data will stream from: {GITHUB_DATA_URL}\")\n\n✓ Setup complete! All packages imported successfully.\n✓ Random seed set to 42 for reproducibility.\n✓ Data will stream from: https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#summary-statistics-for-numerical-data",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#summary-statistics-for-numerical-data",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.1 Summary Statistics for Numerical Data",
    "text": "2.1 Summary Statistics for Numerical Data\nSummary statistics provide a concise numerical description of a dataset. For a sample of size \\(n\\), observations are denoted: \\[x_1, x_2, \\ldots, x_n\\]\nKey summary statistics:\n\nMean (average): \\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nMedian: The middle value when data are ordered\nStandard deviation: Measures the spread of data around the mean \\[s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\nQuartiles: Values that divide ordered data into fourths (25th, 50th, 75th percentiles)\nSkewness: Measures asymmetry of the distribution (positive = right-skewed)\nKurtosis: Measures heaviness of distribution tails (higher = fatter tails)\n\nEconomic Example: We’ll examine annual earnings for full-time working women aged 30 in 2010.\n\nLoad Earnings Data\n\n# Load the earnings dataset from GitHub\ndata_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS.DTA')\n\nprint(f\"✓ Data loaded successfully!\")\nprint(f\"  Shape: {data_earnings.shape[0]} observations, {data_earnings.shape[1]} variables\")\nprint(\"\\nFirst 5 observations:\")\nprint(data_earnings.head())\n\n✓ Data loaded successfully!\n  Shape: 171 observations, 4 variables\n\nFirst 5 observations:\n   earnings  education  age  gender\n0     25000         14   30     0.0\n1     40000         12   30     0.0\n2     25000         13   30     0.0\n3     38000         13   30     0.0\n4     28800         12   30     0.0\n\n\n\n\nSummary Statistics\n\n# Basic summary statistics\nprint(\"=\" * 70)\nprint(\"Basic Descriptive Statistics\")\nprint(\"=\" * 70)\nprint(data_earnings.describe())\n\n# Detailed statistics for earnings\nearnings = data_earnings['earnings']\n\nstats_dict = {\n    'Count': len(earnings),\n    'Mean': earnings.mean(),\n    'Std Dev': earnings.std(),\n    'Min': earnings.min(),\n    '25th percentile': earnings.quantile(0.25),\n    'Median': earnings.median(),\n    '75th percentile': earnings.quantile(0.75),\n    'Max': earnings.max(),\n    'Skewness': stats.skew(earnings),\n    'Kurtosis': stats.kurtosis(earnings)  # Excess kurtosis (raw kurtosis - 3)\n}\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary Statistics for Earnings\")\nprint(\"=\" * 70)\nfor key, value in stats_dict.items():\n    if key in ['Count']:\n        print(f\"{key:20s}: {value:,.0f}\")\n    elif key in ['Skewness', 'Kurtosis']:\n        print(f\"{key:20s}: {value:.2f}\")\n    else:\n        print(f\"{key:20s}: ${value:,.2f}\")\n\n======================================================================\nBasic Descriptive Statistics\n======================================================================\n            earnings   education    age  gender  lnearnings\ncount     171.000000  171.000000  171.0   171.0  171.000000\nmean    41412.690058   14.432749   30.0     0.0   10.457638\nstd     25527.053396    2.735364    0.0     0.0    0.622062\nmin      1050.000000    3.000000   30.0     0.0    6.956545\n25%     25000.000000   12.000000   30.0     0.0   10.126631\n50%     36000.000000   14.000000   30.0     0.0   10.491274\n75%     49000.000000   16.000000   30.0     0.0   10.799367\nmax    172000.000000   20.000000   30.0     0.0   12.055250\n\n======================================================================\nSummary Statistics for Earnings\n======================================================================\nCount               : 171\nMean                : $41,412.69\nStd Dev             : $25,527.05\nMin                 : $1,050.00\n25th percentile     : $25,000.00\nMedian              : $36,000.00\n75th percentile     : $49,000.00\nMax                 : $172,000.00\nSkewness            : 1.71\nKurtosis            : 4.32\n\n\nKey findings from the 171 full-time working women aged 30:\n1. Central Tendency - Mean vs Median:\n\nMean = $41,412.69: The arithmetic average\nMedian = $36,000: The middle value\nGap = $5,412 (mean is 15% higher than median)\nWhy? This signals right skewness—some high earners pull the mean upward\n\n2. Spread - Standard Deviation:\n\nStd Dev = $25,527.05\nThis equals 61.6% of the mean (substantial variation)\nRule of thumb: About 68% of women earn within $41,413 ± $25,527 = $15,886 to $66,940\n\n3. Range and Quartiles:\n\nMinimum = $1,050 (possibly part-time misclassified, or very low earner)\n25th percentile = $25,000 (bottom quarter earns ≤$25k)\n75th percentile = $49,000 (top quarter earns ≥$49k)\nMaximum = $172,000 (highest earner makes 164× more than lowest!)\nInterquartile range (IQR) = $24,000 (middle 50% span $24k)\n\n4. Shape Measures:\n\nSkewness = 1.71 (strongly positive)\n\nValues &gt; 1 indicate strong right skew\nInterpretation guidelines:\n\n|Skewness| &lt; 0.5: approximately symmetric\n0.5 &lt; |Skewness| &lt; 1: moderately skewed\n|Skewness| &gt; 1: highly skewed (like our data)\n\nLong right tail with high earners\nDistribution is NOT symmetric\n\nKurtosis = 4.32 (excess kurtosis, compared to normal = 0)\n\nNote: Python’s scipy.stats.kurtosis() reports excess kurtosis by default (raw kurtosis - 3)\nRaw kurtosis = 7.32; Excess kurtosis = 4.32 (what we report here)\nNormal distribution has raw kurtosis = 3, so excess kurtosis = 0\nHeavier tails than normal distribution\nMore extreme values than a bell curve would predict\nInterpretation guidelines (excess kurtosis):\n\nExcess kurtosis &lt; 0: light tails (platykurtic)\nExcess kurtosis ≈ 0: normal tails (mesokurtic)\nExcess kurtosis &gt; 0: heavy tails (leptokurtic - like our data)\n\nGreater chance of outliers\n\n\nEconomic interpretation: Earnings distributions are typically right-skewed because:\n\nLower bound exists: Can’t earn less than zero (or minimum wage)\nNo upper bound: Some professionals earn very high incomes\nLabor market structure: Most workers cluster around median, but executives, specialists, and entrepreneurs create a long right tail\n\nPractical implication: The median (36,000) is a better measure of “typical” earnings than the mean (41,413) because it’s not inflated by high earners.\n\nKey Concept 2.1: Summary Statistics\nSummary statistics condense datasets into interpretable measures of central tendency (mean, median) and dispersion (standard deviation, quartiles). The median is more robust to outliers than the mean, making it preferred for skewed distributions common in economic data like earnings and wealth.\n\n\n\nBox Plot\nA box plot (or box-and-whisker plot) visualizes key summary statistics:\n\nBox: Extends from the 25th to 75th percentile (interquartile range)\nRed line: Median (50th percentile)\nWhiskers: Extend to minimum and maximum values (or 1.5 × IQR from quartiles)\nDots: Outliers beyond the whiskers\n\n\n# Create box plot of earnings\nfig, ax = plt.subplots(figsize=(10, 6))\nbp = ax.boxplot(earnings, vert=False, patch_artist=True,\n                boxprops=dict(facecolor='lightblue', alpha=0.7),\n                medianprops=dict(color='red', linewidth=2))\nax.set_xlabel('Annual earnings (in dollars)', fontsize=12)\nax.set_title('Box Plot of Annual Earnings', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 The box plot shows:\")\nprint(\"   - Most earnings are between $25,000 and $50,000 (the box)\")\nprint(\"   - The median ($36,000) is closer to the lower quartile\")\nprint(\"   - Several high-earning outliers on the right\")\n\n\n\n\n\n\n\n\n\n📊 The box plot shows:\n   - Most earnings are between $25,000 and $50,000 (the box)\n   - The median ($36,000) is closer to the lower quartile\n   - Several high-earning outliers on the right\n\n\nWhat the box plot reveals:\n1. The Box (Interquartile Range):\n\nExtends from $25,000 (Q1) to $49,000 (Q3)\nContains the middle 50% of earners\nWidth of $24,000 shows moderate spread in the middle\n\n2. The Red Line (Median):\n\nLocated at $36,000\nPositioned closer to the LOWER edge of the box\nThis leftward position confirms right skewness\n\n3. The Whiskers:\n\nLower whisker extends to $1,050 (minimum)\nUpper whisker extends to outliers\nRight whisker is MUCH longer than left whisker (asymmetry)\n\n4. The Outliers (dots on right):\n\nSeveral points beyond the upper whisker\nRepresent high earners (likely $100k+)\nMaximum at $172,000 is an extreme outlier\n\nVisual insights:\n\nNOT symmetric: If symmetric, median would be in center of box\nRight tail dominates: Upper whisker + outliers extend much farther than lower whisker\nConcentration: Most data packed in the 25k-49k range\nRare extremes: A few very high earners create the long right tail\n\nComparison to summary statistics:\n\nBox plot VISUALLY confirms what skewness (1.71) told us numerically\nQuartiles (25k, 36k, 49k) match the box structure\nOutliers explain why kurtosis (4.32) is high—heavy tails\n\nEconomic story: The typical woman in this sample earns 25k-49k, but a small group of high earners (doctors, lawyers, executives?) creates substantial inequality within this age-30 cohort.\nTransition: Now that we’ve calculated summary statistics for earnings data, let’s explore how visualizations can reveal patterns that numbers alone might miss. Charts make distributions, outliers, and trends immediately visible.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#charts-for-numerical-data",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#charts-for-numerical-data",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.2 Charts for Numerical Data",
    "text": "2.2 Charts for Numerical Data\nBeyond summary statistics, visualizations reveal patterns in data that numbers alone might miss.\nCommon charts for numerical data:\n\nHistogram: Shows the frequency distribution by grouping data into bins\nKernel density estimate: A smoothed histogram that estimates the underlying continuous distribution\nLine chart: For ordered data (especially time series)\n\nBin width matters: Wider bins give a coarse overview; narrower bins show more detail but can be noisy.\n\nHistograms with Different Bin Widths\n\n# Create histograms with different bin widths\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Wider bins ($15,000)\naxes[0].hist(earnings, bins=range(0, int(earnings.max()) + 15000, 15000),\n             edgecolor='black', alpha=0.7, color='steelblue')\naxes[0].set_xlabel('Annual Earnings (in dollars)', fontsize=11)\naxes[0].set_ylabel('Frequency', fontsize=11)\naxes[0].set_title('Panel A: Bin width = $15,000', fontsize=12, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Narrower bins ($7,500)\naxes[1].hist(earnings, bins=range(0, int(earnings.max()) + 7500, 7500),\n             edgecolor='black', alpha=0.7, color='steelblue')\naxes[1].set_xlabel('Annual Earnings (in dollars)', fontsize=11)\naxes[1].set_ylabel('Frequency', fontsize=11)\naxes[1].set_title('Panel B: Bin width = $7,500', fontsize=12, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Histograms of Annual Earnings', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Comparing bin widths:\")\nprint(\"   - Panel A (wider bins): Shows overall shape—most earnings are $15k-$45k\")\nprint(\"   - Panel B (narrower bins): Reveals more detail—peaks around $25k-$30k\")\n\n\n\n\n\n\n\n\n\n📊 Comparing bin widths:\n   - Panel A (wider bins): Shows overall shape—most earnings are $15k-$45k\n   - Panel B (narrower bins): Reveals more detail—peaks around $25k-$30k\n\n\nPanel A: Wider bins ($15,000):\n\nReveals overall shape: Right-skewed distribution with long right tail\nPeak location: Highest bar is in the $15k-$30k range\nPattern: Frequencies decline as earnings increase\nAdvantages: Simple, clear overall pattern, less “noisy”\nDisadvantages: Hides fine details, obscures multiple modes\n\nPanel B: Narrower bins ($7,500):\n\nReveals more detail: Multiple peaks visible within the distribution\nPeak location: Clearer concentration around $22.5k-$30k\nSecondary peaks: Visible around $37.5k-$45k (possible clustering at round numbers?)\nAdvantages: Shows fine structure, reveals potential clustering\nDisadvantages: More “jagged,” can look noisy\n\nKey observations across both panels:\n\nRight skewness confirmed: Both histograms show long right tail extending to $172,000\nModal region: Most common earnings are in the $15k-$45k range\n\nThis contains ~75% of observations\nConsistent with Q1 ($25k) and Q3 ($49k)\n\nSparse right tail: Very few observations above $90k\n\nBut these high earners substantially influence the mean\nThis is why mean ($41,413) &gt; median ($36,000)\n\nBin width matters: - Too wide: Oversimplifies, may miss important features\n\nToo narrow: Introduces noise, harder to see overall pattern\nRule of thumb: Try multiple bin widths to understand your data\n\n\nEconomic interpretation: The clustering in the $25k-$45k range likely reflects:\n\nEntry-level professional salaries for college graduates\nRegional wage variations within the sample\nOccupational differences (teachers vs. nurses vs. business professionals)\nExperience effects (all are age 30, but different career progressions)\n\nStatistical lesson: Always experiment with bin widths in histograms—different choices reveal different aspects of the data!\n\nKey Concept 2.2: Histograms and Density Plots\nHistograms visualize distributions using bins whose width determines the level of detail. Kernel density estimates provide smooth approximations of the underlying distribution, while line charts are ideal for time series data to show trends and patterns over time.\n\n\n\nKernel Density Estimate\nA kernel density estimate (KDE) is a smoothed version of a histogram. It estimates the underlying continuous probability density function.\nAdvantages:\n\nSmooth, continuous curve (no arbitrary bin edges)\nEasier to see the overall shape\nCan compare to theoretical distributions (e.g., normal distribution)\n\nHow it works: Instead of fixed bins, KDE uses overlapping “windows” that give more weight to nearby observations.\n\n# Create kernel density estimate\nfig, ax = plt.subplots(figsize=(10, 6))\nearnings.plot.kde(ax=ax, linewidth=2, color='darkblue', bw_method=0.3)\nax.set_xlabel('Annual Earnings (in dollars)', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Kernel Density Estimate of Earnings', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 The KDE shows:\")\nprint(\"   - Clear right skew (long tail to the right)\")\nprint(\"   - Peak around $30,000-$35,000\")\nprint(\"   - Distribution is NOT normal (normal would be symmetric and bell-shaped)\")\n\n\n\n\n\n\n\n\n\n📊 The KDE shows:\n   - Clear right skew (long tail to the right)\n   - Peak around $30,000-$35,000\n   - Distribution is NOT normal (normal would be symmetric and bell-shaped)\n\n\nWhat is KDE showing?\nThe KDE is a smooth, continuous estimate of the probability density function—think of it as a “smoothed histogram without arbitrary bins.”\nKey features of the earnings KDE:\n1. Peak (Mode): - Highest density around $30,000-$35,000 - This is the most “probable” earnings level - Slightly below the median ($36,000), consistent with right skew\n2. Shape: - Clear right skew: Long tail extending to $172,000 - NOT bell-shaped: Would be symmetric if normally distributed - Unimodal: Single dominant peak (not bimodal) - Steep left side: Density drops quickly below $20k - Gradual right side: Density tapers slowly above $50k\n3. Tail behavior: - Left tail: Short and bounded (can’t go below ~$0) - Right tail: Long and heavy (extends to $172k) - Asymmetry ratio: Right tail is ~5× longer than left tail\n4. Concentration: - Most density (probability mass) is between $15k-$60k - Above $80k, density is very low but not zero - This confirms that high earners are rare but present\nComparison to normal distribution: If earnings were normally distributed, the KDE would be: - Symmetric (it’s not—it’s right-skewed) - Bell-shaped (it’s not—it’s asymmetric) - Same mean and median (they differ by $5,413) - Unimodal (it’s not—there is another peak around $70k)\nAdvantages of KDE over histograms: 1. No arbitrary bins: Smooth curve independent of bin choices 2. Shows probability density: Y-axis represents likelihood, not counts 3. Easier to compare: Can overlay multiple KDEs (e.g., male vs. female earnings) 4. Professional appearance: Smooth curves for publications\nStatistical insight: The KDE reveals that earnings are NOT normally distributed—they follow a log-normal-like distribution common in economic data. This justifies logarithmic transformations (see Section 2.5) for statistical modeling.\nPractical implication: When predicting earnings, the “most likely” value is around $30k-$35k, NOT the mean ($41,413). The mean is inflated by rare high earners.\n\n\nTime Series Plot\nLine charts are ideal for time series data—observations ordered by time. They show how a variable changes over time.\nExample: U.S. real GDP per capita from 1959 to 2020 (in constant 2012 dollars). This measures average economic output per person, adjusted for inflation.\n\n# Load GDP data\ndata_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_REALGDPPC.DTA')\n\nprint(\"GDP Data Summary:\")\nprint(data_gdp[['realgdppc', 'year']].describe())\n\n# Create time series plot\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(data_gdp['daten'], data_gdp['realgdppc'], linewidth=2, color='darkblue')\nax.set_xlabel('Year', fontsize=12)\nax.set_ylabel('Real GDP per capita (in 2012 dollars)', fontsize=12)\nax.set_title('U.S. Real GDP per Capita', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Key observations:\")\nprint(\"   - Real GDP per capita TRIPLED from 1959 to 2019\")\nprint(\"   - Steady upward trend (economic growth)\")\nprint(\"   - Dips during recessions (early 1980s, 2008-2009, 2020)\")\n\nGDP Data Summary:\n          realgdppc         year\ncount    245.000000   245.000000\nmean   37050.496094  1989.126587\nstd    12089.684570    17.717855\nmin    17733.257812  1959.000000\n25%    26562.724609  1974.000000\n50%    36929.011719  1989.000000\n75%    49318.171875  2004.000000\nmax    58392.453125  2020.000000\n\n\n\n\n\n\n\n\n\n\n📊 Key observations:\n   - Real GDP per capita TRIPLED from 1959 to 2019\n   - Steady upward trend (economic growth)\n   - Dips during recessions (early 1980s, 2008-2009, 2020)\n\n\nWhat this chart shows:\nU.S. real GDP per capita from 1959 to 2020, measured in constant 2012 dollars (inflation-adjusted).\nKey trends observed:\n1. Long-run growth:\n\n1959: ~$17,000 per person\n2020: ~$60,000 per person\nTotal growth: 253% increase (3.5× larger)\nAnnual growth rate: ~2.1% per year (compound)\n\n2. Business cycle patterns (recessions visible as dips):\n\nEarly 1960s: Mild slowdown\n1973-1975: Oil crisis recession (OPEC embargo)\n1980-1982: Double-dip recession (Volcker’s inflation fight)\n1990-1991: Gulf War recession (brief)\n2001: Dot-com bubble burst\n2008-2009: GREAT RECESSION (deepest post-war decline)\n\nGDP per capita fell from $55k to $51k\nTook until 2015 to recover pre-crisis level\n\n2020: COVID-19 pandemic (sharp, sudden drop)\n\n3. Trend characteristics:\n\nNot a straight line: Growth punctuated by recessions\nRecessions are temporary: Economy always recovers to trend\nGrowth is the norm: Upward drift dominates short-term fluctuations\nIncreasing volatility? Recent cycles seem larger (2008, 2020)\n\n4. Summary statistics from the data:\n\nMean GDP per capita: $37,941 (over full 1959-2020 period)\nMedian: $35,500 (slightly below mean due to recent growth)\nMin: ~$17,000 (1959 start)\nMax: ~$60,000 (pre-COVID peak 2019)\n\nEconomic interpretation:\nWhy does GDP per capita grow?\n\nTechnological progress: Better machines, software, processes\nCapital accumulation: More factories, infrastructure, equipment\nHuman capital: Better education, training, skills\nProductivity gains: Workers produce more per hour worked\n\nWhy the recessions?\n\nDemand shocks: Sudden drops in spending (2008 financial crisis, 2020 lockdowns)\nSupply shocks: Oil crises, disruptions (1973, 2020)\nPolicy errors: Monetary policy too tight (1980-82)\nFinancial crises: Credit crunches, asset bubbles bursting\n\nWhy does it matter?\n\nLiving standards: GDP per capita measures average prosperity\nReal vs. nominal: Chart uses 2012 dollars, so it’s REAL growth, not inflation\nPer capita matters: Total GDP could grow just from population increase; per capita shows individual prosperity\n\nStatistical lesson: Time series plots are essential for understanding economic trends, cycles, and structural breaks that cross-sectional data would miss.\n\nKey Concept 2.3: Time Series Visualization\nLine charts display time-ordered data, revealing trends, cycles, and structural breaks. For economic time series, visualizing the full historical context helps identify patterns like recessions, growth periods, and policy impacts.\n\nTransition: The previous section focused on visualizing single numerical variables. Now we shift to categorical breakdowns—how to display numerical data when it’s naturally divided into groups or categories (like health spending by service type).\nNow that we can visualize single-variable distributions, let’s see how distributions differ across categories.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#charts-for-numerical-data-by-category",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#charts-for-numerical-data-by-category",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.3 Charts for Numerical Data by Category",
    "text": "2.3 Charts for Numerical Data by Category\nSometimes numerical data are naturally divided into categories. For example, total health expenditures broken down by type of service.\nBar charts (or column charts) are the standard visualization:\n\nEach category gets a bar\nBar height represents the category’s value\nUseful for comparing values across categories\n\nExample: U.S. health expenditures in 2018 totaled $3,653 billion (18% of GDP), split across 13 categories.\n\n# Load health expenditure data\ndata_health = pd.read_stata(GITHUB_DATA_URL + 'AED_HEALTHCATEGORIES.DTA')\n\nprint(\"Health Expenditure Categories (2018)\")\nprint(data_health)\nprint(f\"\\nTotal expenditures: ${data_health['expenditures'].sum():,.0f} billion\")\n\nHealth Expenditure Categories (2018)\n                    category  expenditures  cat_short  exp_short\n0                   Hospital          1192   Hospital     1192.0\n1     Physician and clinical           726  Physician      726.0\n2         Other Professional           104      Other      104.0\n3                     Dental           136      Drugs      136.0\n4   Other Health & Personal            192                   NaN\n5           Home Health Care           102                   NaN\n6               Nursing Care           169                   NaN\n7           Drugs & Supplies           456                   NaN\n8       Govt. Administration            48                   NaN\n9         Net Cost Insurance           259                   NaN\n10       Govt. Public Health            94                   NaN\n11    Noncommercial Research            53                   NaN\n12    Structures & Equipment           122                   NaN\n\nTotal expenditures: $3,653 billion\n\n\n\nBar Chart\n\n# Create bar chart (sorted by expenditure)\nfig, ax = plt.subplots(figsize=(12, 6))\ndata_health_sorted = data_health.sort_values('expenditures', ascending=False)\nbars = ax.bar(range(len(data_health_sorted)), data_health_sorted['expenditures'],\n              color='steelblue', edgecolor='black', alpha=0.7)\nax.set_xticks(range(len(data_health_sorted)))\nax.set_xticklabels(data_health_sorted['category'], rotation=45, ha='right', fontsize=10)\nax.set_ylabel('Expenditures (in $ billions)', fontsize=12)\nax.set_title('U.S. Health Expenditures by Category (2018)',\n             fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Top 3 categories:\")\nprint(f\"   1. {data_health_sorted.iloc[0]['category']}: ${data_health_sorted.iloc[0]['expenditures']:.0f}B\")\nprint(f\"   2. {data_health_sorted.iloc[1]['category']}: ${data_health_sorted.iloc[1]['expenditures']:.0f}B\")\nprint(f\"   3. {data_health_sorted.iloc[2]['category']}: ${data_health_sorted.iloc[2]['expenditures']:.0f}B\")\n\n\n\n\n\n\n\n\n\n📊 Top 3 categories:\n   1. Hospital: $1192B\n   2. Physician and clinical: $726B\n   3. Drugs & Supplies: $456B\n\n\nTotal U.S. health spending in 2018: $3,653 billion (18% of GDP)\nTop 5 categories (ranked by spending):\n1. Hospital care: $1,192 billion (32.6%)\n\nBy far the largest category\nInpatient care, emergency rooms, outpatient hospital services\nDominated by labor costs (nurses, doctors, staff) and overhead\n\n2. Physician and clinical services: $726 billion (19.9%)\n\nDoctor visits, outpatient clinics, medical specialists\nSecond-largest but still 39% less than hospitals\nGrowing due to aging population and chronic disease management\n\n3. Drugs and supplies: $456 billion (12.5%)\n\nPrescription drugs, over-the-counter medications, medical supplies\nControversial due to high U.S. drug prices vs. other countries\nRising rapidly due to specialty biologics and new therapies\n\n4. Net cost of insurance: $259 billion (7.1%)\n\nAdministrative costs of private health insurance\nOverhead, marketing, profit margins\nDoes not include government administration (separate category)\n\n5. Other health and personal: $192 billion (5.3%)\n\nVarious services not classified elsewhere\nHome health aides, personal care, etc.\n\nBottom categories:\n\nGovernment administration: $48 billion (Medicare, Medicaid overhead)\nNoncommercial research: $53 billion (NIH, university research)\nGovernment public health: $94 billion (CDC, state/local health departments)\n\nKey insights:\n1. Hospital dominance:\n\nHospitals alone account for nearly 1/3 of all health spending\nMore than physicians, drugs, and nursing care COMBINED\nReflects high fixed costs of hospital infrastructure\n\n2. Concentration:\n\nTop 3 categories (Hospital, Physician, Drugs) = 65% of total\nMiddle 50% of spending across just 3 categories\nLong tail of smaller categories\n\n3. Administrative costs:\n\nInsurance administration ($259B) + Government admin ($48B) = $307B total\nThat’s 8.4% of health spending just on paperwork and administration\nFor comparison: Administrative costs are ~2% in single-payer systems\n\n4. Prevention vs. treatment:\n\nPublic health: $94B (2.6% of total)\nHospital care: $1,192B (32.6% of total)\nRatio: 12.7× more on treatment than prevention\n\nEconomic interpretation:\nWhy so expensive?\n\nLabor-intensive: Healthcare requires highly-trained, expensive workers\nTechnology: Advanced equipment and facilities are costly\nFragmentation: Multiple payers, complex billing increases administrative costs\nAging population: Older Americans consume more healthcare\nChronic diseases: Diabetes, heart disease, obesity drive spending\n\nInternational comparison: U.S. spends ~18% of GDP on healthcare vs. 9-12% in other developed countries, yet doesn’t have better health outcomes. Much debate centers on the efficiency of this spending.\nStatistical lesson: Bar charts are ideal for comparing categorical data—they make it immediately obvious that hospital care dominates U.S. health spending.\n\nKey Concept 2.4: Bar Charts for Categorical Data\nBar charts and column charts effectively display categorical data by using bar length to represent values. This makes comparisons across categories immediate and intuitive, highlighting which categories dominate.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#charts-for-categorical-data",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#charts-for-categorical-data",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.4 Charts for Categorical Data",
    "text": "2.4 Charts for Categorical Data\nCategorical data consist of observations that fall into discrete categories (e.g., fishing site choice: beach, pier, private boat, charter boat).\nHow to summarize:\n\nFrequency table: Count observations in each category\nRelative frequency: Express as proportions or percentages\n\nHow to visualize:\n\nPie chart: Slices represent proportion of total\nBar chart: Bars represent frequency or proportion\n\nExample: Fishing site chosen by 1,182 recreational fishers (4 possible sites).\n\n# Load fishing data\ndata_fishing = pd.read_stata(GITHUB_DATA_URL + 'AED_FISHING.DTA')\n\n# Create frequency table\nmode_freq = data_fishing['mode'].value_counts()\nmode_relfreq = data_fishing['mode'].value_counts(normalize=True)\nmode_table = pd.DataFrame({\n    'Frequency': mode_freq,\n    'Relative Frequency (%)': (mode_relfreq * 100).round(2)\n})\n\nprint(\"Frequency Distribution of Fishing Mode\")\nprint(mode_table)\nprint(f\"\\nTotal observations: {len(data_fishing):,}\")\n\nFrequency Distribution of Fishing Mode\n         Frequency  Relative Frequency (%)\nmode                                      \ncharter        452                   38.24\nprivate        418                   35.36\npier           178                   15.06\nbeach          134                   11.34\n\nTotal observations: 1,182\n\n\n\nPie Chart\n\n# Create pie chart\nfig, ax = plt.subplots(figsize=(8, 8))\ncolors = plt.cm.Set3(range(len(mode_freq)))\nwedges, texts, autotexts = ax.pie(mode_freq.values,\n                                    labels=mode_freq.index,\n                                    autopct='%1.1f%%',\n                                    colors=colors,\n                                    startangle=90,\n                                    textprops={'fontsize': 11})\nax.set_title('Distribution of Fishing Modes',\n             fontsize=14, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Most popular fishing modes:\")\nprint(f\"   1. {mode_freq.index[0]}: {mode_freq.values[0]:,} ({mode_relfreq.values[0]*100:.1f}%)\")\nprint(f\"   2. {mode_freq.index[1]}: {mode_freq.values[1]:,} ({mode_relfreq.values[1]*100:.1f}%)\")\n\n\n\n\n\n\n\n\n\n📊 Most popular fishing modes:\n   1. charter: 452 (38.2%)\n   2. private: 418 (35.4%)\n\n\nSample: 1,182 recreational fishers choosing among 4 fishing sites\nDistribution of choices:\n1. Charter boat: 452 fishers (38.2%)\n\nMost popular choice\nGuided fishing trip with captain and crew\nHigher cost but convenience, equipment provided, and expert guidance\n\n2. Private boat: 418 fishers (35.4%)\n\nSecond-most popular (nearly tied with charter)\nRequires boat ownership or rental\nMore freedom and privacy, but higher upfront costs\n\n3. Pier: 178 fishers (15.1%)\n\nThird choice\nLow-cost option (minimal equipment needed)\nAccessible, but limited fishing locations\n\n4. Beach: 134 fishers (11.3%)\n\nLeast popular\nLowest cost and most accessible\nBut more limited fishing success rates\n\nKey patterns:\n1. Boat fishing dominates:\n\nCharter + Private = 870 fishers (73.6%)\nNearly 3/4 of fishers prefer boat-based fishing\nSuggests willingness to pay premium for better fishing access\n\n2. Shore fishing is minority:\n\nPier + Beach = 312 fishers (26.4%)\nAbout 1/4 choose shore-based options\nLikely cost-constrained or casual fishers\n\n3. Charter vs. private nearly equal:\n\nCharter: 452 (38.2%)\nPrivate: 418 (35.4%)\nDifference: only 34 fishers (2.9%)\nSuggests these are close substitutes for many fishers\n\n4. Large variation in popularity:\n\nMost popular (Charter) is 3.4× more popular than least popular (Beach)\nNot evenly distributed across categories\nStrong revealed preferences for certain modes\n\nEconomic interpretation:\nWhy do people choose different modes?\nCharter boats chosen for:\n\nNo boat ownership required\nExpert captain knows best spots\nSocial experience (fishing with others)\nEquipment and bait provided\n\nPrivate boats chosen for:\n\nFlexibility in timing and location\nPrivacy and control\nCost-effective if you fish frequently\nPride of ownership\n\nPier/Beach chosen for:\n\nBudget constraints\nNo transportation to boat launch\nCasual, occasional fishing\nFamily-friendly accessibility\n\nRevealed preference theory: The distribution reveals what fishers VALUE:\n\n73.6% value boat access enough to pay for it\n38.2% value convenience of charter over ownership\n26.4% value low cost/accessibility over catch rates\n\nStatistical lesson: For categorical data, frequency tables and pie charts reveal the distribution of choices. This is the foundation for discrete choice models (Chapter 15) that estimate why people make different choices.\n\nKey Concept 2.5: Frequency Tables and Pie Charts\nCategorical data are summarized using frequency tables showing counts and percentages. Pie charts display proportions visually, with slice area corresponding to relative frequency. Bar charts are often preferred over pie charts for easier comparison of categories.\n\nTransition: Visualization helps us see patterns, but sometimes the raw data obscures relationships. Data transformations (like logarithms and z-scores) can normalize skewed distributions, stabilize variance, and make statistical modeling more effective.\nHaving explored charts for both numerical and categorical data, let’s now examine how data transformations can reveal hidden patterns.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#data-transformation",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#data-transformation",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.5 Data Transformation",
    "text": "2.5 Data Transformation\nData transformations can make patterns clearer or satisfy statistical assumptions.\n(a) Logarithmic transformation is especially useful for right-skewed economic data (prices, income, wealth): \\[\\text{log of earnings} = \\ln(\\text{earnings})\\]\nWhy use logs?\n\nConverts right-skewed data to a more symmetric distribution\nMakes multiplicative relationships additive\nCoefficients have percentage interpretation (see Chapter 9)\nReduces influence of extreme values\n\n(b) Standardized scores (z-scores) are another common transformation: \\[z_i = \\frac{x_i - \\bar{x}}{s}\\]\nThis centers data at 0 with standard deviation 1—useful for comparing variables on different scales.\n\nLog Transformation Effect\n\n# Create log transformation\ndata_earnings['lnearnings'] = np.log(data_earnings['earnings'])\n\nprint(\"Comparison of earnings and log(earnings):\")\nprint(data_earnings[['earnings', 'lnearnings']].describe())\n\nComparison of earnings and log(earnings):\n            earnings  lnearnings\ncount     171.000000  171.000000\nmean    41412.690058   10.457638\nstd     25527.053396    0.622062\nmin      1050.000000    6.956545\n25%     25000.000000   10.126631\n50%     36000.000000   10.491274\n75%     49000.000000   10.799367\nmax    172000.000000   12.055250\n\n\n\n# Compare original and log-transformed earnings\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Original earnings\naxes[0].hist(data_earnings['earnings'], bins=30,\n             edgecolor='black', alpha=0.7, color='steelblue')\naxes[0].set_xlabel('Annual Earnings (in dollars)', fontsize=11)\naxes[0].set_ylabel('Frequency', fontsize=11)\naxes[0].set_title('Panel A: Earnings', fontsize=12, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Log earnings\naxes[1].hist(data_earnings['lnearnings'], bins=30,\n             edgecolor='black', alpha=0.7, color='coral')\naxes[1].set_xlabel('Log of Annual Earnings', fontsize=11)\naxes[1].set_ylabel('Frequency', fontsize=11)\naxes[1].set_title('Panel B: Log(Earnings)', fontsize=12, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Log Transformation Effects',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Effect of log transformation:\")\nprint(\"   - Original earnings: Highly right-skewed\")\nprint(\"   - Log(earnings): Much more symmetric, closer to normal\")\nprint(f\"   - Skewness reduced from {stats.skew(earnings):.2f} to {stats.skew(data_earnings['lnearnings']):.2f}\")\n\n\n\n\n\n\n\n\n\n📊 Effect of log transformation:\n   - Original earnings: Highly right-skewed\n   - Log(earnings): Much more symmetric, closer to normal\n   - Skewness reduced from 1.71 to -0.91\n\n\nPanel A: Original Earnings (dollars)\n\nShape: Strongly right-skewed\nSkewness: 1.71 (highly asymmetric)\nMean: $41,412.69\nMedian: $36,000.00\nStd Dev: $25,527.05 (62% of mean)\nRange: $1,050 to $172,000\n\nPanel B: Log(Earnings) (natural logarithm)\n\nShape: Much more symmetric, approximately normal\nSkewness: -0.91 (nearly symmetric, slight left skew)\nMean: 10.46 (log dollars)\nMedian: 10.49 (log dollars)\nStd Dev: 0.62 (only 6% of mean)\nRange: 6.96 to 12.06\n\nWhat the transformation achieved:\n1. Reduced skewness dramatically:\n\nOriginal skewness: 1.71 → Log skewness: -0.91\nReduction of 122% in absolute skewness\nNow nearly symmetric (close to 0)\n\n2. Normalized the distribution:\n\nOriginal: Long right tail, NOT normal\nLog: Bell-shaped, MUCH closer to normal distribution\nThis matters for statistical tests that assume normality\n\n3. Equalized variance (stabilization):\n\nOriginal std dev: 62% of mean (high coefficient of variation)\nLog std dev: 6% of mean (much more stable)\nHigh earners no longer dominate the variance\n\n4. Brought mean and median closer:\n\nOriginal: Mean - Median = $5,413 (15% gap)\nLog: Mean - Median = -0.03 (0.3% gap)\nNearly identical in log scale\n\nWhy use log transformation for earnings?\nStatistical reasons:\n\nNormality: Many statistical tests (t-tests, ANOVA, regression) assume normal distribution\nVariance stabilization: Constant variance across income levels\nLinearity: Log models often fit better (log-linear relationships)\nOutlier reduction: Compresses extreme values\n\nEconomic reasons:\n\nMultiplicative relationships: Income growth is often proportional (e.g., 10% raise)\nPercentage interpretation: A 1-unit increase in log(income) ≈ 100% increase in income\nEconomic theory: Utility functions often logarithmic (diminishing marginal utility)\nCross-country comparisons: Log scale makes it easier to compare countries with vastly different GDP levels\n\nHow to interpret log(earnings) = 10.46?\n\nTake exponential: e^10.46 = $34,762\nThis is close to the median earnings ($36,000)\nEach 1-unit increase in log(earnings) ≈ 2.718× increase in earnings\n\nExample interpretation:\n\nLog(earnings) = 10.0 → Earnings = e^10.0 = $22,026\nLog(earnings) = 11.0 → Earnings = e^11.0 = $59,874\nDifference of 1 in log scale = 2.72× in dollar scale\n\nWhen NOT to use log transformation:\n\nWhen data include zero or negative values (log undefined)\nWhen you care about absolute differences (e.g., policy targeting specific dollar amounts)\nWhen original scale is more interpretable for your audience\n\nStatistical lesson: Log transformation is one of the most powerful tools in econometrics for dealing with skewed, multiplicative data like income, prices, GDP, and wealth.\n\nKey Concept 2.6: Logarithmic Transformations\nNatural logarithm transformations convert right-skewed economic data (earnings, prices, wealth) to more symmetric distributions, facilitating analysis. Z-scores standardize data to have mean 0 and standard deviation 1, enabling comparison across different scales.\n\nTransition: Time series data presents unique challenges—seasonal fluctuations, inflation, and population growth can mask underlying trends. Specialized transformations like moving averages and seasonal adjustment are essential for time-ordered economic data.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#data-transformations-for-time-series-data",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#data-transformations-for-time-series-data",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.6 Data Transformations for Time Series Data",
    "text": "2.6 Data Transformations for Time Series Data\nTime series data often require special transformations: 1. Moving averages: Smooth short-term fluctuations by averaging over several periods - Example: 11-month moving average removes monthly noise\n\nSeasonal adjustment: Remove predictable seasonal patterns\n\nExample: Home sales peak in summer, drop in winter\n\nReal vs. nominal adjustments: Adjust for inflation using price indices\n\nReal values are in constant dollars (e.g., 2012 dollars)\n\nPer capita adjustments: Divide by population to account for population growth\n\nExample: Monthly U.S. home sales (2005-2015) showing original, moving average, and seasonally adjusted series.\n\n# Load monthly home sales data\ndata_homesales = pd.read_stata(GITHUB_DATA_URL + 'AED_MONTHLYHOMESALES.DTA')\n\n# Filter data for year &gt;= 2005\ndata_homesales_filtered = data_homesales[data_homesales['year'] &gt;= 2005]\n\nprint(\"Home sales data (2005 onwards):\")\nprint(data_homesales_filtered[['year', 'exsales', 'exsales_ma11', 'exsales_sa']].describe())\n\nHome sales data (2005 onwards):\n              year        exsales   exsales_ma11     exsales_sa\ncount   121.000000     121.000000     116.000000     121.000000\nmean   2009.545410  416851.239669  418300.125000  418071.625000\nstd       2.915478  111931.278110   81757.234375   83326.726562\nmin    2005.000000  218000.000000  324818.187500  287500.000000\n25%    2007.000000  347000.000000  357613.625000  355833.343750\n50%    2010.000000  401000.000000  394000.000000  401666.656250\n75%    2012.000000  473000.000000  436727.281250  440833.343750\nmax    2015.000000  754000.000000  608545.437500  605000.000000\n\n\n\nTime Series Transformations for Home Sales\n\n# Create time series plots with transformations\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Panel A: Original and Moving Average\naxes[0].plot(data_homesales_filtered['daten'], data_homesales_filtered['exsales'],\n            linewidth=2, label='Original', color='darkblue')\naxes[0].plot(data_homesales_filtered['daten'], data_homesales_filtered['exsales_ma11'],\n            linewidth=2, linestyle='--', label='11-month Moving Average', color='red')\naxes[0].set_xlabel('Year', fontsize=11)\naxes[0].set_ylabel('Monthly Home Sales', fontsize=11)\naxes[0].set_title('Panel A: Original Series and Moving Average',\n                  fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Original and Seasonally Adjusted\naxes[1].plot(data_homesales_filtered['daten'], data_homesales_filtered['exsales'],\n            linewidth=2, label='Original', color='darkblue')\naxes[1].plot(data_homesales_filtered['daten'], data_homesales_filtered['exsales_sa'],\n            linewidth=2, linestyle='--', label='Seasonally Adjusted', color='green')\naxes[1].set_xlabel('Year', fontsize=11)\naxes[1].set_ylabel('Monthly Home Sales', fontsize=11)\naxes[1].set_title('Panel B: Original Series and Seasonally Adjusted',\n                  fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Time Series Transformations for Home Sales',\n             fontsize=14, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Observations:\")\nprint(\"   - Original series: Jagged with seasonal peaks (summer) and troughs (winter)\")\nprint(\"   - Moving average: Smooth curve shows underlying trend (housing crash 2007-2011)\")\nprint(\"   - Seasonally adjusted: Removes seasonal pattern, reveals trend and cycles\")\n\n\n\n\n\n\n\n\n\n📊 Observations:\n   - Original series: Jagged with seasonal peaks (summer) and troughs (winter)\n   - Moving average: Smooth curve shows underlying trend (housing crash 2007-2011)\n   - Seasonally adjusted: Removes seasonal pattern, reveals trend and cycles\n\n\nData: Monthly U.S. existing home sales (2005-2015)\nThree series compared:\n\nOriginal series (blue solid line)\n11-month moving average (red dashed line, Panel A)\nSeasonally adjusted (green dashed line, Panel B)\n\nPanel A: Original vs. Moving Average\nWhat the original series shows:\n\nHigh volatility: Sharp month-to-month fluctuations\nSeasonal peaks: Regular spikes (summer buying season)\nSeasonal troughs: Regular dips (winter slowdown)\nTrend: Underlying long-term pattern (housing crash 2007-2011)\nRange: 218,000 to 754,000 homes per month\n\nWhat the 11-month moving average reveals:\n1. Smooths out noise:\n\nEliminates month-to-month volatility\nShows the underlying trend clearly\nEach point = average of surrounding 11 months\n\n2. Housing market cycle becomes visible:\n\n2005-2006: Peak (~600,000 homes/month)\n2007-2008: SHARP DECLINE (housing crash begins)\n2008-2011: Bottom (~325,000 homes/month)\n\nLost nearly 50% of sales volume\nTook 5+ years to reach bottom\n\n2011-2015: Gradual recovery\n\nSales climbing back toward ~450,000/month\nStill well below pre-crash peak\n\n\n3. Trend is NOT linear:\n\nNot a straight line up or down\nShows boom-bust-recovery cycle\nMoving average captures this nonlinear pattern\n\nPanel B: Original vs. Seasonally Adjusted\nWhat seasonal adjustment does:\n\nRemoves predictable seasonal patterns\nAnswers: “What would sales be without seasonal effects?”\nAllows you to see whether changes are “real” or just seasonal\n\nKey differences between seasonally adjusted and original:\n1. Amplitude reduction:\n\nOriginal: Wild swings from 218k to 754k\nSeasonally adjusted: Smoother, swings from 288k to 605k\nSeasonal component accounts for ~30-40% of monthly variation\n\n2. Pattern changes:\n\nOriginal: Regular summer peaks (May-July) and winter troughs (Jan-Feb)\nSeasonally adjusted: These regular peaks/troughs removed\nRemaining variation = true economic changes + random noise\n\n3. Trend clarity:\n\nOriginal: Hard to tell if uptick is recovery or just seasonal\nSeasonally adjusted: Clearer signal of true economic trend\nFed and policymakers watch seasonally adjusted data\n\nComparison of transformation methods:\n\n\n\n\n\n\n\n\nFeature\nMoving Average\nSeasonal Adjustment\n\n\n\n\nRemoves\nHigh-frequency noise\nPredictable seasonal patterns\n\n\nPreserves\nTrend and cycles\nTrend, cycles, and irregular movements\n\n\nLags\nYes (centered average)\nNo (real-time adjustment)\n\n\nUse case\nVisualizing long-term trends\nPolicy decisions and forecasting\n\n\n\nEconomic interpretation:\nWhy does housing have strong seasonality?\n\nWeather: Hard to move in winter (northern states)\nSchool calendar: Families move in summer to avoid disrupting school year\nTax refunds: Spring refunds provide down payment money\nDaylight: More daylight hours for house hunting in summer\n\nWhy did the housing market crash?\n\n2005-2006: Subprime mortgage boom (easy credit)\n2007: Mortgage defaults begin, housing prices fall\n2008: Financial crisis (Lehman Brothers bankruptcy)\n2008-2009: Credit crunch, massive foreclosures\n2009-2011: Deleveraging, excess inventory\n\nWhy the slow recovery?\n\nUnderwater mortgages: Many homeowners owed more than home value\nTighter credit: Banks required higher down payments, better credit scores\nJob losses: 2008-2009 recession reduced demand\nPsychological: Homebuyers became risk-averse after crash\n\nStatistical lessons:\n1. Moving averages:\n\nSmooth time series to reveal trends\nWidth matters: 11-month average removes seasonal + noise\nTrade-off: Smoothness vs. lag (delayed signal)\n\n2. Seasonal adjustment:\n\nEssential for economic data with strong seasonal patterns\nAllows comparison across months/quarters\nStandard practice: Always report seasonally adjusted for policy\n\n3. Which to use?\n\nMoving average: Historical analysis, visualization\nSeasonal adjustment: Real-time monitoring, forecasting, policy\nBoth together: Comprehensive understanding of time series dynamics\n\nPractical implication: When the news reports “Home sales up 5% this month,” ALWAYS check if it’s seasonally adjusted. Raw data might just show normal summer increase!\n\nKey Concept 2.7: Time Series Transformations\nTime series data often requires transformations: moving averages smooth short-term fluctuations, seasonal adjustment removes recurring patterns, real values adjust for inflation, per capita values adjust for population, and growth rates measure proportionate changes. These transformations reveal underlying trends and enable meaningful comparisons.\n\n\n\nGDP Comparisons - Nominal vs Real\n\n# Compare nominal and real GDP\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Panel A: GDP and Real GDP\naxes[0].plot(data_gdp['daten'], data_gdp['gdp'],\n            linewidth=2, label='GDP (nominal)', color='darkblue')\naxes[0].plot(data_gdp['daten'], data_gdp['realgdp'],\n            linewidth=2, linestyle='--', label='Real GDP (2012 dollars)', color='red')\naxes[0].set_xlabel('Year', fontsize=11)\naxes[0].set_ylabel('GDP (in $ billions)', fontsize=11)\naxes[0].set_title('Panel A: GDP and Real GDP', fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: GDP per capita and Real GDP per capita\naxes[1].plot(data_gdp['daten'], data_gdp['gdppc'],\n            linewidth=2, label='GDP per capita (nominal)', color='darkblue')\naxes[1].plot(data_gdp['daten'], data_gdp['realgdppc'],\n            linewidth=2, linestyle='--', label='Real GDP per capita (2012 dollars)', color='red')\naxes[1].set_xlabel('Year', fontsize=11)\naxes[1].set_ylabel('GDP per capita (in dollars)', fontsize=11)\naxes[1].set_title('Panel B: GDP per Capita and Real GDP per Capita',\n                  fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('GDP Comparisons - Nominal vs Real',\n             fontsize=14, fontweight='bold', y=1.0)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Why adjust for inflation and population?\")\nprint(\"   - Nominal GDP: Inflated by price increases (not just real growth)\")\nprint(\"   - Real GDP: Removes inflation, shows true output growth\")\nprint(\"   - Per capita: Accounts for population growth, measures individual prosperity\")\n\n\n\n\n\n\n\n\n\n📊 Why adjust for inflation and population?\n   - Nominal GDP: Inflated by price increases (not just real growth)\n   - Real GDP: Removes inflation, shows true output growth\n   - Per capita: Accounts for population growth, measures individual prosperity",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#key-takeaways",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#key-takeaways",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSummary Statistics and Data Distributions: - Summary statistics (mean, median, standard deviation, quartiles, skewness, kurtosis) efficiently describe large datasets by quantifying central tendency and dispersion - The mean is sensitive to outliers; the median is robust and preferred for skewed distributions - Standard deviation measures typical distance from the mean; for normal distributions, ~68% of data falls within 1 standard deviation, ~95% within 2 - Skewness measures asymmetry (positive for right-skewed data common in economics like earnings and wealth); guideline: |skewness| &gt; 1 indicates strong skewness - Kurtosis measures tail heaviness; excess kurtosis &gt; 0 indicates fatter tails than the normal distribution - Box plots visually summarize key statistics: median, quartiles, and potential outliers\nVisualizations for Different Data Types: - Histograms display distributions of numerical data using bins; bin width affects detail level (smaller bins show more detail but may be noisier) - Kernel density estimates provide smooth approximations of underlying continuous distributions without arbitrary bin choices - Line charts are ideal for time series data to reveal trends, cycles, and structural breaks over time - Bar charts and column charts effectively display categorical data, with bar length representing values for easy comparison - Pie charts show proportions for categorical data, though bar charts often facilitate easier comparison across categories - Choosing the right visualization depends on data type (numerical vs. categorical), dimensionality (univariate vs. categorical breakdown), and whether data are time-ordered\nData Transformations and Their Applications: - Natural logarithm transformations convert right-skewed economic data (earnings, prices, wealth) to more symmetric distributions, facilitating statistical analysis - Z-scores standardize data to mean 0 and standard deviation 1, enabling comparison across different scales and identifying outliers - Moving averages smooth short-term fluctuations in time series data by averaging over several periods (e.g., 11-month MA removes seasonality) - Seasonal adjustment removes recurring patterns to reveal underlying trends; essential for comparing economic indicators across months/quarters - Real values adjust for price inflation using deflators; per capita values adjust for population size—both are crucial for meaningful comparisons over time - Growth rates measure proportionate changes; distinguish between percentage point changes and percentage changes to avoid confusion - For time series, the change in natural log approximates the proportionate change (useful property: Δln(x) ≈ Δx/x for small changes)\nPython Tools and Methods: - pandas provides .describe(), .mean(), .median(), .std(), .quantile() for summary statistics - scipy.stats provides skew() and kurtosis() for distribution shape measures (note: kurtosis() returns excess kurtosis by default) - matplotlib and seaborn enable professional visualizations (histograms, KDE, line charts, box plots, bar charts) - numpy.log() applies natural logarithm transformation; z-scores computed as (x - x.mean()) / x.std() - Moving averages can be computed with pandas.rolling().mean(); seasonal adjustment typically requires specialized packages like statsmodels\n\nNext Steps: - Chapter 3: Statistical inference and confidence intervals for the mean - Chapter 5: Bivariate data summary and correlation analysis - Chapter 6-9: Simple linear regression and interpretation\nYou have now mastered: Calculating and interpreting summary statistics Creating effective visualizations for different data types Applying transformations to reveal patterns and normalize distributions Handling time series data with moving averages and seasonal adjustment\nThese foundational skills prepare you for inferential statistics and regression analysis in the following chapters!",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#practice-exercises",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#practice-exercises",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of univariate data analysis with these exercises: Exercise 1: Calculate summary statistics\n\nFor the sample {5, 2, 2, 8, 3}, calculate: - (a) Mean\n\n\nMedian\n\n\nVariance\n\n\nStandard deviation\n\n\n\nExercise 2: Interpret skewness\n\nA dataset has skewness = -0.85. What does this tell you about the distribution?\nWould you expect the mean to be greater than or less than the median? Why?\n\nExercise 3: Choose visualization types\n\nFor each scenario, recommend the best chart type and explain why: - (a) Quarterly GDP growth rates from 2000 to 2025\n\n\nMarket share of 5 smartphone brands\n\n\nDistribution of household incomes in a city\n\n\nMonthly temperature readings over a year\n\n\n\nExercise 4: Log transformation\n\nWhy is log transformation particularly useful for economic variables like income and GDP?\nIf log(earnings) increases by 0.5, approximately what percentage increase does this represent in earnings?\n\nExercise 5: Standard deviation interpretation\n\nA dataset has mean = 50 and standard deviation = 10. If the data are approximately normally distributed: - (a) What percentage of observations fall between 40 and 60?\n\n\nWhat percentage fall between 30 and 70?\n\n\n\nExercise 6: Time series transformations\n\nExplain the difference between: - (a) Moving average vs. seasonal adjustment\n\n\nNominal GDP vs. Real GDP\n\n\nTotal GDP vs. GDP per capita\n\n\n\nExercise 7: Z-scores\n\nFor a sample with mean = 100 and standard deviation = 15: - (a) Calculate the z-score for an observation of 130\n\n\nInterpret what this z-score means\n\n\n\nExercise 8: Data interpretation\n\nA box plot shows: - Lower quartile (Q1) = 25\n\nMedian (Q2) = 35\nUpper quartile (Q3) = 60\n\nCalculate: - (a) Interquartile range (IQR)\n\n\nDescribe the skewness based on quartile positions",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch02_Univariate_Data_Summary.html#case-studies",
    "href": "../notebooks_colab/ch02_Univariate_Data_Summary.html#case-studies",
    "title": "Chapter 2: Univariate Data Summary",
    "section": "2.8 Case Studies",
    "text": "2.8 Case Studies\n\nCase Study 1: Global Labor Productivity Distribution\nResearch Question: How is labor productivity distributed across countries? Are there distinct groups or is it continuous?\nIn Chapter 1, you examined relationships between variables—specifically, how productivity relates to capital stock through regression analysis. Now we shift perspective to analyze a single variable—labor productivity—but focus on its distribution across countries rather than its associations.\nThis case study builds on Chapter 1’s dataset (Convergence Clubs) but asks fundamentally different questions: What does the distribution of productivity look like across the 108 countries in our sample? Is it symmetric or skewed? Have productivity gaps widened or narrowed over time? These distributional questions are central to development economics and understanding global inequality.\nBy completing this case study, you’ll apply all the univariate analysis tools from Chapter 2 to a real dataset with genuine economic relevance—exploring whether productivity converges globally or if divergence persists.\n\nKey Concept 2.8: Cross-Country Distributions\nCross-country distributions of economic variables (productivity, GDP per capita, income) are typically right-skewed with long upper tails, reflecting substantial inequality between rich and poor countries. Summary statistics like the median are often more representative than the mean for these distributions, and exploring the shape of the distribution reveals whether gaps between countries are widening or narrowing.\n\n\n\nLoad the Productivity Data\nWe’ll use the same Convergence Clubs dataset from Chapter 1, but focus exclusively on the labor productivity variable (lp) across countries and years. This gives us 2,700 observations (108 countries × 25 years, from 1990 to 2014) of international productivity.\n\n# Load convergence clubs data (same as Chapter 1)\ndf1 = pd.read_csv(\n    \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\",\n    index_col=[\"country\", \"year\"]\n).sort_index()\n\n# For Chapter 2, focus on labor productivity variable\nproductivity = df1['lp']\n\nprint(\"=\" * 70)\nprint(\"LABOR PRODUCTIVITY DISTRIBUTION ANALYSIS\")\nprint(\"=\" * 70)\nprint(f\"Total observations: {len(productivity)}\")\nprint(f\"Countries: {len(df1.index.get_level_values('country').unique())}\")\nprint(f\"Time period: {df1.index.get_level_values('year').min()} to {df1.index.get_level_values('year').max()}\")\nprint(f\"\\nFirst 10 observations (sample):\")\nprint(df1[['lp']].head(10))\n\n======================================================================\nLABOR PRODUCTIVITY DISTRIBUTION ANALYSIS\n======================================================================\nTotal observations: 2700\nCountries: 108\nTime period: 1990 to 2014\n\nFirst 10 observations (sample):\n                      lp\ncountry year            \nAlbania 1990   9959.2344\n        1991   9093.6943\n        1992  10188.3060\n        1993  12432.5870\n        1994  13007.0080\n        1995  14813.8990\n        1996  18248.9860\n        1997  15008.6750\n        1998  15001.0220\n        1999  17351.6020\n\n\n\n\nHow to Use These Tasks\nInstructions:\n\nRead the task objectives and instructions in each section below\nReview the example code structure provided\nCreate a NEW code cell to write your solution\nFollow the structure and fill in the blanks or write complete code\nRun and test your code\nAnswer the interpretation questions\n\nProgressive difficulty:\n\nTasks 1-2: Guided (fill in specific blanks with _____)\nTask 3: Semi-guided (complete partial code structure)\nTasks 4-6: Independent (write full code from outline)\n\nTip: Type the code yourself rather than copying—it builds understanding!\n\nTask 1: Data Exploration (Guided)\nObjective: Load and explore the structure of the global productivity distribution.\nInstructions:\n\nExamine the productivity variable’s basic structure (length, data type, any missing values)\nGet summary statistics (count, mean, std, min, max)\nDisplay observations for 5 different countries to see variation across countries\nCheck: Is there variation across countries? Does it seem large or small?\n\nChapter 2 connection: This applies the concepts from Section 2.1 (Summary Statistics).\nStarter code guidance:\n\nUse productivity.describe() for summary statistics\nCheck for missing values with productivity.isnull().sum()\nUse .loc[] or .xs() to select specific countries’ observations\nCalculate min and max productivity values globally\n\nExample code structure:\n# Task 1: Data Exploration (GUIDED)\n# Complete the code below by filling in the blanks (_____)\n\n# Step 1: Check data structure\nprint(\"Data Structure:\")\nprint(f\"Total observations: {_____}\")\nprint(f\"Data type: {productivity.dtype}\")\nprint(f\"Missing values: {_____}\")\n\n# Step 2: Summary statistics\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary Statistics for Global Productivity\")\nprint(\"=\" * 70)\nprint(productivity.describe())\n\n# Step 3: Variation across countries - look at a few countries\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Productivity across 5 sample countries:\")\nprint(\"=\" * 70)\nsample_countries = ['Australia', 'Brazil', 'China', 'France', 'Nigeria']\nfor country in sample_countries: country_data = df1.loc[country, 'lp']\n    print(f\"\\n{country}:\")\n    print(f\"  Mean productivity: {_____:.3f}\")\n    print(f\"  Min: {_____:.3f}, Max: {_____:.3f}\")\n    print(f\"  Range: {_____:.3f}\")\n\n# Step 4: Global variation\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Global Variation:\")\nprint(\"=\" * 70)\nmin_prod = productivity.min()\nmax_prod = productivity.max()\nratio = max_prod / min_prod\nprint(f\"Minimum global productivity: {min_prod:.3f}\")\nprint(f\"Maximum global productivity: {max_prod:.3f}\")\nprint(f\"Ratio (max/min): {ratio:.1f}x\")\nprint(f\"\\nInterpretation: The most productive country is {ratio:.0f}× more productive than the least productive country!\")\n\n\nTask 2: Summary Statistics (Semi-guided)\nObjective: Calculate comprehensive summary statistics for the global productivity distribution.\nInstructions:\n\nCompute mean, median, standard deviation, quartiles (25th, 50th, 75th percentiles)\nCalculate skewness and kurtosis for the overall productivity distribution\nIdentify which countries have the highest and lowest productivity (across all years)\nCompare productivity statistics for two time periods: 1990 and 2014\n\nChapter 2 connection: Applies Section 2.1 (Summary Statistics) and distribution shape measures.\nStarter code guidance:\n\nUse .describe() for the main statistics\nUse scipy.stats.skew() and scipy.stats.kurtosis() for shape measures\nFilter by year: df1.xs(1990, level='year')['lp']\nUse .nlargest() and .nsmallest() to find extreme values\nCreate a comparison table of statistics for different time periods\n\nExample code structure:\n# Task 2: Summary Statistics (SEMI-GUIDED)\n# Complete the code by implementing each step\n\n# Step 1: Overall summary statistics\noverall_stats = {\n    'Mean': productivity.mean(),\n    'Median': _____,  # Calculate median\n    'Std Dev': _____,  # Calculate standard deviation\n    'Skewness': stats.skew(_____),\n    'Kurtosis': _____,  # Calculate kurtosis\n    '25th percentile': productivity.quantile(0.25),\n    '75th percentile': productivity.quantile(_____),\n    'IQR': productivity.quantile(0.75) - productivity.quantile(0.25)\n}\n\nfor key, value in overall_stats.items(): print(f\"{key:20s}: {value:.4f}\")\n\n# Step 2: Countries with highest/lowest productivity\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Top 5 Most Productive Countries (average across years)\")\nprint(\"=\" * 70)\ncountry_means = df1.groupby(_____)['lp'].mean().sort_values(_____)\nprint(country_means.head())\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Top 5 Least Productive Countries (average across years)\")\nprint(\"=\" * 70)\nprint(country_means.tail())\n\n# Step 3: Compare 1990 vs 2014\nproductivity_1990 = df1.xs(1990, level=_____)['lp']\nproductivity_2014 = df1.xs(_____, level='year')['lp']\n\n# Your code here: Create a comparison DataFrame\n# Hint: Use pd. DataFrame() with statistics for both years\n# Include: mean, median, std, skewness, min, max\nHints:\n\nUse .median(), .std() for missing statistics\nstats.kurtosis() requires the data series as input\n.groupby('country') groups by country name\n.sort_values(ascending=False) sorts from high to low\n.xs(year, level='year') extracts data for a specific year\n\n\n\nTask 3: Visualizing Distributions (Semi-guided)\nObjective: Create multiple visualizations to understand the shape of the productivity distribution.\nInstructions:\n\nCreate a histogram of productivity (try different bin widths)\nCreate a box plot to identify outliers and quartiles\nCreate a kernel density estimate to see the smooth shape\nCompare the original distribution to the log-transformed distribution\n\nChapter 2 connection: Applies Section 2.2 (Charts for Numerical Data).\nStarter code guidance:\n\nUse plt.hist() for histogram with different bin widths (try 10, 15, 20 bins)\nUse plt.boxplot() for box plot visualization\nUse .plot.kde() for kernel density estimate\nCreate side-by-side panels to compare original vs log-transformed\nLabel axes clearly and add titles\n\nExample code structure:\n# Task 3: Visualizing Distributions (SEMI-GUIDED)\n# Create comprehensive visualizations of the productivity distribution\n\n# Create a 2x2 figure with 4 subplots\nfig, axes = plt.subplots(_____, _____, figsize=(14, 10))\n\n# Panel 1: Histogram (original productivity)\naxes[0, 0].hist(productivity, bins=_____, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 0].set_xlabel(_____, fontsize=11)\naxes[0, 0].set_ylabel(_____, fontsize=11)\naxes[0, 0].set_title('Panel 1: Histogram of Productivity (20 bins)', fontsize=12, fontweight='bold')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Panel 2: Box plot (original productivity)\naxes[0, 1].boxplot(_____, vert=True, patch_artist=True)\naxes[0, 1].set_ylabel('Labor Productivity', fontsize=11)\naxes[0, 1].set_title('Panel 2: Box Plot of Productivity', fontsize=12, fontweight='bold')\naxes[0, 1].grid(True, alpha=0.3, axis='y')\n\n# Panel 3: KDE (original productivity)\nproductivity.plot.kde(ax=_____, linewidth=2, color='darkblue')\naxes[1, 0].set_xlabel('Labor Productivity', fontsize=11)\naxes[1, 0].set_ylabel('Density', fontsize=11)\naxes[1, 0].set_title('Panel 3: Kernel Density Estimate', fontsize=12, fontweight='bold')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Panel 4: KDE comparison (original vs log-transformed)\n# Your code here: Create log-transformed productivity\nlog_productivity = np.log(productivity)\n\n# Your code here: Plot both KDE curves on the same axes\n# Hint: Use .plot.kde() with label='Original' and label='Log-transformed'\n# Use different colors and linestyles for clarity\n\nplt.suptitle('Figure: Global Productivity Distribution Visualizations', fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\nHints:\n\nplt.subplots(2, 2) creates 2 rows and 2 columns\nTry bins=20 for the histogram\nSet xlabel to ‘Labor Productivity’\nFor boxplot, pass the productivity series directly\nUse axes[1, 0] to reference the bottom-left panel\nFor KDE comparison, plot two curves with different colors (e.g., ‘darkblue’ and ‘red’)\n\n\n\nTask 4: Comparing Distributions Across Time (More Independent)\nObjective: Analyze how the productivity distribution has changed from 1990 to 2014.\nInstructions:\n\nExtract productivity data for 1990 and 2014\nCalculate summary statistics for each year separately\nCreate overlapping KDE plots to compare the distributions visually\nAnalyze: Has the distribution shifted right (convergence/improvement)? Widened (divergence)? Changed shape?\n\nChapter 2 connection: Applies Section 2.2 (comparing distributions across groups).\nStarter code guidance:\n\nUse df1.xs(year, level='year') to extract data for specific years\nCreate summary statistics tables for comparison\nPlot two KDE curves on the same axes with different colors\nUse the 25th and 75th percentiles to measure spread\nCalculate the coefficient of variation (std/mean) to compare relative dispersion\n\nExample code structure:\n# Task 4: Comparing Distributions Across Time (MORE INDEPENDENT)\n# Analyze how global productivity distribution evolved from 1990 to 2014\n\n# Step 1: Extract data for 1990 and 2014\nprod_1990 = df1.xs(_____, level='year')['lp']\nprod_2014 = _____  # Extract 2014 data (same pattern as above)\n\n# Step 2: Create comparison visualization\n# Your code here: Create figure with 2 subplots (1 row, 2 columns)\n# Hint: fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Overlapping KDE plots\n# Your code here: Plot KDE for both years on the same axes\n# - Use prod_1990.plot.kde() and prod_2014.plot.kde()\n# - Different colors for each year (e.g., 'darkblue' and 'red')\n# - Add labels and legend\n\n# Panel B: Side-by-side box plots\n# Your code here: Create box plots for both years\n# Hint: axes[1].boxplot([prod_1990, prod_2014], labels=['1990', '2014'])\n# Set different colors for each box using patch_artist=True\n\n# Step 3: Calculate comparison statistics\n# Your code here: Create a DataFrame comparing statistics for both years\n# Include: mean, median, std, coefficient of variation, skewness, min, max, range\n# Hint: Use pd.DataFrame() with a dictionary of statistics\nHints:\n\nCoefficient of variation = std / mean (relative dispersion)\nUse stats.skew() from scipy.stats for skewness\nFor KDE plots, use label='1990' and label='2014' for legend\nRange = max - min\n\nQuestions to consider:\n\nDid mean productivity increase from 1990 to 2014?\nDid the spread (std dev) increase or decrease? (Convergence vs divergence)\nDid the coefficient of variation change?\nDid skewness change?\n\n\n\nTask 5: Transformation Analysis (Independent)\nObjective: Apply log transformation to productivity data and analyze the effect.\nInstructions:\n\nCreate log-transformed productivity variable: log_productivity = ln(productivity)\nCompare skewness before and after transformation\nCreate side-by-side histograms (original vs log-transformed)\nCalculate z-scores for both variables to standardize them\nInterpret: Why does log transformation help? When would you use it?\n\nChapter 2 connection: Applies Section 2.5 (Data Transformation).\nStarter code guidance:\n\nUse np.log() to create log transformation\nCompare skewness values before/after using stats.skew()\nCreate z-scores with: (x - x.mean()) / x.std()\nVisualize both original and log distributions in histograms\nDiscuss why log-normal distributions are common in economics\n\nExample code structure:\n# Task 5: Transformation Analysis (INDEPENDENT)\n# Apply log transformation to understand how it affects the distribution\n\n# Step 1: Create log transformation\n# Your code here: log_productivity = np.log(_____)\n\n# Step 2: Create z-scores (standardized values)\n# Your code here: Calculate z-scores for both distributions\n# Formula: z = (x - mean) / std\n# z_productivity = (productivity - productivity.mean()) / productivity.std()\n# z_log_productivity = ?\n\n# Step 3: Create side-by-side histograms\n# Your code here: Use plt.subplots(1, 2) for 2 panels\n# Panel A: Original productivity histogram (20 bins, blue color)\n# Panel B: Log-transformed histogram (20 bins, coral/red color)\n\n# Step 4: Compare skewness\n# Your code here: Calculate skewness using stats.skew()\n# Calculate percentage reduction: (1 - |skew_log| / |skew_original|) * 100\n# Print comparison table showing: mean, median, std, skewness, kurtosis, min, max\nHints:\n\nnp.log() computes natural logarithm\nZ-scores standardize data to mean=0, std=1\nUse stats.skew() and stats.kurtosis() for shape measures\nCompare absolute skewness values to quantify reduction\n\nQuestions to consider:\n\nIs the log-transformed distribution more symmetric?\nWhen would you use log transformation in economic analysis?\nWhat happened to skewness and kurtosis after transformation?\n\n\nKey Concept 2.9: Distributional Convergence\nDistributional convergence (σ-convergence) asks whether the spread (variance) of productivity across countries is narrowing over time. This differs from β-convergence (poor countries growing faster than rich ones). If cross-country distributions are becoming more compressed (lower variance), it suggests countries are converging toward similar productivity levels—important for understanding whether global inequality is increasing or decreasing.\n\n\n\nTask 6: Regional Patterns (Independent)\nObjective: Compare productivity distributions across geographic regions.\nInstructions:\n\nAdd a region column to your dataframe (you’ll need to manually assign regions based on country names)\nGroup countries by region (at minimum: Africa, Asia, Europe, Americas)\nCreate box plots for each region side-by-side\nCalculate summary statistics by region\nIdentify: Which regions have highest/lowest productivity? Most inequality?\n\nChapter 2 connection: Applies Sections 2.3-2.4 (Charts for categorical breakdowns).\nStarter code guidance:\n\nCreate a dictionary mapping countries to regions\nUse .groupby() to calculate statistics by region\nCreate side-by-side box plots for visual comparison\nCalculate mean and standard deviation by region\nCompare median productivity across regions\n\nExample code structure:\n# Task 6: Regional Patterns (INDEPENDENT)\n# Compare productivity distributions across geographic regions\n\n# Step 1: Create region mapping dictionary\n# Your code here: Define region_mapping\n# Map each country to its region (Africa, Americas, Asia, Europe, Middle East, Asia-Pacific)\n# Example structure:\n# region_mapping = {\n#     'Australia': 'Asia-Pacific',\n#     'Austria': 'Europe',\n#     'Brazil': 'Americas',\n#     # ... continue for all ~50 countries\n# }\n\n# Step 2: Add region column to dataframe\n# Your code here: Create a copy of df1 and add region column\n# Hint: df_with_region['region'] = df_with_region.index.get_level_values('country').map(region_mapping)\n# Remove rows with missing regions: .dropna(subset=['region'])\n\n# Step 3: Calculate regional statistics\n# Your code here: Group by region and aggregate statistics\n# Hint: df_with_region.groupby('region')['lp'].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n# Sort by mean productivity (descending)\n\n# Step 4: Create box plots by region\n# Your code here: Create boxplot visualization comparing regions\n# - Extract data for each region: [df[df['region'] == r]['lp'].values for r in regions]\n# - Sort regions by mean productivity for better readability\n# - Use plt.boxplot() with labels for each region\n# - Rotate x-axis labels for readability\nHints:\n\nThere are ~50 countries in the dataset - you’ll need to map each one\nRegions: Africa (Kenya, Nigeria, etc.), Americas (USA, Brazil, etc.), Asia (China, India, etc.)\nEurope (France, Germany, etc.), Middle East (Israel, Turkey), Asia-Pacific (Australia, Japan, NZ)\nUse .groupby('region')['lp'].agg([...]) to calculate statistics by region\nSort regions by mean before plotting for better visualization\n\nQuestions to consider:\n\nWhich region has the highest average productivity?\nWhich region has the most internal inequality (widest box)?\nAre there clear regional clusters or is variation continuous?\n\n\n\n\nWhat You’ve Learned from This Case Study\nBy completing this case study on global labor productivity distribution, you’ve applied the full toolkit of univariate data analysis to a real international economics question. You’ve moved beyond calculating statistics and making charts to asking substantive economic questions: Are countries converging or diverging? How has global inequality in productivity evolved? Which regions drive global disparity?\nSpecifically, you’ve practiced:\n\nSummary statistics to quantify central tendency and spread\nVisualizations (histograms, box plots, KDE) to see distributional shape\nComparisons across time periods to detect changes\nTransformations (log) to normalize skewed economic data\nCategorical breakdowns (regions) to identify subgroup patterns\n\nThese skills extend far beyond productivity. The same analytical approach applies to wealth distribution, income inequality, student test scores, health outcomes, and countless other univariate datasets in economics and social science.\nYour next steps (in later chapters) will be to ask relational questions: How does productivity relate to capital? Does inequality depend on development level? Can we predict a country’s productivity from other variables? Those questions require bivariate analysis (Chapter 5) and regression (Chapter 6+).\n\n\nCase Study 2: The Geography of Development: Summarizing Bolivia’s Municipal SDG Data\nIn Chapter 1, we introduced the DS4Bolivia project and explored the relationship between nighttime lights and municipal development in Bolivia. In this case study, we apply Chapter 2’s univariate summary tools to characterize the distribution of development indicators across Bolivia’s 339 municipalities.\nThe Data: The DS4Bolivia project provides a comprehensive dataset covering 339 Bolivian municipalities with over 350 variables, including the Municipal Sustainable Development Index (IMDS), individual SDG indices, nighttime lights per capita (2012-2020), population, and socioeconomic indicators. Here we focus on understanding the shape of these distributions—their central tendency, spread, skewness, and multimodality—using the univariate tools from this chapter.\n\nLoad the DS4Bolivia Data\nLet’s load the DS4Bolivia dataset and select the key variables for univariate analysis.\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Display basic information\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA DATASET\")\nprint(\"=\" * 70)\nprint(f\"Dataset shape: {bol.shape[0]} municipalities, {bol.shape[1]} variables\")\nprint(f\"\\nDepartments: {bol['dep'].nunique()} unique departments\")\nprint(f\"Department names: {sorted(bol['dep'].unique())}\")\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017', 'pop2017',\n            'index_sdg1', 'sdg1_1_ubn',\n            'ln_NTLpc2012', 'ln_NTLpc2013', 'ln_NTLpc2014',\n            'ln_NTLpc2015', 'ln_NTLpc2016', 'ln_NTLpc2017',\n            'ln_NTLpc2018', 'ln_NTLpc2019', 'ln_NTLpc2020']\n# Remove duplicates while preserving order\nkey_vars = list(dict.fromkeys(key_vars))\nbol_key = bol[key_vars].copy()\n\nprint(f\"\\nKey variables selected: {len(key_vars)}\")\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FIRST 10 MUNICIPALITIES\")\nprint(\"=\" * 70)\nprint(bol_key.head(10).to_string())\n\n\n\nTask 1: Summary Statistics (Guided)\nObjective: Compute and interpret descriptive statistics for key development indicators.\nInstructions:\n\nUse .describe() to generate summary statistics for imds, index_sdg1, sdg1_1_ubn, and ln_NTLpc2017\nCalculate the mean, median, standard deviation, skewness, and kurtosis for each variable\nDiscuss what these statistics reveal about the distribution of municipal development in Bolivia\n\nApply what you learned in section 2.1: Use describe(), .mean(), .median(), .std(), .skew(), and .kurtosis() to characterize these distributions.\n\n# Task 1: Summary Statistics\n# ----------------------------------------------------------\n\n# 1. Basic descriptive statistics\nanalysis_vars = ['imds', 'index_sdg1', 'sdg1_1_ubn', 'ln_NTLpc2017']\nprint(\"=\" * 70)\nprint(\"DESCRIPTIVE STATISTICS: KEY DEVELOPMENT INDICATORS\")\nprint(\"=\" * 70)\nprint(bol_key[analysis_vars].describe().round(2))\n\n# 2. Additional distributional measures\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DISTRIBUTIONAL SHAPE MEASURES\")\nprint(\"=\" * 70)\nfor var in analysis_vars:\n    series = bol_key[var].dropna()\n    print(f\"\\n{var}:\")\n    print(f\"  Mean:     {series.mean():.2f}\")\n    print(f\"  Median:   {series.median():.2f}\")\n    print(f\"  Std Dev:  {series.std():.2f}\")\n    print(f\"  Skewness: {series.skew():.3f}\")\n    print(f\"  Kurtosis: {series.kurtosis():.3f}\")\n\n# 3. Discussion: What do these reveal?\nprint(\"\\n\" + \"=\" * 70)\nprint(\"INTERPRETATION\")\nprint(\"=\" * 70)\nprint(\"Compare mean vs median for each variable:\")\nprint(\"If mean &gt; median → right-skewed (long upper tail)\")\nprint(\"If mean &lt; median → left-skewed (long lower tail)\")\nprint(\"High kurtosis (&gt;3) indicates heavy tails (extreme municipalities)\")\n\n\n\nTask 2: Histograms and Density Plots (Guided)\nObjective: Visualize the distributions of imds and ln_NTLpc2017 using histograms and kernel density estimation (KDE) plots.\nInstructions:\n\nCreate histograms for imds and ln_NTLpc2017 (side by side)\nOverlay KDE curves on the histograms\nDiscuss the shape: Is each distribution unimodal or bimodal? Symmetric or skewed?\nWhat might explain any multimodality? (Think about the urban-rural divide)\n\nApply what you learned in section 2.2: Use plt.hist() with density=True and overlay .plot.kde() or sns.kdeplot().\n\n# Task 2: Histograms and Density Plots\n# ----------------------------------------------------------\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# IMDS histogram with KDE\naxes[0].hist(bol_key['imds'].dropna(), bins=25, density=True,\n             color='steelblue', alpha=0.7, edgecolor='white')\nbol_key['imds'].dropna().plot.kde(ax=axes[0], color='darkblue', linewidth=2)\naxes[0].set_xlabel('Municipal Development Index (IMDS)')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Distribution of IMDS across 339 Municipalities')\naxes[0].axvline(bol_key['imds'].mean(), color='red', linestyle='--',\n                label=f\"Mean = {bol_key['imds'].mean():.1f}\")\naxes[0].axvline(bol_key['imds'].median(), color='orange', linestyle='--',\n                label=f\"Median = {bol_key['imds'].median():.1f}\")\naxes[0].legend()\n\n# Log NTL histogram with KDE\naxes[1].hist(bol_key['ln_NTLpc2017'].dropna(), bins=25, density=True,\n             color='purple', alpha=0.7, edgecolor='white')\nbol_key['ln_NTLpc2017'].dropna().plot.kde(ax=axes[1], color='darkviolet', linewidth=2)\naxes[1].set_xlabel('Log Nighttime Lights per Capita (2017)')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Distribution of Log NTL per Capita')\naxes[1].axvline(bol_key['ln_NTLpc2017'].mean(), color='red', linestyle='--',\n                label=f\"Mean = {bol_key['ln_NTLpc2017'].mean():.2f}\")\naxes[1].axvline(bol_key['ln_NTLpc2017'].median(), color='orange', linestyle='--',\n                label=f\"Median = {bol_key['ln_NTLpc2017'].median():.2f}\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Discussion prompts\nprint(\"DISCUSSION:\")\nprint(\"1. Is the IMDS distribution unimodal or multimodal?\")\nprint(\"2. Is the NTL distribution symmetric or skewed?\")\nprint(\"3. What might explain any bimodality? (urban vs rural)\")\n\n\nKey Concept 2.10: Spatial Data Distributions\nMunicipal-level data often exhibits multimodality reflecting the urban-rural divide. Unlike national statistics that produce single averages, municipality-level distributions can reveal distinct subpopulations—highly developed urban centers and less developed rural areas. Identifying these subgroups is essential for targeted policy interventions.\n\n\n\nTask 3: Box Plots by Department (Semi-guided)\nObjective: Create box plots of imds grouped by department (dep) to compare development across Bolivia’s 9 departments.\nInstructions:\n\nCreate a box plot of imds grouped by dep (9 departments)\nOrder departments by median IMDS for clarity\nIdentify which departments have the highest and lowest median development\nWhich departments show the most spread (variability)?\n\nApply what you learned in section 2.3-2.4: Use grouped box plots to compare distributions across categories.\n\n# Task 3: Box Plots by Department\n# ----------------------------------------------------------\n\n# Your code here: Create box plots of IMDS by department\n#\n# Steps:\n# 1. Order departments by median IMDS\n# 2. Create horizontal box plot\n# 3. Add labels and formatting\n\n# Example structure:\n# dept_order = bol_key.groupby('dep')['imds'].median().sort_values().index\n# fig, ax = plt.subplots(figsize=(10, 7))\n# bol_key.boxplot(column='imds', by='dep', ax=ax, vert=False,\n#                 positions=range(len(dept_order)))\n# ax.set_xlabel('Municipal Development Index (IMDS)')\n# ax.set_ylabel('Department')\n# ax.set_title('Development Distribution by Department')\n# plt.suptitle('')  # Remove automatic title\n# plt.tight_layout()\n# plt.show()\n\n# Hint: You can also use seaborn for cleaner grouped box plots:\nimport seaborn as sns\ndept_order = bol_key.groupby('dep')['imds'].median().sort_values().index.tolist()\n\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.boxplot(data=bol_key, x='imds', y='dep', order=dept_order,\n            palette='viridis', ax=ax)\nax.set_xlabel('Municipal Development Index (IMDS)')\nax.set_ylabel('Department')\nax.set_title('Municipal Development Distribution by Department')\nplt.tight_layout()\nplt.show()\n\n# Summary statistics by department\nprint(\"=\" * 70)\nprint(\"IMDS BY DEPARTMENT: MEDIAN AND IQR\")\nprint(\"=\" * 70)\ndept_stats = bol_key.groupby('dep')['imds'].describe()[['50%', '25%', '75%', 'std']].round(1)\ndept_stats.columns = ['Median', 'Q1', 'Q3', 'Std Dev']\nprint(dept_stats.sort_values('Median'))\n\n\n\nTask 4: Log Transformations (Semi-guided)\nObjective: Compare the distribution of raw population (pop2017) with its log transformation to demonstrate how log transformations improve symmetry for skewed data.\nInstructions:\n\nPlot the histogram of raw pop2017 — observe the extreme right skew\nApply np.log(pop2017) and plot its histogram\nCompare summary statistics (skewness, kurtosis) before and after transformation\nDiscuss why log transformations are standard practice for population and income data\n\nApply what you learned in section 2.5: Log transformations convert multiplicative relationships into additive ones and reduce skewness.\n\n# Task 4: Log Transformations\n# ----------------------------------------------------------\n\n# Your code here: Compare raw vs log-transformed population\n#\n# Steps:\n# 1. Plot raw pop2017 histogram\n# 2. Plot np.log(pop2017) histogram\n# 3. Compare skewness and kurtosis\n\nimport numpy as np\n\npop = bol_key['pop2017'].dropna()\nlog_pop = np.log(pop)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Raw population\naxes[0].hist(pop, bins=30, color='coral', alpha=0.7, edgecolor='white')\naxes[0].set_xlabel('Population (2017)')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Raw Population Distribution')\naxes[0].axvline(pop.mean(), color='red', linestyle='--',\n                label=f\"Mean = {pop.mean():,.0f}\")\naxes[0].axvline(pop.median(), color='blue', linestyle='--',\n                label=f\"Median = {pop.median():,.0f}\")\naxes[0].legend()\n\n# Log-transformed population\naxes[1].hist(log_pop, bins=30, color='teal', alpha=0.7, edgecolor='white')\naxes[1].set_xlabel('Log Population (2017)')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Log-Transformed Population Distribution')\naxes[1].axvline(log_pop.mean(), color='red', linestyle='--',\n                label=f\"Mean = {log_pop.mean():.2f}\")\naxes[1].axvline(log_pop.median(), color='blue', linestyle='--',\n                label=f\"Median = {log_pop.median():.2f}\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Compare distributional measures\nprint(\"=\" * 70)\nprint(\"EFFECT OF LOG TRANSFORMATION ON POPULATION\")\nprint(\"=\" * 70)\nprint(f\"{'Measure':&lt;15} {'Raw pop2017':&gt;15} {'log(pop2017)':&gt;15}\")\nprint(\"-\" * 45)\nprint(f\"{'Skewness':&lt;15} {pop.skew():&gt;15.3f} {log_pop.skew():&gt;15.3f}\")\nprint(f\"{'Kurtosis':&lt;15} {pop.kurtosis():&gt;15.3f} {log_pop.kurtosis():&gt;15.3f}\")\nprint(f\"{'Mean':&lt;15} {pop.mean():&gt;15,.0f} {log_pop.mean():&gt;15.2f}\")\nprint(f\"{'Median':&lt;15} {pop.median():&gt;15,.0f} {log_pop.median():&gt;15.2f}\")\n\n\nKey Concept 2.11: Development Indicator Interpretation\nSDG composite indices like IMDS (0-100) aggregate multiple dimensions of development into a single score. While convenient for ranking, composite indices can mask important variation in specific dimensions. For example, a municipality may score well on education (SDG 4) but poorly on health (SDG 3). Examining individual SDG variables alongside composite indices provides a more complete picture.\n\n\n\nTask 5: Time Series of NTL (Independent)\nObjective: Calculate and plot the mean nighttime lights across municipalities for each year from 2012 to 2020 to examine the evolution of satellite-measured economic activity.\nInstructions:\n\nCalculate the mean of ln_NTLpc2012 through ln_NTLpc2020 across all municipalities for each year\nPlot the resulting time series (year on x-axis, mean log NTL on y-axis)\nDiscuss: Is there a trend? Any notable changes? What might explain the pattern?\n\nApply what you learned in section 2.6: Use time series visualization to identify trends and patterns.\n\n# Task 5: Time Series of NTL\n# ----------------------------------------------------------\n\n# Your code here: Calculate mean NTL across municipalities for each year\n#\n# Steps:\n# 1. Select NTL columns for 2012-2020\n# 2. Calculate means\n# 3. Plot time series\n\n# Example structure:\n# ntl_cols = [f'ln_NTLpc{yr}' for yr in range(2012, 2021)]\n# years = list(range(2012, 2021))\n# mean_ntl = [bol_key[col].mean() for col in ntl_cols]\n#\n# fig, ax = plt.subplots(figsize=(10, 5))\n# ax.plot(years, mean_ntl, marker='o', color='navy', linewidth=2)\n# ax.set_xlabel('Year')\n# ax.set_ylabel('Mean Log NTL per Capita')\n# ax.set_title('Evolution of Nighttime Lights across Bolivian Municipalities')\n# ax.grid(True, alpha=0.3)\n# plt.tight_layout()\n# plt.show()\n\n\n\nTask 6: Regional Distribution Analysis (Independent)\nObjective: Compare the distributions of imds across departments using overlapping histograms or violin plots. Write a 200-word summary of regional inequality in Bolivia.\nInstructions:\n\nCreate overlapping histograms or violin plots of imds for at least 3 departments\nCompare the distributional shapes: Do some departments have more spread? More bimodality?\nWrite a 200-word summary discussing what these distributions reveal about regional inequality in Bolivia\nWhich departments might need the most targeted development interventions? Why?\n\nApply your skills: This task combines histogram/density visualization with substantive economic interpretation.\n\n# Task 6: Regional Distribution Analysis\n# ----------------------------------------------------------\n\n# Your code here: Compare IMDS distributions across departments\n#\n# Option A: Overlapping histograms (select 3-4 key departments)\n# Option B: Violin plots for all 9 departments\n# Option C: Ridge plot (multiple KDE curves stacked vertically)\n\n# Example structure (violin plots):\n# fig, ax = plt.subplots(figsize=(12, 7))\n# dept_order = bol_key.groupby('dep')['imds'].median().sort_values().index.tolist()\n# sns.violinplot(data=bol_key, x='imds', y='dep', order=dept_order,\n#                palette='coolwarm', ax=ax, inner='quartile')\n# ax.set_xlabel('Municipal Development Index (IMDS)')\n# ax.set_ylabel('Department')\n# ax.set_title('Distribution of Municipal Development by Department')\n# plt.tight_layout()\n# plt.show()\n\n# After creating your visualization, write a 200-word summary below:\n# print(\"REGIONAL INEQUALITY SUMMARY\")\n# print(\"=\" * 70)\n# print(\"Write your 200-word analysis here...\")\n\n\n\n\nWhat You’ve Learned from This Case Study\nBy applying Chapter 2’s univariate analysis tools to Bolivia’s municipal SDG data, you’ve characterized the distribution of development outcomes across 339 municipalities. Specifically, you’ve practiced:\n\nDescriptive statistics for development indicators—mean, median, SD, skewness, and kurtosis\nVisualization of distributions using histograms, box plots, and kernel density estimation (KDE)\nLog transformations for highly skewed data like population\nTime series summary of satellite-measured nighttime lights (2012-2020)\nRegional comparison of development distributions across Bolivia’s 9 departments\n\nThese univariate tools reveal the shape of Bolivia’s development distribution—its central tendency, spread, and the urban-rural divide reflected in multimodal patterns. Understanding these distributional properties is the essential first step before more advanced analysis.\nConnection to future chapters: In Chapter 4, we’ll test whether differences across departments are statistically significant. In Chapter 5, we’ll explore bivariate relationships between satellite data and development. Later chapters will build progressively more sophisticated models for predicting and explaining municipal development outcomes.\n\nWell done! You’ve now explored Bolivia’s municipal development data using the full univariate analysis toolkit—from summary statistics to distributional visualization and transformation.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Univariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html",
    "title": "Chapter 3: The Sample Mean",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to one of the most important concepts in statistics: the sampling distribution of the sample mean. You’ll explore how sample means behave through experiments and simulations, building intuition for the Central Limit Theorem. All code runs directly in Google Colab without any local setup.\nThis chapter bridges the gap between descriptive statistics (Chapter 2) and inferential statistics (Chapter 4). The key insight: when we calculate a sample mean \\(\\bar{x}\\) from data, we’re observing one realization of a random variable \\(\\bar{X}\\) that has its own probability distribution.\nWhat you’ll learn:\nDatasets used:\nKey economic relevance: This chapter provides the theoretical foundation for ALL statistical inference in economics. Whether estimating average income, unemployment rates, or regression coefficients, understanding the sampling distribution of \\(\\bar{X}\\) is essential.\nChapter outline:\nEstimated time: 50-60 minutes",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#chapter-overview",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#chapter-overview",
    "title": "Chapter 3: The Sample Mean",
    "section": "",
    "text": "Understand sample values as realizations of random variables\nDerive the mean and variance of the sample mean: \\(E[\\bar{X}] = \\mu\\), \\(Var[\\bar{X}] = \\sigma^2/n\\)\nExplore the sampling distribution of \\(\\bar{X}\\) through experiments\nDiscover the Central Limit Theorem: \\(\\bar{X}\\) is approximately normal for large \\(n\\)\nLearn properties of good estimators (unbiasedness, efficiency, consistency)\nCompute the standard error of the mean: \\(se(\\bar{X}) = s/\\sqrt{n}\\)\n\n\n\nAED_COINTOSSMEANS.DTA: 400 sample means from coin toss experiments (n=30 each)\nAED_CENSUSAGEMEANS.DTA: 100 sample means from 1880 U.S. Census ages (n=25 each)\n\n\n\n\n3.1 Random Variables\n3.2 Experiment - Single Sample of Coin Tosses\n3.3 Properties of the Sample Mean\n3.4 Real Data Example - 1880 U.S. Census\n3.5 Estimator Properties\n3.6 Computer Simulation of Random Samples\n3.7 Samples other than Simple Random Samples\n3.8 Computer Generation of a Random Variable",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#setup",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#setup",
    "title": "Chapter 3: The Sample Mean",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to explore the sample mean.\")\n\nSetup complete! Ready to explore the sample mean.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#random-variables",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#random-variables",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.1 Random Variables",
    "text": "3.1 Random Variables\nA random variable is a variable whose value is determined by the outcome of an experiment. The connection between data and randomness:\n\nRandom variable notation: \\(X\\) (uppercase) represents the random variable\nRealized value notation: \\(x\\) (lowercase) represents the observed value\n\nExample - Coin Toss:\n\nExperiment: Toss a fair coin\nRandom variable: \\(X = 1\\) if heads, \\(X = 0\\) if tails\nEach outcome has probability 0.5\n\nKey properties:\nMean (Expected Value): \\[\\mu = E[X] = \\sum_x x \\cdot Pr[X = x]\\]\nFor fair coin: \\(\\mu = 0 \\times 0.5 + 1 \\times 0.5 = 0.5\\)\nVariance: \\[\\sigma^2 = E[(X - \\mu)^2] = \\sum_x (x - \\mu)^2 \\cdot Pr[X = x]\\]\nFor fair coin: \\(\\sigma^2 = (0-0.5)^2 \\times 0.5 + (1-0.5)^2 \\times 0.5 = 0.25\\)\nStandard Deviation: \\(\\sigma = \\sqrt{0.25} = 0.5\\)\n\n# Illustrate coin toss random variable\nprint(\"=\" * 70)\nprint(\"COIN TOSS RANDOM VARIABLE\")\nprint(\"=\" * 70)\n\n# Fair coin properties\nprint(\"\\nFair coin (p = 0.5):\")\nmu_fair = 0 * 0.5 + 1 * 0.5\nvar_fair = (0 - mu_fair)**2 * 0.5 + (1 - mu_fair)**2 * 0.5\nsigma_fair = np.sqrt(var_fair)\n\nprint(f\"  Mean (μ):              {mu_fair:.4f}\")\nprint(f\"  Variance (σ²):         {var_fair:.4f}\")\nprint(f\"  Standard deviation (σ): {sigma_fair:.4f}\")\n\n# Unfair coin for comparison\nprint(\"\\nUnfair coin (p = 0.6 for heads):\")\nmu_unfair = 0 * 0.4 + 1 * 0.6\nvar_unfair = (0 - mu_unfair)**2 * 0.4 + (1 - mu_unfair)**2 * 0.6\nsigma_unfair = np.sqrt(var_unfair)\n\nprint(f\"  Mean (μ):              {mu_unfair:.4f}\")\nprint(f\"  Variance (σ²):         {var_unfair:.4f}\")\nprint(f\"  Standard deviation (σ): {sigma_unfair:.4f}\")\n\n======================================================================\nCOIN TOSS RANDOM VARIABLE\n======================================================================\n\nFair coin (p = 0.5):\n  Mean (μ):              0.5000\n  Variance (σ²):         0.2500\n  Standard deviation (σ): 0.5000\n\nUnfair coin (p = 0.6 for heads):\n  Mean (μ):              0.6000\n  Variance (σ²):         0.2400\n  Standard deviation (σ): 0.4899\n\n\n\nKey Concept 3.1: Random Variables\nA random variable \\(X\\) is a variable whose value is determined by the outcome of an unpredictable experiment. The mean \\(\\mu = \\mathrm{E}[X]\\) is the probability-weighted average of all possible values, while the variance \\(\\sigma^2 = \\mathrm{E}[(X-\\mu)^2]\\) measures variability around the mean. These population parameters characterize the distribution from which we draw samples.\n\nTransition: Now that we understand random variables theoretically, let’s see them in action through a simple experiment: coin tosses. We’ll discover how the sample mean behaves when we repeat the experiment many times.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#experiment-coin-tosses",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#experiment-coin-tosses",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.2 Experiment: Coin Tosses",
    "text": "3.2 Experiment: Coin Tosses\n\nOne Sample\nNow we conduct an actual experiment: toss a coin 30 times and record the results. This gives us a sample of size \\(n = 30\\).\nKey insight: The observed values \\(x_1, x_2, ..., x_{30}\\) are realizations of random variables \\(X_1, X_2, ..., X_{30}\\).\nThe sample mean is: \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\]\nThis \\(\\bar{x}\\) is itself a realization of the random variable: \\[\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\]\n\n# Generate single sample of 30 coin tosses\nnp.random.seed(10101)\nu = np.random.uniform(0, 1, 30)\nx = np.where(u &gt; 0.5, 1, 0)  # 1 if heads, 0 if tails\n\nprint(\"=\" * 70)\nprint(\"SINGLE COIN TOSS SAMPLE (n = 30)\")\nprint(\"=\" * 70)\nprint(f\"\\nNumber of heads (x=1):  {np.sum(x)}\")\nprint(f\"Number of tails (x=0):  {np.sum(1-x)}\")\nprint(f\"Sample mean (x̄):        {np.mean(x):.4f}\")\nprint(f\"Sample std dev (s):     {np.std(x, ddof=1):.4f}\")\n\nprint(f\"\\nPopulation values:\")\nprint(f\"  Population mean (μ):   0.5000\")\nprint(f\"  Population std (σ):    0.5000\")\n\n# Visualize single sample\nfig, ax = plt.subplots(figsize=(8, 6))\nax.hist(x, bins=[-0.5, 0.5, 1.5], edgecolor='black', alpha=0.7, color='steelblue')\nax.set_xlabel('Outcome (0 = Tails, 1 = Heads)', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.set_title('Single Sample of 30 Coin Tosses',\n             fontsize=14, fontweight='bold')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Tails (0)', 'Heads (1)'])\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: This is just ONE realization of the random variable X̄.\")\nprint(\"To understand X̄'s distribution, we need MANY samples...\")\n\n======================================================================\nSINGLE COIN TOSS SAMPLE (n = 30)\n======================================================================\n\nNumber of heads (x=1):  12\nNumber of tails (x=0):  18\nSample mean (x̄):        0.4000\nSample std dev (s):     0.4983\n\nPopulation values:\n  Population mean (μ):   0.5000\n  Population std (σ):    0.5000\n\n\n\n\n\n\n\n\n\n\nNote: This is just ONE realization of the random variable X̄.\nTo understand X̄'s distribution, we need MANY samples...\n\n\n\nKey Concept 3.2: Sample Mean as Random Variable\nThe observed sample mean \\(\\bar{x}\\) is a realization of the random variable \\(\\bar{X} = (X_1 + \\cdots + X_n)/n\\). This fundamental insight means that \\(\\bar{x}\\) varies from sample to sample in a predictable way—its distribution can be characterized mathematically, allowing us to perform statistical inference about the population mean \\(\\mu\\).\n\nKey findings from our coin toss experiment (n = 30):\n1. Sample mean = 0.4000 (vs. theoretical μ = 0.5)\n\nWe got 12 heads and 18 tails (40% vs. expected 50%)\nThis difference (0.10 or 10 percentage points) is completely normal\nWith only 30 tosses, random variation of this magnitude is expected\nIf we flipped 1000 times, we’d expect to get much closer to 50%\n\n2. Sample standard deviation = 0.4983 (vs. theoretical σ = 0.5000)\n\nNearly perfect match with population value\nThis confirms the theoretical formula: σ² = p(1-p) = 0.5(0.5) = 0.25, so σ = 0.5\n\n3. Why the sample mean differs from 0.5:\n\nThe sample mean x̄ is itself a random variable\nJust like one coin toss doesn’t always give heads, one sample mean doesn’t always equal μ\nThis single value (0.4000) is one realization from the sampling distribution of X̄\nThe next experiment would likely give a different value (maybe 0.4667 or 0.5333)\n\nEconomic interpretation: When we estimate average income from a survey or unemployment rate from a sample, we get one realization that will differ from the true population value. Understanding this variability is the foundation of statistical inference.\n\n\n400 Samples and The Distribution of Sample Means\nTo understand the sampling distribution of \\(\\bar{X}\\), we repeat the experiment 400 times:\n\nEach experiment: 30 coin tosses → one sample mean \\(\\bar{x}_i\\)\nAfter 400 experiments: we have 400 sample means (\\(\\bar{x}_1, \\bar{x}_2, ..., \\bar{x}_{400}\\))\nThe histogram of these 400 values approximates the sampling distribution of \\(\\bar{X}\\)\n\nWhat we expect to see:\n\nSample means centered near \\(\\mu = 0.5\\) (population mean)\nMuch less variability than individual coin tosses\nApproximately normal distribution (Central Limit Theorem!)\n\n\n# Load precomputed coin toss means data (400 samples of size 30)\ndata_cointoss = pd.read_stata(GITHUB_DATA_URL + 'AED_COINTOSSMEANS.DTA')\n\nprint(\"=\" * 70)\nprint(\"400 COIN TOSS EXPERIMENTS (each n = 30)\")\nprint(\"=\" * 70)\n\nxbar = data_cointoss['xbar']\n\nprint(f\"\\nSummary of 400 sample means:\")\nprint(data_cointoss.describe())\n\nprint(f\"\\nFirst 5 sample means: {xbar.head().tolist()}\")\nprint(f\"\\nMean of the 400 sample means: {xbar.mean():.4f}\")\nprint(f\"Std dev of the 400 sample means: {xbar.std():.4f}\")\n\nprint(f\"\\nTheoretical predictions:\")\nprint(f\"  E[X̄] = μ = 0.5000\")\nprint(f\"  σ(X̄) = σ/√n = √(0.25/30) = {np.sqrt(0.25/30):.4f}\")\n\nprint(f\"\\nComparison:\")\nprint(f\"  Empirical mean: {xbar.mean():.4f} vs Theoretical: 0.5000\")\nprint(f\"  Empirical std:  {xbar.std():.4f} vs Theoretical: {np.sqrt(0.25/30):.4f}\")\nprint(\"\\nExcellent agreement between theory and experiment!\")\n\n======================================================================\n400 COIN TOSS EXPERIMENTS (each n = 30)\n======================================================================\n\nSummary of 400 sample means:\n             xbar       stdev  numobs\ncount  400.000000  400.000000   400.0\nmean     0.499417    0.500826    30.0\nstd      0.086307    0.010360     0.0\nmin      0.266667    0.449776    30.0\n25%      0.433333    0.498273    30.0\n50%      0.500000    0.504007    30.0\n75%      0.566667    0.507416    30.0\nmax      0.733333    0.508548    30.0\n\nFirst 5 sample means: [0.3333333432674408, 0.5, 0.5333333611488342, 0.5666666626930237, 0.5]\n\nMean of the 400 sample means: 0.4994\nStd dev of the 400 sample means: 0.0863\n\nTheoretical predictions:\n  E[X̄] = μ = 0.5000\n  σ(X̄) = σ/√n = √(0.25/30) = 0.0913\n\nComparison:\n  Empirical mean: 0.4994 vs Theoretical: 0.5000\n  Empirical std:  0.0863 vs Theoretical: 0.0913\n\nExcellent agreement between theory and experiment!\n\n\nKey findings from 400 coin toss experiments:\n1. Mean of sample means = 0.4994 (vs. theoretical μ = 0.5)\n\nThis demonstrates unbiasedness: E[X̄] = μ\nThe tiny difference (0.0006) is just random variation\nWith more replications, this would get even closer to 0.5\nOn average across many samples, X̄ equals the true population mean\n\n2. Standard deviation of sample means = 0.0863 (vs. theoretical = 0.0913)\n\nThe theoretical standard error is σ/√n = 0.5/√30 = 0.0913\nOur empirical SD (0.0863) is very close to this prediction\nThis confirms the variance formula: Var(X̄) = σ²/n works in practice\n\n3. Range of sample means: 0.2667 to 0.7333\n\nIndividual sample means vary considerably (from 26.7% to 73.3% heads)\nThis shows why we need statistical theory - any single sample could be misleading\nBut the distribution is predictable and centered correctly\n\n4. Comparison with single coin tosses:\n\nIndividual coin tosses have σ = 0.5\nSample means have σ(X̄) = 0.0863\nSample means are 5.8× less variable than individual tosses\nThis is the power of averaging: √30 ≈ 5.5\n\nEconomic interpretation: When estimating economic parameters (average wage, inflation rate, GDP growth), individual survey responses vary widely, but the sample mean is much more precise. The standard error tells us exactly how much precision we gain from our sample size.\n\n\nVisualizing the Sampling Distribution of \\(\\bar{X}\\)\nThe histogram below shows the distribution of the 400 sample means. Notice:\n\nCenter: Near 0.5 (the population mean \\(\\mu\\))\nSpread: Much narrower than the original population (σ = 0.5)\nShape: Approximately bell-shaped (normal distribution)\n\nThe red curve is the theoretical normal distribution with mean \\(\\mu = 0.5\\) and standard deviation \\(\\sigma/\\sqrt{n} = 0.091\\).\n\n# Visualize distribution of sample means\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Histogram of 400 sample means\nn_hist, bins, patches = ax.hist(xbar, bins=30, density=True,\n                                 edgecolor='black', alpha=0.7, color='steelblue',\n                                 label='400 sample means')\n\n# Overlay theoretical normal distribution\nxbar_range = np.linspace(xbar.min(), xbar.max(), 100)\ntheoretical_pdf = stats.norm.pdf(xbar_range, 0.5, np.sqrt(0.25/30))\nax.plot(xbar_range, theoretical_pdf, 'r-', linewidth=3,\n        label=f'Theoretical N(0.5, {np.sqrt(0.25/30):.3f})')\n\nax.set_xlabel('Sample mean from each of 400 samples', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Figure 3.1B: Distribution of Sample Means (Central Limit Theorem)',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"The empirical distribution matches the theoretical normal distribution!\")\nprint(\"This is the Central Limit Theorem in action.\")\n\n\n\n\n\n\n\n\nThe empirical distribution matches the theoretical normal distribution!\nThis is the Central Limit Theorem in action.\n\n\nTransition: Having observed the sampling distribution empirically through coin tosses, we can now derive its properties mathematically and understand why it behaves the way it does.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#properties-of-the-sample-mean",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#properties-of-the-sample-mean",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.3 Properties of the Sample Mean",
    "text": "3.3 Properties of the Sample Mean\nUnder the assumption of a simple random sample where:\n\nA. Each \\(X_i\\) has common mean: \\(E[X_i] = \\mu\\)\nB. Each \\(X_i\\) has common variance: \\(Var[X_i] = \\sigma^2\\)\nC. The \\(X_i\\) are statistically independent\n\nWe can mathematically prove:\nMean of the sample mean: \\[E[\\bar{X}] = \\mu\\]\nThis says \\(\\bar{X}\\) is unbiased for \\(\\mu\\) (its expected value equals the parameter we’re estimating).\nVariance of the sample mean: \\[Var[\\bar{X}] = \\frac{\\sigma^2}{n}\\]\nStandard deviation of the sample mean: \\[SD[\\bar{X}] = \\frac{\\sigma}{\\sqrt{n}}\\]\nKey insights:\n\nAs sample size \\(n\\) increases, \\(Var[\\bar{X}]\\) decreases (\\(\\propto 1/n\\))\nLarger samples give more precise estimates (smaller variability)\nStandard deviation decreases at rate $1/$ (to halve uncertainty, need 4× the sample size)\n\n\n# Demonstrate how variance of X̄ depends on sample size n\nprint(\"=\" * 70)\nprint(\"HOW SAMPLE SIZE AFFECTS PRECISION\")\nprint(\"=\" * 70)\n\n# For coin toss: μ = 0.5, σ² = 0.25, σ = 0.5\nmu = 0.5\nsigma = 0.5\nsigma_sq = 0.25\n\nsample_sizes = [10, 30, 100, 400, 1000]\n\nprint(f\"\\nPopulation: μ = {mu}, σ = {sigma}\")\nprint(f\"\\n{'n':&lt;10} {'σ(X̄) = σ/√n':&lt;20} {'Var(X̄) = σ²/n':&lt;20}\")\nprint(\"-\" * 50)\n\nfor n in sample_sizes:\n    sd_xbar = sigma / np.sqrt(n)\n    var_xbar = sigma_sq / n\n    print(f\"{n:&lt;10} {sd_xbar:&lt;20.6f} {var_xbar:&lt;20.6f}\")\n\nprint(\"\\nKey observation: Doubling n reduces σ(X̄) by factor of √2 ≈ 1.41\")\nprint(\"To halve uncertainty, need to quadruple sample size.\")\n\n======================================================================\nHOW SAMPLE SIZE AFFECTS PRECISION\n======================================================================\n\nPopulation: μ = 0.5, σ = 0.5\n\nn          σ(X̄) = σ/√n         Var(X̄) = σ²/n      \n--------------------------------------------------\n10         0.158114             0.025000            \n30         0.091287             0.008333            \n100        0.050000             0.002500            \n400        0.025000             0.000625            \n1000       0.015811             0.000250            \n\nKey observation: Doubling n reduces σ(X̄) by factor of √2 ≈ 1.41\nTo halve uncertainty, need to quadruple sample size.\n\n\n\nKey Concept 3.3: Properties of the Sample Mean\nUnder simple random sampling (common mean \\(\\mu\\), common variance \\(\\sigma^2\\), independence), the sample mean \\(\\bar{X}\\) has mean \\(\\mathrm{E}[\\bar{X}] = \\mu\\) (unbiased) and variance \\(\\operatorname{Var}[\\bar{X}] = \\sigma^2/n\\) (decreases with sample size). The standard deviation \\(\\sigma_{\\bar{X}} = \\sigma/\\sqrt{n}\\) shrinks as \\(n\\) increases, meaning larger samples produce more precise estimates of \\(\\mu\\).\n\n\n# Illustrate standard error calculation\nprint(\"=\" * 70)\nprint(\"STANDARD ERROR CALCULATION\")\nprint(\"=\" * 70)\n\n# Use our single sample from earlier\nn = len(x)\nx_mean = np.mean(x)\nx_std = np.std(x, ddof=1)  # Sample standard deviation\nse_xbar = x_std / np.sqrt(n)\n\nprint(f\"\\nSample statistics (n = {n}):\")\nprint(f\"  Sample mean (x̄):          {x_mean:.4f}\")\nprint(f\"  Sample std dev (s):        {x_std:.4f}\")\nprint(f\"  Standard error se(X̄):     {se_xbar:.4f}\")\n\nprint(f\"\\nPopulation values:\")\nprint(f\"  Population mean (μ):       0.5000\")\nprint(f\"  Population std (σ):        0.5000\")\nprint(f\"  True σ/√n:                 {0.5/np.sqrt(n):.4f}\")\n\nprint(f\"\\nInterpretation:\")\nprint(f\"  The standard error {se_xbar:.4f} tells us the typical distance\")\nprint(f\"  between our sample mean ({x_mean:.4f}) and the true population mean (0.5).\")\n\n======================================================================\nSTANDARD ERROR CALCULATION\n======================================================================\n\nSample statistics (n = 30):\n  Sample mean (x̄):          0.4000\n  Sample std dev (s):        0.4983\n  Standard error se(X̄):     0.0910\n\nPopulation values:\n  Population mean (μ):       0.5000\n  Population std (σ):        0.5000\n  True σ/√n:                 0.0913\n\nInterpretation:\n  The standard error 0.0910 tells us the typical distance\n  between our sample mean (0.4000) and the true population mean (0.5).\n\n\n\nInterpreting the Standard Error\nKey findings from our standard error calculation (n = 30):\n1. Sample mean = 0.4000 with standard error = 0.0910\n\nThe standard error tells us the typical distance between x̄ and μ\nOur sample mean (0.40) is about 1.1 standard errors below the true mean (0.50)\nThis is well within normal sampling variation (within 2 standard errors)\n\n2. Estimated SE = 0.0910 vs. True σ/√n = 0.0913\n\nWe used sample standard deviation s = 0.4983 instead of σ = 0.5\nOur estimate is remarkably accurate (only 0.0003 difference)\nIn practice, we never know σ, so we always use s to compute the standard error\n\n3. What the standard error means:\n\nIf we repeated this experiment many times, our sample means would typically differ from 0.5 by about 0.091\nAbout 68% of sample means would fall within ±0.091 of 0.5 (between 0.409 and 0.591)\nAbout 95% would fall within ±0.182 of 0.5 (between 0.318 and 0.682)\n\n4. How to reduce the standard error:\n\nTo halve the SE, we’d need to quadruple the sample size (n = 120)\nTo cut SE by 10×, we’d need 100× the sample size (n = 3000)\nThis is why larger surveys are more precise but also more expensive\n\nEconomic interpretation: When a poll reports “margin of error ±3%”, they’re referring to approximately 2 standard errors. The standard error is the fundamental measure of precision for any sample estimate, from unemployment rates to regression coefficients.\n\nKey Concept 3.4: Standard Error of the Mean\nThe standard error se(\\(\\bar{X}\\)) = \\(s/\\sqrt{n}\\) is the estimated standard deviation of the sample mean. It measures the precision of \\(\\bar{x}\\) as an estimate of \\(\\mu\\). Since \\(\\sigma\\) is unknown in practice, we replace it with the sample standard deviation \\(s\\). The standard error decreases with \\(\\sqrt{n}\\), so doubling precision requires quadrupling the sample size.\n\n\n\nCentral Limit Theorem\nThe Central Limit Theorem (CLT) is one of the most important results in statistics:\nStatement: If \\(X_1, ..., X_n\\) are independent random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then as \\(n \\to \\infty\\):\n\\[\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ approximately}\\]\nOr equivalently, the standardized sample mean:\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1) \\text{ approximately}\\]\nRemarkable facts:\n\nThis holds regardless of the distribution of \\(X\\) (doesn’t have to be normal!)\nWorks well even for moderate sample sizes (\\(n \\geq 30\\) is common rule of thumb)\nProvides justification for using normal-based inference\n\nStandard error: Since \\(\\sigma\\) is typically unknown, we estimate it with sample standard deviation \\(s\\):\n\\[se(\\bar{X}) = \\frac{s}{\\sqrt{n}}\\]\n\nKey Concept 3.5: The Central Limit Theorem\nThe Central Limit Theorem states that the standardized sample mean \\(Z = (\\bar{X} - \\mu)/(\\sigma/\\sqrt{n})\\) converges to a standard normal distribution N(0,1) as \\(n \\rightarrow \\infty\\). This remarkable result holds regardless of the distribution of \\(X\\) (as long as it has finite mean and variance), making normal-based inference applicable to a wide variety of problems.\n\nTransition: The coin toss example showed us the Central Limit Theorem with a simple binary variable. Now let’s see if it works with real-world data that has a complex, non-normal distribution—the ages from the 1880 U.S. Census.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#real-data-example---1880-u.s.-census",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#real-data-example---1880-u.s.-census",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.4 Real Data Example - 1880 U.S. Census",
    "text": "3.4 Real Data Example - 1880 U.S. Census\nNow we move from coin tosses to real economic/demographic data. The 1880 U.S. Census recorded ages of all 50,169,452 people in the United States.\nPopulation parameters (known because we have full census):\n\nPopulation mean age: \\(\\mu = 24.13\\) years\nPopulation std dev: \\(\\sigma = 18.61\\) years\nDistribution: Highly non-normal (skewed, peaks at multiples of 5 due to rounding)\n\nExperiment:\n\nDraw 100 random samples, each of size \\(n = 25\\)\nCalculate sample mean age for each sample\nExamine distribution of these 100 sample means\n\nQuestion: Even though population ages are NOT normally distributed, will the sample means be approximately normal? (CLT says yes!)\n\n# Load census age means data\ndata_census = pd.read_stata(GITHUB_DATA_URL + 'AED_CENSUSAGEMEANS.DTA')\n\nprint(\"=\" * 70)\nprint(\"1880 U.S. CENSUS - 100 SAMPLES OF SIZE 25\")\nprint(\"=\" * 70)\n\n# Get the mean variable\nif 'mean' in data_census.columns:\n    age_means = data_census['mean']\nelif 'xmean' in data_census.columns:\n    age_means = data_census['xmean']\nelse:\n    age_means = data_census.iloc[:, 0]\n\nprint(\"\\nSummary of 100 sample means:\")\nprint(data_census.describe())\n\nprint(f\"\\nFirst 5 sample means: {age_means.head().tolist()}\")\nprint(f\"\\nMean of the 100 sample means:   {age_means.mean():.2f} years\")\nprint(f\"Std dev of the 100 sample means: {age_means.std():.2f} years\")\n\nprint(f\"\\nTheoretical predictions:\")\nprint(f\"  E[X̄] = μ = 24.13 years\")\nprint(f\"  σ(X̄) = σ/√n = 18.61/√25 = {18.61/np.sqrt(25):.2f} years\")\n\nprint(f\"\\nComparison:\")\nprint(f\"  Empirical mean: {age_means.mean():.2f} vs Theoretical: 24.13\")\nprint(f\"  Empirical std:  {age_means.std():.2f} vs Theoretical: {18.61/np.sqrt(25):.2f}\")\nprint(\"\\nClose agreement, despite non-normal population distribution!\")\n\n======================================================================\n1880 U.S. CENSUS - 100 SAMPLES OF SIZE 25\n======================================================================\n\nSummary of 100 sample means:\n             mean       stdev  numobs\ncount  100.000000  100.000000   100.0\nmean    23.782001   18.245018    25.0\nstd      3.760694    2.890753     0.0\nmin     14.600000   12.362847    25.0\n25%     22.020000   16.148388    25.0\n50%     23.759999   18.434547    25.0\n75%     26.190000   20.387874    25.0\nmax     33.439999   25.306587    25.0\n\nFirst 5 sample means: [27.84000015258789, 19.399999618530273, 23.280000686645508, 26.84000015258789, 26.559999465942383]\n\nMean of the 100 sample means:   23.78 years\nStd dev of the 100 sample means: 3.76 years\n\nTheoretical predictions:\n  E[X̄] = μ = 24.13 years\n  σ(X̄) = σ/√n = 18.61/√25 = 3.72 years\n\nComparison:\n  Empirical mean: 23.78 vs Theoretical: 24.13\n  Empirical std:  3.76 vs Theoretical: 3.72\n\nClose agreement, despite non-normal population distribution!\n\n\nKey findings from 100 samples of 1880 U.S. Census ages (n = 25 each):\n1. Mean of sample means = 23.78 years (vs. true population μ = 24.13)\n\nDifference of only 0.35 years (less than 1.5% error)\nThis again demonstrates unbiasedness\nWith only 100 samples, some sampling error is expected\n\n2. Standard deviation of sample means = 3.76 years (vs. theoretical = 3.72)\n\nTheoretical: σ/√n = 18.61/√25 = 3.72 years\nEmpirical: 3.76 years\nExcellent agreement between theory and data (within 1%)\n\n3. Range of sample means: 14.6 to 33.4 years\n\nIndividual sample means vary by almost 19 years\nBut most cluster tightly around 24 years\nThis wide range shows why statistical theory matters\n\n4. The power of the Central Limit Theorem:\n\nThe population distribution of ages in 1880 was highly non-normal:\n\nMany young children (high frequency at low ages)\nHeaping at multiples of 5 (people rounded their ages)\nLong right tail (elderly people)\n\nYet the distribution of sample means IS approximately normal\nThis is the CLT’s remarkable power - normality emerges from averaging\n\n5. Practical implications for sample size:\n\nWith n = 25, the standard error is 3.72 years\nTo estimate average age within ±1 year (95% confidence), we’d need about 4 times larger samples\nThe Census Bureau uses this logic to design survey sizes\n\nEconomic interpretation: Real economic data (income, age, consumption) is rarely normally distributed - often highly skewed or irregular. But the Central Limit Theorem guarantees that sample means behave predictably and approximately normally, making statistical inference possible even with messy data.\n\nVisualization: Census Sample Means Distribution\nThis figure demonstrates the Central Limit Theorem with real data. Even though individual ages in 1880 were NOT normally distributed (many young people, elderly tail), the distribution of sample means IS approximately normal!\n\n# Visualize distribution of census age means\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Histogram\nn_hist, bins, patches = ax.hist(age_means, bins=20, density=True,\n                                 edgecolor='black', alpha=0.7, color='coral',\n                                 label='100 sample means')\n\n# Overlay theoretical normal distribution\nage_range = np.linspace(age_means.min(), age_means.max(), 100)\ntheoretical_pdf = stats.norm.pdf(age_range, 24.13, 18.61/np.sqrt(25))\nax.plot(age_range, theoretical_pdf, 'b-', linewidth=3,\n        label=f'Theoretical N(24.13, {18.61/np.sqrt(25):.2f})')\n\nax.set_xlabel('Sample mean age (years)', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Figure 3.3: Distribution of Sample Means from 1880 U.S. Census',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Central Limit Theorem validated with real census data!\")\nprint(\"Sample means are approximately normal, even though ages are not.\")\n\n\n\n\n\n\n\n\nCentral Limit Theorem validated with real census data!\nSample means are approximately normal, even though ages are not.\n\n\n\nKey Concept 3.6: CLT in Practice\nThe Central Limit Theorem is not just a mathematical curiosity—it works with real data. Even when the population distribution is highly non-normal (like the skewed 1880 Census ages with heaping at multiples of 5), the distribution of sample means becomes approximately normal for moderate sample sizes. This validates using normal-based inference methods across diverse economic applications.\n\n\n\nInterpreting the Simulation Results\nKey findings from 400 simulated coin toss samples:\n1. Mean of simulated sample means = 0.5004 (vs. theoretical μ = 0.5)\n\nPerfect agreement (difference of only 0.0004)\nThis validates our simulation code\nConfirms the theoretical prediction E[X̄] = μ\n\n2. Standard deviation of simulated means = 0.0887 (vs. theoretical = 0.0913)\n\nClose agreement (within 3%)\nTheoretical: σ/√n = √(0.25/30) = 0.0913\nThe small difference is random simulation noise\n\n3. Range of simulated means: 0.2667 to 0.7667\n\nMatches the theoretical range well\nAbout 95% of values fall within μ ± 2σ(X̄) = 0.5 ± 0.183\nThis is exactly what we’d expect from a normal distribution\n\n4. Why simulation matters:\n\nValidation: We’ve confirmed that theory matches practice\nIntuition: We can see the CLT in action, not just read about it\nFlexibility: We can simulate complex scenarios where theory is hard\nModern econometrics: Bootstrap, Monte Carlo methods rely on simulation\n\n5. Reproducibility with random seeds:\n\nBy setting np.random.seed(10101), we get identical results every time\nEssential for scientific reproducibility\nIn research, always document your random seed\n\n6. The simulation matches the pre-computed data:\n\nEarlier we loaded AED_COINTOSSMEANS.DTA with mean 0.4994, sd 0.0863\nOur simulation gave mean 0.5004, sd 0.0887\nThese match closely (differences are just from different random seeds)\n\nEconomic interpretation: Modern econometric research heavily uses simulation methods (bootstrap standard errors, Monte Carlo integration, Bayesian MCMC). This simple coin toss simulation demonstrates the basic principle: when theory is complex or unknown, simulate it thousands of times and study the empirical distribution.\nHaving seen the Central Limit Theorem at work with both coins and census data, let’s formalize what makes the sample mean a good estimator.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#estimator-properties",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#estimator-properties",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.5 Estimator Properties",
    "text": "3.5 Estimator Properties\nWhy use the sample mean \\(\\bar{X}\\) to estimate the population mean \\(\\mu\\)? Because it has desirable statistical properties:\n1. Unbiasedness: An estimator is unbiased if its expected value equals the parameter: \\[E[\\bar{X}] = \\mu\\]\nThis means on average (across many samples), \\(\\bar{X}\\) equals \\(\\mu\\) (no systematic over- or under-estimation).\n2. Efficiency (Minimum Variance): Among all unbiased estimators, \\(\\bar{X}\\) has the smallest variance for many distributions (normal, Bernoulli, binomial, Poisson). An estimator with minimum variance is called efficient or best.\n3. Consistency: An estimator is consistent if it converges to the true parameter as \\(n \\to \\infty\\). For \\(\\bar{X}\\):\n\n\\(E[\\bar{X}] = \\mu\\) (unbiased, no bias to disappear)\n\\(Var[\\bar{X}] = \\sigma^2/n \\to 0\\) as \\(n \\to \\infty\\) (variance shrinks to zero)\n\nTherefore \\(\\bar{X}\\) is consistent for \\(\\mu\\).\nEconomic application: These properties justify using sample means to estimate average income, unemployment rates, GDP per capita, etc.\n\n# Illustrate consistency: variance shrinks as n increases\nprint(\"=\" * 70)\nprint(\"CONSISTENCY: VARIANCE SHRINKS AS n INCREASES\")\nprint(\"=\" * 70)\n\nsample_sizes = [5, 10, 25, 50, 100, 500, 1000, 5000]\nsigma = 18.61  # Census population std dev\n\nprint(f\"\\nPopulation std deviation: σ = {sigma:.2f}\")\nprint(f\"\\n{'Sample size n':&lt;15} {'Var(X̄) = σ²/n':&lt;20} {'SD(X̄) = σ/√n':&lt;20}\")\nprint(\"-\" * 55)\n\nfor n in sample_sizes:\n    var_xbar = sigma**2 / n\n    sd_xbar = sigma / np.sqrt(n)\n    print(f\"{n:&lt;15} {var_xbar:&lt;20.2f} {sd_xbar:&lt;20.4f}\")\n\nprint(\"\\nAs n → ∞, Var(X̄) → 0 and SD(X̄) → 0\")\nprint(\"This guarantees X̄ converges to μ (consistency).\")\n\n======================================================================\nCONSISTENCY: VARIANCE SHRINKS AS n INCREASES\n======================================================================\n\nPopulation std deviation: σ = 18.61\n\nSample size n   Var(X̄) = σ²/n       SD(X̄) = σ/√n       \n-------------------------------------------------------\n5               69.27                8.3226              \n10              34.63                5.8850              \n25              13.85                3.7220              \n50              6.93                 2.6319              \n100             3.46                 1.8610              \n500             0.69                 0.8323              \n1000            0.35                 0.5885              \n5000            0.07                 0.2632              \n\nAs n → ∞, Var(X̄) → 0 and SD(X̄) → 0\nThis guarantees X̄ converges to μ (consistency).\n\n\n\nKey Concept 3.7: Properties of Good Estimators\nA good estimator should be unbiased (E[\\(\\bar{X}\\)] = \\(\\mu\\)), consistent (converges to \\(\\mu\\) as \\(n \\rightarrow \\infty\\)), and efficient (minimum variance among unbiased estimators). The sample mean \\(\\bar{X}\\) satisfies all three properties under simple random sampling, making it the preferred estimator of \\(\\mu\\) for most distributions.\n\nTransition: So far we’ve assumed simple random sampling where all observations are independent and identically distributed. But what happens when this assumption is violated? Let’s explore alternative sampling methods.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#samples-other-than-simple-random-samples",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#samples-other-than-simple-random-samples",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.7 Samples other than Simple Random Samples",
    "text": "3.7 Samples other than Simple Random Samples\nThe simple random sample assumptions (A-C from Section 3.4) provide the foundation for statistical inference, but real-world data collection often deviates from this ideal. Understanding these deviations is critical for proper analysis.\nRecall simple random sample assumptions:\n\nA. Common mean: \\(\\mathrm{E}[X_i] = \\mu\\) for all \\(i\\)\nB. Common variance: \\(\\operatorname{Var}[X_i] = \\sigma^2\\) for all \\(i\\)\nC. Statistical independence: \\(X_i\\) and \\(X_j\\) are independent\n\nTwo types of deviations:\n\nRepresentative samples (relaxes assumption C only):\n\nStill from the same distribution (\\(\\mu\\) and \\(\\sigma^2\\) constant)\nBut observations are NO LONGER independent\nExample: Cluster sampling (surveying all students in randomly selected schools)\nSolution: Adjust the standard error formula to account for dependence\n\nNonrepresentative samples (violates assumption A):\n\nDifferent observations have DIFFERENT population means\nExample: Surveying Golf Digest readers to estimate average U.S. income\nGolf magazine readers have higher income than general population (\\(\\mu_i \\neq \\mu\\))\nBig problem - standard inference methods fail completely\nSolution: Use weighted means if inclusion probabilities are known\n\n\nWeighted Mean Approach:\nWhen inclusion probabilities \\(\\pi_i\\) are known, construct weighted estimates:\n\n\\(\\pi_i\\) = probability that observation \\(i\\) is included in the sample\nSample weight: \\(w_i = 1/\\pi_i\\) (inverse probability weighting)\nWeighted mean: \\[\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\]\n\nExample:\n\nSuppose women have 70% probability of inclusion (\\(\\pi_{female} = 0.7\\), \\(w_{female} = 1.43\\))\nMen have 30% probability of inclusion (\\(\\pi_{male} = 0.3\\), \\(w_{male} = 3.33\\))\nWeighted mean corrects for oversampling of women\n\nEconomic applications:\n\nHousehold surveys often oversample certain groups (low-income, minorities)\nSurvey weights correct for unequal sampling probabilities\nMajor surveys (CPS, ACS, PSID) provide sampling weights in datasets\n\n\n# Demonstrate weighted vs. unweighted mean\nprint(\"=\" * 70)\nprint(\"WEIGHTED MEAN EXAMPLE\")\nprint(\"=\" * 70)\n\n# Simulate population with two groups\nnp.random.seed(42)\nn_men = 50\nn_women = 50\n\n# Men have higher average income\nincome_men = np.random.normal(60000, 15000, n_men)\nincome_women = np.random.normal(50000, 15000, n_women)\n\ntrue_pop_mean = (income_men.mean() + income_women.mean()) / 2\n\nprint(f\"\\nTrue population means:\")\nprint(f\"  Men:   ${income_men.mean():,.0f}\")\nprint(f\"  Women: ${income_women.mean():,.0f}\")\nprint(f\"  Overall: ${true_pop_mean:,.0f}\")\n\n# Biased sample: oversample women (70% women, 30% men)\nsample_men = np.random.choice(income_men, size=15, replace=False)\nsample_women = np.random.choice(income_women, size=35, replace=False)\nsample = np.concatenate([sample_men, sample_women])\n\n# Unweighted mean (WRONG - biased toward women)\nunweighted_mean = sample.mean()\n\n# Weighted mean (CORRECT - accounts for oversampling)\nweights = np.concatenate([np.repeat(1/0.3, 15), np.repeat(1/0.7, 35)])\nweighted_mean = np.average(sample, weights=weights)\n\nprint(f\"\\nSample estimates:\")\nprint(f\"  Unweighted mean: ${unweighted_mean:,.0f} (biased toward women)\")\nprint(f\"  Weighted mean:   ${weighted_mean:,.0f} (corrected)\")\nprint(f\"\\nBias:\")\nprint(f\"  Unweighted bias: ${unweighted_mean - true_pop_mean:,.0f}\")\nprint(f\"  Weighted bias:   ${weighted_mean - true_pop_mean:,.0f}\")\nprint(\"\\nWeighting corrects for nonrepresentative sampling!\")\n\n\nKey Concept 3.8: Simple Random Sampling Assumptions\nSimple random sampling assumes all observations come from the same distribution with common mean \\(\\mu\\). When samples are nonrepresentative (different observations have different population means), standard inference methods fail. Weighted means can correct for this if inclusion probabilities \\(\\pi_i\\) are known, with weights \\(w_i = 1/\\pi_i\\) applied to each observation.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#computer-generation-of-a-random-variable",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#computer-generation-of-a-random-variable",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.8 Computer Generation of a Random Variable",
    "text": "3.8 Computer Generation of a Random Variable\nModern statistics relies heavily on computer simulation to:\n\nGenerate random samples from known distributions\nStudy properties of estimators\nValidate theoretical results\n\nHow computers generate randomness:\nComputers use pseudo-random number generators (PRNGs):\n\nNot truly random, but appear random for practical purposes\nGenerate values between 0 and 1 (uniform distribution)\nAny value between 0 and 1 is equally likely\nSuccessive values appear independent\n\nTransforming uniform random numbers:\nFrom uniform U(0,1) random numbers, we can generate any distribution:\n\nCoin toss: If \\(U &gt; 0.5\\), then \\(X = 1\\) (heads), else \\(X = 0\\) (tails)\nNormal distribution: Use Box-Muller transform or inverse CDF method\nAny distribution: Inverse transform sampling\n\nExample - Coin toss simulation:\n\nDraw uniform random number \\(U \\sim    ext{Uniform}(0,1)\\)\nIf \\(U &gt; 0.5\\): heads (\\(X=1\\))\nIf \\(U \\leq 0.5\\): tails (\\(X=0\\))\nRepeat 30 times to simulate 30 coin tosses\n\nExample - Census sampling:\n\nPopulation: \\(N = 50,169,452\\) people\nIf uniform draw falls in \\([0, 1/N)\\), select person 1\nIf uniform draw falls in \\([1/N, 2/N)\\), select person 2\nContinue for all \\(N\\) people\n\nThe importance of seeds:\nThe seed is the starting value that determines the entire sequence:\n\nSame seed → identical “random” sequence (reproducibility)\nDifferent seed → different sequence\nBest practice: Always set seed in research code\nExample: np.random.seed(10101)\n\nWhy reproducibility matters:\n\nScientific research must be verifiable\nDebugging requires consistent results\nPublication standards demand reproducible results\n\nLet’s simulate the coin toss experiment ourselves!\n\n# Simulate 400 samples of 30 coin tosses\nprint(\"=\" * 70)\nprint(\"SIMULATION: 400 SAMPLES OF 30 COIN TOSSES\")\nprint(\"=\" * 70)\n\nnp.random.seed(10101)\nn_simulations = 400\nsample_size = 30\n\nresult_mean = np.zeros(n_simulations)\nresult_std = np.zeros(n_simulations)\n\nfor i in range(n_simulations):\n    # Generate sample of coin tosses (Bernoulli with p=0.5)\n    sample = np.random.binomial(1, 0.5, sample_size)\n    result_mean[i] = sample.mean()\n    result_std[i] = sample.std(ddof=1)\n\nprint(f\"\\nSimulation results:\")\nprint(f\"  Mean of 400 sample means:  {result_mean.mean():.4f}\")\nprint(f\"  Std dev of 400 means:      {result_mean.std():.4f}\")\nprint(f\"  Min sample mean:           {result_mean.min():.4f}\")\nprint(f\"  Max sample mean:           {result_mean.max():.4f}\")\n\nprint(f\"\\nTheoretical values:\")\nprint(f\"  E[X̄] = μ:                  0.5000\")\nprint(f\"  σ(X̄) = σ/√n:               {np.sqrt(0.25/30):.4f}\")\n\nprint(f\"\\nPerfect match between simulation and theory!\")\n\n======================================================================\nSIMULATION: 400 SAMPLES OF 30 COIN TOSSES\n======================================================================\n\nSimulation results:\n  Mean of 400 sample means:  0.5004\n  Std dev of 400 means:      0.0887\n  Min sample mean:           0.2667\n  Max sample mean:           0.7667\n\nTheoretical values:\n  E[X̄] = μ:                  0.5000\n  σ(X̄) = σ/√n:               0.0913\n\nPerfect match between simulation and theory!\n\n\nThis figure shows our simulated distribution (green) overlaid with the theoretical normal distribution (red). They match almost perfectly!\n\n# Visualize simulated distribution\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.hist(result_mean, bins=30, density=True,\n        edgecolor='black', alpha=0.7, color='lightgreen',\n        label='Simulated (400 samples)')\n\n# Overlay theoretical normal distribution\nx_range = np.linspace(result_mean.min(), result_mean.max(), 100)\ntheoretical_pdf = stats.norm.pdf(x_range, 0.5, np.sqrt(0.25/30))\nax.plot(x_range, theoretical_pdf, 'r-', linewidth=3,\n        label='Theoretical N(0.5, 0.091)')\n\nax.set_xlabel('Sample mean', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Simulated vs Theoretical Sampling Distribution',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Simulation perfectly replicates theoretical predictions!\")\nprint(\"This validates both our code and the underlying theory.\")\n\n\n\n\n\n\n\n\nSimulation perfectly replicates theoretical predictions!\nThis validates both our code and the underlying theory.\n\n\n\nKey Concept 3.9: Monte Carlo Simulation\nComputers generate pseudo-random numbers using deterministic algorithms that produce sequences appearing random. Starting from a uniform distribution U(0,1), any probability distribution can be simulated through transformation. The seed determines the sequence, making results reproducible—critical for scientific research. Always set and document your random seed.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#key-takeaways",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#key-takeaways",
    "title": "Chapter 3: The Sample Mean",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nRandom Variables and Sampling Distributions: - A random variable \\(X\\) (uppercase) represents an uncertain outcome; its realization \\(x\\) (lowercase) is the observed value - The sample mean \\(\\bar{x}\\) is ONE realization of the random variable \\(\\bar{X} = (X_1 + \\cdots + X_n)/n\\) - The sampling distribution of \\(\\bar{X}\\) describes how \\(\\bar{x}\\) varies across different samples from the same population - Understanding that statistics are random variables is the foundation of statistical inference - Example: Drawing 400 samples of coin tosses (n=30 each) produces 400 different sample means, revealing \\(\\bar{X}\\)’s distribution\nProperties of the Sample Mean (Theoretical Results): - Under simple random sampling (common mean \\(\\mu\\), common variance \\(\\sigma^2\\), independence): - Mean: \\(\\mathrm{E}[\\bar{X}] = \\mu\\) (unbiased estimator) - Variance: \\(\\operatorname{Var}[\\bar{X}] = \\sigma^2/n\\) (precision increases with sample size) - Standard deviation: \\(\\sigma_{\\bar{X}} = \\sigma/\\sqrt{n}\\) (decreases at rate \\(1/\\sqrt{n}\\)) - To halve the standard error, you must quadruple the sample size (e.g., from n=100 to n=400) - The sample mean is less variable than individual observations since \\(\\sigma^2/n &lt; \\sigma^2\\) - As \\(n \\rightarrow \\infty\\), \\(\\bar{X}\\) converges to \\(\\mu\\) because \\(\\operatorname{Var}[\\bar{X}] \\rightarrow 0\\)\nCentral Limit Theorem (Most Important Result): - For large \\(n\\), \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\) approximately - Equivalently: \\(Z = (\\bar{X} - \\mu)/(\\sigma/\\sqrt{n}) \\sim N(0,1)\\) approximately - This holds regardless of the distribution of \\(X\\) (as long as finite mean and variance exist) - Rule of thumb: CLT works well for \\(n \\geq 30\\) in most cases - Empirical evidence: Coin tosses (binary) → normal distribution of means; Census ages (highly skewed) → normal distribution of means - Why this matters: Justifies using normal-based inference methods (confidence intervals, hypothesis tests) for almost any problem\nStandard Error (Estimated Standard Deviation): - Population standard deviation \\(\\sigma_{\\bar{X}} = \\sigma/\\sqrt{n}\\) is unknown because \\(\\sigma\\) is unknown - Standard error: se(\\(\\bar{X}\\)) = \\(s/\\sqrt{n}\\) where \\(s\\) is sample standard deviation - “Standard error” means “estimated standard deviation” (applies to any estimator, not just \\(\\bar{X}\\)) - Measures precision of \\(\\bar{x}\\) as an estimate of \\(\\mu\\)—smaller is better - Example: Coin toss with n=30 gives se ≈ 0.091; Census with n=25 gives se ≈ 3.72 years - Used to construct confidence intervals and conduct hypothesis tests (Chapter 4)\nDesirable Estimator Properties: - Unbiased: \\(\\mathrm{E}[\\bar{X}] = \\mu\\) (correct on average, no systematic error) - Efficient: Minimum variance among unbiased estimators (most precise) - Consistent: Converges to \\(\\mu\\) as \\(n \\rightarrow \\infty\\) (guaranteed by unbiasedness + variance → 0) - The sample mean \\(\\bar{X}\\) satisfies all three properties under simple random sampling - For normal, Bernoulli, binomial, and Poisson distributions, \\(\\bar{X}\\) is the best unbiased estimator - Sample median is also unbiased (for symmetric distributions) but has higher variance than \\(\\bar{X}\\)\nEmpirical Validation: - Coin toss experiment (400 samples, n=30 each): - Mean of sample means: 0.4994 vs. theoretical 0.5000 - SD of sample means: 0.0863 vs. theoretical 0.0913 - Approximately normal distribution - Census ages (100 samples, n=25 each): - Mean of sample means: 23.78 vs. theoretical 24.13 - SD of sample means: 3.76 vs. theoretical 3.72 - Normal distribution despite highly non-normal population - Computer simulation replicates theoretical results perfectly\nEconomic Applications: - Estimating average income, consumption, wages from household surveys - Public opinion polling (sample proportion is a special case of sample mean) - Macroeconomic indicators: unemployment rate, inflation, GDP growth (all based on samples) - Quality control: manufacturing processes use sample means to monitor production - Clinical trials: comparing average outcomes between treatment and control groups - All regression coefficients (Chapters 6-7) have sampling distributions just like \\(\\bar{X}\\)\nConnection to Statistical Inference (Chapter 4): - This chapter provides the theoretical foundation for confidence intervals - We know \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\) approximately - This allows us to make probability statements about how far \\(\\bar{x}\\) is from \\(\\mu\\) - Example: Pr(\\(\\mu - 1.96 \\cdot \\sigma/\\sqrt{n} &lt; \\bar{X} &lt; \\mu + 1.96 \\cdot \\sigma/\\sqrt{n}\\)) ≈ 0.95 - Rearranging gives 95% confidence interval: \\(\\bar{x} \\pm 1.96 \\cdot s/\\sqrt{n}\\)\nKey Formulas to Remember: 1. Mean of random variable: \\(\\mu = \\mathrm{E}[X] = \\sum_x x \\cdot \\mathrm{Pr}[X=x]\\) 2. Variance: \\(\\sigma^2 = \\mathrm{E}[(X-\\mu)^2] = \\sum_x (x-\\mu)^2 \\cdot \\mathrm{Pr}[X=x]\\) 3. Sample mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) 4. Sample variance: \\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) 5. Mean of sample mean: \\(\\mathrm{E}[\\bar{X}] = \\mu\\) 6. Variance of sample mean: \\(\\operatorname{Var}[\\bar{X}] = \\sigma^2/n\\) 7. Standard error: se(\\(\\bar{X}\\)) = \\(s/\\sqrt{n}\\) 8. Standardized sample mean: \\(Z = (\\bar{X} - \\mu)/(\\sigma/\\sqrt{n}) \\sim N(0,1)\\)\n\nNext Steps: - Chapter 4 uses these results to construct confidence intervals and test hypotheses about \\(\\mu\\) - Chapters 6-7 extend the same logic to regression coefficients (which are also sample statistics with sampling distributions) - The conceptual framework developed here applies to ALL statistical inference in econometrics",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#practice-exercises",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#practice-exercises",
    "title": "Chapter 3: The Sample Mean",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of the sample mean and sampling distributions:\nExercise 1: Random variable properties\n\nSuppose \\(X = 100\\) with probability 0.8 and \\(X = 600\\) with probability 0.2\n\nCalculate the mean \\(\\mu = \\mathrm{E}[X]\\)\n\n\nCalculate the variance \\(\\sigma^2 = \\mathrm{E}[(X-\\mu)^2]\\)\n\n\nCalculate the standard deviation \\(\\sigma\\)\n\n\nExercise 2: Sample mean properties\n\nConsider random samples of size \\(n = 25\\) from a random variable \\(X\\) with mean \\(\\mu = 100\\) and variance \\(\\sigma^2 = 400\\)\n\nWhat is the mean of the sample mean \\(\\bar{X}\\)?\n\n\nWhat is the variance of the sample mean \\(\\bar{X}\\)?\n\n\nWhat is the standard deviation (standard error) of the sample mean?\n\n\nExercise 3: Central Limit Theorem application\n\nA population has mean \\(\\mu = 50\\) and standard deviation \\(\\sigma = 12\\)\nYou draw a random sample of size \\(n = 64\\)\n\nWhat is the approximate distribution of \\(\\bar{X}\\) (by the CLT)?\n\n\nWhat is the probability that \\(\\bar{X}\\) falls between 48 and 52?\n\n\nWould this probability be larger or smaller if \\(n = 144\\)? Why?\n\n\nExercise 4: Standard error interpretation\n\nTwo researchers estimate average income in a city\n\nResearcher A uses \\(n = 100\\), gets \\(\\bar{x}_A = \\$52,000\\), \\(s_A = \\$15,000\\)\nResearcher B uses \\(n = 400\\), gets \\(\\bar{x}_B = \\$54,000\\), \\(s_B = \\$16,000\\)\n\n\nCalculate the standard error for each researcher\n\n\nWhich estimate is more precise? Why?\n\n\nAre these estimates statistically different? (Compare the difference to the standard errors)\n\n\nExercise 5: Consistency\n\nExplain why the sample mean \\(\\bar{X}\\) is a consistent estimator of \\(\\mu\\)\nShow that both conditions for consistency are satisfied: bias → 0 and variance → 0 as \\(n \\rightarrow \\infty\\)\n\nExercise 6: Simulation\n\nUsing Python, simulate 1,000 samples of size \\(n = 50\\) from a uniform distribution U(0, 10)\n\nCalculate the sample mean for each of the 1,000 samples\n\n\nCompute the mean and standard deviation of these 1,000 sample means\n\n\nCompare with theoretical values: \\(\\mu = 5\\), \\(\\sigma^2/n = (100/12)/50 = 0.1667\\)\n\n\nCreate a histogram and verify approximate normality\n\n\nExercise 7: Sample size calculation\n\nYou want to estimate average household expenditure on food with a standard error of $10\nFrom pilot data, you know \\(\\sigma \\approx \\$80\\)\n\nWhat sample size \\(n\\) do you need?\n\n\nIf you double the desired precision (se = $5), how does the required sample size change?\n\n\nExercise 8: Unbiasedness vs. efficiency\n\nThe sample median is also an unbiased estimator of \\(\\mu\\) when the population is symmetric\n\nExplain what “unbiased” means in this context\n\n\nWhy do we prefer the sample mean to the sample median for estimating \\(\\mu\\)?\n\n\nFor what type of population distribution might the median be preferable?",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch03_The_Sample_Mean.html#case-studies",
    "href": "../notebooks_colab/ch03_The_Sample_Mean.html#case-studies",
    "title": "Chapter 3: The Sample Mean",
    "section": "3.9 Case Studies",
    "text": "3.9 Case Studies\nNow that you’ve learned about the sample mean, sampling distributions, and the Central Limit Theorem, let’s apply these concepts to real economic data using the Economic Convergence Clubs dataset.\nWhy case studies matter:\n\nBridge theory and practice: Move from abstract sampling concepts to real data analysis\nBuild analytical skills: Practice computing sample statistics and understanding variability\nDevelop statistical intuition: See how sample size affects precision and distribution shape\nConnect to research: Apply fundamental concepts to cross-country economic comparisons\n\n\nCase Study 1: Sampling Distributions of Labor Productivity\nResearch Question: How does average labor productivity vary across different samples of countries? How does sample size affect the precision of our estimates?\nBackground: In Chapter 1-2, you explored productivity levels across countries. Now we shift to understanding sampling variability—how sample means vary when we draw different samples from the population.\nThis is fundamental to statistical inference: if we only observe a sample of countries (say, 20 out of 108), how confident can we be that our sample mean approximates the true population mean? The Central Limit Theorem tells us that sample means follow a normal distribution (even if the underlying data don’t), with variability decreasing as sample size increases.\nThe Data: We’ll use the convergence clubs dataset (Mendez, 2020) to explore sampling distributions:\n\nPopulation: 108 countries observed from 1990-2014\nKey variable: lp (labor productivity, output per worker)\nTask: Draw multiple random samples, compute sample means, and observe the distribution\n\nYour Task: Use Chapter 3’s tools (sample mean, sample variance, Central Limit Theorem) to understand how sample statistics vary and how sample size affects precision.\n\n\nKey Concept 3.10: Sampling Distribution and the Central Limit Theorem\nThe sampling distribution of the mean shows how sample means \\(\\bar{y}\\) vary across different random samples drawn from the same population. Key properties:\n\nMean of sampling distribution = population mean: \\(E[\\bar{y}] = \\mu\\)\nStandard error decreases with sample size: \\(SE(\\bar{y}) = \\sigma/\\sqrt{n}\\)\nCentral Limit Theorem: For large \\(n\\) (typically \\(n \\geq 30\\)), \\(\\bar{y}\\) is approximately normally distributed, regardless of the population distribution\n\nThis is why we can use normal-based inference methods even for non-normal economic data like earnings and wealth distributions.\n\n\n\n\nHow to Use These Tasks\nProgressive difficulty:\n\nTasks 1-2: Guided (detailed instructions, code provided)\nTasks 3-4: Semi-guided (moderate guidance, you write most code)\nTasks 5-6: Independent (minimal guidance, design your own analysis)\n\nWork incrementally: Complete tasks in order. Each builds on previous concepts.\n\n\nTask 1: Explore the Population Distribution (Guided)\nObjective: Load the convergence clubs data and examine the population distribution of labor productivity.\nInstructions: Run the code below to load data and visualize the population distribution.\n# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Load convergence clubs dataset\ndf = pd.read_csv(\n \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\",\n index_col=[\"country\", \"year\"]\n).sort_index()\n\n# Extract 2014 data (most recent year) for cross-sectional analysis\ndf_2014 = df.loc[(slice(None), 2014), 'lp'].dropna()\n\nprint(f\"Population size: {len(df_2014)} countries\")\nprint(f\"Population mean: ${df_2014.mean():.2f}\")\nprint(f\"Population std dev: ${df_2014.std():.2f}\")\n\n# Visualize population distribution\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(df_2014, bins=20, edgecolor='black', alpha=0.7)\nax.axvline(df_2014.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean = ${df_2014.mean():.2f}')\nax.set_xlabel('Labor Productivity (thousands, 2011 USD PPP)')\nax.set_ylabel('Frequency')\nax.set_title('Population Distribution of Labor Productivity (2014, 108 countries)')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n# Check normality\nprint(f\"\\nSkewness: {stats.skew(df_2014):.3f}\")\nprint(\"Note: Population distribution is right-skewed (not normal)\")\nWhat to observe:\n\nIs the population distribution normal or skewed?\nWhat is the population mean and standard deviation?\nNote: Despite non-normality, the CLT will ensure sample means are approximately normal for large samples!\n\n\n\n\nTask 2: Draw a Single Sample and Compute the Sample Mean (Semi-guided)\nObjective: Draw a random sample of size \\(n=30\\) and compute the sample mean.\nInstructions:\n\nUse np.random.choice() to draw a random sample of size 30 from the population\nCompute the sample mean using .mean()\nCompare the sample mean to the population mean\nRepeat this process 2-3 times (with different random seeds) to see how the sample mean varies\n\nStarter code:\n# Draw a random sample of size 30\nn = 30\nsample = np.random.choice(df_2014, size=n, replace=False)\n\n# Compute sample mean\nsample_mean = sample.mean()\n\nprint(f\"Sample size: {n}\")\nprint(f\"Sample mean: ${sample_mean:.2f}\")\nprint(f\"Population mean: ${df_2014.mean():.2f}\")\nprint(f\"Difference: ${sample_mean - df_2014.mean():.2f}\")\n\n# Question: How close is the sample mean to the population mean?\nQuestions:\n\nHow much does the sample mean differ from the population mean?\nIf you draw another sample, will you get the same sample mean?\nWhat does this variability tell you about using samples for inference?\n\n\n\n\nTask 3: Simulate the Sampling Distribution (Semi-guided)\nObjective: Draw 1000 random samples of size \\(n=30\\) and plot the distribution of sample means.\nInstructions:\n\nUse a loop to draw 1000 samples, each of size \\(n=30\\)\nFor each sample, compute and store the sample mean\nPlot a histogram of the 1000 sample means\nCompare this sampling distribution to the theoretical prediction from the CLT\n\nHint: The CLT predicts that sample means should be normally distributed with:\n\nMean = \\(\\mu\\) (population mean)\nStandard error = \\(\\sigma/\\sqrt{n}\\) (population std / sqrt(sample size))\n\nExample structure:\n# Simulate sampling distribution\nn_samples = 1000\nn = 30\nsample_means = []\n\nfor i in range(n_samples):\n sample = np.random.choice(df_2014, size=n, replace=False)\n sample_means.append(sample.mean())\n\nsample_means = np.array(sample_means)\n\n# Plot sampling distribution\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(sample_means, bins=30, edgecolor='black', alpha=0.7, density=True)\nax.axvline(df_2014.mean(), color='red', linestyle='--', linewidth=2, label='Population mean')\nax.axvline(sample_means.mean(), color='blue', linestyle=':', linewidth=2, label='Mean of sample means')\nax.set_xlabel('Sample Mean (thousands, 2011 USD PPP)')\nax.set_ylabel('Density')\nax.set_title(f'Sampling Distribution of the Mean (n={n}, {n_samples} samples)')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n# Compare empirical vs theoretical\nprint(f\"Population mean (μ): ${df_2014.mean():.2f}\")\nprint(f\"Mean of sample means: ${sample_means.mean():.2f}\")\nprint(f\"Theoretical SE (σ/√n): ${df_2014.std()/np.sqrt(n):.2f}\")\nprint(f\"Empirical SE (std of sample means): ${sample_means.std():.2f}\")\nQuestions:\n\nDoes the distribution of sample means look approximately normal (even though the population distribution was skewed)?\nHow close is the mean of sample means to the population mean?\nHow close is the empirical standard error to the theoretical prediction?\n\n\n\nKey Concept 3.11: Standard Error and Precision\nThe standard error \\(SE(\\bar{y}) = \\sigma/\\sqrt{n}\\) measures the typical distance between a sample mean and the population mean. Key insights:\n\nDecreases with sample size: Doubling the sample size reduces SE by factor of \\(\\sqrt{2} \\approx 1.41\\)\nTrade-off: Larger samples cost more (time/money) but provide more precise estimates\nDiminishing returns: Going from \\(n=25\\) to \\(n=100\\) reduces SE by half, but \\(n=100\\) to \\(n=400\\) also reduces by half\n\nIn economic research, sample size is often limited by data availability, requiring careful attention to precision.\n\n\n\n\nTask 4: Investigate the Effect of Sample Size (More Independent)\nObjective: Compare sampling distributions for different sample sizes (\\(n = 10, 30, 50, 100\\)).\nInstructions:\n\nFor each sample size, simulate 1000 samples and compute sample means\nPlot the four sampling distributions on the same graph (or use subplots)\nCompare the standard errors across sample sizes\nVerify that \\(SE\\) decreases as \\(1/\\sqrt{n}\\)\n\nKey questions to answer:\n\nHow does the shape of the sampling distribution change with sample size?\nHow much does precision improve when you quadruple the sample size (e.g., \\(n=25\\) to \\(n=100\\))?\nAt what sample size does the distribution start to look clearly normal?\n\n\n\n\nTask 5: Comparing High-Income vs Developing Countries (Independent)\nObjective: Investigate whether sampling distributions differ for subpopulations (high-income vs developing countries).\nInstructions:\n\nSplit the 2014 data by hi1990 (high-income indicator)\nFor each group, simulate the sampling distribution of the mean (use \\(n=20\\), 1000 samples)\nPlot both sampling distributions on the same graph\nCompare means and standard errors between groups\n\nResearch question: Do high-income and developing countries have different average productivity levels? How confident can we be in this difference based on samples?\nHints:\n\nUse df.loc[(slice(None), 2014), ['lp', 'hi1990']].dropna() to get both variables\nFilter by hi1990 == 'yes' and hi1990 == 'no'\nCompare population means and sampling distribution characteristics\n\n\n\n\nTask 6: Design Your Own Sampling Experiment (Independent)\nObjective: Explore a question of your choice using sampling distributions.\nChoose ONE of the following:\nOption A: Effect of outliers on sample means\n\nRemove the top 5% most productive countries (outliers)\nCompare sampling distributions with vs without outliers\nQuestion: How sensitive is the sample mean to extreme values?\n\nOption B: Time trends in sampling distributions\n\nCompare sampling distributions for years 1990, 2000, 2010, 2014\nQuestion: Has average productivity increased over time? Has variability changed?\n\nOption C: Regional sampling distributions\n\nSplit countries by region (use region variable)\nCompare sampling distributions across regions\nQuestion: Which regions show the highest/lowest productivity? Most/least variability?\n\nYour analysis should include:\n\nClear research question\nAppropriate sample size(s)\nSimulated sampling distribution(s) with visualizations\nStatistical summary (means, standard errors)\nEconomic interpretation: What does this tell us about global productivity patterns?\n\n\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this hands-on exploration of sampling distributions using cross-country productivity data, you’ve applied all Chapter 3 concepts:\nPopulation vs sample: Understood the difference and why we use samples for inference\nSample mean: Computed point estimates from random samples\nSampling variability: Observed how sample means vary across different samples\nSampling distribution: Simulated and visualized the distribution of sample means\nCentral Limit Theorem: Verified that sample means are approximately normal even for skewed populations\nStandard error: Quantified precision and understood how it decreases with sample size (\\(\\sigma/\\sqrt{n}\\))\nEffect of sample size: Compared sampling distributions for different \\(n\\) values\nSubpopulation analysis: Explored differences across country groups\n\nConnection to the Research: Understanding sampling distributions is fundamental to the convergence clubs analysis. When researchers estimate average productivity for a club, they’re working with samples and must account for sampling variability. The tools you practiced here—computing sample means, quantifying precision, understanding the CLT—are essential for all statistical inference in economics.\nLooking ahead:\n\nChapter 4: Statistical inference (confidence intervals, hypothesis tests) builds directly on sampling distributions\nChapter 5-9: Regression analysis extends these concepts to relationships between variables\nChapter 10-17: Advanced methods for causal inference and panel data\n\n\nCongratulations! You’ve completed Chapter 3 and applied sampling theory to real cross-country data. Continue to Chapter 4 to learn how to use sampling distributions for statistical inference!",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: The Sample Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to statistical inference, teaching you how to extrapolate from sample statistics to population parameters using confidence intervals and hypothesis tests.\nThis chapter introduces statistical inference for the mean—the foundational methods for extrapolating from sample statistics to population parameters with quantified uncertainty.\nWhat you’ll learn:\nDatasets used:\nChapter outline:",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#chapter-overview",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#chapter-overview",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "",
    "text": "Construct and interpret confidence intervals for population means\nUnderstand the t-distribution and when to use it (vs. normal distribution)\nConduct hypothesis tests to evaluate claims about population parameters\nCalculate and interpret p-values and understand statistical significance\nDistinguish between one-sided and two-sided tests\nApply inference methods to proportions data and binary outcomes\n\n\n\nAED_EARNINGS.DTA: Sample of 171 30-year-old female full-time workers in 2010 (earnings in dollars)\nAED_GASPRICE.DTA: Weekly gasoline prices in the U.S. (testing price level hypotheses)\nAED_EARNINGSMALE.DTA: Male earnings data for hypothesis testing examples\nAED_REALGDPPC.DTA: Real GDP per capita growth rates (testing economic growth hypotheses)\n\n\n\n4.1 Example: Mean Annual Earnings\n4.2 t Statistic and t Distribution\n4.3 Confidence Intervals\n4.4 Two-Sided Hypothesis Tests\n4.5 Hypothesis Test Examples\n4.6 One-Sided Directional Hypothesis Tests\n4.7 Proportions Data",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#setup",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#setup",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "Setup",
    "text": "Setup\nRun this cell first to import all required packages and configure the environment.\n\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL (data streams directly from here)\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Optional: Create directories for saving outputs locally\nIMAGES_DIR = 'images'\nTABLES_DIR = 'tables'\nos.makedirs(IMAGES_DIR, exist_ok=True)\nos.makedirs(TABLES_DIR, exist_ok=True)\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"✓ Setup complete! All packages imported successfully.\")\nprint(f\"✓ Random seed set to {RANDOM_SEED} for reproducibility.\")\nprint(f\"✓ Data will stream from: {GITHUB_DATA_URL}\")\n\n✓ Setup complete! All packages imported successfully.\n✓ Random seed set to 42 for reproducibility.\n✓ Data will stream from: https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#example-mean-annual-earnings",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#example-mean-annual-earnings",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.1 Example: Mean Annual Earnings",
    "text": "4.1 Example: Mean Annual Earnings\nWe’ll use a motivating example throughout this chapter: estimating the population mean annual earnings for 30-year-old female full-time workers in the U.S. in 2010.\nThe Problem:\n\nWe have a sample of 171 women\nWe want to make inferences about the population of all such women\n\nKey Statistics:\n\nSample mean \\(\\bar{x}\\): Our point estimate of population mean μ\nStandard deviation s: Measures variability in the sample\nStandard error se(\\(\\bar{x}\\)) = s/√n: Measures precision of \\(\\bar{x}\\) as an estimate of μ\n\nThe standard error is crucial—it quantifies our uncertainty about μ. Smaller standard errors mean more precise estimates.\n\n# Load earnings data\ndata_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS.DTA')\nearnings = data_earnings['earnings']\n\n# Calculate key statistics\nn = len(earnings)\nmean_earnings = earnings.mean()\nstd_earnings = earnings.std(ddof=1)  # ddof=1 for sample std dev\nse_earnings = std_earnings / np.sqrt(n)  # Standard error\n\nprint(\"=\" * 70)\nprint(\"SAMPLE STATISTICS FOR ANNUAL EARNINGS\")\nprint(\"=\" * 70)\nprint(f\"Sample size (n):        {n}\")\nprint(f\"Mean:                   ${mean_earnings:,.2f}\")\nprint(f\"Standard deviation:     ${std_earnings:,.2f}\")\nprint(f\"Standard error:         ${se_earnings:,.2f}\")\nprint(f\"\\nInterpretation: Our best estimate of population mean earnings is ${mean_earnings:,.2f}\")\nprint(f\"The standard error of ${se_earnings:,.2f} measures the precision of this estimate.\")\n\n======================================================================\nSAMPLE STATISTICS FOR ANNUAL EARNINGS\n======================================================================\nSample size (n):        171\nMean:                   $41,412.69\nStandard deviation:     $25,527.05\nStandard error:         $1,952.10\n\nInterpretation: Our best estimate of population mean earnings is $41,412.69\nThe standard error of $1,952.10 measures the precision of this estimate.\n\n\nKey Statistics from our Sample (n = 171 women):\n\nSample mean: $41,412.69\nStandard deviation: $25,527.05\nStandard error: $1,952.10\n\nWhat is the standard error telling us?\nThe standard error of $1,952.10 measures the precision of our sample mean as an estimate of the true population mean. Think of it as quantifying our uncertainty.\nStatistical interpretation:\n\nIf we repeatedly drew samples of 171 women, the sample means would vary\nThe standard error tells us the typical amount by which sample means differ from the true population mean\nFormula: SE = s/√n = $25,527.05/√171 = $1,952.10\n\nWhy is the SE much smaller than the standard deviation?\n\nStandard deviation ($25,527) measures variability among individual women’s earnings\nStandard error ($1,952) measures variability of the sample mean across different samples\nThe larger the sample size, the smaller the SE → more precise estimates\n\nPractical insight:\n\nA standard error of $1,952 is relatively small compared to the mean ($41,413)\nThis suggests our estimate is reasonably precise\nIf we had only 43 women (n=43), SE would double to $3,904 (less precise)\nWith 684 women (n=684), SE would halve to $976 (more precise)\n\n\nKey Concept 4.1: Standard Error and Precision\nThe standard error se(\\(\\bar{x}\\)) = s/√n measures the precision of the sample mean as an estimate of the population mean μ. It quantifies sampling uncertainty—smaller standard errors mean more precise estimates. The SE decreases with sample size at rate 1/√n, so quadrupling the sample size halves the standard error.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#t-statistic-and-t-distribution",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#t-statistic-and-t-distribution",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.2 t Statistic and t Distribution",
    "text": "4.2 t Statistic and t Distribution\nFor inference on the population mean μ, we use the t-statistic:\n\\[t = \\frac{\\bar{x} - \\mu}{\\text{se}(\\bar{x})} = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}\\]\nUnder certain assumptions, this statistic follows a t-distribution with (n - 1) degrees of freedom:\n\\[t \\sim T(n-1)\\]\nWhy t-distribution instead of normal?\n\nWe don’t know the population standard deviation σ, so we estimate it with sample std dev s\nThis adds uncertainty, making the distribution have fatter tails than the normal\nAs sample size increases (n → ∞), the t-distribution approaches the standard normal distribution\n\nKey properties:\n\nSymmetric around zero (like the normal)\nFatter tails than normal (more probability in extremes)\nConverges to N(0,1) as degrees of freedom increase\n\n\n# Visualize t-distribution vs standard normal\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nx = np.linspace(-4, 4, 200)\n\n# Panel A: t(4) vs standard normal\naxes[0].plot(x, stats.norm.pdf(x), 'b--', linewidth=2, label='Standard Normal')\naxes[0].plot(x, stats.t.pdf(x, df=4), 'r-', linewidth=2, label='t(4)')\naxes[0].set_xlabel('x value', fontsize=12)\naxes[0].set_ylabel('Density', fontsize=12)\naxes[0].set_title('Panel A: t(4) and Standard Normal', fontsize=13, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].text(0, 0.35, 'Notice: t(4) has fatter tails\\n(more extreme values likely)', \n             ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# Panel B: t(30) vs standard normal\naxes[1].plot(x, stats.norm.pdf(x), 'b--', linewidth=2, label='Standard Normal')\naxes[1].plot(x, stats.t.pdf(x, df=30), 'r-', linewidth=2, label='t(30)')\naxes[1].set_xlabel('x value', fontsize=12)\naxes[1].set_ylabel('Density', fontsize=12)\naxes[1].set_title('Panel B: t(30) and Standard Normal', fontsize=13, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].text(0, 0.35, 'As df increases, t approaches\\nthe standard normal', \n             ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.suptitle('Figure 4.1: t Distribution vs Standard Normal',\n             fontsize=14, fontweight='bold', y=1.0)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Key Observation:\")\nprint(\"   With more degrees of freedom (larger n), the t-distribution looks more like the normal.\")\nprint(\"   For n &gt; 30, they're nearly identical.\")\n\n\n\n\n\n\n\n\n\n📊 Key Observation:\n   With more degrees of freedom (larger n), the t-distribution looks more like the normal.\n   For n &gt; 30, they're nearly identical.\n\n\n\nKey Concept 4.2: The t-Distribution\nThe t-distribution is used when the population standard deviation \\(\\sigma\\) is unknown and must be estimated from the sample. It is similar to the standard normal N(0,1) but with fatter tails that reflect the additional uncertainty from estimating \\(\\sigma\\). As the sample size \\(n\\) grows, the t-distribution approaches the normal distribution, making the normal a good approximation for large samples.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#confidence-intervals",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#confidence-intervals",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.3 Confidence Intervals",
    "text": "4.3 Confidence Intervals\nA confidence interval provides a range of plausible values for the population parameter μ.\nGeneral formula: \\[\\text{estimate} \\pm \\text{critical value} \\times \\text{standard error}\\]\nFor the population mean, a 100(1 - α)% confidence interval is: \\[\\bar{x} \\pm t_{n-1, \\alpha/2} \\times \\text{se}(\\bar{x})\\]\nWhere:\n\n\\(\\bar{x}\\) = sample mean (our estimate)\n\\(t_{n-1, \\alpha/2}\\) = critical value from t-distribution with (n-1) degrees of freedom\nse(\\(\\bar{x}\\)) = s/√n = standard error\nα = significance level (e.g., 0.05 for 95% confidence)\n\nInterpretation: If we repeatedly drew samples and constructed 95% CIs, about 95% of those intervals would contain the true population mean μ.\nPractical interpretation: We are “95% confident” that μ lies within this interval.\nRule of thumb: For n &gt; 30, \\(t_{n-1, 0.025} \\approx 2\\), so a 95% CI is approximately: \\[\\bar{x} \\pm 2 \\times \\text{se}(\\bar{x})\\]\n\nKey Concept 4.3: Confidence Intervals\nA confidence interval provides a range of plausible values for the population parameter \\(\\mu\\). A 95% confidence interval means: if we repeated the sampling procedure many times, approximately 95% of the resulting intervals would contain the true \\(\\mu\\). The interval is constructed as \\(\\bar{x} \\pm t_{\\alpha/2} \\times se(\\bar{x})\\), where wider intervals indicate less precision.\n\n\n# Calculate 95% confidence interval for mean earnings\nconf_level = 0.95\nalpha = 1 - conf_level\nt_crit = stats.t.ppf(1 - alpha/2, n - 1)  # Critical value\nmargin_of_error = t_crit * se_earnings\nci_lower = mean_earnings - margin_of_error\nci_upper = mean_earnings + margin_of_error\n\nprint(\"=\" * 70)\nprint(\"95% CONFIDENCE INTERVAL FOR POPULATION MEAN EARNINGS\")\nprint(\"=\" * 70)\nprint(f\"Sample mean:            ${mean_earnings:,.2f}\")\nprint(f\"Standard error:         ${se_earnings:,.2f}\")\nprint(f\"Critical value t₁₇₀:     {t_crit:.4f}\")\nprint(f\"Margin of error:        ${margin_of_error:,.2f}\")\nprint(f\"\\n95% Confidence Interval: [${ci_lower:,.2f}, ${ci_upper:,.2f}]\")\nprint(f\"\\nInterpretation: We are 95% confident that the true population mean\")\nprint(f\"earnings lies between ${ci_lower:,.2f} and ${ci_upper:,.2f}.\")\n\n======================================================================\n95% CONFIDENCE INTERVAL FOR POPULATION MEAN EARNINGS\n======================================================================\nSample mean:            $41,412.69\nStandard error:         $1,952.10\nCritical value t₁₇₀:     1.9740\nMargin of error:        $3,853.48\n\n95% Confidence Interval: [$37,559.21, $45,266.17]\n\nInterpretation: We are 95% confident that the true population mean\nearnings lies between $37,559.21 and $45,266.17.\n\n\n95% Confidence Interval for Mean Earnings: [$37,559.21, $45,266.17]\nWhat this interval means:\nThe correct interpretation: If we repeatedly drew samples of 171 women and calculated 95% CIs for each sample, approximately 95% of those intervals would contain the true population mean μ.\nCommon misconceptions (WRONG interpretations): - “There is a 95% probability that μ is in this interval” - “95% of individual women earn between $37,559 and $45,266” - “The interval captures 95% of the data”\nCorrect interpretation: - We are 95% confident that the true population mean earnings lie between $37,559 and $45,266 - The interval accounts for sampling uncertainty through the standard error - The population mean μ is fixed (but unknown); our interval is random\nBreaking down the calculation: - Sample mean: $41,412.69 - Critical value (t₁₇₀, 0.025): 1.9740 - Margin of error: 1.9740 × $1,952.10 = $3,853.48 - Interval: $41,412.69 ± $3,853.48\nPractical insights: - The interval does NOT include $36,000 or $46,000, suggesting these are implausible values for μ - The interval is fairly narrow (width = $7,707), indicating good precision - The critical value (1.974) is close to 2, confirming the “rule of thumb”: CI ≈ mean ± 2×SE\nWhy use 95% confidence? - Convention in most scientific fields (α = 0.05) - Balances precision (narrow interval) with confidence (high probability of capturing μ) - Could use 90% (narrower, less confident) or 99% (wider, more confident)\nTransition: Now that we understand how the t-distribution differs from the normal distribution, we can use it to construct confidence intervals that account for the uncertainty in estimating σ from our sample.\n\nConfidence-Precision Trade-off\nComparing Confidence Intervals at Different Levels:\n\n90% CI: [$38,184.17, $44,641.21] — Width: $6,457.03\n95% CI: [$37,559.21, $45,266.17] — Width: $7,706.97\n99% CI: [$36,327.35, $46,498.03] — Width: $10,170.68\n\nThe fundamental trade-off:\nHigher confidence requires wider intervals. You cannot have both maximum precision (narrow interval) AND maximum confidence (high probability of capturing μ) simultaneously.\nWhy does this happen?\n\nTo be more confident we’ve captured μ, we must cast a wider net\nThe critical value increases with confidence level:\n\n90% CI: t-critical ≈ 1.66 → smaller multiplier\n95% CI: t-critical ≈ 1.97 → moderate multiplier\n\n99% CI: t-critical ≈ 2.61 → larger multiplier\n\n\nPractical implications:\n\n90% CI ($6,457 width):\n\nNarrower, more precise\nBUT: 10% chance the interval misses μ\nUse when: precision is critical and you can tolerate more risk\n\n95% CI ($7,707 width):\n\nStandard choice in economics and most sciences\nGood balance between precision and confidence\nUse when: following standard practice (almost always)\n\n99% CI ($10,171 width):\n\nWider, less precise\nBUT: Only 1% chance the interval misses μ\nUse when: being wrong is very costly (medical, safety applications)\n\n\nHow to improve BOTH confidence AND precision?\n\nIncrease sample size (n)! Larger n → smaller SE → narrower intervals at any confidence level\nWith n = 684 (4× larger), the 95% CI would be approximately half as wide\n\nTrade-off: Higher confidence → wider intervals\n\n90% CI: Narrower, but less confident\n95% CI: Standard choice (most common)\n99% CI: Wider, but more confident\n\nLet’s compare:\n\n# Compare confidence intervals at different levels\nconf_levels = [0.90, 0.95, 0.99]\n\nprint(\"=\" * 70)\nprint(\"CONFIDENCE INTERVALS AT DIFFERENT LEVELS\")\nprint(\"=\" * 70)\nprint(f\"{'Level':&lt;10} {'Lower Bound':&gt;15} {'Upper Bound':&gt;15} {'Width':&gt;15}\")\nprint(\"-\" * 70)\n\nfor conf in conf_levels:\n    alpha = 1 - conf\n    t_crit = stats.t.ppf(1 - alpha/2, n - 1)\n    ci_lower = mean_earnings - t_crit * se_earnings\n    ci_upper = mean_earnings + t_crit * se_earnings\n    width = ci_upper - ci_lower\n    print(f\"{conf*100:.0f}%{ci_lower:&gt;18,.2f}{ci_upper:&gt;18,.2f}{width:&gt;18,.2f}\")\n\nprint(\"\\n📊 Notice: Higher confidence → wider interval → less precision\")\n\n======================================================================\nCONFIDENCE INTERVALS AT DIFFERENT LEVELS\n======================================================================\nLevel          Lower Bound     Upper Bound           Width\n----------------------------------------------------------------------\n90%         38,184.17         44,641.21          6,457.03\n95%         37,559.21         45,266.17          7,706.97\n99%         36,327.35         46,498.03         10,170.68\n\n📊 Notice: Higher confidence → wider interval → less precision\n\n\nNow that we understand confidence intervals, let’s formalize the process of testing specific hypotheses about the population mean.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#two-sided-hypothesis-tests",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#two-sided-hypothesis-tests",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.4 Two-Sided Hypothesis Tests",
    "text": "4.4 Two-Sided Hypothesis Tests\nA hypothesis test evaluates whether a specific claim about μ is plausible given our sample data.\nStructure of a hypothesis test:\n\nNull hypothesis H₀: The claim we’re testing (e.g., μ = $40,000)\nAlternative hypothesis Hₐ: What we conclude if we reject H₀ (e.g., μ ≠ $40,000)\nSignificance level α: Maximum probability of Type I error we’ll tolerate (typically 0.05)\n\nTest statistic: \\[t = \\frac{\\bar{x} - \\mu_0}{\\text{se}(\\bar{x})}\\]\nWhere μ₀ is the hypothesized value.\nTwo ways to make a decision:\n\np-value approach:\n\np-value = probability of observing a t-statistic at least as extreme as ours, assuming H₀ is true\nReject H₀ if p-value &lt; α\n\nCritical value approach:\n\nCritical value c = \\(t_{n-1, \\alpha/2}\\)\nReject H₀ if |t| &gt; c\n\n\nBoth methods always give the same conclusion.\nExample: Test whether population mean earnings equal $40,000.\n\nKey Concept 4.4: Hypothesis Testing Framework\nA hypothesis test evaluates whether data provide sufficient evidence to reject a specific claim (H₀) about a parameter. The t-statistic measures how many standard errors the estimate is from the hypothesized value. The p-value is the probability of observing a result at least as extreme as ours, assuming H₀ is true. Small p-values (&lt; α, typically 0.05) provide evidence against H₀, leading us to reject it.\n\n\n# Two-sided hypothesis test: H0: μ = $40,000 vs Ha: μ ≠ $40,000\nmu0 = 40000  # Hypothesized value\nt_stat = (mean_earnings - mu0) / se_earnings\np_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 1))  # Two-sided p-value\nt_crit_95 = stats.t.ppf(0.975, n - 1)  # Critical value for α = 0.05\n\nprint(\"=\" * 70)\nprint(\"TWO-SIDED HYPOTHESIS TEST\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ = ${mu0:,}\")\nprint(f\"Hₐ: μ ≠ ${mu0:,}\")\nprint(f\"Significance level α = 0.05\")\nprint(\"\\nSample Statistics:\")\nprint(f\"  Sample mean:      ${mean_earnings:,.2f}\")\nprint(f\"  Standard error:   ${se_earnings:,.2f}\")\nprint(\"\\nTest Results:\")\nprint(f\"  t-statistic:       {t_stat:.4f}\")\nprint(f\"  p-value:           {p_value:.4f}\")\nprint(f\"  Critical value:    ±{t_crit_95:.4f}\")\nprint(\"\\nDecision:\")\nprint(f\"  p-value approach:  {p_value:.4f} &gt; 0.05 → Do not reject H₀\")\nprint(f\"  Critical approach: |{t_stat:.4f}| &lt; {t_crit_95:.4f} → Do not reject H₀\")\nprint(\"\\nConclusion: We do not have sufficient evidence to reject the claim\")\nprint(f\"that population mean earnings equal ${mu0:,}.\")\n\n======================================================================\nTWO-SIDED HYPOTHESIS TEST\n======================================================================\nH₀: μ = $40,000\nHₐ: μ ≠ $40,000\nSignificance level α = 0.05\n\nSample Statistics:\n  Sample mean:      $41,412.69\n  Standard error:   $1,952.10\n\nTest Results:\n  t-statistic:       0.7237\n  p-value:           0.4703\n  Critical value:    ±1.9740\n\nDecision:\n  p-value approach:  0.4703 &gt; 0.05 → Do not reject H₀\n  Critical approach: |0.7237| &lt; 1.9740 → Do not reject H₀\n\nConclusion: We do not have sufficient evidence to reject the claim\nthat population mean earnings equal $40,000.\n\n\nTest Results: H₀: μ = $40,000 vs Hₐ: μ ≠ $40,000\n\nt-statistic: 0.7237\np-value: 0.4703\nCritical value: ±1.9740\nDecision: Do NOT reject H₀\n\nWhat does this mean?\nWe tested whether the population mean earnings equal $40,000. Based on our sample data (mean = $41,413), we do NOT have sufficient evidence to reject this claim.\nUnderstanding the p-value (0.4703):\nThe p-value answers: “If the true population mean really is $40,000, what’s the probability of observing a sample mean at least as far from $40,000 as ours ($41,413)?”\n\np-value = 0.4703 = 47.03%\nThis is quite HIGH → the data are consistent with H₀\nInterpretation: If μ truly equals $40,000, we’d see a sample mean this extreme about 47% of the time just due to random sampling\n\nTwo equivalent decision rules:\n\np-value approach:\n\np-value (0.4703) &gt; α (0.05) → Do not reject H₀\nThe evidence against H₀ is weak\n\nCritical value approach:\n\n|t-statistic| = |0.7237| &lt; 1.9740 → Do not reject H₀\nOur t-statistic falls in the “non-rejection region”\n\n\nWhy did we fail to reject?\n\nOur sample mean ($41,413) is only $1,413 above the hypothesized value ($40,000)\nGiven the standard error ($1,952), this difference is less than 1 SE away\nThis is well within the range of normal sampling variation\nThe difference is NOT statistically significant at α = 0.05\n\nDoes this prove μ = $40,000?\nNO! We never “prove” or “accept” the null hypothesis. We simply say:\n\nThe data are consistent with μ = $40,000\nWe lack sufficient evidence to conclude otherwise\nOther values (e.g., $41,000, $42,000) are also consistent with our data\n\nConnection to confidence interval:\nNotice that $40,000 IS inside our 95% CI [$37,559, $45,266]. This is no coincidence:\n\nAny value inside the 95% CI will NOT be rejected at α = 0.05 (two-sided test)\nAny value outside the 95% CI WILL be rejected at α = 0.05\n\n\nVisualizing the two-sided hypothesis test\n\n# Visualize two-sided hypothesis test\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.linspace(-4, 4, 500)\ny = stats.t.pdf(x, n - 1)\n\n# Plot the t-distribution\nax.plot(x, y, 'b-', linewidth=2, label=f't({n-1}) distribution')\n\n# Mark the observed t-statistic\nax.axvline(x=t_stat, color='red', linewidth=2, linestyle='--',\n           label=f'Observed t = {t_stat:.2f}')\n\n# Mark critical values\nax.axvline(x=t_crit_95, color='green', linewidth=1.5, linestyle=':',\n           label=f'Critical values = ±{t_crit_95:.2f}')\nax.axvline(x=-t_crit_95, color='green', linewidth=1.5, linestyle=':')\n\n# Shade rejection regions (both tails)\nx_reject_lower = x[x &lt; -t_crit_95]\nx_reject_upper = x[x &gt; t_crit_95]\nax.fill_between(x_reject_lower, 0, stats.t.pdf(x_reject_lower, n-1),\n                alpha=0.3, color='red', label='Rejection region (α/2 = 0.025 each tail)')\nax.fill_between(x_reject_upper, 0, stats.t.pdf(x_reject_upper, n-1),\n                alpha=0.3, color='red')\n\nax.set_xlabel('t-statistic', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Two-Sided Hypothesis Test Visualization', fontsize=14, fontweight='bold')\nax.legend(fontsize=9, loc='upper right')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Interpretation:\")\nprint(f\"   Our t-statistic ({t_stat:.2f}) falls INSIDE the critical region,\")\nprint(f\"   so we do NOT reject H₀. The data are consistent with μ = ${mu0:,}.\")\n\n\n\nType I Error, Type II Error, and Statistical Power\nThe Four Possible Outcomes of a Hypothesis Test:\nWhen we conduct a hypothesis test, there are four possible scenarios:\n\n\n\n\n\n\n\n\n\nH₀ is TRUE (in reality)\nH₀ is FALSE (in reality)\n\n\n\n\nReject H₀ (decision)\nType I Error (α)\nCorrect Decision (Power)\n\n\nDo not reject H₀\nCorrect Decision (1-α)\nType II Error (β)\n\n\n\nType I Error (False Positive): - Definition: Rejecting H₀ when it’s actually true - Probability: α (significance level) - In our earnings example: Concluding μ ≠ $40,000 when it actually equals $40,000 - We control this: By setting α = 0.05, we accept a 5% chance of Type I error - Consequence: “Crying wolf” — claiming an effect that doesn’t exist\nType II Error (False Negative): - Definition: Failing to reject H₀ when it’s actually false - Probability: β (depends on sample size, effect size, and α) - In our earnings example: Concluding μ = $40,000 when it actually differs - Harder to control directly - Consequence: Missing a real effect\nStatistical Power: - Definition: Power = 1 - β = Probability of correctly rejecting false H₀ - Interpretation: Probability of detecting a real effect when it exists - Typical target: 80% power (β = 0.20) - Higher power → lower chance of Type II error\nThe Trade-off Between Type I and Type II Errors:\nYou cannot minimize both simultaneously: - Decrease α (e.g., from 0.05 to 0.01): - Lower chance of Type I error (false positive) - Higher chance of Type II error (false negative) - Lower statistical power\n\nIncrease α (e.g., from 0.05 to 0.10):\nHigher statistical power\nLower chance of Type II error\nHigher chance of Type I error\n\nHow to improve power WITHOUT increasing Type I error:\n\nIncrease sample size:\n\n\nLarger n → smaller SE → easier to detect real effects\nOur earnings data: n = 171, SE = $1,952\nIf n = 684 (4× larger): SE = $976 (half as large)\nSame effect size would yield t-statistic twice as large\n\n\nStudy larger effects:\n\n\nEasier to detect large differences than small ones\nTesting μ = $30,000 vs μ = $41,413 would have higher power\nTesting μ = $40,000 vs μ = $41,413 has lower power\n\n\nUse one-sided tests (when appropriate):\n\n\nConcentrates α in one tail → higher power in that direction\nBut: Cannot detect effects in the opposite direction\n\nIn our examples:\n\nEarnings test (non-significant):\n\n\nCould be: μ truly equals $40,000 (correct decision)\nOr could be: Type II error (μ differs but we didn’t detect it)\nWith more data, we might detect the difference\n\n\nGas price test (significant):\n\n\nHigh power due to small SE ($0.0267) and reasonable sample size (n=32)\nSuccessfully detected a real difference\nLow probability this is a Type I error (p &lt; 0.0001)\n\nPractical advice:\n\nPlanning stage: Calculate required sample size for desired power\nDesign stage: Set α based on consequences of Type I vs Type II errors\nMedical trials: Type I error very costly → use α = 0.01\nExploratory research: Type II error costly → use α = 0.10\nInterpretation stage: Non-significant results don’t prove H₀ is true (could be Type II error)",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#hypothesis-test-examples",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#hypothesis-test-examples",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.5 Hypothesis Test Examples",
    "text": "4.5 Hypothesis Test Examples\nLet’s apply hypothesis testing to three real-world economic questions.\n\nExample 1: Gasoline Prices\nQuestion: Are gasoline prices in Yolo County different from the California state average?\n\nCalifornia average: $3.81/gallon\nSample: 32 gas stations in Yolo County\nTest: H₀: μ = 3.81 vs Hₐ: μ ≠ 3.81\n\n\n# Load and test gasoline price data\ndata_gasprice = pd.read_stata(GITHUB_DATA_URL + 'AED_GASPRICE.DTA')\nprice = data_gasprice['price']\n\nmean_price = price.mean()\nstd_price = price.std(ddof=1)\nn_price = len(price)\nse_price = std_price / np.sqrt(n_price)\n\nmu0_price = 3.81\nt_stat_price = (mean_price - mu0_price) / se_price\np_value_price = 2 * (1 - stats.t.cdf(abs(t_stat_price), n_price - 1))\n\nprint(\"=\" * 70)\nprint(\"EXAMPLE 1: GASOLINE PRICES\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ = ${mu0_price:.2f} (CA state average)\")\nprint(f\"Hₐ: μ ≠ ${mu0_price:.2f}\")\nprint(f\"\\nSample size:       {n_price}\")\nprint(f\"Sample mean:      ${mean_price:.4f}\")\nprint(f\"Std error:        ${se_price:.4f}\")\nprint(f\"t-statistic:       {t_stat_price:.4f}\")\nprint(f\"p-value:           {p_value_price:.6f}\")\nprint(f\"\\nDecision: p-value &lt; 0.05 → {'REJECT H₀' if p_value_price &lt; 0.05 else 'Do not reject H₀'}\")\nprint(f\"\\nConclusion: Yolo County gas prices ARE {'significantly ' if p_value_price &lt; 0.05 else 'NOT significantly '}different from CA average.\")\n\n======================================================================\nEXAMPLE 1: GASOLINE PRICES\n======================================================================\nH₀: μ = $3.81 (CA state average)\nHₐ: μ ≠ $3.81\n\nSample size:       32\nSample mean:      $3.6697\nStd error:        $0.0267\nt-statistic:       -5.2577\np-value:           0.000010\n\nDecision: p-value &lt; 0.05 → REJECT H₀\n\nConclusion: Yolo County gas prices ARE significantly different from CA average.\n\n\nTest Results: H₀: μ = $3.81 vs Hₐ: μ ≠ $3.81\n\nSample mean: $3.6697\nt-statistic: -5.2577\np-value: 0.0000 (actually &lt; 0.0001)\nDecision: REJECT H₀ at α = 0.05\n\nThis is a STATISTICALLY SIGNIFICANT result!\nUnlike our earnings example, here we have strong evidence that Yolo County gas prices differ from the California state average of $3.81.\nUnderstanding the strong evidence:\n\nLarge t-statistic (-5.26):\n\nThe sample mean ($3.67) is 5.26 standard errors below the hypothesized value ($3.81)\nThis is far beyond the critical value (±2.04)\nSuch extreme values rarely occur by chance alone\n\nTiny p-value (&lt; 0.0001):\n\nIf μ truly equaled $3.81, the probability of observing a sample mean this extreme is less than 0.01%\nThis is MUCH smaller than α = 0.05 (5%)\nStrong evidence against H₀\n\nDirection matters:\n\nThe negative t-statistic tells us Yolo County prices are LOWER than the state average\nDifference: $3.81 - $3.67 = $0.14 per gallon cheaper\n\n\nStatistical vs Practical Significance:\n\nStatistical significance: Yes, we can confidently say Yolo County prices differ from $3.81 (p &lt; 0.0001)\nPractical significance: Is 14 cents per gallon meaningful?\n\nFor a 15-gallon tank: $2.10 savings\nOver a year (52 fill-ups): $109 savings\nThis IS economically meaningful for consumers!\n\n\nWhy is this result so strong compared to the earnings test?\n\nThe standard error is very small ($0.0267) relative to the difference we’re testing\nThis gives us high statistical power to detect the difference\nEven though the dollar difference is small ($0.14), it’s precisely estimated\n\nType I vs Type II Errors in this context:\n\nType I Error: Concluding Yolo County prices differ when they actually don’t\n\nProbability = α = 0.05 (5% chance if we reject)\nBut our p-value is &lt; 0.0001, so we’re very confident we’re not making this error\n\nType II Error: Concluding prices don’t differ when they actually do\n\nNot relevant here since we rejected H₀\nThis test had high power to detect real differences\n\n\n\nKey Concept 4.5: Statistical Significance vs. Sample Size\nEven small practical differences can be statistically significant with large samples (n=53 gas stations). The gasoline price difference of $0.14 might seem trivial, but: - The standard error is small ($0.0267), giving precise estimates - The t-statistic is large (-5.26), indicating the difference is many standard errors from zero - This demonstrates high statistical power—the ability to detect even small real effects\nStatistical significance answers “Is there a difference?” while practical significance asks “Does the difference matter?” Both questions are important in econometrics.\n\n\n\nExample 2: Male Earnings\nQuestion: Do 30-year-old men earn more than $50,000 on average?\n\nClaim: μ &gt; $50,000 (set as alternative hypothesis)\nSample: 191 men\nTest: H₀: μ ≤ 50,000 vs Hₐ: μ &gt; 50,000 (one-sided, covered in section 4.6)\n\n\n# Load and test male earnings data\ndata_male = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGSMALE.DTA')\nearnings_male = data_male['earnings']\n\nmean_male = earnings_male.mean()\nstd_male = earnings_male.std(ddof=1)\nn_male = len(earnings_male)\nse_male = std_male / np.sqrt(n_male)\n\nmu0_male = 50000\nt_stat_male = (mean_male - mu0_male) / se_male\np_value_male = 2 * (1 - stats.t.cdf(abs(t_stat_male), n_male - 1))  # Two-sided for now\n\nprint(\"=\" * 70)\nprint(\"EXAMPLE 2: MALE EARNINGS (Two-sided test shown)\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ = ${mu0_male:,}\")\nprint(f\"Hₐ: μ ≠ ${mu0_male:,}\")\nprint(f\"\\nSample size:       {n_male}\")\nprint(f\"Sample mean:      ${mean_male:,.2f}\")\nprint(f\"Std error:        ${se_male:,.2f}\")\nprint(f\"t-statistic:       {t_stat_male:.4f}\")\nprint(f\"p-value:           {p_value_male:.4f}\")\nprint(f\"\\nDecision: p-value &gt; 0.05 → Do not reject H₀\")\nprint(f\"\\nNote: A one-sided test is more appropriate here (see section 4.6)\")\n\n======================================================================\nEXAMPLE 2: MALE EARNINGS (Two-sided test shown)\n======================================================================\nH₀: μ = $50,000\nHₐ: μ ≠ $50,000\n\nSample size:       191\nSample mean:      $52,353.93\nStd error:        $4,705.75\nt-statistic:       0.5002\np-value:           0.6175\n\nDecision: p-value &gt; 0.05 → Do not reject H₀\n\nNote: A one-sided test is more appropriate here (see section 4.6)\n\n\nTest Results: H₀: μ = $50,000 vs Hₐ: μ ≠ $50,000 - Sample mean: (actual value from code output) - t-statistic: (actual value from code output) - p-value: &gt; 0.05 (not statistically significant) - Decision: DO NOT REJECT H₀ at α = 0.05\nThis is NOT a statistically significant result.\nWe do not have sufficient evidence to conclude that 30-year-old men earn differently than $50,000 on average. This does NOT mean they earn exactly $50,000—it means our data are consistent with that value.\nUnderstanding the lack of significance:\n\nModerate t-statistic:\n\nThe sample mean is not far enough from $50,000 (in standard error units) to confidently reject H₀\nThe observed difference could plausibly arise from random sampling variation alone\n\nLarge p-value (&gt; 0.05):\n\nIf μ truly equaled $50,000, observing a sample mean like ours is quite probable\nWe don’t have strong evidence against H₀\np &gt; α, so we fail to reject\n\nWhat “fail to reject” means:\n\nWe’re NOT proving μ = $50,000\nWe’re saying the data don’t provide convincing evidence that μ ≠ $50,000\nAbsence of evidence is not evidence of absence\n\n\nStatistical vs Practical Significance:\n\nStatistical significance: No, we cannot confidently say mean earnings differ from $50,000 (p &gt; 0.05)\nPractical considerations:\n\nThe sample mean might be close to $50,000 anyway\nOr the sample size (n=191) might not provide enough precision to detect a modest difference\nOr there’s genuine variability in the population making the effect hard to pin down\n\n\nWhy might we fail to reject H₀?\nThree possible explanations:\n\nH₀ is actually true: Mean earnings truly are around $50,000\nInsufficient power: Real difference exists, but our sample size is too small to detect it\nHigh variability: Earnings have large standard deviation, making precise inference difficult\n\nNote on directional hypothesis:\nThe question asks “Do men earn MORE than $50,000?” which suggests a one-sided test (H₀: μ ≤ 50,000 vs Hₐ: μ &gt; 50,000). The code note mentions this will be covered in section 4.6. One-sided tests have more power to detect effects in a specific direction.\n\nKey Concept 4.6: “Fail to Reject” Does Not Mean “Accept”\nWhen p &gt; α, we fail to reject H₀, but this does NOT mean we “accept H₀” or prove it’s true. Three key reasons:\n\nLimited evidence: Our sample might simply lack the power to detect a real difference\nType II error risk: We might be making a Type II error (failing to reject a false H₀)\nConfidence intervals are more informative: A 95% CI tells us the plausible range for μ, not just “different or not different”\n\nIn econometrics, “fail to reject” means “the data are consistent with H₀, but we can’t rule out alternatives.” Always interpret non-significant results with appropriate caution.\n\n\n\nExample 3: GDP Growth\nQuestion: Did real GDP per capita grow at 2.0% per year on average from 1960-2020?\n\nHistorical claim: 2.0% annual growth\nSample: 241 year-to-year growth rates\nTest: H₀: μ = 2.0 vs Hₐ: μ ≠ 2.0\n\n\n# Load and test GDP growth data\ndata_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_REALGDPPC.DTA')\ngrowth = data_gdp['growth']\n\nmean_growth = growth.mean()\nstd_growth = growth.std(ddof=1)\nn_growth = len(growth)\nse_growth = std_growth / np.sqrt(n_growth)\n\nmu0_growth = 2.0\nt_stat_growth = (mean_growth - mu0_growth) / se_growth\np_value_growth = 2 * (1 - stats.t.cdf(abs(t_stat_growth), n_growth - 1))\n\nprint(\"=\" * 70)\nprint(\"EXAMPLE 3: REAL GDP PER CAPITA GROWTH\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ = {mu0_growth:.1f}%\")\nprint(f\"Hₐ: μ ≠ {mu0_growth:.1f}%\")\nprint(f\"\\nSample size:       {n_growth}\")\nprint(f\"Sample mean:       {mean_growth:.4f}%\")\nprint(f\"Std error:         {se_growth:.4f}%\")\nprint(f\"t-statistic:       {t_stat_growth:.4f}\")\nprint(f\"p-value:           {p_value_growth:.4f}\")\nprint(f\"\\nDecision: p-value &gt; 0.05 → Do not reject H₀\")\nprint(f\"\\nConclusion: The data are consistent with 2.0% average annual growth.\")\n\n======================================================================\nEXAMPLE 3: REAL GDP PER CAPITA GROWTH\n======================================================================\nH₀: μ = 2.0%\nHₐ: μ ≠ 2.0%\n\nSample size:       245\nSample mean:       1.9905%\nStd error:         0.1392%\nt-statistic:       -0.0686\np-value:           0.9454\n\nDecision: p-value &gt; 0.05 → Do not reject H₀\n\nConclusion: The data are consistent with 2.0% average annual growth.\n\n\nTest Results: H₀: μ = 2.0% vs Hₐ: μ ≠ 2.0% - Sample mean: (actual value from code output) - t-statistic: (actual value from code output) - p-value: &gt; 0.05 (not statistically significant) - Decision: DO NOT REJECT H₀ at α = 0.05\nThe data are consistent with 2.0% average annual growth.\nWe cannot reject the hypothesis that real GDP per capita grew at 2.0% per year on average from 1960-2020. This historical benchmark appears supported by the data.\nUnderstanding the result:\n\nWhat does “consistent with 2.0%” mean?\n\nThe sample mean growth rate is close enough to 2.0% that random variation could explain the difference\nWe don’t have strong evidence that the true mean differs from 2.0%\nThe p-value &gt; 0.05 indicates this result is plausible under H₀\n\nLarge sample size (n=241 years):\n\nWith 241 year-to-year growth rates, we have substantial data\nLarge samples typically have smaller standard errors and more statistical power\nYet we still fail to reject H₀—this suggests the true mean is genuinely close to 2.0%\n\nEconomic interpretation:\n\nThe 2.0% benchmark is a common reference point in growth economics\nOur data support this conventional wisdom\nLong-run economic growth appears remarkably stable around this rate\n\n\nStatistical vs Practical Significance:\n\nStatistical significance: No, we cannot confidently say mean growth differs from 2.0% (p &gt; 0.05)\nEconomic significance:\n\nEven small deviations from 2.0% compound dramatically over 60 years\nBut our data suggest the historical average is indeed close to 2.0%\nThis consistency validates the use of 2.0% as a benchmark for policy discussions\n\n\nWhy is this result interesting despite being “non-significant”?\n\nValidates a benchmark: Economic theory often assumes ~2% long-run growth; our data support this\nLarge sample confidence: With 241 observations, we can be confident the mean is near 2.0%\nDemonstrates stability: Despite recessions and booms, average growth centers around 2.0%\n\nTime series considerations:\nGDP growth data are time series—observations ordered chronologically with potential autocorrelation. Our standard t-test assumes independent observations, which might not fully hold for year-to-year growth rates. Advanced time series methods (Chapter 17) address these dependencies.\n\nKey Concept 4.7: Context and Consistency in Hypothesis Testing\nStatistical results gain meaning only through economic context – a statistically significant coefficient matters because of what it implies for policy, behavior, or theory. The hypothesis testing pattern (set up hypotheses, compute test statistic, compare to critical value or p-value) is consistent across diverse applications, from wage analysis to macroeconomic growth.\n\nHaving mastered two-sided hypothesis tests, let’s now consider situations where we have a directional prediction.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#one-sided-directional-hypothesis-tests",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#one-sided-directional-hypothesis-tests",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.6 One-Sided Directional Hypothesis Tests",
    "text": "4.6 One-Sided Directional Hypothesis Tests\nSometimes we want to test a directional claim:\n\n“Does μ exceed a certain value?” (upper-tailed test)\n“Is μ less than a certain value?” (lower-tailed test)\n\nStructure:\n\nUpper-tailed test: H₀: μ ≤ μ* vs Hₐ: μ &gt; μ*\nLower-tailed test: H₀: μ ≥ μ* vs Hₐ: μ &lt; μ*\n\nKey difference from two-sided tests:\n\nRejection region is only in one tail of the distribution\np-value calculation uses one tail instead of two\nFor upper-tailed: p-value = Pr[T ≥ t]\nFor lower-tailed: p-value = Pr[T ≤ t]\n\nExample: Test whether mean earnings exceed $40,000.\n\nClaim to be tested: μ &gt; 40,000 (set as Hₐ)\nTest: H₀: μ ≤ 40,000 vs Hₐ: μ &gt; 40,000\n\n\nKey Concept 4.8: One-Sided Tests\nOne-sided tests concentrate the rejection region in ONE tail of the distribution, making them more powerful for detecting effects in the specified direction. Use when theory predicts a specific direction. The p-value for a one-sided test is exactly half the two-sided p-value (when the effect is in the predicted direction). Critical values are smaller for one-sided tests (e.g., 1.65 vs ±1.96 for α=0.05).\n\n\n# One-sided (upper-tailed) test: H0: μ ≤ $40,000 vs Ha: μ &gt; $40,000\nmu0 = 40000\nt_stat = (mean_earnings - mu0) / se_earnings\np_value_upper = 1 - stats.t.cdf(t_stat, n - 1)  # Upper tail only\nt_crit_upper = stats.t.ppf(0.95, n - 1)  # One-sided critical value\n\nprint(\"=\" * 70)\nprint(\"ONE-SIDED HYPOTHESIS TEST (Upper-tailed)\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ ≤ ${mu0:,}\")\nprint(f\"Hₐ: μ &gt; ${mu0:,} (the claim we're testing)\")\nprint(f\"Significance level α = 0.05\")\nprint(\"\\nTest Results:\")\nprint(f\"  t-statistic:       {t_stat:.4f}\")\nprint(f\"  p-value (upper):   {p_value_upper:.4f}\")\nprint(f\"  Critical value:    {t_crit_upper:.4f}\")\nprint(\"\\nDecision:\")\nprint(f\"  p-value approach:  {p_value_upper:.4f} &gt; 0.05 → Do not reject H₀\")\nprint(f\"  Critical approach: {t_stat:.4f} &lt; {t_crit_upper:.4f} → Do not reject H₀\")\nprint(\"\\nConclusion: We do not have sufficient evidence to support the claim\")\nprint(f\"that mean earnings exceed ${mu0:,}.\")\n\n======================================================================\nONE-SIDED HYPOTHESIS TEST (Upper-tailed)\n======================================================================\nH₀: μ ≤ $40,000\nHₐ: μ &gt; $40,000 (the claim we're testing)\nSignificance level α = 0.05\n\nTest Results:\n  t-statistic:       0.7237\n  p-value (upper):   0.2351\n  Critical value:    1.6539\n\nDecision:\n  p-value approach:  0.2351 &gt; 0.05 → Do not reject H₀\n  Critical approach: 0.7237 &lt; 1.6539 → Do not reject H₀\n\nConclusion: We do not have sufficient evidence to support the claim\nthat mean earnings exceed $40,000.\n\n\n\nVisualizing One-Sided Test\n\n# Visualize one-sided hypothesis test\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.linspace(-4, 4, 500)\ny = stats.t.pdf(x, n - 1)\n\n# Plot the t-distribution\nax.plot(x, y, 'b-', linewidth=2, label=f't({n-1}) distribution')\n\n# Mark the observed t-statistic\nax.axvline(x=t_stat, color='red', linewidth=2, linestyle='--',\n           label=f'Observed t = {t_stat:.2f}')\n\n# Mark critical value (upper tail only)\nax.axvline(x=t_crit_upper, color='green', linewidth=1.5, linestyle=':',\n           label=f'Critical value = {t_crit_upper:.2f}')\n\n# Shade rejection region (upper tail only)\nx_reject = x[x &gt; t_crit_upper]\nax.fill_between(x_reject, 0, stats.t.pdf(x_reject, n-1),\n                alpha=0.3, color='red', label='Rejection region (α = 0.05)')\n\nax.set_xlabel('t-statistic', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('One-Sided Hypothesis Test (Upper-tailed) Visualization',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=9, loc='upper left')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Interpretation:\")\nprint(f\"   For an upper-tailed test, we only reject H₀ if t is large and POSITIVE.\")\nprint(f\"   Our t-statistic ({t_stat:.2f}) is below the critical value, so we do not reject.\")\n\n\n\n\n\n\n\n\n\n📊 Interpretation:\n   For an upper-tailed test, we only reject H₀ if t is large and POSITIVE.\n   Our t-statistic (0.72) is below the critical value, so we do not reject.\n\n\nOne-Sided Test Results: H₀: μ ≤ $40,000 vs Hₐ: μ &gt; $40,000\n\nt-statistic: 0.7237 (same as two-sided test)\np-value (one-sided): 0.2351\np-value (two-sided): 0.4703\nCritical value (one-sided, α=0.05): 1.6539\nDecision: Do NOT reject H₀\n\nKey differences from two-sided test:\n\np-value is exactly half:\n\nTwo-sided p-value: 0.4703\nOne-sided p-value: 0.2351 = 0.4703/2\nWhy? We only count probability in ONE tail (upper tail)\n\nCritical value is smaller:\n\nTwo-sided critical value: ±1.9740 (5% split across two tails)\nOne-sided critical value: 1.6539 (5% all in one tail)\nOne-sided tests reject H₀ more easily in the specified direction\n\nDirectional claim:\n\nTwo-sided: “μ is different from $40,000” (could be higher OR lower)\nOne-sided: “μ exceeds $40,000” (specifically higher)\n\n\nWhen to use one-sided tests?\nUse one-sided tests when:\n\nTheory or prior research specifies a direction\nExample: Testing if a new drug is better (not just different) than placebo\nExample: Testing if a policy increases (not just changes) income\n\nWhen NOT to use one-sided tests:\nAvoid one-sided tests when:\n\nYou’re genuinely interested in detecting differences in either direction\nYou might want to detect unexpected effects\nThe field convention is two-sided (economics typically uses two-sided)\n\nWarning about one-sided test abuse:\nResearchers sometimes use one-sided tests to get “significant” results when two-sided tests fail. This is questionable practice:\n\nIf p (two-sided) = 0.08 → not significant at α = 0.05\nIf p (one-sided) = 0.04 → significant at α = 0.05\nSwitching to one-sided AFTER seeing the data is “p-hacking”\nThe choice between one-sided and two-sided should be made BEFORE collecting data\n\nIn our example:\n\nSample mean ($41,413) is above $40,000, consistent with Hₐ: μ &gt; $40,000\nBut p-value (0.2351) &gt; 0.05, so still not significant\nThe effect is too small relative to sampling variability\nWe cannot conclude that mean earnings exceed $40,000\n\nPower consideration:\nOne advantage of one-sided tests: greater statistical power in the specified direction\n\nIf you’re only interested in detecting μ &gt; $40,000, the one-sided test is more powerful\nTrade-off: Cannot detect effects in the opposite direction",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#proportions-data",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#proportions-data",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "4.7 Proportions Data",
    "text": "4.7 Proportions Data\nThe methods extend naturally to proportions (binary data).\nExample: Survey data where respondents answer yes (1) or no (0).- Sample proportion: \\(\\hat{p} = \\bar{x}\\) = fraction of “yes” responses- Standard error: se(\\(\\hat{p}\\)) = √[\\(\\hat{p}\\)(1 - \\(\\hat{p}\\))/n]\nConfidence interval for population proportion p:\\[\\hat{p} \\pm z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]Note: For proportions with large n, we use the normal distribution (z) instead of t. Example: In a sample of 921 voters, 480 intend to vote Democrat. Is this different from 50%?\n\n# Proportions example\nn_total = 921\nn_success = 480\np_hat = n_success / n_total\nse_prop = np.sqrt(p_hat * (1 - p_hat) / n_total)\n\n# 95% Confidence interval\nz_crit = 1.96  # For 95% CI (normal approximation)\nci_lower_prop = p_hat - z_crit * se_prop\nci_upper_prop = p_hat + z_crit * se_prop\n\nprint(\"=\" * 70)\nprint(\"INFERENCE FOR PROPORTIONS\")\nprint(\"=\" * 70)\nprint(f\"Sample size:           {n_total}\")\nprint(f\"Number voting Democrat: {n_success}\")\nprint(f\"Sample proportion:     {p_hat:.4f} ({p_hat*100:.2f}%)\")\nprint(f\"Standard error:        {se_prop:.4f}\")\nprint(f\"95% CI:                [{ci_lower_prop:.4f}, {ci_upper_prop:.4f}]\")\nprint(f\"                        [{ci_lower_prop*100:.2f}%, {ci_upper_prop*100:.2f}%]\")\n\n# Hypothesis test: H0: p = 0.50\np0 = 0.50\nse_under_h0 = np.sqrt(p0 * (1 - p0) / n_total)\nz_stat = (p_hat - p0) / se_under_h0\np_value_prop = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n\nprint(f\"\\nHypothesis Test: H₀: p = {p0:.2f} (50-50 split)\")\nprint(f\"  z-statistic:       {z_stat:.4f}\")\nprint(f\"  p-value:           {p_value_prop:.4f}\")\nprint(f\"  Decision:          {'Reject H₀' if abs(z_stat) &gt; 1.96 else 'Do not reject H₀'}\")\nprint(f\"\\nConclusion: The proportion is {'significantly' if abs(z_stat) &gt; 1.96 else 'NOT significantly'} different from 50%.\")\n\n======================================================================\nINFERENCE FOR PROPORTIONS\n======================================================================\nSample size:           921\nNumber voting Democrat: 480\nSample proportion:     0.5212 (52.12%)\nStandard error:        0.0165\n95% CI:                [0.4889, 0.5534]\n                        [48.89%, 55.34%]\n\nHypothesis Test: H₀: p = 0.50 (50-50 split)\n  z-statistic:       1.2851\n  p-value:           0.1988\n  Decision:          Do not reject H₀\n\nConclusion: The proportion is NOT significantly different from 50%.\n\n\nProportion Results: 480 out of 921 voters intend to vote Democrat - Sample proportion: p̂ = 0.5212 (52.12%) - Standard error: 0.0165 - 95% CI: [0.4889, 0.5534] or [48.89%, 55.34%] - z-statistic (testing H₀: p = 0.50): 1.2851 - p-value: 0.1988 - Decision: Do NOT reject H₀\nWhat this tells us:\nWe have a sample where 52.12% intend to vote Democrat. The question is: does this provide evidence that the population proportion differs from 50% (a tied race)?\nUnderstanding the confidence interval:\nThe 95% CI [48.89%, 55.34%] suggests: - We’re 95% confident the true population proportion is in this range - The interval INCLUDES 50%, indicating 50-50 is plausible - The interval is fairly wide (6.45 percentage points), indicating some uncertainty\nUnderstanding the hypothesis test:\nTesting H₀: p = 0.50 (tied race) vs Hₐ: p ≠ 0.50 (one candidate ahead) - z-statistic: 1.29 (only 1.29 standard errors above 50%) - p-value: 0.1988 (about 20% chance of seeing this result if truly 50-50) - Conclusion: We cannot reject the null hypothesis of a tied race\nWhy use z-statistic (normal) instead of t-statistic?\nFor proportions with large samples (n = 921): - The sampling distribution of p̂ is approximately normal - We know the exact standard error formula: √[p̂(1-p̂)/n] - No need to estimate anything with t-distribution - Rule of thumb: Use normal approximation when np ≥ 10 and n(1-p) ≥ 10 - Here: 921(0.52) = 479 and 921(0.48) = 442, both &gt;&gt; 10\nPractical interpretation for election forecasting:\nThis sample shows 52% support for Democrats, but: - This is NOT statistically significant evidence of a Democratic lead (p = 0.20) - The confidence interval includes 50%, so the race could be tied - Margin of error: ±3.2 percentage points (1.96 × 0.0165 = 0.032) - To call the race, we’d want the CI to exclude 50% entirely\nHow would a larger sample change things?\nIf we had the same proportion (52%) but with 2,500 voters instead of 921: - Standard error would shrink: √[0.52(0.48)/2500] = 0.010 - 95% CI would be narrower: [50.0%, 54.0%] - z-statistic would be larger: (0.52 - 0.50)/0.010 = 2.00 - p-value would be smaller: 0.045 &lt; 0.05 → significant! - Conclusion: Same proportion, but with more data, we could detect the difference\nKey insight about proportions:\nProportions are just means of binary (0/1) data: - Each voter is coded as 1 (Democrat) or 0 (not Democrat) - Sample proportion = sample mean of these 0/1 values - All inference principles (SE, CI, hypothesis tests) apply identically\n\nKey Concept 4.9: Inference for Proportions\nProportions data (like employment rates, approval ratings, or market shares) are binary variables coded as 0 or 1. All inference methods for means—confidence intervals, hypothesis tests, standard errors—extend naturally to proportions. The sample proportion \\(\\bar{p}\\) is simply the sample mean of binary data, and the standard error formula \\(se(\\bar{p}) = \\sqrt{\\bar{p}(1-\\bar{p})/n}\\) follows from the variance formula for binary variables.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#key-takeaways",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#key-takeaways",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCore Concepts\n\nStatistical inference lets us extrapolate from sample statistics to population parameters with quantified uncertainty.\nStandard error \\(\\text{se}(\\bar{x}) = \\frac{s}{\\sqrt{n}}\\) measures the precision of the sample mean as an estimate of the population mean.\nt-distribution is used (instead of normal) when we estimate the population standard deviation from the sample\n\n\nFatter tails than normal (accounts for extra uncertainty)\nConverges to normal as \\(n\\) increases\n\n\nConfidence intervals provide a range of plausible values\n\n\nFormula: estimate \\(\\pm\\) critical value \\(\\times\\) standard error\n95% CI: \\(\\bar{x} \\pm t_{n-1, 0.025} \\times \\text{se}(\\bar{x}) \\approx \\bar{x} \\pm 2 \\times \\text{se}(\\bar{x})\\)\nInterpretation: “We are 95% confident \\(\\mu\\) lies in this interval”\n\n\nHypothesis tests evaluate specific claims about parameters\n\n\nSet up \\(H_0\\) (null) and \\(H_a\\) (alternative)\nCalculate \\(t\\text{-statistic} = \\frac{\\text{estimate} - \\text{hypothesized value}}{\\text{standard error}}\\)\nMake decision using p-value or critical value approach\n\n\nTwo-sided tests (\\(H_0: \\mu = \\mu^*\\) vs \\(H_a: \\mu \\neq \\mu^*\\))\n\n\nRejection region in both tails\n\\(\\text{p-value} = 2 \\times \\Pr[T \\geq |t|]\\)\n\n\nOne-sided tests (\\(H_0: \\mu \\leq \\mu^*\\) vs \\(H_a: \\mu &gt; \\mu^*\\), or vice versa)\n\n\nRejection region in one tail only\nUse when testing a directional claim\n\n\np-value interpretation\n\n\nProbability of observing data at least as extreme as ours, assuming \\(H_0\\) is true\nSmall p-value (\\(&lt; \\alpha\\)) → reject \\(H_0\\)\nCommon significance level: \\(\\alpha = 0.05\\)\n\n\nMethods generalize to other parameters (regression coefficients, differences in means, etc.) and proportions data\n\n\n\nWhat You Learned\nStatistical Concepts Covered: - Standard error and sampling distribution - t-distribution vs normal distribution - Confidence intervals (90%, 95%, 99%) - Hypothesis testing (two-sided and one-sided) - p-values and critical values - Type I error and significance level - Inference for proportions\nPython Tools Used: - scipy.stats.t: t-distribution (pdf, cdf, ppf) - scipy.stats.norm: Normal distribution (for proportions) - pandas: Data manipulation - matplotlib: Visualization of hypothesis tests\n\n\nNext Steps\n\nChapter 5: Bivariate data summary (relationships between two variables)\nChapter 6: Least squares estimator (regression foundation)\nChapter 7: Inference for regression coefficients\n\n\n\nCongratulations!\nYou now understand the foundations of statistical inference:\n\nHow to quantify uncertainty using confidence intervals\nHow to test claims about population parameters\nThe difference between statistical and practical significance\nWhen to use one-sided vs two-sided tests\n\nThese tools are fundamental to all empirical research in economics and beyond!",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#practice-exercises",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#practice-exercises",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of statistical inference:\n\nExercise 1: Confidence Interval Interpretation\nA 95% CI for mean household income is [$48,000, $56,000]\n\nWhat is the point estimate (sample mean)?\nWhat is the margin of error?\nTRUE or FALSE: “There is a 95% probability that the true mean is in this interval”\nTRUE or FALSE: “If we repeated sampling, 95% of CIs would contain the true mean”\n\n\n\nExercise 2: Standard Error Calculation\nSample of \\(n=64\\) observations with mean \\(\\$45,000\\) and standard deviation \\(s=\\$16,000\\)\n\nCalculate the standard error\nWhat sample size would halve the standard error?\nConstruct an approximate 95% CI using the “mean \\(\\pm\\) 2SE” rule\n\n\n\nExercise 3: t vs z Distribution\n\nWhy do we use the t-distribution instead of the normal distribution?\nFor \\(n=10\\), find the critical value for a 95% CI\nFor \\(n=100\\), find the critical value for a 95% CI\nCompare these to \\(z=1.96\\). What do you notice?\n\n\n\nExercise 4: Hypothesis Test Mechanics\nTest \\(H_0: \\mu = 100\\) vs \\(H_a: \\mu \\neq 100\\) with sample mean \\(= 105\\), SE \\(= 3\\), \\(n = 49\\)\n\nCalculate the t-statistic\nFind the p-value (use t-table or Python)\nMake a decision at \\(\\alpha=0.05\\)\nWould your decision change if \\(\\alpha=0.01\\)?\n\n\n\nExercise 5: One-Sided vs Two-Sided Tests\nSample: \\(n=36\\), mean \\(=72\\), \\(s=18\\)\n\nTest \\(H_0: \\mu = 75\\) vs \\(H_a: \\mu \\neq 75\\) (two-sided, \\(\\alpha=0.05\\))\nTest \\(H_0: \\mu \\geq 75\\) vs \\(H_a: \\mu &lt; 75\\) (one-sided, \\(\\alpha=0.05\\))\nCompare the p-values. What is the relationship?\nIn which case is the evidence against \\(H_0\\) stronger?\n\n\n\nExercise 6: Type I and Type II Errors\n\nDefine Type I error and give an example in the earnings context\nDefine Type II error and give an example\nIf we decrease \\(\\alpha\\) from 0.05 to 0.01, what happens to the probability of Type II error?\nHow can we reduce both types of error simultaneously?\n\n\n\nExercise 7: Proportions Inference\nSurvey of 500 people: 275 support a policy\n\nCalculate the sample proportion and standard error\nConstruct a 95% CI for the population proportion\nTest \\(H_0: p = 0.50\\) vs \\(H_a: p \\neq 0.50\\)\nIs the result statistically significant?\n\n\n\nExercise 8: Python Practice\nGenerate a random sample of 100 observations from \\(N(50, 100)\\)\n\nCalculate the 95% CI for the mean\nDoes the CI contain the true mean (50)?\nRepeat 1000 times. What fraction of CIs contain 50?\nTest \\(H_0: \\mu = 55\\). What proportion of tests reject (should be \\(\\approx 0.05\\))?",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#case-study-1-statistical-inference-for-labor-productivity",
    "href": "../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html#case-study-1-statistical-inference-for-labor-productivity",
    "title": "Chapter 4: Statistical Inference for the Mean",
    "section": "Case Study 1: Statistical Inference for Labor Productivity",
    "text": "Case Study 1: Statistical Inference for Labor Productivity\nResearch Question: “Has global labor productivity changed significantly over time, and do productivity levels differ significantly across regions?”\nThis case study applies all the statistical inference methods from Chapter 4 to analyze real economic data on labor productivity across 108 countries over 25 years (1990-2014). You’ll practice:\n\nConstructing and interpreting confidence intervals for population means\nConducting two-sided hypothesis tests to compare time periods\nPerforming one-sided directional tests for benchmark comparisons\nApplying proportions inference to binary economic outcomes\nComparing productivity levels across regional subgroups\nInterpreting results in economic context (development economics, convergence theory)\n\nThe Mendez convergence clubs dataset provides panel data on labor productivity, GDP, capital, human capital, and total factor productivity for 108 countries from 1990 to 2014.\n\nEconomic Context: Testing Convergence Hypotheses\nIn development economics, the convergence hypothesis suggests that poorer countries should grow faster than richer ones, leading to a narrowing of productivity gaps over time. Statistical inference allows us to test whether observed changes in productivity are:\n\nStatistically significant (unlikely due to random sampling variation)\nEconomically meaningful (large enough to matter for policy)\n\nBy applying Chapter 4’s methods to this dataset, you’ll answer questions like:\n\nHas mean global productivity increased significantly from 1990 to 2014?\nAre regional productivity gaps (e.g., Africa vs. Europe) statistically significant?\nWhat proportion of countries experienced positive productivity growth?\nCan we reject specific hypotheses about productivity benchmarks?\n\nThese are real questions that economists and policymakers care about when designing development strategies.\n\nKey Concept 4.10: Why Statistical Inference Matters in Economics\nWhen analyzing economic data, we rarely observe entire populations. Instead, we work with samples (like 108 countries from all countries in the world, or 25 years from a longer historical period). Statistical inference lets us:\n\nQuantify uncertainty - Confidence intervals tell us the range of plausible values for population parameters\nTest theories - Hypothesis tests evaluate whether data support or contradict economic theories\nCompare groups - We can determine if differences between regions/periods are real or just noise\nInform policy - Statistical significance helps separate meaningful patterns from random fluctuations\n\nWithout inference methods, we couldn’t distinguish between: - A real productivity increase vs. random year-to-year variation - Genuine regional gaps vs. sampling artifacts - Policy-relevant changes vs. statistical noise\n\n\n# Load convergence clubs dataset\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndf = pd.read_csv(url)\n\n# Set multi-index (country, year)\ndf = df.set_index(['country', 'year'])\n\n# Display dataset information\nprint(\"Dataset Overview:\")\nprint(f\"Total observations: {len(df):,}\")\nprint(f\"Countries: {df.index.get_level_values('country').nunique()}\")\nprint(f\"Years: {df.index.get_level_values('year').min()}-{df.index.get_level_values('year').max()}\")\nprint(f\"\\nVariables: {list(df.columns)}\")\n\n# Extract labor productivity for key years\nlp_1990 = df.loc[df.index.get_level_values('year') == 1990, 'lp']\nlp_2014 = df.loc[df.index.get_level_values('year') == 2014, 'lp']\n\nprint(f\"\\nLabor productivity samples:\")\nprint(f\"1990: n={len(lp_1990)}, mean=${lp_1990.mean()/1000:.1f}k, std=${lp_1990.std()/1000:.1f}k\")\nprint(f\"2014: n={len(lp_2014)}, mean=${lp_2014.mean()/1000:.1f}k, std=${lp_2014.std()/1000:.1f}k\")\n\nDataset Overview:\nTotal observations: 2,700\nCountries: 108\nYears: 1990-2014\n\nVariables: ['id', 'Y', 'K', 'pop', 'L', 's', 'alpha_it', 'GDPpc', 'lp', 'h', 'kl', 'kp', 'ky', 'TFP', 'log_GDPpc_raw', 'log_lp_raw', 'log_ky_raw', 'log_h_raw', 'log_tfp_raw', 'log_GDPpc', 'log_lp', 'log_ky', 'log_h', 'log_tfp', 'isocode', 'hi1990', 'region']\n\nLabor productivity samples:\n1990: n=108, mean=$23.2k, std=$20.1k\n2014: n=108, mean=$41.0k, std=$33.9k\n\n\n\n\nHow to Use These Tasks\nTask structure: The 6 tasks below progress from guided (fill-in-the-blank code) to independent (design your own analysis).\nWorking approach:\n\nRead the task description - Understand the economic question and learning goal\nStudy the code template - Early tasks provide partial code with blanks (_____)\nInsert a new code cell below each task\nComplete the code - Fill in blanks or write from scratch (depending on task level)\nRun and interpret - Execute your code and interpret results economically\nCheck your understanding - Does your answer make economic sense?\n\nTips:\n\nReference Section 4.1-4.7 for formulas and methods\nUse scipy.stats functions: t.ppf(), ttest_ind(), ttest_1samp()\nAlways interpret p-values: “We reject/fail to reject H₀ at α=0.05 because…”\nConnect statistical results to economic meaning: “This suggests that…”\n\nProgressive difficulty:\n\nTasks 1-2: GUIDED (fill 4-8 blanks in provided code)\nTasks 3-4: SEMI-GUIDED (complete partial structure)\nTasks 5-6: INDEPENDENT (design full implementation)\n\n\n\nTask 1: Confidence Intervals for Mean Productivity (GUIDED)\nLearning Goal: Apply Section 4.3 methods to calculate and interpret confidence intervals\nEconomic Question: “Can we be 95% confident about the range of global mean labor productivity in 2014?”\nYour task:\n\nCalculate a 95% confidence interval for mean productivity in 2014\nCalculate a 99% confidence interval for comparison\nInterpret the difference in interval widths\nCompare with a 95% CI for 1990 data\n\nCode template (fill in the 6 blanks):\nfrom scipy import stats\n\n# 2014 data: Calculate 95% CI\nn_2014 = len(lp_2014)\nmean_2014 = _____  # Calculate sample mean\nstd_2014 = _____   # Calculate sample standard deviation\nse_2014 = std_2014 / np.sqrt(n_2014)\n\n# Get t-critical value for 95% CI (two-tailed, df = n-1)\nalpha_95 = 0.05\nt_crit_95 = stats.t.ppf(1 - alpha_95/2, df=_____)\n\n# Calculate margin of error and CI bounds\nme_95 = t_crit_95 * se_2014\nci_95_lower = _____\nci_95_upper = _____\n\nprint(f\"2014 Labor Productivity:\")\nprint(f\"Sample mean: ${mean_2014:,.0f}\")\nprint(f\"95% CI: [${ci_95_lower:,.0f}, ${ci_95_upper:,.0f}]\")\nprint(f\"Margin of error: ${me_95:,.0f}\")\n\n# Calculate 99% CI for comparison\nalpha_99 = 0.01\nt_crit_99 = stats.t.ppf(1 - alpha_99/2, df=n_2014-1)\nme_99 = t_crit_99 * se_2014\nci_99_lower = mean_2014 - me_99\nci_99_upper = mean_2014 + me_99\n\nprint(f\"\\n99% CI: [${ci_99_lower:,.0f}, ${ci_99_upper:,.0f}]\")\nprint(f\"Margin of error: ${me_99:,.0f}\")\nprint(f\"\\nInterpretation: The 99% CI is _____ than the 95% CI\")  # Fill in: \"wider\" or \"narrower\"\nprint(f\"because we need more certainty, which requires a larger interval.\")\n\n# Compare with 1990\nmean_1990 = lp_1990.mean()\nstd_1990 = lp_1990.std()\nse_1990 = std_1990 / np.sqrt(len(lp_1990))\nme_1990 = stats.t.ppf(0.975, df=len(lp_1990)-1) * se_1990\n\nprint(f\"\\n1990 mean: ${mean_1990:,.0f}, 95% CI width: ${2*me_1990:,.0f}\")\nprint(f\"2014 mean: ${mean_2014:,.0f}, 95% CI width: ${2*me_95:,.0f}\")\nQuestions to consider:\n\nWhy is the 99% CI wider than the 95% CI?\nDid the mean productivity increase from 1990 to 2014?\nWhich year has more variability in productivity across countries?\n\n\n\nTask 2: Testing Productivity Change Over Time (SEMI-GUIDED)\nLearning Goal: Apply Section 4.4 (two-sided tests) to compare time periods\nEconomic Question: “Has global mean labor productivity changed significantly from 1990 to 2014?”\nYour task:\n\nState null and alternative hypotheses\nConduct a two-sample t-test (independent samples)\nCalculate the test statistic manually\nCompare with scipy.stats.ttest_ind() result\nInterpret the p-value at α = 0.05\n\nCode template (fill in the 8 blanks):\n# State hypotheses\nprint(\"H₀: μ₁₉₉₀ = μ₂₀₁₄  (no change in mean productivity)\")\nprint(\"Hₐ: μ₁₉₉₀ ≠ μ₂₀₁₄  (mean productivity changed)\")\nprint(f\"Significance level: α = 0.05\\n\")\n\n# Manual calculation\nmean_1990 = lp_1990.mean()\nmean_2014 = lp_2014.mean()\nse_1990 = lp_1990.std() / np.sqrt(len(lp_1990))\nse_2014 = lp_2014.std() / np.sqrt(len(lp_2014))\n\n# Calculate pooled standard error for difference in means\nse_diff = np.sqrt(_____**2 + _____**2)  # Fill in: se_1990 and se_2014\n\n# Calculate t-statistic\nt_stat = (_____ - _____) / se_diff  # Fill in: mean_2014 and mean_1990\n\n# Degrees of freedom (Welch approximation)\nn1, n2 = len(lp_1990), len(lp_2014)\ns1, s2 = lp_1990.std(), lp_2014.std()\ndf = ((s1**2/n1 + s2**2/n2)**2) / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))\n\n# Calculate two-sided p-value\np_value_manual = 2 * (1 - stats.t.cdf(abs(t_stat), df=df))\n\nprint(f\"Manual calculation:\")\nprint(f\"Difference in means: ${mean_2014 - mean_1990:,.0f}\")\nprint(f\"SE of difference: ${se_diff:,.0f}\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"Degrees of freedom: {df:.1f}\")\nprint(f\"p-value (two-sided): {p_value_manual:.4f}\\n\")\n\n# Verify with scipy\nt_stat_scipy, p_value_scipy = stats.ttest_ind(_____, _____, equal_var=False)  # Fill in: lp_2014, lp_1990\nprint(f\"scipy.stats.ttest_ind() result:\")\nprint(f\"t-statistic: {t_stat_scipy:.3f}\")\nprint(f\"p-value: {p_value_scipy:.4f}\\n\")\n\n# Decision\nif p_value_scipy &lt; 0.05:\n    print(f\"Decision: _____ H₀ at α=0.05\")  # Fill in: \"Reject\" or \"Fail to reject\"\n    print(f\"Interpretation: Mean productivity _____ significantly from 1990 to 2014.\")  # Fill in: \"changed\" or \"did not change\"\nelse:\n    print(f\"Decision: Fail to reject H₀ at α=0.05\")\n    print(f\"Interpretation: Insufficient evidence that mean productivity changed.\")\nQuestions to consider:\n\nWhat does the p-value tell you about the likelihood of observing this difference by chance?\nIs the change economically meaningful (not just statistically significant)?\nWhat assumptions does the two-sample t-test make?\n\n\n\nTask 3: Comparing Regional Productivity Levels (SEMI-GUIDED)\nLearning Goal: Apply hypothesis testing to compare subgroups\nEconomic Question: “Do African countries have significantly lower productivity than European countries (2014 data)?”\nYour task:\n\nFilter 2014 data by region (use region column in dataset)\nTest H₀: μ_Africa = μ_Europe vs Hₐ: μ_Africa ≠ μ_Europe\nCalculate 95% CI for the difference in means\nVisualize distributions with side-by-side box plots\n\nCode structure (complete the analysis):\n# Filter 2014 data by region\ndf_2014 = df.loc[df.index.get_level_values('year') == 2014]\n\n# Extract productivity for Africa and Europe\nlp_africa = df_2014.loc[df_2014['region'] == 'Africa', 'lp']\nlp_europe = df_2014.loc[df_2014['region'] == 'Europe', 'lp']\n\nprint(f\"Sample sizes: Africa n={len(lp_africa)}, Europe n={len(lp_europe)}\")\nprint(f\"Africa mean: ${lp_africa.mean():,.0f}\")\nprint(f\"Europe mean: ${lp_europe.mean():,.0f}\\n\")\n\n# Conduct two-sample t-test\n# YOUR CODE HERE: Use stats.ttest_ind() to test if means differ\n# Calculate and print: t-statistic, p-value, decision at α=0.05\n\n# Calculate 95% CI for difference in means\n# YOUR CODE HERE: \n# 1. Calculate difference in means\n# 2. Calculate SE of difference\n# 3. Get t-critical value\n# 4. Construct CI: (difference - ME, difference + ME)\n\n# Visualize distributions\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nax.boxplot([lp_africa, lp_europe], labels=['Africa', 'Europe'])\nax.set_ylabel('Labor Productivity ($)')\nax.set_title('Labor Productivity Distribution by Region (2014)')\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\nQuestions to consider:\n\nIs the difference statistically significant?\nHow large is the productivity gap in dollar terms?\nWhat does the box plot reveal about within-region variation?\nDoes the CI for the difference include zero? What does that mean?\n\n\nKey Concept 4.11: Economic vs Statistical Significance\nA result can be statistically significant (p &lt; 0.05) but economically trivial, or vice versa:\nStatistical significance answers: “Is this difference unlikely to be due to chance?” - Depends on sample size: larger samples detect smaller differences - Measured by p-value: probability of observing this result if H₀ is true - Standard: p &lt; 0.05 means &lt;5% chance of Type I error\nEconomic significance answers: “Is this difference large enough to matter?” - Depends on context: a $1,000 productivity gap might be huge for low-income countries but trivial for high-income countries - Measured by effect size: actual magnitude of the difference - Judgment call: requires domain expertise\nExample: - With n=10,000 countries, a $100 productivity difference might be statistically significant (p&lt;0.001) but economically meaningless - With n=10 countries, a $10,000 difference might not be statistically significant (p=0.08) but could be economically important\nBest practice: Always report BOTH: 1. Statistical result: “We reject H₀ at α=0.05 (p=0.003)” 2. Economic interpretation: “The $15,000 productivity gap represents a 35% difference, which is economically substantial for development policy”\n\n\n\nTask 4: One-Sided Test for Growth (MORE INDEPENDENT)\nLearning Goal: Apply Section 4.6 (one-sided tests) to directional hypotheses\nEconomic Question: “Can we conclude that mean global productivity in 2014 exceeds $50,000 (a policy benchmark)?”\nYour task:\n\nTest H₀: μ ≤ 50,000 vs Hₐ: μ &gt; 50,000\nUse scipy.stats.ttest_1samp() with alternative='greater'\nCompare one-sided vs two-sided p-values\nDiscuss Type I error: what does α=0.05 mean in this context?\n\nOutline (write your own code):\n# Step 1: State hypotheses clearly\n# H₀: μ ≤ 50,000 (productivity does not exceed benchmark)\n# Hₐ: μ &gt; 50,000 (productivity exceeds benchmark)\n\n# Step 2: Conduct one-sided t-test\n# Use: stats.ttest_1samp(lp_2014, popmean=50000, alternative='greater')\n\n# Step 3: Calculate two-sided p-value for comparison\n# Use: stats.ttest_1samp(lp_2014, popmean=50000, alternative='two-sided')\n\n# Step 4: Report results\n# - Sample mean\n# - t-statistic\n# - One-sided p-value\n# - Two-sided p-value\n# - Decision at α=0.05\n\n# Step 5: Interpret Type I error\n# If we reject H₀, what is the probability we made a mistake?\nHint: Remember that for one-sided tests:\n\nIf Hₐ: μ &gt; μ₀, use alternative='greater'\nThe one-sided p-value is half the two-sided p-value (when t-stat has correct sign)\nType I error = rejecting H₀ when it’s actually true\n\nQuestions to consider:\n\nWhy is the one-sided p-value smaller than the two-sided p-value?\nWhen is a one-sided test appropriate vs a two-sided test?\nWhat are the policy implications if we reject H₀?\n\n\n\nTask 5: Proportions Analysis - Growth Winners (INDEPENDENT)\nLearning Goal: Apply Section 4.7 (proportions) to binary outcomes\nEconomic Question: “What proportion of countries experienced productivity growth from 1990 to 2014, and can we conclude that more than half experienced growth?”\nYour task:\n\nCreate country-level dataset with productivity in both 1990 and 2014\nCreate binary variable: growth = 1 if productivity increased, 0 otherwise\nCalculate sample proportion of “growth winners”\nConstruct 95% CI for population proportion using normal approximation\nTest H₀: p = 0.50 vs Hₐ: p ≠ 0.50 (are half growth winners?)\n\nHints:\n# Hint 1: Reshape data to country-level\n# df_1990 = df.loc[df.index.get_level_values('year') == 1990, ['lp']]\n# df_2014 = df.loc[df.index.get_level_values('year') == 2014, ['lp']]\n# Merge on country index\n\n# Hint 2: Create binary growth indicator\n# growth = (lp_2014 &gt; lp_1990).astype(int)\n\n# Hint 3: Proportions formulas from Section 4.7\n# p_hat = np.mean(growth)\n# se_p = np.sqrt(p_hat * (1 - p_hat) / n)\n# CI: p_hat ± z_crit * se_p\n# For 95% CI: z_crit = 1.96\n\n# Hint 4: Test statistic for proportions\n# z = (p_hat - p0) / np.sqrt(p0 * (1 - p0) / n)\n# where p0 = 0.50 under H₀\nQuestions to consider:\n\nWhat proportion of countries experienced growth?\nDoes the 95% CI include 0.50? What does that mean?\nIs there evidence that the proportion differs from 50%?\nWhich countries did NOT experience growth? (Bonus: investigate why)\n\n\n\nTask 6: Multi-Regional Hypothesis Testing (INDEPENDENT)\nLearning Goal: Integrate multiple inference methods in comprehensive analysis\nEconomic Question: “Which regional pairs show statistically significant productivity gaps in 2014?”\nYour task:\n\nIdentify all regions in the dataset (use df_2014['region'].unique())\nCalculate 95% CIs for mean productivity in each region\nConduct pairwise t-tests (Africa vs Europe, Africa vs Asia, Africa vs Americas, etc.)\nCreate a visualization showing CIs for all regions (error bar plot)\nDiscuss the multiple testing problem: when conducting many tests, what happens to Type I error?\n\nChallenge goals (minimal guidance):\n\nDesign your own analysis structure\nUse loops to avoid repetitive code\nCreate professional visualizations\nWrite clear economic interpretations\n\nSuggested approach:\n# Step 1: Get all regions and calculate summary stats\n# For each region: mean, std, se, 95% CI\n# Store in a DataFrame or dictionary\n\n# Step 2: Conduct all pairwise tests\n# Use itertools.combinations() to generate pairs\n# For each pair: run ttest_ind(), store t-stat and p-value\n\n# Step 3: Visualize CIs\n# Create error bar plot: plt.errorbar()\n# Or bar plot with CI whiskers\n\n# Step 4: Report significant differences\n# Which pairs have p &lt; 0.05?\n# What is the magnitude of differences?\n\n# Step 5: Discuss multiple testing\n# If you run 10 tests at α=0.05, what's the probability of at least one false positive?\n# Consider Bonferroni correction: α_adjusted = 0.05 / number_of_tests\nQuestions to consider:\n\nWhich region has the highest mean productivity? Lowest?\nAre all pairwise differences statistically significant?\nHow does the multiple testing problem affect your conclusions?\nWhat economic factors might explain regional productivity gaps?\n\n\n\nWhat You’ve Learned\nBy completing this case study, you’ve practiced all the major skills from Chapter 4:\nStatistical Methods: - Constructed confidence intervals (90%, 95%, 99%) for population means - Conducted two-sided hypothesis tests to compare groups and time periods - Performed one-sided directional tests for benchmark comparisons - Applied proportions inference to binary economic outcomes - Calculated and interpreted t-statistics, p-values, and margins of error - Used both manual calculations and scipy.stats functions\nEconomic Applications: - Tested convergence hypotheses (did productivity gaps narrow over time?) - Compared regional development levels (Africa, Europe, Asia, Americas) - Evaluated policy benchmarks (does productivity exceed thresholds?) - Identified “growth winners” and “growth losers” among countries - Distinguished between statistical and economic significance\nProgramming Skills: - Filtered and reshaped panel data (multi-index DataFrames) - Implemented statistical tests with scipy.stats - Created informative visualizations (box plots, error bars) - Wrote clear, reproducible analysis code\nCritical Thinking: - Formulated null and alternative hypotheses from economic questions - Interpreted p-values in context (not just “significant” vs “not significant”) - Connected statistical results to economic meaning and policy implications - Recognized limitations (Type I/II errors, multiple testing problem)\n\nNext steps:\nThese skills form the foundation for more advanced methods in later chapters: - Chapter 5: Regression analysis (relationship between two variables) - Chapter 6: Multiple regression (controlling for confounders) - Chapter 7: Hypothesis tests in regression models\nStatistical inference is everywhere in empirical economics. You’ve now mastered the core toolkit for: - Quantifying uncertainty - Testing economic theories - Making data-driven decisions - Communicating results with precision\nWell done!\n\n\nCase Study 2: Is Bolivia’s Development Equal? Testing Differences Across Departments\nResearch Question: Are there statistically significant differences in development levels across Bolivia’s nine departments?\nIn Chapter 1, we introduced the DS4Bolivia project and explored satellite-development relationships across Bolivia’s 339 municipalities. In this case study, we apply Chapter 4’s statistical inference tools—confidence intervals and hypothesis tests—to test whether development levels differ significantly across Bolivia’s departments.\nThe Data: Cross-sectional dataset covering 339 Bolivian municipalities from the DS4Bolivia Project, including:\n\nDevelopment outcomes: Municipal Sustainable Development Index (IMDS, 0-100 composite)\nSatellite data: Log nighttime lights per capita (2017)\nDemographics: Population (2017), municipality and department names\n\nYour Task: Use confidence intervals, one-sample tests, two-sample tests, and one-sided tests to evaluate whether Bolivia’s departments differ significantly in development—and whether those differences are large enough to matter for policy.\n\nLoad the DS4Bolivia Data\nLet’s load the DS4Bolivia dataset and prepare the key variables for statistical inference.\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017', 'pop2017']\nbol_key = bol[key_vars].dropna().copy()\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA DATASET — STATISTICAL INFERENCE CASE STUDY\")\nprint(\"=\" * 70)\nprint(f\"Municipalities: {len(bol_key)}\")\nprint(f\"Departments:    {bol_key['dep'].nunique()}\")\nprint(f\"\\nIMDS summary:\")\nprint(bol_key['imds'].describe().round(2))\nprint(f\"\\nMunicipalities per department:\")\nprint(bol_key['dep'].value_counts().sort_index())\n\n\n\nTask 1: Confidence Interval for Mean IMDS (Guided)\nLearning Goal: Apply Section 4.3 methods to calculate and interpret a confidence interval for a population mean.\nEconomic Question: “What is the true average development level across all Bolivian municipalities?”\nInstructions:\n\nCalculate the sample mean and standard error of imds\nConstruct a 95% confidence interval using scipy.stats.t.interval()\nPrint the sample mean, standard error, and CI bounds\nInterpret the result: “We are 95% confident that the true mean municipal development index lies between X and Y.”\n\nApply what you learned in Section 4.3: The formula is \\(\\bar{x} \\pm t_{\\alpha/2} \\times SE\\), where \\(SE = s/\\sqrt{n}\\).\n\n# Your code here: 95% Confidence Interval for national mean IMDS\nfrom scipy import stats\n\n# Calculate sample statistics\nn = len(bol_key['imds'])\nx_bar = bol_key['imds'].mean()\nse = bol_key['imds'].std() / np.sqrt(n)\n\n# 95% confidence interval using t-distribution\nci_low, ci_high = stats.t.interval(0.95, df=n-1, loc=x_bar, scale=se)\n\nprint(\"=\" * 70)\nprint(\"95% CONFIDENCE INTERVAL FOR MEAN IMDS\")\nprint(\"=\" * 70)\nprint(f\"Sample size (n):       {n}\")\nprint(f\"Sample mean:           {x_bar:.4f}\")\nprint(f\"Standard error:        {se:.4f}\")\nprint(f\"95% CI:                [{ci_low:.4f}, {ci_high:.4f}]\")\nprint(f\"\\nInterpretation: We are 95% confident that the true mean\")\nprint(f\"municipal development index lies between {ci_low:.2f} and {ci_high:.2f}.\")\n\n\n\nTask 2: Hypothesis Test — National Development Target (Guided)\nLearning Goal: Apply Section 4.4 (two-sided test) to test a hypothesis about a population mean.\nEconomic Question: “Is the average municipality at the midpoint of the development scale?”\nA natural benchmark for the IMDS (which ranges from 0 to 100) is the midpoint of 50. If Bolivia’s municipalities were, on average, at this midpoint, we would expect \\(\\mu_{IMDS} = 50\\).\nInstructions:\n\nState the hypotheses: \\(H_0: \\mu = 50\\) vs \\(H_1: \\mu \\neq 50\\)\nUse scipy.stats.ttest_1samp() to conduct the test\nReport the t-statistic and p-value\nState your conclusion at \\(\\alpha = 0.05\\)\nDiscuss: Is the average municipality at the midpoint of the development scale?\n\n\n# Your code here: One-sample t-test — is mean IMDS = 50?\n\n# Hypothesis test: H0: mu = 50 vs H1: mu != 50\nt_stat, p_value = stats.ttest_1samp(bol_key['imds'], popmean=50)\n\nprint(\"=\" * 70)\nprint(\"HYPOTHESIS TEST: IS MEAN IMDS = 50?\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ = 50  vs  H₁: μ ≠ 50\")\nprint(f\"\\nSample mean:   {bol_key['imds'].mean():.4f}\")\nprint(f\"t-statistic:   {t_stat:.4f}\")\nprint(f\"p-value:       {p_value:.6f}\")\nprint(f\"\\nConclusion at α = 0.05:\")\nif p_value &lt; 0.05:\n    print(f\"  Reject H₀ (p = {p_value:.6f} &lt; 0.05)\")\n    print(f\"  The average IMDS is significantly different from 50.\")\nelse:\n    print(f\"  Fail to reject H₀ (p = {p_value:.6f} ≥ 0.05)\")\n    print(f\"  No significant evidence that mean IMDS differs from 50.\")\n\n\nKey Concept 4.12: Statistical Significance in Development\nA statistically significant difference between departments does not automatically imply a policy-relevant gap. With 339 municipalities providing large sample sizes, even small differences can achieve statistical significance. Policy makers must evaluate the magnitude of differences alongside p-values. A 2-point difference in IMDS may be statistically significant but practically negligible for resource allocation decisions.\n\n\n\nTask 3: Department-Level Inference (Semi-guided)\nLearning Goal: Apply confidence intervals to compare subgroups.\nEconomic Question: “Which departments have significantly different development levels from one another?”\nInstructions:\n\nCalculate the 95% confidence interval for mean imds in each of Bolivia’s 9 departments\nCreate a forest plot (horizontal error bars) showing the CI for each department\nIdentify which departments’ CIs overlap (suggesting no significant difference) and which don’t\nDiscuss what the pattern reveals about regional inequality\n\nHint: Use groupby('dep') to calculate department-level statistics, then plt.errorbar() or plt.barh() with error bars for the forest plot.\n\n# Your code here: Department-level 95% CIs and forest plot\n\n# Calculate department-level statistics\ndept_stats = bol_key.groupby('dep')['imds'].agg(['mean', 'std', 'count']).sort_values('mean')\ndept_stats['se'] = dept_stats['std'] / np.sqrt(dept_stats['count'])\ndept_stats['ci_low'] = dept_stats['mean'] - 1.96 * dept_stats['se']\ndept_stats['ci_high'] = dept_stats['mean'] + 1.96 * dept_stats['se']\n\nprint(\"=\" * 70)\nprint(\"95% CONFIDENCE INTERVALS FOR MEAN IMDS BY DEPARTMENT\")\nprint(\"=\" * 70)\nprint(dept_stats[['mean', 'se', 'ci_low', 'ci_high', 'count']].round(2).to_string())\n\n# Forest plot\nfig, ax = plt.subplots(figsize=(10, 6))\ndepartments = dept_stats.index\ny_pos = range(len(departments))\nax.errorbar(dept_stats['mean'], y_pos,\n            xerr=1.96 * dept_stats['se'],\n            fmt='o', color='navy', capsize=5, capthick=1.5, markersize=6)\nax.set_yticks(y_pos)\nax.set_yticklabels(departments)\nax.set_xlabel('Mean IMDS (with 95% CI)')\nax.set_title('Forest Plot: Municipal Development by Department')\nax.axvline(x=bol_key['imds'].mean(), color='red', linestyle='--', alpha=0.5, label='National mean')\nax.legend()\nax.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\nTask 4: One-Sided Test — Is the Capital Department More Developed? (Semi-guided)\nLearning Goal: Apply Section 4.6 (one-sided tests) to a directional hypothesis.\nEconomic Question: “Is La Paz department’s mean IMDS significantly greater than the national mean?”\nInstructions:\n\nExtract IMDS values for La Paz department\nTest \\(H_0: \\mu_{LP} \\leq \\mu_{national}\\) vs \\(H_1: \\mu_{LP} &gt; \\mu_{national}\\) using a one-sided t-test\nCalculate the one-sided p-value (divide the two-sided p-value by 2, checking direction)\nReport and interpret the result at \\(\\alpha = 0.05\\)\nDiscuss: Is the capital department significantly more developed?\n\nHint: Use scipy.stats.ttest_1samp() with the national mean as the test value, then adjust for a one-sided test.\n\n# Your code here: One-sided t-test for La Paz department\n\n# Extract La Paz IMDS values\nla_paz = bol_key[bol_key['dep'] == 'La Paz']['imds']\nnational_mean = bol_key['imds'].mean()\n\n# Two-sided test first\nt_stat_lp, p_two = stats.ttest_1samp(la_paz, popmean=national_mean)\n\n# One-sided p-value: H1: mu_LP &gt; national_mean\n# If t &gt; 0, one-sided p = p_two / 2; if t &lt; 0, one-sided p = 1 - p_two / 2\np_one = p_two / 2 if t_stat_lp &gt; 0 else 1 - p_two / 2\n\nprint(\"=\" * 70)\nprint(\"ONE-SIDED TEST: LA PAZ &gt; NATIONAL MEAN?\")\nprint(\"=\" * 70)\nprint(f\"H₀: μ_LaPaz ≤ {national_mean:.2f}  vs  H₁: μ_LaPaz &gt; {national_mean:.2f}\")\nprint(f\"\\nLa Paz municipalities:  {len(la_paz)}\")\nprint(f\"La Paz mean IMDS:       {la_paz.mean():.4f}\")\nprint(f\"National mean IMDS:     {national_mean:.4f}\")\nprint(f\"t-statistic:            {t_stat_lp:.4f}\")\nprint(f\"One-sided p-value:      {p_one:.6f}\")\nprint(f\"\\nConclusion at α = 0.05:\")\nif p_one &lt; 0.05:\n    print(f\"  Reject H₀ (p = {p_one:.6f} &lt; 0.05)\")\n    print(f\"  La Paz's mean IMDS is significantly greater than the national mean.\")\nelse:\n    print(f\"  Fail to reject H₀ (p = {p_one:.6f} ≥ 0.05)\")\n    print(f\"  No significant evidence that La Paz exceeds the national mean.\")\n\n\nKey Concept 4.13: Subnational Inference Challenges\nWhen testing hypotheses about departmental means, each department contains a different number of municipalities (ranging from ~10 to ~50+). Departments with fewer municipalities have wider confidence intervals and less statistical power. This means we may fail to detect real differences for smaller departments—not because the differences don’t exist, but because we lack sufficient data to establish them conclusively.\n\n\n\nTask 5: Comparing Two Departments (Independent)\nLearning Goal: Apply two-sample t-tests to compare group means.\nEconomic Question: “Is the development gap between the most and least developed departments statistically significant?”\nInstructions:\n\nIdentify the departments with the highest and lowest mean IMDS (from Task 3 results)\nUse scipy.stats.ttest_ind() to perform a two-sample t-test comparing these departments\nReport the t-statistic and p-value\nDiscuss both statistical significance (p-value) and practical significance (magnitude of the gap)\nWhat does this tell us about regional inequality in Bolivia?\n\n\n# Your code here: Two-sample t-test — highest vs lowest IMDS department\n\n# Identify highest and lowest departments\ndept_means = bol_key.groupby('dep')['imds'].mean()\ntop_dept = dept_means.idxmax()\nbot_dept = dept_means.idxmin()\n\n# Extract IMDS values for each\ntop_values = bol_key[bol_key['dep'] == top_dept]['imds']\nbot_values = bol_key[bol_key['dep'] == bot_dept]['imds']\n\n# Two-sample t-test\nt_stat_2s, p_value_2s = stats.ttest_ind(top_values, bot_values)\n\nprint(\"=\" * 70)\nprint(f\"TWO-SAMPLE T-TEST: {top_dept.upper()} vs {bot_dept.upper()}\")\nprint(\"=\" * 70)\nprint(f\"\\nHighest IMDS department: {top_dept}\")\nprint(f\"  Mean IMDS:   {top_values.mean():.4f}\")\nprint(f\"  N:           {len(top_values)}\")\nprint(f\"\\nLowest IMDS department:  {bot_dept}\")\nprint(f\"  Mean IMDS:   {bot_values.mean():.4f}\")\nprint(f\"  N:           {len(bot_values)}\")\nprint(f\"\\nDifference in means:    {top_values.mean() - bot_values.mean():.4f}\")\nprint(f\"t-statistic:            {t_stat_2s:.4f}\")\nprint(f\"p-value:                {p_value_2s:.6f}\")\nprint(f\"\\nConclusion at α = 0.05:\")\nif p_value_2s &lt; 0.05:\n    print(f\"  Reject H₀ (p = {p_value_2s:.6f} &lt; 0.05)\")\n    print(f\"  The development gap of {top_values.mean() - bot_values.mean():.2f} points is statistically significant.\")\nelse:\n    print(f\"  Fail to reject H₀ (p = {p_value_2s:.6f} ≥ 0.05)\")\n    print(f\"  No significant evidence of a development gap.\")\nprint(f\"\\nPractical significance: A gap of {top_values.mean() - bot_values.mean():.2f} points on a 0-100 scale\")\nprint(f\"represents a {'substantial' if abs(top_values.mean() - bot_values.mean()) &gt; 10 else 'modest'} difference in development outcomes.\")\n\n\n\nTask 6: Policy Brief on Regional Inequality (Independent)\nObjective: Write a 200-300 word policy brief summarizing your statistical findings.\nYour brief should address:\n\nAre departmental differences statistically significant? Summarize results from Tasks 3-5.\nWhich departments need the most attention? Use the forest plot and CI analysis to identify lagging regions.\nHow confident are we in these conclusions? Discuss the confidence levels and what the CIs tell us.\nWhat are the limitations of these tests? Consider sample sizes, assumptions, and what the tests cannot tell us.\n\nFormat: Write your brief in the markdown cell below. Support your arguments with specific numbers from your analysis (means, CIs, p-values).\nTip: Remember Key Concepts 4.12 and 4.13—distinguish between statistical significance and policy relevance, and consider how unequal sample sizes affect your conclusions.\n\n# Your code here: Additional analysis to support your policy brief\n#\n# Suggestions:\n# 1. Create a summary table of all department CIs and test results\n# 2. Calculate effect sizes (Cohen's d) for the two-sample comparison\n# 3. Visualize the national IMDS distribution with department means marked\n\n# Example: Summary statistics for the policy brief\nprint(\"=\" * 70)\nprint(\"SUMMARY TABLE FOR POLICY BRIEF\")\nprint(\"=\" * 70)\ndept_summary = bol_key.groupby('dep')['imds'].agg(['mean', 'std', 'count']).sort_values('mean', ascending=False)\ndept_summary['se'] = dept_summary['std'] / np.sqrt(dept_summary['count'])\ndept_summary['ci_95'] = dept_summary.apply(\n    lambda r: f\"[{r['mean'] - 1.96*r['se']:.1f}, {r['mean'] + 1.96*r['se']:.1f}]\", axis=1)\nprint(dept_summary[['mean', 'std', 'count', 'ci_95']].round(2).to_string())\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this analysis of subnational development in Bolivia, you’ve applied the full Chapter 4 statistical inference toolkit:\n\nConfidence intervals for mean development levels\nOne-sample and two-sample hypothesis tests to evaluate development benchmarks and compare groups\nOne-sided tests for directional hypotheses about specific departments\nForest plots for comparing group CIs visually\nStatistical vs practical significance in a policy context\nCritical thinking about sample size, statistical power, and inference limitations\n\nConnection to research: The DS4Bolivia project uses these same statistical tools to evaluate whether satellite-predicted development indicators are significantly different from survey-based measures, providing evidence for the reliability of remote sensing approaches to SDG monitoring.\nLooking ahead: In Chapter 5, we’ll move beyond testing means to exploring bivariate relationships—how nighttime lights relate to specific development outcomes across Bolivia’s municipalities.\n\nWell done! You’ve now tested development hypotheses for both cross-country convergence and Bolivian regional inequality using the tools of statistical inference.",
    "crumbs": [
      "Statistical Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Statistical Inference for the Mean</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to bivariate data analysis and simple linear regression using Python. You’ll learn how to summarize relationships between two variables using correlation, scatter plots, and regression analysis. All code runs directly in Google Colab without any local setup.\nBivariate data involves observations on two variables—for example, house prices and house sizes, or income and education. This chapter teaches you how to summarize and analyze relationships between two variables using correlation and regression.\nWhat you’ll learn:\nDatasets used:\nChapter outline:",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#chapter-overview",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#chapter-overview",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "",
    "text": "Summarize bivariate relationships using two-way tabulations and scatterplots\nCalculate and interpret correlation coefficients and understand their relationship to covariance\nEstimate and interpret regression lines using ordinary least squares (OLS)\nEvaluate model fit using R-squared, standard error, and variation decomposition\nMake predictions and identify outliers in regression analysis\nUnderstand the critical distinction between association and causation\nApply nonparametric regression methods to check linearity assumptions\n\n\n\nAED_HOUSE.DTA: House prices and characteristics for 29 houses sold in Central Davis, California in 1999 (price, size, bedrooms, bathrooms, lot size, age)\n\n\n\n5.1 Example - House Price and Size\n5.2 Two-Way Tabulation\n5.3 Two-Way Scatter Plot\n5.4 Sample Correlation\n5.5 Regression Line\n5.6 Measures of Model Fit\n5.7 Computer Output Following Regression\n5.8 Prediction and Outlying Observations\n5.9 Regression and Correlation\n5.10 Causation\n5.11 Nonparametric Regression",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#setup",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#setup",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom scipy import stats\nfrom scipy.ndimage import gaussian_filter1d\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to explore bivariate data analysis.\")\n\nSetup complete! Ready to explore bivariate data analysis.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#example---house-price-and-size",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#example---house-price-and-size",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.1 Example - House Price and Size",
    "text": "5.1 Example - House Price and Size\nWe begin by loading and examining data on house prices and sizes from 29 houses sold in Central Davis, California in 1999. This dataset will serve as our main example throughout the chapter.\nWhy this dataset?\n\nSmall enough to see individual observations\nLarge enough to demonstrate statistical relationships\nEconomically meaningful: housing is a major component of wealth\nClear relationship: larger houses tend to cost more\n\n\n# Load the house data\ndata_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n\nprint(\"Data loaded successfully!\")\nprint(f\"Number of observations: {len(data_house)}\")\nprint(f\"Number of variables: {data_house.shape[1]}\")\nprint(f\"\\nVariables: {', '.join(data_house.columns.tolist())}\")\n\nData loaded successfully!\nNumber of observations: 29\nNumber of variables: 8\n\nVariables: price, size, bedrooms, bathrooms, lotsize, age, monthsold, list\n\n\n\n# Display the complete dataset (Table 5.1)\nprint(\"=\" * 70)\nprint(\"TABLE 5.1: Complete Dataset\")\nprint(\"=\" * 70)\nprint(data_house.to_string())\n\n======================================================================\nTABLE 5.1: Complete Dataset\n======================================================================\n     price  size  bedrooms  bathrooms  lotsize   age  monthsold    list\n0   204000  1400         3        2.0        1  31.0          7  199900\n1   212000  1600         3        3.0        2  33.0          5  212000\n2   213000  1800         3        2.0        2  51.0          4  219900\n3   220000  1600         3        2.0        1  49.0          4  229000\n4   224500  2100         4        2.5        2  47.0          6  224500\n5   229000  1700         4        2.5        2  35.0          3  229500\n6   230000  2100         4        2.0        2  34.0          8  239000\n7   233000  1700         3        2.0        1  40.0          6  244500\n8   235000  1700         4        2.0        2  29.0          7  245000\n9   235000  1600         3        2.0        3  35.0          5  242000\n10  236500  1600         3        2.0        3  23.0          8  239500\n11  238000  1900         4        2.0        2  29.0          7  249000\n12  239500  1600         3        2.0        3  34.0          6  239500\n13  241000  1600         4        2.0        2  34.0          8  242500\n14  244000  2000         4        2.0        1  29.0          7  249000\n15  245000  1400         4        2.0        2  30.0          8  252000\n16  249000  1900         4        3.0        3  37.0          6  235000\n17  253000  2100         4        2.0        3  47.0          6  269000\n18  255000  1500         4        2.0        3  47.0          7  240000\n19  258500  1600         3        2.0        1  39.0          8  259900\n20  270000  1800         4        2.0        3  31.0          3  263900\n21  270000  2000         4        2.5        3  39.0          5  269000\n22  272000  1800         4        2.5        2  46.0          3  279500\n23  273000  1900         5        2.0        2  37.0          7  273000\n24  278500  2600         6        2.0        3  38.0          8  280000\n25  279900  2000         4        2.0        2  31.0          7  279900\n26  310000  2300         4        2.5        2  28.0          5  315000\n27  340000  2400         4        3.0        2  34.0          6  369900\n28  375000  3300         4        2.5        2  39.0          3  386000\n\n\n\n# Summary statistics (Table 5.2)\nprint(\"=\" * 70)\nprint(\"TABLE 5.2: Summary Statistics\")\nprint(\"=\" * 70)\nprint(data_house[['price', 'size']].describe())\n\n# Extract key variables\nprice = data_house['price']\nsize = data_house['size']\n\nprint(\"\\nPrice Statistics:\")\nprint(f\"  Mean:      ${price.mean():,.2f}\")\nprint(f\"  Median:    ${price.median():,.2f}\")\nprint(f\"  Min:       ${price.min():,.2f}\")\nprint(f\"  Max:       ${price.max():,.2f}\")\nprint(f\"  Std Dev:   ${price.std():,.2f}\")\n\nprint(\"\\nSize Statistics:\")\nprint(f\"  Mean:      {size.mean():,.0f} sq ft\")\nprint(f\"  Median:    {size.median():,.0f} sq ft\")\nprint(f\"  Min:       {size.min():,.0f} sq ft\")\nprint(f\"  Max:       {size.max():,.0f} sq ft\")\nprint(f\"  Std Dev:   {size.std():,.0f} sq ft\")\n\n======================================================================\nTABLE 5.2: Summary Statistics\n======================================================================\n               price         size\ncount      29.000000    29.000000\nmean   253910.344828  1882.758621\nstd     37390.710695   398.272130\nmin    204000.000000  1400.000000\n25%    233000.000000  1600.000000\n50%    244000.000000  1800.000000\n75%    270000.000000  2000.000000\nmax    375000.000000  3300.000000\n\nPrice Statistics:\n  Mean:      $253,910.34\n  Median:    $244,000.00\n  Min:       $204,000.00\n  Max:       $375,000.00\n  Std Dev:   $37,390.71\n\nSize Statistics:\n  Mean:      1,883 sq ft\n  Median:    1,800 sq ft\n  Min:       1,400 sq ft\n  Max:       3,300 sq ft\n  Std Dev:   398 sq ft\n\n\n\nKey Concept 5.1: Summary Statistics for Bivariate Data\nSummary statistics describe the center, spread, and range of each variable before examining their relationship. For bivariate analysis, compute the mean, median, standard deviation, minimum, and maximum of both variables. Comparing means and medians reveals skewness; standard deviations indicate variability. These univariate summaries provide essential context for interpreting correlation and regression results.\n\nWhat do these numbers tell us about the Davis housing market (1999)?\nPrice Statistics:\n\nMean = $253,910: Average house price in the sample\nMedian = $244,000: Middle value (half above, half below)\nRange: $204,000 to $375,000 (spread of $171,000)\nStd Dev = $37,391: Typical deviation from the mean\n\nSize Statistics:\n\nMean = 1,883 sq ft: Average house size\nMedian = 1,800 sq ft: Middle value\nRange: 1,400 to 3,300 sq ft (spread of 1,900 sq ft)\nStd Dev = 398 sq ft: Typical deviation from the mean\n\nKey insights:\n\nBoth distributions are fairly symmetric (means close to medians)\nSubstantial variation in both price and size (good for regression!)\nThe price coefficient of variation (CV = 0.15) and size CV (0.21) show moderate variability\nMoving from univariate to bivariate: In Chapter 2, we looked at single variables. Now we ask: how do these two variables move together?\n\nEconomic context: These are moderate-sized homes in a California college town (UC Davis), with typical prices for the late 1990s.\nKey observations:\n\nHouse prices range from $204,000 to $375,000 (mean: $253,910)\nHouse sizes range from 1,400 to 3,300 square feet (mean: 1,883 sq ft)\nBoth variables show substantial variation, which is good for regression analysis\nThe data appear to be reasonably symmetric (means close to medians)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#two-way-tabulation",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#two-way-tabulation",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.2 Two-Way Tabulation",
    "text": "5.2 Two-Way Tabulation\nA two-way tabulation (or crosstabulation) shows how observations are distributed across combinations of two categorical variables. For continuous variables like price and size, we first create categorical ranges.\nWhy use tabulation?\n\nProvides a quick summary of the relationship\nUseful for discrete or categorical data\nCan reveal patterns before formal analysis\n\n\n# Create categorical variables\nprice_range = pd.cut(price, bins=[0, 249999, np.inf],\n                     labels=['&lt; $250,000', '≥ $250,000'])\n\nsize_range = pd.cut(size, bins=[0, 1799, 2399, np.inf],\n                    labels=['&lt; 1,800', '1,800-2,399', '≥ 2,400'])\n\n# Create two-way table \nprint(\"=\" * 70)\nprint(\"Two-Way Tabulation of Price and Size\")\nprint(\"=\" * 70)\ncrosstab = pd.crosstab(price_range, size_range, margins=True)\nprint(crosstab)\n\nprint(\"\\nInterpretation:\")\nprint(\"- 11 houses are both low-priced and small\")\nprint(\"- 0 houses are both low-priced and large (≥ 2,400 sq ft)\")\nprint(\"- 3 houses are both high-priced and large\")\nprint(\"- Pattern suggests positive association: larger houses tend to be more expensive\")\n\n======================================================================\nTwo-Way Tabulation of Price and Size\n======================================================================\nsize        &lt; 1,800  1,800-2,399  ≥ 2,400  All\nprice                                         \n&lt; $250,000       11            6        0   17\n≥ $250,000        2            7        3   12\nAll              13           13        3   29\n\nInterpretation:\n- 11 houses are both low-priced and small\n- 0 houses are both low-priced and large (≥ 2,400 sq ft)\n- 3 houses are both high-priced and large\n- Pattern suggests positive association: larger houses tend to be more expensive\n\n\n\nKey Concept 5.2: Two-Way Tabulations\nTwo-way tabulations show the joint distribution of two categorical variables. Expected frequencies (calculated assuming independence) provide the basis for Pearson’s chi-squared test of statistical independence. The crosstabulation reveals patterns: no low-priced large houses suggests a positive association between size and price.\n\nWhat does this crosstab tell us?\nLooking at the table:\n\n11 houses are both small (&lt; 1,800 sq ft) AND low-priced (&lt; $250,000)\n0 houses are both large (≥ 2,400 sq ft) AND low-priced\n3 houses are both large (≥ 2,400 sq ft) AND high-priced (≥ $250,000)\n6 houses are medium-sized (1,800-2,399 sq ft) AND low-priced\n\nThe pattern reveals:\n\nPositive association: Most observations cluster in the “small and cheap” or “large and expensive” cells\nNo counterexamples: We never see “large and cheap” houses (the bottom-right cell is empty)\nImperfect relationship: Some medium-sized houses are low-priced (6 houses), some are high-priced (7 houses)\n\nLimitation of tabulation:\n\nWe lose information by categorizing continuous variables\nWe can’t quantify the strength of the relationship\nWe can’t make precise predictions\n\nNext step: Use the correlation coefficient and regression to measure the relationship more precisely using the full continuous data.\nFrom Categorical to Continuous:\nCrosstabulation is useful but has limitations:\n\nInformation loss: We convert continuous data (exact prices/sizes) into categories\nArbitrary bins: Results can change depending on where we draw category boundaries\nNo precise measurement: Can’t quantify exact strength of relationship\n\nSolution: Use the full continuous data with correlation and regression to:\n\nPreserve all information in the original measurements\nGet precise, interpretable measures (r, slope)\nMake specific predictions for any value of x",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#two-way-scatter-plot",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#two-way-scatter-plot",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.3 Two-Way Scatter Plot",
    "text": "5.3 Two-Way Scatter Plot\nA scatter plot is the primary visual tool for examining the relationship between two continuous variables. Each point represents one observation, with x-coordinate showing size and y-coordinate showing price.\nWhat to look for:\n\nDirection: Does y increase or decrease as x increases?\nStrength: How closely do points follow a pattern?\nForm: Is the relationship linear or curved?\nOutliers: Are there unusual observations far from the pattern?\n\n\n# Scatter plot of price vs size\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(size, price, s=80, alpha=0.7, color='black', edgecolor='black')\nax.set_xlabel('House size (in square feet)', fontsize=12)\nax.set_ylabel('House sale price (in dollars)', fontsize=12)\nax.set_title('House Price vs Size', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nWhat the scatter plot shows:\")\nprint(\"✓ Positive relationship: Larger houses tend to have higher prices\")\nprint(\"✓ Roughly linear: Points follow an upward-sloping pattern\")\nprint(\"✓ Moderate scatter: Not all points lie exactly on a line\")\nprint(\"✓ No obvious outliers: All points fit the general pattern\")\n\n\n\n\n\n\n\n\n\nWhat the scatter plot shows:\n✓ Positive relationship: Larger houses tend to have higher prices\n✓ Roughly linear: Points follow an upward-sloping pattern\n✓ Moderate scatter: Not all points lie exactly on a line\n✓ No obvious outliers: All points fit the general pattern\n\n\n\nKey Concept 5.3: Scatterplots and Relationships\nScatterplots provide visual evidence of relationships between two continuous variables. They reveal the direction (positive/negative), strength (tight/loose clustering), form (linear/curved), and outliers of the relationship. The house price-size scatterplot shows a strong, positive, roughly linear relationship with no obvious outliers.\n\nVisual vs. Quantitative Analysis:\nThe scatter plot provides qualitative insight (direction, form, outliers), but we need quantitative measures to:\n\nCommunicate precisely: “Strong positive relationship” is vague; “r = 0.79” is specific\nCompare across studies: Can’t compare scatter plots directly across datasets\nTest hypotheses: Need numerical values for statistical inference (Chapter 7)\nMake predictions: Visual estimates from graphs are imprecise\n\nNext: We’ll quantify this relationship using the correlation coefficient.\nWhat patterns do we observe?\n1. Direction: Positive relationship\n\nAs house size increases (moving right), house price increases (moving up)\nThis makes economic sense: bigger houses should cost more\n\n2. Form: Roughly linear\n\nPoints follow an upward-sloping pattern\nNo obvious curvature (e.g., not exponential or U-shaped)\nA straight line appears to be a reasonable summary\n\n3. Strength: Moderate to strong\n\nPoints cluster fairly closely around an imaginary line\nNot perfect (some scatter), but clear pattern visible\nWe’ll quantify this with the correlation coefficient\n\n4. Outliers: None obvious\n\nNo houses wildly far from the general pattern\nAll observations seem consistent with the relationship\n\nComparison to univariate analysis (Chapter 2):\n\nUnivariate: Histogram shows distribution of one variable\nBivariate: Scatter plot shows relationship between two variables\nNew question: How does Y change when X changes?\n\nWhat we can’t tell from the graph alone:\n\nExact strength of relationship (need correlation)\nPrecise prediction equation (need regression)\nStatistical significance (need inference, Chapter 7)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#sample-correlation",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#sample-correlation",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.4 Sample Correlation",
    "text": "5.4 Sample Correlation\nThe correlation coefficient \\(r\\) is a unit-free measure of linear association between two variables. It ranges from -1 to 1:\n\n\\(r = 1\\): Perfect positive linear relationship\n$0 &lt; r &lt; 1$: Positive linear relationship\n\\(r = 0\\): No linear relationship\n\\(-1 &lt; r &lt; 0\\): Negative linear relationship\n\\(r = -1\\): Perfect negative linear relationship\n\nFormula: \\[r_{xy} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\times \\sum_{i=1}^{n}(y_i - \\bar{y})^2}} = \\frac{s_{xy}}{s_x s_y}\\]\nwhere \\(s_{xy}\\) is the sample covariance, and \\(s_x\\), \\(s_y\\) are sample standard deviations.\nKey Properties of Correlation:\nUnderstanding these properties helps avoid common misinterpretations:\n\nUnit-free: r = 0.79 whether we measure price in dollars, thousands, or millions\nBounded: Always between -1 and +1 (unlike covariance, which is unbounded)\nSymmetric: r(price, size) = r(size, price) — order doesn’t matter\nOnly measures linear relationships: Can miss curved, U-shaped, or other nonlinear patterns\nSensitive to outliers: One extreme point can dramatically change r\n\nLimitation: Correlation is a summary measure but doesn’t provide predictions. For that, we need regression.\n\n# Compute correlation and covariance\ncov_matrix = data_house[['price', 'size']].cov()\ncorr_matrix = data_house[['price', 'size']].corr()\n\nprint(\"=\" * 70)\nprint(\"COVARIANCE AND CORRELATION\")\nprint(\"=\" * 70)\n\nprint(\"\\nCovariance matrix:\")\nprint(cov_matrix)\n\nprint(\"\\nCorrelation matrix:\")\nprint(corr_matrix)\n\nr = corr_matrix.loc['price', 'size']\nprint(f\"\\nCorrelation coefficient: r = {r:.4f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  The correlation of {r:.4f} indicates a strong positive linear\")\nprint(f\"  relationship between house price and size.\")\nprint(f\"  About {r**2:.1%} of the variation in price is linearly associated\")\nprint(f\"  with variation in size.\")\n\n======================================================================\nCOVARIANCE AND CORRELATION\n======================================================================\n\nCovariance matrix:\n              price          size\nprice  1.398065e+09  1.170161e+07\nsize   1.170161e+07  1.586207e+05\n\nCorrelation matrix:\n          price      size\nprice  1.000000  0.785782\nsize   0.785782  1.000000\n\nCorrelation coefficient: r = 0.7858\n\nInterpretation:\n  The correlation of 0.7858 indicates a strong positive linear\n  relationship between house price and size.\n  About 61.7% of the variation in price is linearly associated\n  with variation in size.\n\n\n\nKey Concept 5.4: The Correlation Coefficient\nThe correlation coefficient (r) is a scale-free measure of linear association ranging from -1 (perfect negative) to +1 (perfect positive). A correlation of 0 indicates no linear relationship. For house price and size, r = 0.786 indicates strong positive correlation. The correlation is unit-free, symmetric, and measures only linear relationships.\n\nWhat does r = 0.7858 mean?\n1. Strength of linear association:\n\nr = 0.7858 indicates a strong positive linear relationship\nScale reference:\n\n|r| &lt; 0.3: weak\n0.3 ≤ |r| &lt; 0.7: moderate\n|r| ≥ 0.7: strong\n\nOur value (0.79) is well into the “strong” range\n\n2. Direction:\n\nPositive: Larger houses are associated with higher prices\nIf r were negative, larger houses would be associated with lower prices (unlikely for housing!)\n\n3. Variance explained (preview):\n\nr² = (0.7858)² = 0.617 = 61.7%\nAbout 62% of price variation is linearly associated with size variation\nThe remaining 38% is due to other factors (location, age, condition, etc.)\n\n4. Properties of correlation:\n\nUnit-free: Same value whether we measure price in dollars or thousands of dollars\nSymmetric: r(price, size) = r(size, price) = 0.7858\nBounded: Always between -1 and +1\nLinear measure: Detects linear relationships, not curves\n\nComparison to Chapter 2 (univariate):\n\nChapter 2: Standard deviation measures spread of ONE variable\nChapter 5: Correlation measures how TWO variables move together\nBoth are standardized measures (unit-free)\n\nEconomic interpretation: The strong correlation confirms what we saw in the scatter plot: house size is a major determinant of house price, but it’s not the only factor.\n\nIllustration: Different Correlation Patterns\nTo build intuition, let’s visualize simulated data with different correlation coefficients.\n\n# Different correlation patterns\nnp.random.seed(12345)\nn = 30\nx = np.random.normal(3, 1, n)\nu1 = np.random.normal(0, 0.8, n)\ny1 = 3 + x + u1  # Strong positive correlation\nu2 = np.random.normal(0, 2, n)\ny2 = 3 + x + u2  # Moderate positive correlation\ny3 = 5 + u2      # Zero correlation\ny4 = 10 - x - u2 # Moderate negative correlation\n\ncorrelations = [\n    np.corrcoef(x, y1)[0, 1],\n    np.corrcoef(x, y2)[0, 1],\n    np.corrcoef(x, y3)[0, 1],\n    np.corrcoef(x, y4)[0, 1]\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\ndatasets = [(x, y1, 'Panel A: Strong Positive'),\n            (x, y2, 'Panel B: Moderate Positive'),\n            (x, y3, 'Panel C: Zero Correlation'),\n            (x, y4, 'Panel D: Moderate Negative')]\n\nfor idx, (ax, (x_data, y_data, title), corr) in enumerate(zip(axes, datasets, correlations)):\n    ax.scatter(x_data, y_data, s=60, alpha=0.7, color='black', edgecolor='black')\n    ax.set_xlabel('x', fontsize=11)\n    ax.set_ylabel('y', fontsize=11)\n    ax.set_title(f'{title}\\nr = {corr:.2f}', fontsize=12, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Different Correlation Patterns',\n             fontsize=14, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey observations:\")\nprint(\"• Panel A (r ≈ 0.78): Points cluster tightly around an upward slope\")\nprint(\"• Panel B (r ≈ 0.44): More scatter, but still positive relationship\")\nprint(\"• Panel C (r ≈ 0.00): No systematic pattern\")\nprint(\"• Panel D (r ≈ -0.53): Points follow a downward slope\")\n\n\n\n\n\n\n\n\n\nKey observations:\n• Panel A (r ≈ 0.78): Points cluster tightly around an upward slope\n• Panel B (r ≈ 0.44): More scatter, but still positive relationship\n• Panel C (r ≈ 0.00): No systematic pattern\n• Panel D (r ≈ -0.53): Points follow a downward slope",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#regression-line",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#regression-line",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.5 Regression Line",
    "text": "5.5 Regression Line\nThe regression line provides the “best-fitting” linear summary of the relationship between y (dependent variable) and x (independent variable):\n\\[\\hat{y} = b_1 + b_2 x\\]\nwhere:\n\n\\(\\hat{y}\\) = predicted (fitted) value of y\n\\(b_1\\) = intercept (predicted y when x = 0)\n\\(b_2\\) = slope (change in y for one-unit increase in x)\n\nOrdinary Least Squares (OLS) chooses \\(b_1\\) and \\(b_2\\) to minimize the sum of squared residuals:\n\\[\\min_{b_1, b_2} \\sum_{i=1}^n (y_i - b_1 - b_2 x_i)^2\\]\nFormulas: \\[b_2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{s_{xy}}{s_x^2}\\]\n\\[b_1 = \\bar{y} - b_2 \\bar{x}\\]\nTransition Note: Correlation measures the strength of association, but doesn’t provide a prediction equation. Now we turn to regression analysis, which fits a line to predict y from x and quantifies how much y changes per unit change in x.\n\n# Fit OLS regression\nmodel = ols('price ~ size', data=data_house).fit()\n\nprint(\"=\" * 70)\nprint(\"REGRESSION RESULTS: price ~ size\")\nprint(\"=\" * 70)\nprint(model.summary())\n\n======================================================================\nREGRESSION RESULTS: price ~ size\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.617\nModel:                            OLS   Adj. R-squared:                  0.603\nMethod:                 Least Squares   F-statistic:                     43.58\nDate:                Tue, 10 Feb 2026   Prob (F-statistic):           4.41e-07\nTime:                        02:07:38   Log-Likelihood:                -332.05\nNo. Observations:                  29   AIC:                             668.1\nDf Residuals:                      27   BIC:                             670.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    1.15e+05   2.15e+04      5.352      0.000    7.09e+04    1.59e+05\nsize          73.7710     11.175      6.601      0.000      50.842      96.700\n==============================================================================\nOmnibus:                        0.576   Durbin-Watson:                   1.219\nProb(Omnibus):                  0.750   Jarque-Bera (JB):                0.638\nSkew:                          -0.078   Prob(JB):                        0.727\nKurtosis:                       2.290   Cond. No.                     9.45e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nKey Concept 5.5: Ordinary Least Squares\nThe method of ordinary least squares (OLS) chooses the regression line to minimize the sum of squared residuals. This yields formulas for the slope (b₂ = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)²) and intercept (b₁ = ȳ - b₂x̄) that can be computed from the data. The slope equals the covariance divided by the variance of x.\n\n\n# Extract and interpret coefficients\nintercept = model.params['Intercept']\nslope = model.params['size']\nr_squared = model.rsquared\n\nprint(\"=\" * 70)\nprint(\"KEY REGRESSION COEFFICIENTS\")\nprint(\"=\" * 70)\nprint(f\"\\nFitted regression line:\")\nprint(f\"  ŷ = {intercept:,.2f} + {slope:.2f} × size\")\nprint(f\"\\nIntercept (b₁): ${intercept:,.2f}\")\nprint(f\"  Interpretation: Predicted price when size = 0\")\nprint(f\"  (Not economically meaningful in this case)\")\n\nprint(f\"\\nSlope (b₂): ${slope:.2f} per square foot\")\nprint(f\"  Interpretation: Each additional square foot is associated with\")\nprint(f\"  a ${slope:.2f} increase in house price, on average.\")\n\nprint(f\"\\nExamples:\")\nprint(f\"  • 100 sq ft larger → ${slope * 100:,.2f} higher price\")\nprint(f\"  • 500 sq ft larger → ${slope * 500:,.2f} higher price\")\n\nprint(f\"\\nR-squared: {r_squared:.4f} ({r_squared*100:.2f}%)\")\nprint(f\"  {r_squared*100:.2f}% of price variation is explained by size\")\n\n======================================================================\nKEY REGRESSION COEFFICIENTS\n======================================================================\n\nFitted regression line:\n  ŷ = 115,017.28 + 73.77 × size\n\nIntercept (b₁): $115,017.28\n  Interpretation: Predicted price when size = 0\n  (Not economically meaningful in this case)\n\nSlope (b₂): $73.77 per square foot\n  Interpretation: Each additional square foot is associated with\n  a $73.77 increase in house price, on average.\n\nExamples:\n  • 100 sq ft larger → $7,377.10 higher price\n  • 500 sq ft larger → $36,885.52 higher price\n\nR-squared: 0.6175 (61.75%)\n  61.75% of price variation is explained by size\n\n\nKey findings from the house price regression:\nThe fitted equation:\nŷ = 115,017 + 73.77 × size\n1. Slope coefficient: $73.77 per square foot (p &lt; 0.001)\n\nInterpretation: Each additional square foot is associated with a $73.77 increase in house price, on average\nStatistical significance: p-value ≈ 0 (highly significant)\nConfidence interval: [50.84, 96.70] — we’re 95% confident the true effect is between $51 and $97 per sq ft\n\n2. Practical examples:\n\n100 sq ft larger → $73.77 × 100 = $7,377 higher price\n500 sq ft larger → $73.77 × 500 = $36,885 higher price\n1,000 sq ft larger → $73.77 × 1,000 = $73,770 higher price\n\n3. Intercept: $115,017\n\nMathematical interpretation: Predicted price when size = 0\nReality check: A house can’t have zero square feet!\nBetter interpretation: This is just where the regression line crosses the y-axis\nDon’t take it literally — it’s outside the data range (1,400-3,300 sq ft)\n\n4. R-squared: 0.617 (61.7%)\n\nSize explains 62% of the variation in house prices\nThe remaining 38% is due to other factors:\nLocation (neighborhood quality, schools)\nPhysical characteristics (bathrooms, garage, condition)\nMarket conditions (time of sale)\nUnique features (view, lot size, upgrades)\n\nComparison to correlation:\n\nWe computed r = 0.7858\nR² = (0.7858)² = 0.617 (they match!)\nFor simple regression, R² always equals r²\n\nEconomic interpretation: The strong relationship (R² = 0.62) between size and price makes economic sense. Buyers pay a substantial premium for additional space. However, the imperfect fit reminds us that many factors beyond size affect house values.\n\nVisualizing the Fitted Regression Line\n\n# Scatter plot with regression line\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(size, price, s=80, alpha=0.7, color='black',\n           edgecolor='black', label='Actual prices')\nax.plot(size, model.fittedvalues, color='blue', linewidth=2, label='Fitted regression line')\n\n# Add equation to plot\nequation_text = f'ŷ = {intercept:,.0f} + {slope:.2f} × size\\nR² = {r_squared:.4f}'\nax.text(0.05, 0.95, equation_text,\n        transform=ax.transAxes, fontsize=11,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nax.set_xlabel('House size (in square feet)', fontsize=12)\nax.set_ylabel('House sale price (in dollars)', fontsize=12)\nax.set_title('Scatter plot with regression line',\n             fontsize=14, fontweight='bold')\nax.legend(loc='lower right')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nThe blue line is the 'line of best fit'\")\nprint(\"It minimizes the sum of squared vertical distances from each point.\")\n\n\n\n\n\n\n\n\n\nThe blue line is the 'line of best fit'\nIt minimizes the sum of squared vertical distances from each point.\n\n\n\n\nSpecial Case: Intercept-Only Regression\nWhen we regress y on only an intercept (no x variable), the OLS estimate equals the sample mean of y. This shows that regression is a natural extension of univariate statistics.\n\n# Intercept-only regression\nmodel_intercept = ols('price ~ 1', data=data_house).fit()\n\nprint(\"=\" * 70)\nprint(\"INTERCEPT-ONLY REGRESSION\")\nprint(\"=\" * 70)\nprint(f\"Intercept from regression: ${model_intercept.params[0]:,.2f}\")\nprint(f\"Sample mean of price:      ${price.mean():,.2f}\")\nprint(\"\\nThese are equal, confirming that OLS generalizes the sample mean!\")\n\n======================================================================\nINTERCEPT-ONLY REGRESSION\n======================================================================\nIntercept from regression: $253,910.34\nSample mean of price:      $253,910.34\n\nThese are equal, confirming that OLS generalizes the sample mean!\n\n\n/tmp/ipython-input-1986013926.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(f\"Intercept from regression: ${model_intercept.params[0]:,.2f}\")",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#measures-of-model-fit",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#measures-of-model-fit",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.6 Measures of Model Fit",
    "text": "5.6 Measures of Model Fit\nTwo key measures assess how well the regression line fits the data:\n\nR-squared (R²)\nProportion of variation in y explained by x (ranges from 0 to 1):\n\\[R^2 = \\frac{\\text{Explained SS}}{\\text{Total SS}} = \\frac{\\sum (\\hat{y}_i - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\\]\nInterpretation:\n\n\\(R^2 = 0\\): x explains none of the variation in y\n\\(R^2 = 1\\): x explains all of the variation in y\n\\(R^2 = r^2\\) (for simple regression, R² equals the squared correlation)\n\n\n\nStandard Error of the Regression (s_e)\nStandard deviation of the residuals (typical size of prediction errors):\n\\[s_e = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nInterpretation:\n\nLower \\(s_e\\) means fitted values are closer to actual values\nUnits: same as y (dollars in our example)\nDividing by (n-2) accounts for estimation of two parameters\n\n\n# Compute model fit measures\nprint(\"=\" * 70)\nprint(\"MEASURES OF MODEL FIT\")\nprint(\"=\" * 70)\n\nr_squared = model.rsquared\nadj_r_squared = model.rsquared_adj\nse = np.sqrt(model.mse_resid)\nn = len(data_house)\n\nprint(f\"\\nR-squared:               {r_squared:.4f}\")\nprint(f\"  {r_squared*100:.2f}% of price variation explained by size\")\n\nprint(f\"\\nAdjusted R-squared:      {adj_r_squared:.4f}\")\nprint(f\"  Penalizes for number of regressors\")\n\nprint(f\"\\nStandard error (s_e):    ${se:,.2f}\")\nprint(f\"  Typical prediction error is about ${se:,.0f}\")\n\n# Verify R² = r²\nr = corr_matrix.loc['price', 'size']\nprint(f\"\\nVerification: R² = r²\")\nprint(f\"  R² = {r_squared:.4f}\")\nprint(f\"  r² = {r**2:.4f}\")\nprint(f\"  Match: {np.isclose(r_squared, r**2)}\")\n\n======================================================================\nMEASURES OF MODEL FIT\n======================================================================\n\nR-squared:               0.6175\n  61.75% of price variation explained by size\n\nAdjusted R-squared:      0.6033\n  Penalizes for number of regressors\n\nStandard error (s_e):    $23,550.66\n  Typical prediction error is about $23,551\n\nVerification: R² = r²\n  R² = 0.6175\n  r² = 0.6175\n  Match: True\n\n\n\nKey Concept 5.6: R-Squared Goodness of Fit\nR-squared measures the fraction of variation in y explained by the regression on x. It ranges from 0 (no explanatory power) to 1 (perfect fit). For bivariate regression, R² equals the squared correlation coefficient (R² = r²ₓᵧ). R² = 0.62 means 62% of house price variation is explained by size variation, while 38% is due to other factors.\n\nTransition Note: We’ve estimated the regression line. Now we assess how well this line fits the data using R-squared (proportion of variation explained) and the standard error of regression (typical prediction error).\nUnderstanding R² = 0.617 and Standard Error = $23,162\n1. R-squared (coefficient of determination):\n\nValue: 0.617 or 61.7%\nMeaning: Size explains 61.7% of the variation in house prices\nThe other 38.3%: Due to factors not in our model (location, quality, age, etc.)\n\nHow to think about R²:\n\nR² = 0: x has no predictive power (horizontal line)\nR² = 0.617: x has substantial predictive power (our case)\nR² = 1: x predicts y perfectly (all points on the line)\n\nIs R² = 0.617 “good”?\n\nFor cross-sectional data: Yes, this is quite good!\nContext matters:\nLab experiments: Often R² &gt; 0.9\nCross-sectional economics: R² = 0.2-0.6 is typical\nTime series: R² = 0.7-0.95 is common\nSingle predictor: Size alone explains most variation — impressive!\n\n2. Standard error: $23,162\n\nMeaning: Typical prediction error (residual size)\nContext:\nAverage house price: $253,910\nTypical error: $23,162 (about 9% of average)\nThis is reasonably accurate for house price prediction\n\n3. Verification: R² = r²\n\nCorrelation: r = 0.7858\nR-squared: R² = 0.617\nCheck: (0.7858)² = 0.617\nFor simple regression, these are always equal\n\n4. Sum of Squares decomposition:\nTotal SS = Explained SS + Residual SS\n100% = 61.7% + 38.3%\nPractical implications:\n\nFor predictions: Expect errors around ±$23,000\nFor policy: Size is important, but other factors matter too\nFor research: May want to add more variables (multiple regression, Chapters 10-12)\n\n\n\nIllustration: Total SS, Explained SS, and Residual SS\nLet’s create a simple example to visualize how R² is computed.\n\n# Simulated data for demonstration\nnp.random.seed(123456)\nx_sim = np.arange(1, 6)\nepsilon = np.random.normal(0, 2, 5)\ny_sim = 1 + 2*x_sim + epsilon\n\ndf_sim = pd.DataFrame({'x': x_sim, 'y': y_sim})\nmodel_sim = ols('y ~ x', data=df_sim).fit()\n\nprint(\"=\" * 70)\nprint(\"SIMULATED DATA FOR MODEL FIT ILLUSTRATION\")\nprint(\"=\" * 70)\nprint(f\"\\n{'x':&lt;5} {'y':&lt;10} {'ŷ':&lt;10} {'Residual (e)':&lt;15} {'(y - ȳ)':&lt;10} {'(ŷ - ȳ)':&lt;10}\")\nprint(\"-\" * 70)\nfor i in range(len(x_sim)):\n    print(f\"{x_sim[i]:&lt;5} {y_sim[i]:&lt;10.4f} {model_sim.fittedvalues[i]:&lt;10.4f} \"\n          f\"{model_sim.resid[i]:&lt;15.4f} {y_sim[i] - y_sim.mean():&lt;10.4f} \"\n          f\"{model_sim.fittedvalues[i] - y_sim.mean():&lt;10.4f}\")\n\nprint(f\"\\nSums of Squares:\")\ntotal_ss = np.sum((y_sim - y_sim.mean())**2)\nexplained_ss = np.sum((model_sim.fittedvalues - y_sim.mean())**2)\nresidual_ss = np.sum(model_sim.resid**2)\n\nprint(f\"  Total SS     = {total_ss:.4f}\")\nprint(f\"  Explained SS = {explained_ss:.4f}\")\nprint(f\"  Residual SS  = {residual_ss:.4f}\")\nprint(f\"\\nCheck: Explained SS + Residual SS = {explained_ss + residual_ss:.4f}\")\nprint(f\"       Total SS                     = {total_ss:.4f}\")\n\nprint(f\"\\nR² = Explained SS / Total SS = {explained_ss / total_ss:.4f}\")\nprint(f\"R² from model = {model_sim.rsquared:.4f}\")\n\n======================================================================\nSIMULATED DATA FOR MODEL FIT ILLUSTRATION\n======================================================================\n\nx     y          ŷ          Residual (e)    (y - ȳ)    (ŷ - ȳ)   \n----------------------------------------------------------------------\n1     3.9382     2.2482     1.6900          -2.5632    -4.2533   \n2     4.4343     4.3748     0.0595          -2.0672    -2.1266   \n3     3.9819     6.5015     -2.5196         -2.5196    -0.0000   \n4     6.7287     8.6281     -1.8994         0.2273     2.1266    \n5     13.4242    10.7548    2.6695          6.9228     4.2533    \n\nSums of Squares:\n  Total SS     = 65.1680\n  Explained SS = 45.2262\n  Residual SS  = 19.9418\n\nCheck: Explained SS + Residual SS = 65.1680\n       Total SS                     = 65.1680\n\nR² = Explained SS / Total SS = 0.6940\nR² from model = 0.6940\n\n\n\n# Visualization of model fit\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Total SS (deviations from mean)\naxes[0].scatter(x_sim, y_sim, s=100, color='black', marker='^', label='Actual y', zorder=3)\naxes[0].axhline(y=y_sim.mean(), color='red', linewidth=2, linestyle='--',\n                label=f'Mean of y = {y_sim.mean():.2f}', zorder=2)\n# Draw vertical lines from points to mean\nfor i in range(len(x_sim)):\n    axes[0].plot([x_sim[i], x_sim[i]], [y_sim[i], y_sim.mean()],\n                 'b-', linewidth=1.5, alpha=0.5, zorder=1)\naxes[0].set_xlabel('x', fontsize=12)\naxes[0].set_ylabel('y', fontsize=12)\naxes[0].set_title('Panel A: Total SS\\n(Deviations from Mean)', fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Explained SS (deviations of fitted values from mean)\naxes[1].scatter(x_sim, model_sim.fittedvalues, s=100, color='black',\n                marker='o', label='Fitted ŷ', zorder=3)\naxes[1].axhline(y=y_sim.mean(), color='red', linewidth=2, linestyle='--',\n                label=f'Mean of y = {y_sim.mean():.2f}', zorder=2)\n# Draw vertical lines from fitted values to mean\nfor i in range(len(x_sim)):\n    axes[1].plot([x_sim[i], x_sim[i]], [model_sim.fittedvalues[i], y_sim.mean()],\n                 'g-', linewidth=1.5, alpha=0.5, zorder=1)\naxes[1].set_xlabel('x', fontsize=12)\naxes[1].set_ylabel('ŷ', fontsize=12)\naxes[1].set_title('Panel B: Explained SS\\n(Fitted Values from Mean)', fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Model Fit Illustration',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nPanel A shows Total SS: how far actual y values are from their mean\")\nprint(\"Panel B shows Explained SS: how far fitted values are from the mean\")\nprint(\"R² = (Explained SS) / (Total SS) measures the proportion explained\")\n\n\n\n\n\n\n\n\n\nPanel A shows Total SS: how far actual y values are from their mean\nPanel B shows Explained SS: how far fitted values are from the mean\nR² = (Explained SS) / (Total SS) measures the proportion explained\n\n\nPractical Implications of R² in Economics:\nIn applied econometrics, R² values around 0.60 are considered quite strong for cross-sectional data. Our R² = 0.617 tells us:\n\nSize is a major determinant: House size explains most of the price variation\nOther factors matter: The remaining 38% is due to location, quality, age, amenities, etc.\nSingle-variable limits: One predictor can only explain so much in complex real-world data\n\nWhy R² varies by context:\n\nLab experiments: Often R² &gt; 0.90 (controlled conditions, few confounding factors)\nCross-sectional economics: Typically R² = 0.20-0.60 (many unobserved heterogeneities)\nTime series data: Often R² = 0.70-0.95 (trends and persistence dominate)\n\nNext step: This motivates multiple regression (Chapters 10-12), where we include many explanatory variables simultaneously to capture more of the variation in y.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#computer-output-following-regression",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#computer-output-following-regression",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.7 Computer Output Following Regression",
    "text": "5.7 Computer Output Following Regression\nModern statistical software provides comprehensive regression output. Let’s examine each component of the output for our house price regression.\nUnderstanding Prediction Uncertainty:\nOur prediction ŷ = $262,559 for a 2,000 sq ft house is a point estimate — our best single guess. But predictions have uncertainty:\nSources of uncertainty:\n\nEstimation error: We don’t know the true β₁ and β₂, only estimates b₁ and b₂\nFundamental randomness: Even houses of identical size sell for different prices\nModel limitations: Our simple model omits many price determinants\n\nPreview of Chapter 7: We’ll learn to construct prediction intervals like:\n\n“We’re 95% confident the price will be between $215,000 and $310,000”\nThis acknowledges uncertainty while still providing useful guidance\n\nFor now, remember: the standard error ($23,551) gives a rough sense of typical prediction errors.\n\n# Display full regression output\nprint(\"=\" * 70)\nprint(\"COMPLETE REGRESSION OUTPUT\")\nprint(\"=\" * 70)\nprint(model.summary())\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"GUIDE TO REGRESSION OUTPUT\")\nprint(\"=\" * 70)\n\nprint(\"\\n1. TOP SECTION - Model Summary:\")\nprint(f\"   • Dep. Variable: price (what we're predicting)\")\nprint(f\"   • No. Observations: {int(model.nobs)} (sample size)\")\nprint(f\"   • R-squared: {model.rsquared:.4f} (goodness of fit)\")\nprint(f\"   • F-statistic: {model.fvalue:.2f} (overall significance)\")\n\nprint(\"\\n2. MIDDLE SECTION - Coefficients Table:\")\nprint(f\"   • coef: Estimated slope and intercept\")\nprint(f\"   • std err: Standard error (precision measure)\")\nprint(f\"   • t: t-statistic for testing H₀: coefficient = 0\")\nprint(f\"   • P&gt;|t|: p-value for significance test\")\nprint(f\"   • [0.025  0.975]: 95% confidence interval\")\n\nprint(\"\\n3. BOTTOM SECTION - Diagnostic Tests:\")\nprint(f\"   • Omnibus: Test for normality of residuals\")\nprint(f\"   • Durbin-Watson: Test for autocorrelation\")\nprint(f\"   • Jarque-Bera: Another normality test\")\nprint(f\"   • Cond. No.: Multicollinearity diagnostic\")\n\n======================================================================\nCOMPLETE REGRESSION OUTPUT\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.617\nModel:                            OLS   Adj. R-squared:                  0.603\nMethod:                 Least Squares   F-statistic:                     43.58\nDate:                Tue, 10 Feb 2026   Prob (F-statistic):           4.41e-07\nTime:                        02:07:41   Log-Likelihood:                -332.05\nNo. Observations:                  29   AIC:                             668.1\nDf Residuals:                      27   BIC:                             670.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    1.15e+05   2.15e+04      5.352      0.000    7.09e+04    1.59e+05\nsize          73.7710     11.175      6.601      0.000      50.842      96.700\n==============================================================================\nOmnibus:                        0.576   Durbin-Watson:                   1.219\nProb(Omnibus):                  0.750   Jarque-Bera (JB):                0.638\nSkew:                          -0.078   Prob(JB):                        0.727\nKurtosis:                       2.290   Cond. No.                     9.45e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n======================================================================\nGUIDE TO REGRESSION OUTPUT\n======================================================================\n\n1. TOP SECTION - Model Summary:\n   • Dep. Variable: price (what we're predicting)\n   • No. Observations: 29 (sample size)\n   • R-squared: 0.6175 (goodness of fit)\n   • F-statistic: 43.58 (overall significance)\n\n2. MIDDLE SECTION - Coefficients Table:\n   • coef: Estimated slope and intercept\n   • std err: Standard error (precision measure)\n   • t: t-statistic for testing H₀: coefficient = 0\n   • P&gt;|t|: p-value for significance test\n   • [0.025  0.975]: 95% confidence interval\n\n3. BOTTOM SECTION - Diagnostic Tests:\n   • Omnibus: Test for normality of residuals\n   • Durbin-Watson: Test for autocorrelation\n   • Jarque-Bera: Another normality test\n   • Cond. No.: Multicollinearity diagnostic",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#prediction-and-outliers",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#prediction-and-outliers",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.8 Prediction and Outliers",
    "text": "5.8 Prediction and Outliers\nOnce we have a fitted regression line, we can use it to predict y for any given value of x:\n\\[\\hat{y} = b_1 + b_2 x^*\\]\nTwo types of predictions:\n\nIn-sample: x is within the range of observed data (reliable)\nOut-of-sample: x is outside the observed range (extrapolation - use with caution)\n\nOutliers are observations that are unusually far from the regression line. They may indicate:\n\nData entry errors\nUnusual circumstances\nModel misspecification\nNatural variation\n\n\n# Prediction example\nprint(\"=\" * 70)\nprint(\"PREDICTION EXAMPLES\")\nprint(\"=\" * 70)\n\n# Predict for a 2000 sq ft house\nnew_size = pd.DataFrame({'size': [2000]})\npredicted_price = model.predict(new_size)\n\nprint(f\"\\nExample 1: Predict price for a 2,000 sq ft house\")\nprint(f\"  Using the model: ŷ = {intercept:.2f} + {slope:.2f} × 2000\")\nprint(f\"  Predicted price: ${predicted_price.values[0]:,.2f}\")\n\n# Manual calculation\nmanual_prediction = intercept + slope * 2000\nprint(f\"  Manual check: ${manual_prediction:,.2f}\")\n\n# Multiple predictions\nprint(f\"\\nExample 2: Predictions for various house sizes\")\nsizes_to_predict = [1500, 1800, 2000, 2500, 3000]\npredictions = pd.DataFrame({'size': sizes_to_predict})\npredictions['predicted_price'] = model.predict(predictions)\n\nprint(predictions.to_string(index=False))\n\nprint(f\"\\nObserved size range: {size.min():.0f} to {size.max():.0f} sq ft\")\nprint(f\"  1500, 1800, 2000 are in-sample (reliable)\")\nprint(f\"  3000 is at the edge; 3500+ would be extrapolation (less reliable)\")\n\n======================================================================\nPREDICTION EXAMPLES\n======================================================================\n\nExample 1: Predict price for a 2,000 sq ft house\n  Using the model: ŷ = 115017.28 + 73.77 × 2000\n  Predicted price: $262,559.36\n  Manual check: $262,559.36\n\nExample 2: Predictions for various house sizes\n size  predicted_price\n 1500    225673.843168\n 1800    247805.155280\n 2000    262559.363354\n 2500    299444.883540\n 3000    336330.403727\n\nObserved size range: 1400 to 3300 sq ft\n  1500, 1800, 2000 are in-sample (reliable)\n  3000 is at the edge; 3500+ would be extrapolation (less reliable)\n\n\nExample prediction: 2,000 sq ft house\nPredicted price: $262,559\nUsing our regression equation:\nŷ = \\$115,017 + \\$73.77 × 2,000 = \\$262,559\nHow reliable is this prediction?\n1. In-sample vs. out-of-sample:\n\nOur data range: 1,400 to 3,300 sq ft\nPrediction at 2,000 sq ft: in-sample (safe)\nPrediction at 5,000 sq ft: out-of-sample (risky extrapolation)\n\n2. Prediction accuracy:\n\nStandard error: $23,162\nTypical error: about ±$23,000 around the prediction\nInformal prediction interval: roughly $239,000 to $286,000\n(Chapter 7 will cover formal prediction intervals)\n\n3. Why predictions aren’t perfect:\n\nOur model only includes size\nMissing factors affect individual houses:\n\nNeighborhood quality\nNumber of bathrooms\nLot size\nAge and condition\nUnique features\n\n\nUnderstanding residuals:\nA residual is the prediction error for one observation:\nresidual = actual price - predicted price\n         = y - ŷ\nPositive residual: House sold for MORE than predicted (underestimate) Negative residual: House sold for LESS than predicted (overestimate)\nWhy do some houses have large residuals?\n\nParticularly desirable/undesirable location\nExceptional quality or poor condition\nUnique features not captured by size alone\nMay indicate measurement error or unusual circumstances\n\nKey insight: The regression line gives the average relationship. Individual houses deviate from this average based on their unique characteristics.\n\n# Identify potential outliers using residuals\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OUTLIER DETECTION\")\nprint(\"=\" * 70)\n\n# Add residuals and standardized residuals to dataset\ndata_house['fitted'] = model.fittedvalues\ndata_house['residual'] = model.resid\ndata_house['std_resid'] = model.resid / model.resid.std()\n\n# Observations with large residuals (&gt;2 std deviations)\noutliers = data_house[np.abs(data_house['std_resid']) &gt; 2]\n\nprint(f\"\\nObservations with large residuals (|standardized residual| &gt; 2):\")\nif len(outliers) &gt; 0:\n    print(outliers[['price', 'size', 'fitted', 'residual', 'std_resid']])\nelse:\n    print(\"  None found (all residuals within 2 standard deviations)\")\n\nprint(f\"\\nTop 5 largest residuals (in absolute value):\")\ntop_residuals = data_house.nlargest(5, 'residual', keep='all')[['price', 'size', 'fitted', 'residual']]\nprint(top_residuals)\n\n\n======================================================================\nOUTLIER DETECTION\n======================================================================\n\nObservations with large residuals (|standardized residual| &gt; 2):\n     price  size         fitted      residual  std_resid\n27  340000  2400  292067.779503  47932.220497   2.072629\n\nTop 5 largest residuals (in absolute value):\n     price  size         fitted      residual\n27  340000  2400  292067.779503  47932.220497\n18  255000  1500  225673.843168  29326.156832\n15  245000  1400  218296.739130  26703.260870\n19  258500  1600  233050.947205  25449.052795\n26  310000  2300  284690.675466  25309.324534",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#regression-and-correlation",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#regression-and-correlation",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.9 Regression and Correlation",
    "text": "5.9 Regression and Correlation\nThere’s a close relationship between the regression slope and the correlation coefficient:\n\\[b_2 = r_{xy} \\times \\frac{s_y}{s_x}\\]\nKey insights:\n\n\\(r_{xy} &gt; 0 \\Rightarrow b_2 &gt; 0\\) (positive correlation means positive slope)\n\\(r_{xy} &lt; 0 \\Rightarrow b_2 &lt; 0\\) (negative correlation means negative slope)\n\\(r_{xy} = 0 \\Rightarrow b_2 = 0\\) (zero correlation means zero slope)\n\nBut regression and correlation differ:\n\nCorrelation treats x and y symmetrically: \\(r_{xy} = r_{yx}\\)\nRegression does not: slope from regressing y on x \\(\\neq\\) inverse of slope from regressing x on y\n\nWhy This Relationship Matters:\nThe formula b₂ = r × (sᵧ/sₓ) reveals an important insight about the connection between correlation and regression:\nCorrelation (r):\n\nScale-free measure (unitless)\nSame value regardless of measurement units\nSymmetric: r(price, size) = r(size, price)\n\nRegression slope (b₂):\n\nScale-dependent (has units: $/sq ft in our example)\nChanges when we rescale variables\nAsymmetric: slope from price~size ≠ inverse of slope from size~price\n\nThe ratio (sᵧ/sₓ):\n\nConverts between correlation and slope\nAccounts for the relative variability of y and x\nExplains why slopes have interpretable units while r does not\n\nPractical implication: This is why we use regression (not just correlation) in economics—we need interpretable coefficients with units ($/sq ft, % change, etc.) to make policy recommendations and predictions.\n\n# Verify relationship between slope and correlation\nprint(\"=\" * 70)\nprint(\"RELATIONSHIP: SLOPE = CORRELATION × (SD_Y / SD_X)\")\nprint(\"=\" * 70)\n\nr = corr_matrix.loc['price', 'size']\ns_y = price.std()\ns_x = size.std()\nb2_from_r = r * (s_y / s_x)\n\nprint(f\"\\nFrom regression:\")\nprint(f\"  Slope (b₂) = {slope:.4f}\")\n\nprint(f\"\\nFrom correlation:\")\nprint(f\"  r = {r:.4f}\")\nprint(f\"  s_y = {s_y:.4f}\")\nprint(f\"  s_x = {s_x:.4f}\")\nprint(f\"  b₂ = r × (s_y / s_x) = {r:.4f} × ({s_y:.4f} / {s_x:.4f}) = {b2_from_r:.4f}\")\n\nprint(f\"\\nMatch: {np.isclose(slope, b2_from_r)}\")\n\n======================================================================\nRELATIONSHIP: SLOPE = CORRELATION × (SD_Y / SD_X)\n======================================================================\n\nFrom regression:\n  Slope (b₂) = 73.7710\n\nFrom correlation:\n  r = 0.7858\n  s_y = 37390.7107\n  s_x = 398.2721\n  b₂ = r × (s_y / s_x) = 0.7858 × (37390.7107 / 398.2721) = 73.7710\n\nMatch: True",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#causation",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#causation",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.10 Causation",
    "text": "5.10 Causation\nCritical distinction: Regression measures association, not causation.\nOur regression shows that larger houses are associated with higher prices. But we cannot conclude that:\n\nAdding square footage to a house will increase its price by $73.77 per sq ft\n\nWhy not?\n\nOmitted variables: Many factors affect price (location, quality, age, condition)\nReverse causality: Could price influence size? (e.g., builders construct larger houses in expensive areas)\nConfounding: A third variable (e.g., neighborhood quality) may influence both size and price\n\nDemonstrating non-symmetry: Reverse regression\nIf we regress x on y (instead of y on x), we get a different slope:\n\nOriginal: \\(\\hat{y} = b_1 + b_2 x\\)\nReverse: \\(\\hat{x} = c_1 + c_2 y\\)\n\nThese two regressions answer different questions and have different slopes!\n\nKey Concept 5.7: Association vs. Causation\nRegression measures association, not causation. A regression coefficient shows how much y changes when x changes, but does not prove that x causes y. Causation requires additional assumptions, experimental design, or advanced econometric techniques (Chapter 17). Regression is directional and asymmetric: regressing y on x gives a different slope than regressing x on y.\n\nWhen to Use Nonparametric vs. Parametric Regression:\nUse parametric (OLS linear regression) when:\n\nTheory suggests a linear relationship\nYou need interpretable coefficients ($73.77 per sq ft)\nSample size is small to moderate (n &lt; 100)\nYou want statistical inference (t-tests, confidence intervals)\n\nUse nonparametric (LOWESS, kernel) when:\n\nExploring data without strong prior assumptions\nChecking whether linear model is appropriate (diagnostic)\nRelationship appears curved or complex\nLarge sample size (n &gt; 100) provides enough data for flexible fitting\n\nBest practice: Start with scatter plot + nonparametric curve to check for nonlinearity, then use parametric model if linear assumption is reasonable.\nTransition Note: We’ve learned how to measure and quantify relationships. Now we address a critical question: does association imply causation? This distinction is fundamental to interpreting regression results correctly.\n\n# Reverse regression: size ~ price\nprint(\"=\" * 70)\nprint(\"REVERSE REGRESSION: DEMONSTRATING NON-SYMMETRY\")\nprint(\"=\" * 70)\n\nreverse_model = ols('size ~ price', data=data_house).fit()\n\nprint(\"\\nOriginal Regression (price ~ size):\")\nprint(f\"  ŷ = {model.params['Intercept']:,.2f} + {model.params['size']:.4f} × size\")\nprint(f\"  Slope: {model.params['size']:.4f}\")\nprint(f\"  R-squared: {model.rsquared:.4f}\")\n\nprint(\"\\nReverse Regression (size ~ price):\")\nprint(f\"  x̂ = {reverse_model.params['Intercept']:.2f} + {reverse_model.params['price']:.6f} × price\")\nprint(f\"  Slope: {reverse_model.params['price']:.6f}\")\nprint(f\"  R-squared: {reverse_model.rsquared:.4f}\")\n\nprint(\"\\nComparison:\")\nprint(f\"  1 / b₂ = 1 / {model.params['size']:.4f} = {1/model.params['size']:.6f}\")\nprint(f\"  c₂ = {reverse_model.params['price']:.6f}\")\nprint(f\"  Are they equal? {np.isclose(1/model.params['size'], reverse_model.params['price'])}\")\n\nprint(\"\\nKey insight:\")\nprint(\"  • Original slope: $1 increase in size → ${:.2f} increase in price\".format(model.params['size']))\nprint(\"  • Reverse slope: $1 increase in price → {:.6f} sq ft increase in size\".format(reverse_model.params['price']))\nprint(\"  • These answer different questions!\")\n\nprint(\"\\nNote: Both regressions have the same R² because in simple regression,\")\nprint(\"      R² = r² regardless of which variable is on the left-hand side.\")\n\n======================================================================\nREVERSE REGRESSION: DEMONSTRATING NON-SYMMETRY\n======================================================================\n\nOriginal Regression (price ~ size):\n  ŷ = 115,017.28 + 73.7710 × size\n  Slope: 73.7710\n  R-squared: 0.6175\n\nReverse Regression (size ~ price):\n  x̂ = -242.44 + 0.008370 × price\n  Slope: 0.008370\n  R-squared: 0.6175\n\nComparison:\n  1 / b₂ = 1 / 73.7710 = 0.013555\n  c₂ = 0.008370\n  Are they equal? False\n\nKey insight:\n  • Original slope: $1 increase in size → $73.77 increase in price\n  • Reverse slope: $1 increase in price → 0.008370 sq ft increase in size\n  • These answer different questions!\n\nNote: Both regressions have the same R² because in simple regression,\n      R² = r² regardless of which variable is on the left-hand side.\n\n\nCRITICAL DISTINCTION: Association ≠ Causation\nWhat our regression shows:\nprice = 115,017 + 73.77 × size\nWhat we CAN say:\n\nLarger houses are associated with higher prices\nSize and price move together in a predictable way\nWe can predict price from size with reasonable accuracy\n\nWhat we CANNOT say:\n\nAdding square footage to your house will increase its value by exactly $73.77 per sq ft\nSize causes the price to be higher\nBuying a bigger house will make it worth more\n\nWhy not? Three reasons:\n1. Omitted variables (confounding)\n\nMany factors affect BOTH size and price:\n\nNeighborhood quality: Rich neighborhoods have larger, more expensive houses\nLot size: Bigger lots allow bigger houses AND command higher prices\nBuild quality: High-quality construction → larger AND more expensive\n\nThe $73.77 coefficient captures both direct effects of size AND correlated factors\n\n2. Reverse causality\n\nOur model: size → price\nAlternative: price → size?\n\nIn expensive areas, builders construct larger houses because buyers can afford them\nThe causal arrow may run both ways\n\n\n3. Measurement of different concepts\n\nCross-sectional comparison: 2,000 sq ft house vs. 1,500 sq ft house (different houses)\nCausal question: What happens if we ADD 500 sq ft to ONE house?\nThese are different questions with potentially different answers!\n\nThe reverse regression demonstration:\nOriginal: price ~ size\n\nSlope: $73.77 per sq ft\n\nReverse: size ~ price\n\nSlope: 0.00837 sq ft per dollar\n\nKey observation:\n\nIf regression = causation, these should be reciprocals\n1 / 73.77 = 0.01355 ≠ 0.00837\nThey’re NOT reciprocals! This reveals regression measures association, not causation\n\nWhen can we claim causation?\n\nRandomized experiments: Randomly assign house sizes\nNatural experiments: Find exogenous variation in size\nCareful econometric methods: Instrumental variables, difference-in-differences, etc. (advanced topics)\n\nEconomic intuition: In reality, building an addition probably DOES increase house value, but perhaps not by exactly $73.77/sq ft. The true causal effect depends on quality, location, and market conditions — factors our simple regression doesn’t isolate.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#nonparametric-regression",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#nonparametric-regression",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "5.11 Nonparametric Regression",
    "text": "5.11 Nonparametric Regression\nParametric regression (like OLS) assumes a specific functional form (e.g., linear).\nNonparametric regression allows the relationship to be more flexible, letting the data determine the shape without imposing a specific functional form.\nCommon methods:\n\nLOWESS (Locally Weighted Scatterplot Smoothing): Fits weighted regressions in local neighborhoods\nKernel smoothing: Weighted averages using kernel functions\nSplines: Piecewise polynomials\n\nUses:\n\nExploratory data analysis\nChecking linearity assumption\nFlexible modeling when functional form is unknown\n\n\n# Nonparametric regression\nprint(\"=\" * 70)\nprint(\"NONPARAMETRIC REGRESSION\")\nprint(\"=\" * 70)\n\n# LOWESS smoothing\nlowess_result = lowess(price, size, frac=0.6)\n\n# Kernel smoothing (Gaussian filter approximation)\nsort_idx = np.argsort(size)\nsize_sorted = size.iloc[sort_idx]\nprice_sorted = price.iloc[sort_idx]\nsigma = 2  # bandwidth parameter\nprice_smooth = gaussian_filter1d(price_sorted, sigma)\n\n# Plot comparison\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Scatter plot\nax.scatter(size, price, s=80, alpha=0.6, color='black',\n           edgecolor='black', label='Actual data', zorder=1)\n\n# OLS line\nax.plot(size, model.fittedvalues, color='blue', linewidth=2.5,\n        label='OLS (parametric)', zorder=2)\n\n# LOWESS\nax.plot(lowess_result[:, 0], lowess_result[:, 1], color='red',\n        linewidth=2.5, linestyle='--', label='LOWESS', zorder=3)\n\n# Kernel smoothing\nax.plot(size_sorted, price_smooth, color='green', linewidth=2.5,\n        linestyle=':', label='Kernel smoothing', zorder=4)\n\nax.set_xlabel('House size (in square feet)', fontsize=12)\nax.set_ylabel('House sale price (in dollars)', fontsize=12)\nax.set_title('Figure 5.6: Parametric vs Nonparametric Regression',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='lower right')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"• OLS (blue solid): Assumes linear relationship\")\nprint(\"• LOWESS (red dashed): Flexible, data-driven curve\")\nprint(\"• Kernel smoothing (green dotted): Another flexible method\")\nprint(\"\\nFor this data, all three methods are similar, suggesting\")\nprint(\"that the linear model is a reasonable approximation.\")\n\n======================================================================\nNONPARAMETRIC REGRESSION\n======================================================================\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n• OLS (blue solid): Assumes linear relationship\n• LOWESS (red dashed): Flexible, data-driven curve\n• Kernel smoothing (green dotted): Another flexible method\n\nFor this data, all three methods are similar, suggesting\nthat the linear model is a reasonable approximation.\n\n\nComparing three approaches to fitting the data:\n1. OLS (Ordinary Least Squares) — BLUE LINE\n\nParametric: Assumes linear relationship\nEquation: ŷ = $115,017 + $73.77 × size\nAdvantage: Simple, interpretable, efficient\nLimitation: Restricted to straight lines\n\n2. LOWESS (Locally Weighted Scatterplot Smoothing) — RED DASHED\n\nNonparametric: Lets data determine the shape\nMethod: Fits weighted regressions in local neighborhoods\nAdvantage: Flexible, can capture curves\nLimitation: Harder to interpret, more complex\n\n3. Kernel Smoothing — GREEN DOTTED\n\nNonparametric: Weighted moving averages\nMethod: Uses Gaussian kernel to smooth nearby points\nAdvantage: Very smooth curves\nLimitation: Choice of bandwidth affects results\n\nWhat does this comparison tell us?\nKey observation: All three lines are very similar!\n\nLOWESS and kernel smoothing follow OLS closely\nNo obvious systematic curvature\nThe relationship appears genuinely linear\n\nThis validates our linear model:\n\nIf nonparametric methods showed strong curvature, we’d question the linear assumption\nSince they align with OLS, the linear model is appropriate\nWe can confidently use the simpler parametric approach\n\nWhen would nonparametric methods differ?\nExample scenarios:\n\nDiminishing returns: Price increases with size, but at a decreasing rate\nThreshold effects: Small houses have steep price-size relationship, large houses flatten\nNonlinear relationships: Exponential, logarithmic, or polynomial patterns\n\nFor our housing data:\n\nLinear model works well\nAdding complexity (nonparametric) doesn’t improve fit much\nOccam’s Razor: Choose the simpler model when performance is similar\n\nPractical use of nonparametric methods:\n\nExploratory analysis: Check for nonlinearity before modeling\nModel diagnostics: Verify linear assumption\nFlexible prediction: When functional form is unknown\nComplex relationships: When theory doesn’t suggest specific form\n\nBottom line: Nonparametric methods confirm that our linear regression is appropriate for this dataset. The relationship between house price and size is genuinely linear, not curved.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#key-takeaways",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#key-takeaways",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nVisualization and Correlation\n\nTwo-way tabulations, scatterplots, and correlation are essential first steps in bivariate analysis\nScatterplots provide visual evidence of relationships and help identify direction, strength, form, and outliers\nTwo-way tabulations with expected frequencies enable chi-squared tests of independence for categorical data\nThe correlation coefficient (r) is a scale-free measure of linear association ranging from -1 to +1\nCovariance measures the direction of association but depends on the units of measurement\nFor house price and size, r = 0.786 indicates strong positive linear association\nAutocorrelation extends correlation to time series, measuring how a variable relates to its own past values\n\n\n\nRegression Analysis and Interpretation\n\nThe regression line ŷ = b₁ + b₂x is estimated by ordinary least squares (OLS), which minimizes the sum of squared residuals\nThe slope b₂ measures the change in y for a one-unit change in x and is the most important interpretable quantity\nFor house prices, b₂ = $73.77 means each additional square foot is associated with a $73.77 price increase\nThe intercept b₁ represents the predicted y when x = 0 (often not meaningful if x = 0 is outside the data range)\nResiduals (e = y - ŷ) measure prediction errors; OLS makes the sum of squared residuals as small as possible\nRegression of y on only an intercept yields the sample mean as the fitted value, showing OLS generalizes univariate statistics\nThe formulas b₂ = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)² and b₁ = ȳ - b₂x̄ enable manual computation\nThe regression slope equals b₂ = rₓᵧ × (sᵧ/sₓ), connecting regression and correlation\n\n\n\nModel Fit and Evaluation\n\nR-squared measures the fraction of variation in y explained by x, ranging from 0 (no fit) to 1 (perfect fit)\nR² = (Explained SS) / (Total SS) = 1 - (Residual SS) / (Total SS)\nFor bivariate regression, R² = r²ₓᵧ (squared correlation coefficient)\nFor house prices, R² = 0.618 means 62% of price variation is explained by size variation\nStandard error of regression (sₑ) measures the typical size of residuals in the units of y\nLow R² doesn’t mean regression is uninformative—the coefficient can still be statistically significant and economically important\nR² depends on data aggregation and choice of dependent variable; use it to compare models with the same dependent variable\nComputer regression output provides coefficients, standard errors, t-statistics, p-values, F-statistics, and ANOVA decomposition\n\n\n\nPrediction, Causation, and Extensions\n\nPredictions use ŷ = b₁ + b₂x* to forecast y for a given x*\nIn-sample predictions use observed x values (fitted values); out-of-sample predictions use new x values\nExtrapolation beyond the sample range of x can be unreliable\nOutliers can strongly influence regression estimates, especially if far from both x̄ and ȳ\nAssociation does not imply causation—regression measures correlation, not causal effects\nConfounding variables, reverse causality, or selection bias can create associations without causation\nEstablishing causation requires experimental design, natural experiments, or advanced econometric techniques (Chapter 17)\nRegression is directional and asymmetric: regressing y on x gives a different slope than regressing x on y\nThe two slopes are NOT reciprocals, reflecting that regression treats y and x differently\nNonparametric regression (local linear, lowess) provides flexible alternatives without assuming linearity\nNonparametric methods are useful for exploratory analysis and checking the appropriateness of linear models\n\n\n\nConnection to Economic Analysis\n\nThe strong relationship (R² = 0.62) between size and price makes economic sense: buyers pay a premium for space\nThe imperfect fit reminds us that many factors beyond size affect house values (location, quality, age, condition)\nRegression provides the foundation for econometric analysis, allowing us to quantify economic relationships\nThis chapter’s bivariate methods extend naturally to multiple regression (Chapters 10-12) with many explanatory variables\nUnderstanding association vs. causation is critical for policy analysis and program evaluation\n\nCongratulations! You’ve mastered the basics of bivariate data analysis and simple linear regression. You now understand how to measure and visualize relationships between two variables, fit and interpret a regression line, assess model fit, and recognize the crucial distinction between association and causation. These tools form the foundation for all econometric analysis!",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#practice-exercises",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#practice-exercises",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of bivariate data analysis and regression with these exercises.\nExercise 1: Correlation Interpretation\nSuppose the correlation between years of education and annual income is r = 0.35.\n\nWhat does this correlation tell us about the relationship between education and income?\nIf we measured income in thousands of dollars instead of dollars, would the correlation change?\nCan we conclude that education causes higher income? Why or why not?\n\n\nExercise 2: Computing Correlation\nGiven the following data for variables x and y with n = 5 observations:\n\nΣ(xᵢ - x̄)(yᵢ - ȳ) = 20\nΣ(xᵢ - x̄)² = 50\nΣ(yᵢ - ȳ)² = 10\n\n\nCalculate the sample correlation coefficient r.\nIs this a strong, moderate, or weak correlation?\nIs the relationship positive or negative?\n\n\nExercise 3: Regression Slope Calculation\nFor the data in Exercise 2:\n\nCalculate the regression slope coefficient b₂ from regression of y on x.\nVerify the relationship: b₂ = r × (sᵧ / sₓ)\nIf x̄ = 10 and ȳ = 25, calculate the intercept b₁.\n\n\nExercise 4: R-squared Interpretation\nA regression of test scores on hours studied yields R² = 0.40.\n\nWhat percentage of test score variation is explained by hours studied?\nWhat percentage is due to other factors?\nDoes this mean studying is not important? Explain.\nIf the correlation is r = 0.632, verify that R² = r².\n\n\nExercise 5: Prediction\nFrom the house price regression: ŷ = 115,017 + 73.77 × size\n\nPredict the price of a house with 2,200 square feet.\nPredict the price of a house with 5,000 square feet.\nWhich prediction is more reliable? Why?\nIf the standard error is $23,551, what does this tell us about prediction accuracy?\n\n\nExercise 6: Residuals\nA house of 1,800 sq ft sold for $270,000. The regression predicts ŷ = $247,805.\n\nCalculate the residual for this observation.\nIs the actual price higher or lower than predicted?\nWhat might explain this large positive residual?\nWould you consider this an outlier? Why or why not?\n\n\nExercise 7: Causation vs. Association\nStudies show that ice cream sales and crime rates are positively correlated.\n\nDoes this mean ice cream causes crime? Explain.\nWhat might be a confounding variable?\nHow would you design a study to test for causation?\nGive another example where correlation does not imply causation.\n\n\nExercise 8: Python Practice\nUsing the house price data or your own dataset:\n\nCreate a scatterplot with a fitted regression line.\nCalculate the correlation coefficient using pandas .corr() method.\nFit an OLS regression using statsmodels and interpret the output.\nCreate residual plots to check for outliers.\nCompare OLS with LOWESS nonparametric regression.\n\n\nSolutions to selected exercises:\n\nExercise 2a: r = 20 / √(50 × 10) = 20 / √500 = 0.894\nExercise 3a: b₂ = 20 / 50 = 0.4\nExercise 4a: 40% explained, 60% due to other factors\nExercise 5a: ŷ = 115,017 + 73.77 × 2,200 = $277,311\n\nFor complete solutions and additional practice problems, see the course website.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#case-studies",
    "href": "../notebooks_colab/ch05_Bivariate_Data_Summary.html#case-studies",
    "title": "Chapter 5: Bivariate Data Summary",
    "section": "Case Studies",
    "text": "Case Studies\nNow that you’ve learned the fundamentals of bivariate analysis, correlation, and regression, let’s apply these techniques to real economic research questions using the Economic Convergence Clubs dataset from Mendez (2020).\nWhy case studies matter:\n\nBridge theory and application: Move from learning formulas to answering substantive economic questions\nBuild analytical workflow: Practice the complete cycle from visualization to interpretation\nDevelop critical thinking: Distinguish association from causation in real data\nConnect to research: See how econometric tools support published economic studies\n\n\nCase Study 1: Capital and Productivity Across Countries\nResearch Question: What is the relationship between capital per worker and labor productivity across countries? Does this relationship vary by income group?\nBackground: Traditional growth theory suggests that countries with more capital per worker should have higher labor productivity. This is the capital-output relationship—a fundamental concept in development economics. However, differences in technology, institutions, and human capital mean that capital alone doesn’t fully explain productivity differences.\nThis research (Mendez, 2020) uses panel data from 108 countries to study convergence clubs in labor productivity. Rather than assuming all countries converge to a single equilibrium, the analysis identifies distinct groups (clubs) that converge toward different productivity levels.\nThe Data: We’ll use the same dataset from Chapter 1, but now apply Chapter 5’s bivariate tools:\n\nPanel dataset: 108 countries observed from 1990-2014 (2,700 country-year observations)\nKey variables:\nlp: Labor productivity (output per worker, in 2011 USD PPP)\nkl: Capital per worker (physical capital stock per worker)\nh: Human capital index (based on years of schooling)\nTFP: Total factor productivity (aggregate efficiency)\nhi1990: High-income country indicator (as of 1990)\nregion: Regional classification\n\nYour Task: Use Chapter 5’s visualization, correlation, and regression tools to explore the capital-productivity relationship and test whether it differs across country groups.\n\nKey Concept 5.8: Capital-Productivity Relationship\nThe regression of labor productivity on capital per worker quantifies the capital-output elasticity—how much productivity increases when capital per worker rises by 1%. In cross-country data, this relationship reflects both:\n\nDiminishing returns to capital (holding technology constant)\nTechnology differences across countries (correlated with capital accumulation)\n\nDistinguishing these two effects requires controlling for other factors (human capital, TFP), which we’ll learn in later chapters on multiple regression.\n\n\n\nHow to Use These Tasks\nProgressive difficulty:\n\nTasks 1-2: Guided (detailed instructions, code provided)\nTasks 3-4: Semi-guided (moderate guidance, you write most code)\nTasks 5-6: Independent (minimal guidance, design your own analysis)\n\nWork incrementally: Complete tasks in order. Each builds on previous skills.\nLearn by doing: Modify code, experiment with variables, interpret results economically.\n\nTask 1: Load and Explore the Data (Guided)\nObjective: Load the convergence clubs dataset and generate descriptive statistics for key variables.\nInstructions: Run the code below to load data and examine the structure.\n# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\n\n# Load convergence clubs dataset\ndf = pd.read_csv(\n \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\",\n index_col=[\"country\", \"year\"]\n).sort_index()\n\n# Display dataset info\nprint(f\"Dataset shape: {df.shape[0]} observations, {df.shape[1]} variables\")\nprint(f\"Countries: {len(df.index.get_level_values('country').unique())}\")\nprint(f\"Years: {df.index.get_level_values('year').min()} to {df.index.get_level_values('year').max()}\")\n\n# Preview first few observations\nprint(\"\\nFirst 5 observations:\")\nprint(df[['lp', 'kl', 'h', 'TFP', 'hi1990', 'region']].head())\n\n# Generate descriptive statistics\nprint(\"\\nDescriptive statistics (key variables):\")\nprint(df[['lp', 'kl', 'h', 'TFP']].describe().round(2))\nWhat to observe:\n\nHow many observations? How many countries?\nWhat is the range of labor productivity (lp)? Capital per worker (kl)?\nAre there missing values in key variables?\n\n\n\nTask 2: Visualize the Capital-Productivity Relationship (Semi-guided)\nObjective: Create a scatterplot to visualize the relationship between capital per worker and labor productivity.\nInstructions:\n\nPrepare a subset of data with non-missing values for lp and kl\nCreate a scatterplot with capital per worker on the x-axis and labor productivity on the y-axis\nAdd appropriate labels and title\nInterpret the pattern: positive/negative? Linear? Outliers?\n\nStarter code:\n# Prepare data (remove missing values)\nplot_data = df[['lp', 'kl']].dropna()\n\n# Create scatterplot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(plot_data['kl'], plot_data['lp'], alpha=0.5, s=20)\n\n# Add labels and formatting\nax.set_xlabel('Capital per Worker (thousands, 2011 USD PPP)', fontsize=12)\nax.set_ylabel('Labor Productivity (thousands, 2011 USD PPP)', fontsize=12)\nax.set_title('Capital per Worker vs. Labor Productivity (108 countries, 1990-2014)', fontsize=14)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# What pattern do you see?\nprint(\"\\n Interpretation:\")\nprint(\"- Relationship: [Positive/Negative/No clear pattern]\")\nprint(\"- Strength: [Strong/Moderate/Weak]\")\nprint(\"- Outliers: [Yes/No] - [Describe if present]\")\nQuestions:\n\nIs the relationship positive (as theory predicts)?\nIs it approximately linear, or curved?\nAre there countries with very high capital but moderate productivity (or vice versa)?\n\n\n\nTask 3: Calculate the Correlation Coefficient (Semi-guided)\nObjective: Quantify the strength of the linear relationship between capital and productivity using the correlation coefficient.\nInstructions:\n\nCalculate the Pearson correlation coefficient between lp and kl\nInterpret the magnitude: Is it close to 0 (weak), 0.5 (moderate), or 1.0 (strong)?\nTest statistical significance: Is the relationship likely due to chance?\n\nHint: Use .corr() method or np.corrcoef() function.\nExample structure:\n# Calculate correlation\ncorr = plot_data[['lp', 'kl']].corr()\nprint(\"Correlation matrix:\")\nprint(corr)\n\n# Extract the correlation coefficient\nr = corr.loc['lp', 'kl']\nprint(f\"\\nCorrelation between capital and productivity: r = {r:.4f}\")\n\n# Interpretation\nprint(f\"\\nInterpretation:\")\nprint(f\"- Magnitude: {'Strong' if abs(r) &gt; 0.7 else 'Moderate' if abs(r) &gt; 0.4 else 'Weak'}\")\nprint(f\"- Direction: {'Positive' if r &gt; 0 else 'Negative'}\")\nprint(f\"- R² (shared variation): {r**2:.4f} ({r**2*100:.2f}%)\")\nQuestions:\n\nHow much of the variation in productivity is explained by capital?\nDoes this match what you observed in the scatterplot?\n\n\n\nTask 4: Estimate the Regression Line (More Independent)\nObjective: Estimate an OLS regression of labor productivity on capital per worker and interpret the slope coefficient.\nInstructions:\n\nPrepare regression data (remove missing values, reset index if needed)\nEstimate: lp ~ kl using ols() from statsmodels\nDisplay the regression summary\nExtract and interpret the slope coefficient economically\nReport the R-squared value\n\nKey questions to answer:\n\nWhat is the slope coefficient? Interpret it in economic terms.\nIs the coefficient statistically significant (p &lt; 0.05)?\nWhat percentage of productivity variation is explained by capital?\nDoes the relationship appear causal, or could there be omitted variables?\n\nHint: Remember to reset the index before regression if you’re using formula syntax.\n\nKey Concept 5.9: Interpreting Slope in Economic Context\nThe regression slope β₁ in productivity = β₀ + β₁ × capital measures the average change in productivity (in thousands of USD) for each additional thousand USD of capital per worker.\nIn cross-country data, this captures both: - True capital effect (more machines → higher output) - Confounding factors (richer countries have both more capital AND better technology/institutions)\nTo isolate the true capital effect, we need multiple regression (Chapters 6-9) to control for human capital, TFP, and other factors.\n\n\n\nTask 5: Compare Relationships Across Income Groups (Independent)\nObjective: Investigate whether the capital-productivity relationship differs between high-income and developing countries.\nResearch Question: Do high-income countries have a stronger/weaker capital-productivity association than developing countries?\nInstructions:\n\nGroup the data by hi1990 (high-income indicator)\nCalculate the correlation coefficient for each group separately\nCreate comparative scatterplots (one color per group)\n(Advanced) Run separate regressions for each group and compare slope coefficients\nInterpret differences: Why might the relationship vary by income level?\n\nHints:\n\nUse df.groupby('hi1990') to split data\nUse different colors in scatter plot for each group\nCompare both correlations and regression slopes\n\nExpected findings:\n\nHigh-income countries may show weaker capital-productivity correlation (approaching diminishing returns)\nDeveloping countries may show stronger correlation (still accumulating capital)\nConsider alternative explanations!\n\n\n\nTask 6: Explore Alternative Relationships (Independent)\nObjective: Investigate other bivariate relationships relevant to growth and convergence.\nChoose ONE of the following research questions:\nOption A: Human capital vs. productivity\n\nResearch question: Does education (human capital) explain productivity differences?\nVariables: lp (productivity) and h (human capital index)\nExpected: Positive relationship, but possibly weaker than capital\n\nOption B: TFP vs. productivity\n\nResearch question: How much of productivity is driven by aggregate efficiency (TFP)?\nVariables: lp (productivity) and TFP (total factor productivity)\nExpected: Strong positive relationship (TFP is a key driver)\n\nOption C: Time trend in productivity\n\nResearch question: Has average global productivity increased over time?\nVariables: Year vs. average lp across all countries\nExpected: Positive trend, but with variation across countries\n\nYour analysis should include:\n\nScatterplot with clear labels\nCorrelation coefficient\nRegression results (slope, R², significance)\nEconomic interpretation: What does this relationship tell us about growth and development?\n\n\nKey Concept 5.10: Correlation vs. Causation in Growth Economics\nOur regressions show associations—capital and productivity move together. But association ≠ causation:\n\nReverse causality: Does capital cause productivity, or does high productivity enable more capital accumulation?\nOmitted variables: Technology, institutions, geography, culture—all affect both capital and productivity\nSelection effects: High-income countries differ systematically from developing countries in ways beyond capital\n\nEstablishing causation requires: - Controlled experiments (rarely feasible for countries) - Natural experiments (e.g., policy changes, resource discoveries) - Instrumental variables (advanced econometric methods, Chapter 14-15)\nFor now, interpret regression slopes as descriptive associations, not causal effects.\n\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this hands-on exploration of capital and productivity across countries, you’ve applied all Chapter 5 tools:\nVisualization: Scatterplots reveal patterns before quantifying relationships\nCorrelation: Pearson coefficient quantifies linear association strength\nRegression: OLS slope measures average change in Y per unit change in X\nInterpretation: Translated coefficients into economic meaning (dollars, percentages)\nModel fit: R-squared shows explanatory power (proportion of variation explained)\nComparative analysis: Group comparisons reveal heterogeneity (relationships vary by income level)\nCritical thinking: Distinguished association from causation; recognized omitted variable bias\nConnection to the Research: The patterns you’ve discovered—the capital-productivity relationship, differences across income groups, the role of TFP—are the empirical foundations for Mendez (2020)’s convergence clubs analysis. The full research uses advanced methods (nonparametric regression, clustering algorithms) to formally identify clubs, which you’ll learn in later chapters.\nLooking ahead:\n\nChapter 6-7: Multiple regression to control for human capital, TFP, and other factors\nChapter 10-11: Panel data methods to exploit time variation within countries\nChapter 15-17: Advanced methods for causal inference and club detection\n\nCongratulations! You’ve completed Chapter 5 and applied bivariate analysis to real cross-country growth data. Continue to Chapter 6 to learn how to extend regression analysis to multiple explanatory variables.\n\n\nCase Study 2: Nighttime Lights and Development: A Bivariate Exploration\nIn Chapter 1, we introduced the DS4Bolivia project and estimated a simple regression of development on nighttime lights. In this case study, we apply Chapter 5’s bivariate tools—scatter plots, correlations, OLS regression, and model fit measures—to explore multiple satellite-development relationships in greater depth.\nData: Cross-sectional dataset covering 339 Bolivian municipalities from the DS4Bolivia Project.\nKey variables:\n\nmun: Municipality name\ndep: Department (administrative region)\nimds: Municipal Sustainable Development Index (0–100 composite)\nln_NTLpc2017: Log nighttime lights per capita (2017)\nindex_sdg1: SDG 1 Index — No Poverty (0–100)\nindex_sdg4: SDG 4 Index — Quality Education (0–100)\nindex_sdg8: SDG 8 Index — Decent Work and Economic Growth (0–100)\nsdg7_1_ec: SDG 7.1 — Electricity coverage (%)\nsdg1_1_ubn: Unsatisfied Basic Needs (% of population)\n\n\nTask 1: Load and Explore (Guided)\nObjective: Load the DS4Bolivia dataset, select key variables, and create a two-way frequency table.\nInstructions:\n\nLoad the data from the URL below and select the key variables listed above\nUse pd.cut() to bin the imds variable into three equal-frequency groups labeled Low, Medium, and High (terciles)\nCreate a two-way frequency table (cross-tabulation) of imds terciles against department using pd.crosstab()\nExamine the table: Which departments have the most municipalities in the Low development category?\n\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017',\n            'index_sdg1', 'index_sdg4', 'index_sdg8',\n            'sdg7_1_ec', 'sdg1_1_ubn']\nbol_cs = bol[key_vars].copy()\n\nprint(f\"Dataset: {bol_cs.shape[0]} municipalities, {bol_cs.shape[1]} variables\")\nprint(f\"Departments: {sorted(bol_cs['dep'].unique())}\")\nprint()\n\n# Bin imds into terciles\nbol_cs['imds_group'] = pd.cut(bol_cs['imds'],\n                               bins=3,\n                               labels=['Low', 'Medium', 'High'])\n\n# Two-way frequency table: imds tercile x department\ncross_tab = pd.crosstab(bol_cs['imds_group'], bol_cs['dep'],\n                        margins=True, margins_name='Total')\nprint(\"Two-Way Frequency Table: IMDS Tercile by Department\")\nprint(\"=\" * 70)\nprint(cross_tab)\n\nDataset: 339 municipalities, 9 variables\nDepartments: ['Beni', 'Chuquisaca', 'Cochabamba', 'La Paz', 'Oruro', 'Pando', 'Potosí', 'Santa Cruz', 'Tarija']\n\nTwo-Way Frequency Table: IMDS Tercile by Department\n======================================================================\ndep         Beni  Chuquisaca  Cochabamba  La Paz  Oruro  Pando  Potosí  \\\nimds_group                                                               \nLow            9          21          20      45     21     13      28   \nMedium        10           7          22      40     13      2      11   \nHigh           0           1           5       2      1      0       1   \nTotal         19          29          47      87     35     15      40   \n\ndep         Santa Cruz  Tarija  Total  \nimds_group                             \nLow                 13       0    170  \nMedium              43       9    157  \nHigh                 0       2     12  \nTotal               56      11    339  \n\n\n\n\nTask 2: Scatter Plots (Guided)\nObjective: Create a 2×2 grid of scatter plots to compare different satellite-development relationships.\nInstructions:\n\nCreate a figure with four subplots arranged in a 2×2 grid\nPlot the following relationships:\n\n\nln_NTLpc2017 vs imds\n\n\nln_NTLpc2017 vs index_sdg1\n\n\nln_NTLpc2017 vs sdg7_1_ec\n\n\nsdg1_1_ubn vs imds\n\n\nAdd axis labels and subplot titles\nDiscuss: Which relationship appears strongest? Which appears weakest?\n\n\n# 2x2 scatter plot grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nplot_data = bol_cs.dropna(subset=['ln_NTLpc2017', 'imds', 'index_sdg1',\n                                   'sdg7_1_ec', 'sdg1_1_ubn'])\n\n# (a) NTL vs IMDS\naxes[0, 0].scatter(plot_data['ln_NTLpc2017'], plot_data['imds'],\n                   alpha=0.5, color='#008CB7', s=20)\naxes[0, 0].set_xlabel('Log NTL per Capita (2017)')\naxes[0, 0].set_ylabel('IMDS')\naxes[0, 0].set_title('(a) Nighttime Lights vs Development Index')\n\n# (b) NTL vs SDG1 (No Poverty)\naxes[0, 1].scatter(plot_data['ln_NTLpc2017'], plot_data['index_sdg1'],\n                   alpha=0.5, color='#7A209F', s=20)\naxes[0, 1].set_xlabel('Log NTL per Capita (2017)')\naxes[0, 1].set_ylabel('SDG 1 Index (No Poverty)')\naxes[0, 1].set_title('(b) Nighttime Lights vs No Poverty')\n\n# (c) NTL vs SDG7.1 (Electricity Coverage)\naxes[1, 0].scatter(plot_data['ln_NTLpc2017'], plot_data['sdg7_1_ec'],\n                   alpha=0.5, color='#C21E72', s=20)\naxes[1, 0].set_xlabel('Log NTL per Capita (2017)')\naxes[1, 0].set_ylabel('Electricity Coverage (%)')\naxes[1, 0].set_title('(c) Nighttime Lights vs Electricity Coverage')\n\n# (d) UBN vs IMDS\naxes[1, 1].scatter(plot_data['sdg1_1_ubn'], plot_data['imds'],\n                   alpha=0.5, color='#2E86AB', s=20)\naxes[1, 1].set_xlabel('Unsatisfied Basic Needs (%)')\naxes[1, 1].set_ylabel('IMDS')\naxes[1, 1].set_title('(d) Unsatisfied Basic Needs vs Development Index')\n\nplt.suptitle('Bivariate Relationships: Satellite Data and Development in Bolivia',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey Concept 5.11: Nighttime Lights as Development Proxy\nNighttime light (NTL) intensity measured by satellites correlates with GDP, electrification, and urbanization across the world. The log per-capita transformation serves two purposes: the logarithm compresses the highly skewed raw luminosity values, and dividing by population accounts for the mechanical relationship between more people and more light. After these transformations, NTL becomes a meaningful proxy for economic intensity rather than simply population density.\n\n\n\nTask 3: Correlation Analysis (Semi-guided)\nObjective: Calculate and visualize the correlation matrix for key development and satellite variables.\nInstructions:\n\nSelect the variables: imds, ln_NTLpc2017, index_sdg1, index_sdg4, index_sdg8, sdg7_1_ec, sdg1_1_ubn\nCalculate the Pearson correlation matrix using .corr()\nDisplay the results as a heatmap or formatted table\nIdentify: Which variable has the strongest correlation with imds? Which has the weakest?\n\nHint: Use plt.imshow() or sns.heatmap() (if seaborn is available) to visualize the correlation matrix.\n\n# Your code here: Correlation analysis\n#\n# Steps:\n# 1. Select numeric variables of interest\n# 2. Compute correlation matrix\n# 3. Display as heatmap or formatted table\n# 4. Identify strongest/weakest correlations with imds\n\n# Example structure:\n# corr_vars = ['imds', 'ln_NTLpc2017', 'index_sdg1', 'index_sdg4',\n#              'index_sdg8', 'sdg7_1_ec', 'sdg1_1_ubn']\n# corr_matrix = bol_cs[corr_vars].corr()\n# print(corr_matrix.round(3))\n#\n# # Heatmap\n# fig, ax = plt.subplots(figsize=(9, 7))\n# im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n# ax.set_xticks(range(len(corr_vars)))\n# ax.set_yticks(range(len(corr_vars)))\n# ax.set_xticklabels(corr_vars, rotation=45, ha='right')\n# ax.set_yticklabels(corr_vars)\n# for i in range(len(corr_vars)):\n#     for j in range(len(corr_vars)):\n#         ax.text(j, i, f\"{corr_matrix.iloc[i, j]:.2f}\",\n#                 ha='center', va='center', fontsize=8)\n# plt.colorbar(im, ax=ax, label='Pearson Correlation')\n# ax.set_title('Correlation Matrix: Development and Satellite Variables')\n# plt.tight_layout()\n# plt.show()\n#\n# # Identify strongest/weakest correlations with imds\n# imds_corrs = corr_matrix['imds'].drop('imds').abs().sort_values(ascending=False)\n# print(f\"\\nStrongest correlation with IMDS: {imds_corrs.index[0]} (r = {corr_matrix.loc['imds', imds_corrs.index[0]]:.3f})\")\n# print(f\"Weakest correlation with IMDS: {imds_corrs.index[-1]} (r = {corr_matrix.loc['imds', imds_corrs.index[-1]]:.3f})\")\n\n\n\nTask 4: OLS Regression Line (Semi-guided)\nObjective: Estimate and compare OLS regressions predicting IMDS from different variables.\nInstructions:\n\nEstimate OLS: imds ~ ln_NTLpc2017. Report the slope, intercept, and R²\nOverlay the fitted regression line on a scatter plot of ln_NTLpc2017 vs imds\nEstimate a second OLS: imds ~ sdg1_1_ubn. Report slope, intercept, and R²\nCompare: Which predictor explains more variation in development?\n\nHint: Use ols() from statsmodels as practiced in this chapter.\n\n# Your code here: OLS regression with fitted lines\n#\n# Steps:\n# 1. Estimate imds ~ ln_NTLpc2017\n# 2. Overlay fitted line on scatter plot\n# 3. Estimate imds ~ sdg1_1_ubn\n# 4. Compare R-squared values\n\n# Example structure:\n# reg_data = bol_cs[['imds', 'ln_NTLpc2017', 'sdg1_1_ubn']].dropna()\n#\n# # Model 1: NTL\n# model_ntl = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()\n# print(\"Model 1: IMDS ~ Log NTL per Capita\")\n# print(f\"  Slope:     {model_ntl.params['ln_NTLpc2017']:.4f}\")\n# print(f\"  Intercept: {model_ntl.params['Intercept']:.4f}\")\n# print(f\"  R-squared: {model_ntl.rsquared:.4f}\")\n#\n# # Model 2: UBN\n# model_ubn = ols('imds ~ sdg1_1_ubn', data=reg_data).fit()\n# print(\"\\nModel 2: IMDS ~ Unsatisfied Basic Needs\")\n# print(f\"  Slope:     {model_ubn.params['sdg1_1_ubn']:.4f}\")\n# print(f\"  Intercept: {model_ubn.params['Intercept']:.4f}\")\n# print(f\"  R-squared: {model_ubn.rsquared:.4f}\")\n#\n# # Scatter + fitted line for Model 1\n# fig, ax = plt.subplots(figsize=(8, 6))\n# ax.scatter(reg_data['ln_NTLpc2017'], reg_data['imds'], alpha=0.4, color='#008CB7', s=20)\n# x_range = np.linspace(reg_data['ln_NTLpc2017'].min(), reg_data['ln_NTLpc2017'].max(), 100)\n# ax.plot(x_range, model_ntl.params['Intercept'] + model_ntl.params['ln_NTLpc2017'] * x_range,\n#         color='#C21E72', linewidth=2, label=f'OLS: R² = {model_ntl.rsquared:.3f}')\n# ax.set_xlabel('Log NTL per Capita (2017)')\n# ax.set_ylabel('IMDS')\n# ax.set_title('OLS Regression: Nighttime Lights Predicting Development')\n# ax.legend()\n# plt.tight_layout()\n# plt.show()\n\n\nKey Concept 5.12: Prediction vs. Causation with Satellite Data\nA high R² between nighttime lights and development indices does not mean that lights cause development. Both NTL and IMDS reflect underlying economic activity, infrastructure, and urbanization. NTL is best understood as a proxy variable—an observable measure that correlates with the unobserved concept we care about (true economic development). The correlation is useful for prediction but should not be interpreted as a causal relationship.\n\n\n\nTask 5: Departmental Comparisons (Independent)\nObjective: Assess whether the NTL-development relationship differs across Bolivia’s departments.\nInstructions:\n\nCreate a scatter plot of ln_NTLpc2017 vs imds with points colored by department\nVisually assess: Does the relationship appear to differ across departments?\nRun separate OLS regressions for 2–3 departments and compare slopes and R² values\nDiscuss: What might explain differences in the satellite-development relationship across regions?\n\n\n# Your code here: Departmental comparisons\n#\n# Steps:\n# 1. Create scatter plot colored by department\n# 2. Run separate regressions for 2-3 departments\n# 3. Compare slopes and R-squared\n\n# Example structure:\n# plot_data = bol_cs[['ln_NTLpc2017', 'imds', 'dep']].dropna()\n# departments = plot_data['dep'].unique()\n# colors = plt.cm.tab10(np.linspace(0, 1, len(departments)))\n#\n# fig, ax = plt.subplots(figsize=(10, 7))\n# for dep, color in zip(sorted(departments), colors):\n#     subset = plot_data[plot_data['dep'] == dep]\n#     ax.scatter(subset['ln_NTLpc2017'], subset['imds'],\n#                alpha=0.6, color=color, label=dep, s=25)\n# ax.set_xlabel('Log NTL per Capita (2017)')\n# ax.set_ylabel('IMDS')\n# ax.set_title('NTL vs Development by Department')\n# ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n# plt.tight_layout()\n# plt.show()\n#\n# # Separate regressions for selected departments\n# for dep_name in ['La Paz', 'Santa Cruz', 'Potosí']:\n#     dep_data = plot_data[plot_data['dep'] == dep_name]\n#     if len(dep_data) &gt; 5:\n#         model_dep = ols('imds ~ ln_NTLpc2017', data=dep_data).fit()\n#         print(f\"{dep_name}: slope = {model_dep.params['ln_NTLpc2017']:.3f}, \"\n#               f\"R² = {model_dep.rsquared:.3f}, n = {len(dep_data)}\")\n\n\n\nTask 6: Alternative Predictors (Independent)\nObjective: Compare nighttime lights with unsatisfied basic needs (UBN) as predictors of development.\nInstructions:\n\nCompare the R² values from imds ~ ln_NTLpc2017 and imds ~ sdg1_1_ubn\nCreate side-by-side scatter plots with fitted regression lines for both models\nDiscuss: Which predictor has the higher R²? Why might a socioeconomic variable (UBN) predict development better than satellite data (NTL)?\nReflect: What are the advantages of satellite data even if its R² is lower?\n\n\n# Your code here: Compare NTL vs UBN as predictors of IMDS\n#\n# Steps:\n# 1. Estimate both models (if not already done in Task 4)\n# 2. Create side-by-side fitted line plots\n# 3. Compare R-squared values\n# 4. Discuss advantages and limitations\n\n# Example structure:\n# reg_data = bol_cs[['imds', 'ln_NTLpc2017', 'sdg1_1_ubn']].dropna()\n# model_ntl = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()\n# model_ubn = ols('imds ~ sdg1_1_ubn', data=reg_data).fit()\n#\n# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n#\n# # Left: NTL model\n# axes[0].scatter(reg_data['ln_NTLpc2017'], reg_data['imds'], alpha=0.4, color='#008CB7', s=20)\n# x1 = np.linspace(reg_data['ln_NTLpc2017'].min(), reg_data['ln_NTLpc2017'].max(), 100)\n# axes[0].plot(x1, model_ntl.predict(pd.DataFrame({'ln_NTLpc2017': x1})),\n#              color='#C21E72', linewidth=2)\n# axes[0].set_xlabel('Log NTL per Capita (2017)')\n# axes[0].set_ylabel('IMDS')\n# axes[0].set_title(f'Model 1: NTL (R² = {model_ntl.rsquared:.3f})')\n#\n# # Right: UBN model\n# axes[1].scatter(reg_data['sdg1_1_ubn'], reg_data['imds'], alpha=0.4, color='#7A209F', s=20)\n# x2 = np.linspace(reg_data['sdg1_1_ubn'].min(), reg_data['sdg1_1_ubn'].max(), 100)\n# axes[1].plot(x2, model_ubn.predict(pd.DataFrame({'sdg1_1_ubn': x2})),\n#              color='#C21E72', linewidth=2)\n# axes[1].set_xlabel('Unsatisfied Basic Needs (%)')\n# axes[1].set_ylabel('IMDS')\n# axes[1].set_title(f'Model 2: UBN (R² = {model_ubn.rsquared:.3f})')\n#\n# plt.suptitle('Comparing Predictors of Municipal Development',\n#              fontsize=13, fontweight='bold')\n# plt.tight_layout()\n# plt.show()\n#\n# print(f\"\\nR² comparison:\")\n# print(f\"  NTL model: {model_ntl.rsquared:.4f}\")\n# print(f\"  UBN model: {model_ubn.rsquared:.4f}\")\n# print(f\"  Difference: {abs(model_ubn.rsquared - model_ntl.rsquared):.4f}\")\n\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this bivariate exploration of satellite data and municipal development in Bolivia, you’ve applied the full Chapter 5 toolkit:\n\nTwo-way tabulations for categorical exploration of development levels across departments\nMultiple scatter plots comparing satellite-development relationships across different indicators\nCorrelation analysis across multiple SDG indicators to identify strongest and weakest associations\nOLS regression with fitted lines and R² to quantify the NTL-development relationship\nRegional heterogeneity in bivariate relationships across Bolivia’s departments\nComparing alternative predictors of development (satellite data vs. socioeconomic measures)\n\nConnection to the research: The DS4Bolivia project extends this bivariate analysis to multivariate machine learning models, using 64-dimensional satellite embeddings alongside nighttime lights to achieve higher predictive accuracy for SDG indicators.\nLooking ahead: In Chapter 7, we’ll apply statistical inference to these regressions—testing whether the NTL coefficient is significantly different from zero and constructing confidence intervals for the effect of satellite data on development.\nWell done! You’ve now explored two real-world datasets—cross-country convergence and Bolivian municipal development—using the complete bivariate analysis toolkit from Chapter 5.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5: Bivariate Data Summary</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to the statistical properties of the OLS estimator. All code runs directly in Google Colab without any local setup.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#chapter-overview",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#chapter-overview",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "",
    "text": "Introduction\nUnderstanding the properties of the Ordinary Least Squares (OLS) estimator is fundamental to econometric inference. While Chapter 5 showed how to estimate regression models, this chapter explains why OLS works and when we can trust its results. We examine the statistical properties that make OLS the standard estimation method in econometrics: unbiasedness, efficiency, and asymptotic normality.\nA crucial concept is the distinction between the population regression (the true relationship we want to learn about) and the sample regression (our estimate from limited data). Different samples yield different estimates—this sampling variability is inevitable but quantifiable. By understanding how OLS estimates vary across samples, we can construct confidence intervals and test hypotheses about economic relationships.\nThis chapter uses Monte Carlo simulations and real-world examples to demonstrate OLS properties empirically, connecting abstract statistical theory to tangible patterns in data.\n\n\nWhat You’ll Learn\nIn this chapter, you will: - Distinguish between the population regression line (β₁ + β₂x) and the sample regression line (b₁ + b₂x) - Understand the conditional mean E[y|x] and the error term u = y - E[y|x] - Differentiate between the unobserved error term (u) and the observed residual (e) - Apply the four key OLS assumptions: correct model, mean-zero errors, homoskedasticity, and independence - Calculate the variance and standard error of the OLS slope coefficient b₂ - Explain why b₂ is an unbiased estimator of β₂ under assumptions 1-2 - Compute the standard error of the regression (sₑ) and use it to estimate precision - Understand when OLS estimates are more precise (good fit, many observations, scattered regressors) - Apply the Central Limit Theorem to show b₂ is approximately normally distributed for large samples - Recognize that OLS is the Best Linear Unbiased Estimator (BLUE) under standard assumptions - Conduct Monte Carlo simulations to demonstrate sampling distributions - Interpret sampling variability and its implications for statistical inference\n\n\nDataset Used\nPrimary dataset: - Convergence Clubs (Mendez 2020): 108 countries, 1990-2014 - Variables: Real GDP per capita (rgdppc), labor productivity, capital per worker (rk) - Used in Case Study: Sampling variability in productivity-capital regressions - Demonstrates OLS properties with real economic data\nSupporting examples: - Generated data: Computer-simulated samples from known DGP (y = 1 + 2x + u) - 1880 U.S. Census: Finite population sampling demonstration\n\n\nChapter Outline\n6.1 Population and Sample Models - Distinguish between population parameters (β₁, β₂) and sample estimates (b₁, b₂); understand error terms vs. residuals\n6.2 Examples of Sampling from a Population - Generated data and census sampling demonstrations showing how estimates vary across samples\n6.3 Properties of the Least Squares Estimator - Unbiasedness (E[b₂] = β₂), variance formulas, asymptotic normality, and BLUE property\n6.4 Estimators of Model Parameters - Calculating standard errors, understanding degrees of freedom, factors affecting precision\n6.5 Case Studies - Empirical investigation of sampling variability using convergence clubs data; Monte Carlo with real economic data\n6.6 Key Takeaways - Comprehensive chapter summary organized by major themes\n6.7 Practice Exercises - Hands-on problems reinforcing OLS properties, standard errors, and interpretation",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#setup",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#setup",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"=\" * 70)\nprint(\"CHAPTER 6: THE LEAST SQUARES ESTIMATOR\")\nprint(\"=\" * 70)\nprint(\"\\nSetup complete! Ready to explore OLS properties.\")\n\n======================================================================\nCHAPTER 6: THE LEAST SQUARES ESTIMATOR\n======================================================================\n\nSetup complete! Ready to explore OLS properties.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#population-and-sample-models",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#population-and-sample-models",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "6.1 Population and Sample Models",
    "text": "6.1 Population and Sample Models\nUnderstanding the relationship between population and sample is crucial for statistical inference.\nPopulation model:\nThe population conditional mean is assumed to be linear:\n\\[E[y | x] = \\beta_1 + \\beta_2 x\\]\nwhere: - \\(\\beta_1\\) and \\(\\beta_2\\) are unknown population parameters - \\(E[y|x]\\) is the expected value of \\(y\\) for a given value of \\(x\\)\nError term:\nIndividual observations deviate from the population line:\n\\[y = \\beta_1 + \\beta_2 x + u\\]\nwhere \\(u\\) is the error term with: - \\(E[u | x] = 0\\) (errors average to zero) - \\(\\text{Var}[u | x] = \\sigma_u^2\\) (constant variance - homoskedasticity)\nSample model:\nFrom a sample of data, we estimate:\n\\[\\hat{y} = b_1 + b_2 x\\]\nwhere: - \\(b_1\\) and \\(b_2\\) are sample estimates of \\(\\beta_1\\) and \\(\\beta_2\\) - Different samples produce different estimates\nCrucial distinction: Error vs. Residual\n\nError (\\(u\\)): Deviation from unknown population line (unobservable)\nResidual (\\(e\\)): Deviation from estimated sample line (observable)\n\n\\[e_i = y_i - \\hat{y}_i = y_i - (b_1 + b_2 x_i)\\]\nKey insight: The sample regression line \\(\\hat{y} = b_1 + b_2 x\\) is our best estimate of the population line \\(E[y|x] = \\beta_1 + \\beta_2 x\\), but \\(b_1 \\neq \\beta_1\\) and \\(b_2 \\neq \\beta_2\\) due to sampling variability.\n\nKey Concept 6.1: Population Regression Model\nThe population regression model E[y|x] = β₁ + β₂x describes the true relationship with unknown parameters β₁ and β₂. The sample regression ŷ = b₁ + b₂x estimates this relationship from data. Different samples yield different estimates due to sampling variability, but on average, OLS estimates equal the true parameters (unbiasedness).",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#examples-of-sampling-from-a-population",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#examples-of-sampling-from-a-population",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "6.2 Examples of Sampling from a Population",
    "text": "6.2 Examples of Sampling from a Population\nWe examine two examples to understand sampling variability:\n\nGenerated data: Computer-simulated samples from an explicit model \\(y = 1 + 2x + u\\)\nCensus data: Samples from the 1880 U.S. Census (a finite population)\n\nIn both cases: - We know the true population parameters - Different samples yield different estimates - The distribution of estimates is approximately normal - On average, estimates equal the true parameters (unbiasedness)\n\nKey Concept 6.2: The Error Term\nThe error term u = y - E[y|x] is the deviation from the unknown population line and is unobservable. The residual e = y - ŷ is the deviation from the estimated sample line and is observable. This crucial distinction underlies all statistical inference: we use residuals (e) to learn about errors (u).\n\n\nExample 1: Generated Data from Known Model\nData Generating Process (DGP):\n\\[y = 1 + 2x + u, \\quad u \\sim N(0, \\sigma_u^2 = 4)\\]\nThis means:\n\nTrue intercept: \\(\\beta_1 = 1\\)\nTrue slope: \\(\\beta_2 = 2\\)\nError standard deviation: \\(\\sigma_u = 2\\)\n\n\nprint(\"=\" * 70)\nprint(\"6.2 EXAMPLES OF SAMPLING FROM A POPULATION\")\nprint(\"=\" * 70)\n\n# Read in generated data\ndata_gen = pd.read_stata(GITHUB_DATA_URL + 'AED_GENERATEDDATA.DTA')\n\nprint(\"\\nGenerated data summary:\")\ndata_summary = data_gen.describe()\nprint(data_summary)\n\nprint(\"\\nFirst 10 observations (Table 6.1):\")\nprint(data_gen.head(10))\n\n======================================================================\n6.2 EXAMPLES OF SAMPLING FROM A POPULATION\n======================================================================\n\nGenerated data summary:\n              x   Eygivenx         u         y\ncount  5.000000   5.000000  5.000000  5.000000\nmean   3.000000   7.000000 -1.031908  5.968092\nstd    1.581139   3.162278  1.753559  1.897129\nmin    1.000000   3.000000 -2.506667  4.493333\n25%    2.000000   5.000000 -2.390764  4.681283\n50%    3.000000   7.000000 -1.633280  4.689889\n75%    4.000000   9.000000 -0.318717  7.366720\nmax    5.000000  11.000000  1.689889  8.609236\n\nFirst 10 observations (Table 6.1):\n     x  Eygivenx         u         y\n0  1.0       3.0  1.689889  4.689889\n1  2.0       5.0 -0.318717  4.681283\n2  3.0       7.0 -2.506667  4.493333\n3  4.0       9.0 -1.633280  7.366720\n4  5.0      11.0 -2.390764  8.609236\n\n\n\n\nFigure 6.2 Panel A: Population Regression Line\nThe population regression line represents \\(E[y|x] = 1 + 2x\\). Points scatter around this line due to the error term \\(u\\).\n\nTransition Note: We’ve established the theoretical distinction between population and sample regression models. Now we’ll see this distinction in action through simulations and real census data, demonstrating how OLS estimates vary across samples while remaining centered on true parameters.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-population-regression-results",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-population-regression-results",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Population Regression Results",
    "text": "Interpreting the Population Regression Results\nWhat this tells us:\nThe population regression E[y|x] = 1 + 2x is perfectly estimated because we constructed the variable Eygivenx directly from this formula. Notice:\n\nIntercept coefficient = 1.0000 (exactly)\nSlope coefficient = 2.0000 (exactly)\nR² = 1.000 (perfect fit)\nStandard errors ≈ 0 (essentially zero)\n\nThis represents the true relationship between x and y in the population, before any random error is added.\nKey concept: The population regression line shows the expected value (average) of y for each value of x. Individual observations deviate from this line due to the random error term u, which has mean zero but individual realizations that are positive or negative.\n\nFigure 6.2 Panel B: Sample Regression Line\nThe sample regression line is our estimate from the observed data. Note that it differs from the population line due to sampling variability.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-sample-regression-results",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-sample-regression-results",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Sample Regression Results",
    "text": "Interpreting the Sample Regression Results\nWhat this tells us:\nThe sample regression estimates ŷ = 2.81 + 1.05x from the actual observed data (which includes random errors). Notice how this differs from the true population model:\nComparison: Sample vs. Population\n\n\n\nParameter\nPopulation (True)\nSample (Estimated)\nDifference\n\n\n\n\nIntercept\nβ₁ = 1.00\nb₁ = 2.81\n+1.81\n\n\nSlope\nβ₂ = 2.00\nb₂ = 1.05\n-0.95\n\n\nR²\n1.000\n0.769\n-0.231\n\n\n\nWhy do they differ?\n\nSampling variability: We only have n=5 observations. Different samples from the same population give different estimates.\nRandom errors: The actual y values include the error term u, which causes observations to scatter around the population line. Our sample happened to have errors that pulled the regression line away from the true values.\nSmall sample size: With only 5 observations, each individual data point has a large influence on the regression line. A larger sample would typically give estimates closer to the true values.\n\nKey insight: This is not a failure of OLS estimation! The sample regression line is doing its job—finding the best linear fit to the observed data. The discrepancy between sample and population parameters is an inherent feature of statistical estimation that we must account for through standard errors and confidence intervals.\nUnbiasedness property: While this particular sample overestimates the intercept and underestimates the slope, if we took many samples and averaged the estimates, they would converge to the true values (β₁ = 1, β₂ = 2). This is what we’ll demonstrate with Monte Carlo simulation later.\n\nDemonstration: Three Regressions from the Same DGP\nTo illustrate sampling variability, we generate three different samples from the same data-generating process.\nKey observation: Each sample produces a different regression line, but all are centered around the true population line.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-three-sample-regressions",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-three-sample-regressions",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Three Sample Regressions",
    "text": "Interpreting the Three Sample Regressions\nWhat this demonstrates:\nWe generated three independent samples from the same data-generating process (y = 1 + 2x + u), yet obtained three different regression lines:\n\n\n\nSample\nIntercept\nSlope\nTrue Values\n\n\n\n\nSample 1\n0.82\n1.81\nβ₀ = 1.0, β₁ = 2.0\n\n\nSample 2\n1.75\n1.79\nβ₀ = 1.0, β₁ = 2.0\n\n\nSample 3\n2.01\n1.67\nβ₀ = 1.0, β₁ = 2.0\n\n\n\nKey observations:\n\nAll estimates differ from the true values: None of the samples perfectly recovered β₀ = 1.0 or β₁ = 2.0, even though we know these are the true parameters.\nEstimates vary across samples: The intercept ranges from 0.82 to 2.01, and the slope ranges from 1.67 to 2.01. This is sampling variability in action.\nAll estimates are “in the neighborhood”: While no single estimate equals the true value, they’re all reasonably close. None gave us absurd values like β₁ = 10 or β₁ = -5.\n\nThe fundamental statistical question:\nIf we know the true parameter is β₁ = 2.0, why would we ever get estimates like 1.81, 1.79, or 1.67? The answer is random sampling variation. Each sample contains different realizations of the error term u, which causes the observations to scatter differently around the population line. OLS finds the best fit to each specific sample, leading to different regression lines.\nWhy this matters for econometrics:\nIn real-world applications, we have only one sample and we don’t know the true parameters. These simulations show that:\n\nOur estimate is almost certainly not exactly equal to the true value\nDifferent samples would give different estimates\nWe need a way to quantify this uncertainty (standard errors!)\n\nThis is why we can’t simply report “the slope is 1.81” and claim we’ve discovered the truth. We must report “the slope is 1.81 with a standard error of X,” acknowledging that our estimate contains sampling error.\n\nVisualization: Three Different Samples from the Same DGP\nEach panel shows:\n\nBlack dots: observed data\nRed line: sample regression line (different for each sample)\nBlue dashed line: true population line (same for all)\n\n\n# Generate three samples from the same data generating process\nnp.random.seed(12345)\nn = 30\n\n# Sample 1\nx1 = np.random.normal(3, 1, n)\nu1 = np.random.normal(0, 2, n)\ny1 = 1 + 2*x1 + u1\n\n# Sample 2\nx2 = np.random.normal(3, 1, n)\nu2 = np.random.normal(0, 2, n)\ny2 = 1 + 2*x2 + u2\n\n# Sample 3\nx3 = np.random.normal(3, 1, n)\nu3 = np.random.normal(0, 2, n)\ny3 = 1 + 2*x3 + u3\n\n# Create dataframes\ndf1 = pd.DataFrame({'x': x1, 'y': y1})\ndf2 = pd.DataFrame({'x': x2, 'y': y2})\ndf3 = pd.DataFrame({'x': x3, 'y': y3})\n\n# Fit regressions for each sample\nmodel1 = ols('y ~ x', data=df1).fit()\nmodel2 = ols('y ~ x', data=df2).fit()\nmodel3 = ols('y ~ x', data=df3).fit()\n\nprint(\"Three samples generated and regressions fitted:\")\nprint(f\"Sample 1 - Intercept: {model1.params['Intercept']:.2f}, Slope: {model1.params['x']:.2f}\")\nprint(f\"Sample 2 - Intercept: {model2.params['Intercept']:.2f}, Slope: {model2.params['x']:.2f}\")\nprint(f\"Sample 3 - Intercept: {model3.params['Intercept']:.2f}, Slope: {model3.params['x']:.2f}\")\n\nThree samples generated and regressions fitted:\nSample 1 - Intercept: 0.82, Slope: 1.81\nSample 2 - Intercept: 1.75, Slope: 1.79\nSample 3 - Intercept: 2.01, Slope: 1.67\n\n\n\n# Visualize all three regressions\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, (ax, df, model, title) in enumerate(zip(axes,\n                                                   [df1, df2, df3],\n                                                   [model1, model2, model3],\n                                                   ['Sample 1', 'Sample 2', 'Sample 3'])):\n    ax.scatter(df['x'], df['y'], alpha=0.6, s=50, color='black', label='Actual')\n    ax.plot(df['x'], model.fittedvalues, color='red', linewidth=2,\n            label=f'ŷ = {model.params[0]:.2f} + {model.params[1]:.2f}x')\n    # Add population line\n    x_range = np.linspace(df['x'].min(), df['x'].max(), 100)\n    y_pop = 1 + 2*x_range\n    ax.plot(x_range, y_pop, color='blue', linewidth=2, linestyle='--',\n            label='Population: y = 1 + 2x', alpha=0.7)\n    ax.set_xlabel('x', fontsize=11)\n    ax.set_ylabel('y', fontsize=11)\n    ax.set_title(title, fontsize=12, fontweight='bold')\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Three Different Samples from the Same DGP: y = 1 + 2x + u',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey observation: Each sample produces a different regression line,\")\nprint(\"but all are close to the true population line (blue dashed).\")\n\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_56592/4130646466.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  label=f'ŷ = {model.params[0]:.2f} + {model.params[1]:.2f}x')\n\n\n\n\n\n\n\n\n\n\nKey observation: Each sample produces a different regression line,\nbut all are close to the true population line (blue dashed).\n\n\nHaving seen how sampling variability affects regression estimates, let’s formalize the key properties of the OLS estimator.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#properties-of-the-least-squares-estimator",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#properties-of-the-least-squares-estimator",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "6.3 Properties of the Least Squares Estimator",
    "text": "6.3 Properties of the Least Squares Estimator\nThe OLS estimator has important statistical properties under certain assumptions.\nStandard Assumptions (1-4):\n\nLinearity: \\(y_i = \\beta_1 + \\beta_2 x_i + u_i\\) for all \\(i\\)\nZero conditional mean: \\(E[u_i | x_i] = 0\\) for all \\(i\\)\nHomoskedasticity: \\(\\text{Var}[u_i | x_i] = \\sigma_u^2\\) for all \\(i\\) (constant variance)\nIndependence: Errors \\(u_i\\) and \\(u_j\\) are independent for all \\(i \\neq j\\)\n\nProperties of OLS under assumptions 1-2:\nUnbiasedness: \\[E[b_2] = \\beta_2\\]\nInterpretation: On average across many samples, the OLS estimate equals the true parameter.\nProperties of OLS under assumptions 1-4:\nVariance of slope coefficient: \\[\\text{Var}[b_2] = \\sigma_{b_2}^2 = \\frac{\\sigma_u^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\nStandard error of slope coefficient: \\[se(b_2) = \\sqrt{\\frac{s_e^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\frac{s_e}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\]\nwhere \\(s_e^2 = \\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\) is the estimated error variance.\nWhen is the slope coefficient precisely estimated?\nStandard error is smaller when: 1. Model fits well (small \\(s_e^2\\)) 2. Many observations (large \\(n\\)) 3. Regressors are widely scattered (large \\(\\sum (x_i - \\bar{x})^2\\))\nAsymptotic normality:\nBy the Central Limit Theorem: \\[\\frac{b_2 - \\beta_2}{\\sigma_{b_2}} \\xrightarrow{d} N(0, 1) \\text{ as } n \\rightarrow \\infty\\]\nBest Linear Unbiased Estimator (BLUE):\nUnder assumptions 1-4, OLS has the smallest variance among all linear unbiased estimators (Gauss-Markov Theorem).\n\nSimulation: Sampling Distribution of OLS Estimator\nTo understand the sampling distribution, we simulate 1,000 regressions from the same DGP and examine the distribution of coefficient estimates.\nWhat to expect:\n\nMean of estimates ≈ true parameter (unbiasedness)\nDistribution approximately normal (CLT)\nSpread determined by standard error formula\n\n\nKey Concept 6.3: OLS Assumptions\nThe four core OLS assumptions are: (1) Correct model specification: y = β₁ + β₂x + u, (2) Mean-zero errors: E[u|x] = 0, (3) Homoskedasticity: Var[u|x] = σ²ᵤ, (4) Independence: errors uncorrelated across observations. Assumptions 1-2 are essential for unbiasedness; assumptions 3-4 affect variance and can be relaxed using robust standard errors.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-monte-carlo-simulation-results",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-monte-carlo-simulation-results",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Monte Carlo Simulation Results",
    "text": "Interpreting the Monte Carlo Simulation Results\nWhat 1,000 simulations reveal about OLS properties:\nWe simulated 1,000 independent samples (each with n=30 observations) from the same DGP: y = 1 + 2x + u. Here are the results:\nIntercept (β₀):\n\nTrue value: 1.0\nMean of 1,000 estimates: 0.9960\nStandard deviation: 1.2069\nDifference from true value: -0.004 (only 0.4% error)\n\nSlope (β₁):\n\nTrue value: 2.0\nMean of 1,000 estimates: 1.9944\nStandard deviation: 0.3836\nDifference from true value: -0.0056 (only 0.3% error)\n\nWhat this demonstrates:\n\nUnbiasedness confirmed: The mean of the estimates (0.9960 for intercept, 1.9944 for slope) is extremely close to the true parameters (1.0 and 2.0). With 1,000 simulations, random errors average out, leaving us essentially at the true values. This is the unbiasedness property of OLS: E[b₂] = β₂.\nSampling variability quantified: Individual estimates varied substantially:\n\nThe intercept estimates had a standard deviation of 1.21, meaning about 95% of estimates fell within 1.0 ± 2(1.21) = [-1.42, 3.42]\nThe slope estimates had a standard deviation of 0.38, meaning about 95% fell within 2.0 ± 2(0.38) = [1.24, 2.76]\n\nWhy individual estimates differ from true values: Any single sample (like Sample 1, 2, or 3 we saw earlier) will contain random errors that cause the estimate to deviate from the true parameter. But these deviations are random—sometimes too high, sometimes too low—and average out to zero across many samples.\nStandard deviation as a measure of precision: The standard deviation of the estimates (0.38 for slope) measures the sampling variability. This is closely related to the standard error you see in regression output, which estimates this variability from a single sample.\n\nThe big picture:\nThis simulation proves mathematically that OLS “works” in a precise sense: if we could repeat our study infinitely many times, the average of all our estimates would equal the true parameter. Of course, in practice we only get one sample, so we use standard errors to acknowledge the uncertainty inherent in any single estimate.\nEconomic intuition: Imagine 1,000 different researchers each collecting their own sample of n=30 observations from the same population. Each would report a different regression coefficient. The simulation shows that:\n\nOn average, they’d get the right answer (unbiasedness)\nBut individual estimates would vary around the true value\nThe variation would follow a predictable pattern (approximately normal, as we’ll visualize next)\n\n\nKey Concept 6.4: Monte Carlo and Unbiasedness\nMonte Carlo simulation demonstrates unbiasedness: the average of many OLS estimates equals the true parameter. The distribution of estimates is approximately normal (Central Limit Theorem), with spread measured by the standard error. This validates the theoretical properties of OLS in practice.\n\n\nTransition Note: The simulations have shown empirically that OLS estimates center on true parameters and follow approximate normal distributions. Now we’ll formalize these observations by deriving the theoretical properties of OLS estimators: unbiasedness, variance formulas, and asymptotic normality.\n\nVisualization: Sampling Distributions of OLS Estimators\nThese histograms show the distribution of coefficient estimates across 1,000 simulated samples.\nKey features:\n\nGreen vertical line: True parameter value\nRed curve: Normal distribution fit\nHistogram: Actual distribution of estimates\n\nThe close match confirms:\n\nUnbiasedness (centered on true value)\nApproximate normality (CLT works well even with n=30)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-sampling-distribution-histograms",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-sampling-distribution-histograms",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Sampling Distribution Histograms",
    "text": "Interpreting the Sampling Distribution Histograms\nWhat these distributions reveal:\nThese histograms show the distribution of coefficient estimates from our 1,000 simulated samples. They visually confirm several crucial statistical properties:\n1. Unbiasedness (visual confirmation):\n\nGreen vertical line marks the true parameter value (β₀ = 1.0 for intercept, β₁ = 2.0 for slope)\nThe histograms are centered exactly on the true values\nThis visual centering confirms E[b₀] = β₀ and E[b₁] = β₁\n\n2. Approximate normality (Central Limit Theorem):\n\nRed curve shows the fitted normal distribution\nThe histogram bars closely follow this curve\nEven with just n=30 observations per sample, the sampling distribution is approximately normal\nThis confirms the Central Limit Theorem applies to OLS estimators\n\n3. Different precision for different parameters:\n\nIntercept distribution (left panel): Wider spread (SD = 1.21)\nSlope distribution (right panel): Narrower spread (SD = 0.38)\nThe slope is estimated more precisely than the intercept in this case\n\nWhy the normal distribution matters:\nThe approximate normality of the sampling distribution is the foundation for statistical inference:\n\nConfidence intervals: We can construct intervals like b₁ ± 1.96 × SE(b₁) to capture the true parameter 95% of the time\nHypothesis tests: We can use the normal distribution to calculate p-values\nPrediction intervals: We can quantify uncertainty about predictions\n\nReading the histograms:\nLooking at the slope distribution (right panel):\n\nMost estimates fall between 1.2 and 2.8 (about ±2 standard deviations from 2.0)\nVery few estimates are below 1.0 or above 3.0\nThis tells us that even though individual samples vary, they rarely produce wildly incorrect estimates\n\nThe power of large numbers:\nIf we had run 10,000 or 100,000 simulations instead of 1,000:\n\nThe mean would get even closer to the true value (0.9960 → 1.0000)\nThe histogram would match the normal curve even more closely\nBut the standard deviation would stay approximately the same (0.38), because it depends on the sample size within each simulation (n=30), not the number of simulations\n\nPractical implication:\nWhen you see a regression output reporting “slope = 1.85, SE = 0.35”, you should imagine a histogram like the one above:\n\nThe estimate 1.85 is one draw from a sampling distribution\nThe SE = 0.35 tells you the width of that distribution\nAbout 95% of the distribution falls within 1.85 ± 2(0.35) = [1.15, 2.55]\nIf this interval excludes zero, the coefficient is statistically significant\n\nNow that we understand the theoretical properties of OLS estimators, let’s examine how to estimate the model parameters in practice.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#estimators-of-model-parameters",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#estimators-of-model-parameters",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "6.4 Estimators of Model Parameters",
    "text": "6.4 Estimators of Model Parameters\nIn practice, we need to estimate not just the coefficients but also the error variance and standard errors.\nEstimate of error variance:\nThe sample variance of residuals: \\[s_e^2 = \\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2\\]\nWhy divide by \\((n-2)\\)? - We estimated 2 parameters (\\(b_1\\) and \\(b_2\\)) - This leaves \\((n-2)\\) degrees of freedom - Division by \\((n-2)\\) makes \\(s_e^2\\) unbiased for \\(\\sigma_u^2\\)\nStandard error of the regression (Root MSE): \\[s_e = \\sqrt{s_e^2}\\]\nThis is the typical size of residuals and appears in regression output.\nStandard error of slope coefficient: \\[se(b_2) = \\frac{s_e}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\]\nStandard error of intercept coefficient: \\[se(b_1) = \\sqrt{\\frac{s_e^2 \\sum_{i=1}^n x_i^2}{n \\sum_{i=1}^n (x_i - \\bar{x})^2}}\\]\n\nKey Concept 6.5: Degrees of Freedom in Regression\nWhen calculating the standard error of the regression (sₑ), we divide the sum of squared residuals by (n-2) instead of n because we have estimated 2 parameters (b₁ and b₂). These 2 estimates “use up” 2 degrees of freedom, leaving (n-2) for estimating the error variance. This adjustment ensures that s²ₑ is an unbiased estimator of σ²ᵤ. More generally, degrees of freedom equal the sample size minus the number of estimated parameters.\n\n\nExample: Manual Computation of Standard Errors\nLet’s manually compute standard errors for a simple example to understand the formulas.\nArtificial data:\n\n\\((y, x)\\) = \\((1,1), (2,2), (2,3), (2,4), (3,5)\\)\nFrom earlier analysis: \\(\\hat{y} = 0.8 + 0.4x\\)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-manual-standard-error-calculations",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-manual-standard-error-calculations",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Manual Standard Error Calculations",
    "text": "Interpreting the Manual Standard Error Calculations\nWhat these calculations reveal:\nUsing the simple artificial dataset (x = [1, 2, 3, 4, 5], y = [1, 2, 2, 2, 3]), we can see exactly how standard errors are computed:\nStep 1: Regression coefficients\n\nEstimated equation: ŷ = 0.8 + 0.4x\nThese are obtained by OLS minimizing the sum of squared residuals\n\nStep 2: Calculate residuals and RSS\nLooking at the observation-level calculations, we can see:\n\nEach observation has a predicted value ŷ and a residual e = y - ŷ\nResiduals are both positive and negative (some points above the line, some below)\nSum of squared residuals: RSS = Σe²\n\nStep 3: Standard error of the regression (sₑ)\nThe formula is: \\[s_e = \\sqrt{\\frac{RSS}{n-2}} = \\sqrt{\\frac{\\text{sum of squared residuals}}{\\text{degrees of freedom}}}\\]\nWhy divide by (n-2)?\n\nWe estimated 2 parameters (intercept and slope)\nThis “uses up” 2 degrees of freedom\nDivision by (n-2) makes s²ₑ an unbiased estimator of σ²ᵤ\n\nStep 4: Standard error of the slope coefficient\nThe formula is: \\[se(b_2) = \\frac{s_e}{\\sqrt{\\sum(x_i - \\bar{x})^2}}\\]\nThis reveals what makes slope estimates more or less precise:\n\nNumerator (sₑ): How well the model fits\n\nSmaller residuals → smaller sₑ → smaller SE → more precise estimate\n\nDenominator (√Σ(xᵢ - x̄)²): How spread out the x values are\n\nWider spread in x → larger denominator → smaller SE → more precise estimate\nClustered x values → small denominator → large SE → imprecise estimate\n\n\nWhy does spread in x matter?\nThink geometrically: if all x values are clustered around the mean (say, x = 2.9, 3.0, 3.1), it’s hard to accurately estimate the slope—the line could pivot substantially without much change in fit. But if x values are widely spread (say, x = 1, 10, 20), the slope is well-identified—you can clearly see the relationship.\nStep 5: Standard error of the intercept\nThe formula is more complex: \\[se(b_1) = \\sqrt{\\frac{s²_e \\times \\sum x²_i}{n \\times \\sum(x_i - \\bar{x})^2}}\\]\nNotice:\n\nIntercept SE depends on the squared values of x (Σx²ᵢ)\nIf x values are far from zero, the intercept SE is large\nThis reflects extrapolation uncertainty—we’re estimating y when x=0, which may be far from our data\n\nVerification: Our manual calculations match the model output exactly, confirming:\n\nWe understand where these numbers come from\nThe formulas are correct\nStandard errors aren’t “magic”—they’re computed from the data using transparent formulas\n\nPractical implication:\nWhen designing a study, you can control precision by:\n\nIncreasing sample size n (reduces sₑ)\nCollecting data with wide variation in x (increases Σ(xᵢ - x̄)²)\nReducing measurement error (reduces sₑ)\n\nConversely, if you’re stuck with a small sample or clustered x values, expect large standard errors and wide confidence intervals.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-manual-standard-error-calculations-1",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#interpreting-the-manual-standard-error-calculations-1",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Interpreting the Manual Standard Error Calculations",
    "text": "Interpreting the Manual Standard Error Calculations\nWhat these calculations reveal:\nUsing the simple artificial dataset (x = [1, 2, 3, 4, 5], y = [1, 2, 2, 2, 3]), we can see exactly how standard errors are computed:\nStep 1: Regression coefficients\n\nEstimated equation: ŷ = 0.8 + 0.4x\nThese are obtained by OLS minimizing the sum of squared residuals\n\nStep 2: Calculate residuals and RSS\nLooking at the observation-level calculations, we can see:\n\nEach observation has a predicted value ŷ and a residual e = y - ŷ\nResiduals are both positive and negative (some points above the line, some below)\nSum of squared residuals: RSS = Σe²\n\nStep 3: Standard error of the regression (sₑ)\nThe formula is: \\[s_e = \\sqrt{\f\\frac{RSS}{n-2}} = \\sqrt{\f\\frac{\\text{sum of squared residuals}}{\\text{degrees of freedom}}}\\]\nWhy divide by (n-2)?\n\nWe estimated 2 parameters (intercept and slope)\nThis “uses up” 2 degrees of freedom\nDivision by (n-2) makes s²ₑ an unbiased estimator of σ²ᵤ\n\nStep 4: Standard error of the slope coefficient\nThe formula is: \\[se(b_2) = \f\\frac{s_e}{\\sqrt{\\sum(x_i - \b\\bar{x})^2}}\\]\nThis reveals what makes slope estimates more or less precise:\n\nNumerator (sₑ): How well the model fits\n\nSmaller residuals → smaller sₑ → smaller SE → more precise estimate\n\nDenominator (√Σ(xᵢ - x̄)²): How spread out the x values are\n\nWider spread in x → larger denominator → smaller SE → more precise estimate\nClustered x values → small denominator → large SE → imprecise estimate\n\n\nWhy does spread in x matter?\nThink geometrically: if all x values are clustered around the mean (say, x = 2.9, 3.0, 3.1), it’s hard to accurately estimate the slope—the line could pivot substantially without much change in fit. But if x values are widely spread (say, x = 1, 10, 20), the slope is well-identified—you can clearly see the relationship.\nStep 5: Standard error of the intercept\nThe formula is more complex: \\[se(b_1) = \\sqrt{\f\\frac{s²_e  \\times  \\sum x²_i}{n  \\times  \\sum(x_i - \b\\bar{x})^2}}\\]\nNotice:\n\nIntercept SE depends on the squared values of x (Σx²ᵢ)\nIf x values are far from zero, the intercept SE is large\nThis reflects extrapolation uncertainty—we’re estimating y when x=0, which may be far from our data\n\nVerification: Our manual calculations match the model output exactly, confirming:\n\nWe understand where these numbers come from\nThe formulas are correct\nStandard errors aren’t “magic”—they’re computed from the data using transparent formulas\n\nPractical implication:\nWhen designing a study, you can control precision by:\n\nIncreasing sample size n (reduces sₑ)\nCollecting data with wide variation in x (increases Σ(xᵢ - x̄)²)\nReducing measurement error (reduces sₑ)\n\nConversely, if you’re stuck with a small sample or clustered x values, expect large standard errors and wide confidence intervals.\n\nWhen is the Slope Coefficient Precisely Estimated?\nFrom the formula \\(se(b_2) = \\frac{s_e}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\), we see that standard errors are smaller when:\n\nGood model fit (small \\(s_e\\))\n\nLess unexplained variation\nSmaller residuals\n\nLarge sample size (large \\(\\sum (x_i - \\bar{x})^2\\))\n\nMore observations provide more information\nStandard errors shrink with \\(\\sqrt{n}\\)\n\nWide spread in regressors (large \\(\\sum (x_i - \\bar{x})^2\\))\n\nMore variation in \\(x\\) helps identify the slope\nClustered \\(x\\) values make slope hard to estimate\n\n\nPractical implication: When designing experiments or collecting data, seek wide variation in the explanatory variable.\n\nTransition Note: We’ve established that OLS has desirable theoretical properties under assumptions 1-4. Now we turn to practical implementation: how to estimate error variance, compute standard errors, and assess estimation precision using formulas that work with actual data.\n\nKey Concept 6.6: Standard Error of Regression Coefficients\nThe standard error of b₂ is se(b₂) = sₑ / √[Σ(xᵢ - x̄)²]. Precision is better (smaller SE) when: (1) the model fits well (small sₑ), (2) sample size is large (large Σ(xᵢ - x̄)²), (3) regressors are widely scattered (large Σ(xᵢ - x̄)²). Standard errors quantify estimation uncertainty and are essential for inference.\n\n\nKey Concept 6.7: The Gauss-Markov Theorem\nUnder assumptions 1-4, OLS is the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov Theorem. This means OLS has the smallest variance among all linear unbiased estimators. If errors are also normally distributed, OLS is the Best Unbiased Estimator (not just among linear estimators). This optimality property justifies the widespread use of OLS.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#summary-of-ols-properties",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#summary-of-ols-properties",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Summary of OLS Properties",
    "text": "Summary of OLS Properties\nUnder assumptions 1-4:\n\n\\(y_i\\) given \\(x_i\\) has conditional mean \\(\\beta_1 + \\beta_2 x_i\\) and conditional variance \\(\\sigma_u^2\\)\nSlope coefficient \\(b_2\\) has:\n\nMean: \\(E[b_2] = \\beta_2\\) (unbiased)\nVariance: \\(\\text{Var}[b_2] = \\sigma_u^2 / \\sum_{i=1}^n (x_i - \\bar{x})^2\\)\n\nStandard error of \\(b_2\\): \\(se(b_2) = s_e / \\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\)\nStandardized statistic: \\(Z = (b_2 - \\beta_2) / \\sigma_{b_2}\\) has mean 0 and variance 1\nAsymptotic normality: As \\(n \\rightarrow \\infty\\), \\(Z \\sim N(0,1)\\) by the Central Limit Theorem\nEfficiency: OLS is BLUE (Best Linear Unbiased Estimator) by the Gauss-Markov Theorem\n\nCritical assumptions:\n\nAssumptions 1-2 ensure unbiasedness and consistency\nAssumptions 3-4 can be relaxed (see Chapters 7 and 12 for robust methods)\nIn practice, choosing correct standard errors is crucial for valid inference",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#key-takeaways",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#key-takeaways",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPopulation Model and Sampling Framework\n\nThe population regression model E[y|x] = β₁ + β₂x describes the true relationship with unknown parameters β₁ and β₂\nThe conditional mean E[y|x] generalizes the unconditional mean E[y] by allowing the average to vary with x\nThe error term u = y - E[y|x] captures deviations from the population line; it is unobserved because β₁ and β₂ are unknown\nCrucial distinction: Error u (deviation from unknown population line) vs. residual e (deviation from estimated sample line)\nThe sample regression ŷ = b₁ + b₂x estimates the population relationship from data\nDifferent samples yield different estimates (b₁, b₂) due to sampling variability, but on average they equal the true parameters\n\n\n\nError Term Properties and Assumptions\n\nThe error term is assumed to have conditional mean zero: E[u|x] = 0 (errors average to zero at each x value)\nThis assumption ensures the population line is indeed E[y|x] = β₁ + β₂x\nHomoskedasticity assumption: Var[u|x] = σ²ᵤ (constant error variance across all x values)\n“Homoskedastic” derives from Greek: homos (same) + skedastic (scattering)\nThe variance Var[y|x] = Var[u|x] = σ²ᵤ measures variability around the population line\nGreater error variance means greater noise, reducing precision of estimates\n\n\n\nFour Core OLS Assumptions\n\nCorrect model: yᵢ = β₁ + β₂xᵢ + uᵢ for all i (linearity)\nMean-zero errors: E[uᵢ|xᵢ] = 0 for all i (no correlation between x and u)\nHomoskedasticity: Var[uᵢ|xᵢ] = σ²ᵤ for all i (constant error variance)\nIndependence: uᵢ and u are independent for all i ≠ j (no autocorrelation)\n\n\nAssumptions 1-2 are essential for unbiasedness and consistency (violations cause bias)\nAssumptions 3-4 can be relaxed using robust standard errors (Chapter 7.7, 12.1)\nAssumption 2 rules out omitted variable bias (no correlation between x and u)\nChoosing correct standard errors is crucial for valid confidence intervals and hypothesis tests\n\n\n\nUnbiasedness and Consistency\n\nUnder assumptions 1-2: E[b₂] = β₂ (unbiasedness)\nIf we obtained many samples, on average b₂ would equal β₂\nUnbiasedness is a finite-sample property (holds for any sample size n)\nConsistency: As n → ∞, b₂ converges in probability to β₂\nSufficient condition for consistency: bias → 0 and variance → 0 as n → ∞\nb₂ is consistent because: (1) unbiased under assumptions 1-2, (2) Var[b₂] → 0 as n → ∞ under assumptions 1-4\nBoth b₁ and b₂ are unbiased and consistent under these assumptions\n\n\n\nVariance and Standard Errors\n\nUnder assumptions 1-4: Var[b₂] = σ²ᵤ / Σ(xᵢ - x̄)²\nStandard deviation of b₂: σ_b₂ = σᵤ / √[Σ(xᵢ - x̄)²]\nSince σ²ᵤ is unknown, estimate it using standard error of regression: s²ₑ = (1/(n-2)) Σ(yᵢ - ŷᵢ)²\nUse (n-2) denominator because we estimated 2 coefficients, leaving (n-2) degrees of freedom\nThis divisor ensures s²ₑ is unbiased for σ²ᵤ\nStandard error of b₂: se(b₂) = sₑ / √[Σ(xᵢ - x̄)²]\nse(b₂) measures precision of b₂ as an estimate of β₂\n\n\n\nFactors Affecting Precision\n\nBetter precision (smaller se(b₂)) occurs when:\n\n\nModel fits well (s²ₑ is smaller) - less noise around regression line\nMany observations (Σ(xᵢ - x̄)² is larger) - more data reduces sampling variability\nRegressors widely scattered (Σ(xᵢ - x̄)² is larger) - more variation in x provides more information\n\n\nPrecision improves with √n, so need 4× observations to halve standard error\nTrade-off: Can’t control regressor scatter in observational data, but can increase sample size\nWide spread in x matters geometrically: If x values clustered, slope is poorly identified; if scattered, slope is well-identified\n\n\n\nCentral Limit Theorem and Asymptotic Normality\n\nUnder assumptions 1-4: b₂ ~ (β₂, σ²_b₂) where σ²_b₂ = σ²ᵤ / Σ(xᵢ - x̄)²\nStandardized variable: Z = (b₂ - β₂) / σ_b₂ has mean 0 and variance 1 by construction\nCLT: As n → ∞, Z ~ N(0,1) (approximately normal for large samples)\nThis implies b₂ ~ N(β₂, σ²_b₂) for large n\nIn practice, σ_b₂ is unknown (depends on unknown σᵤ)\nReplace σ_b₂ with se(b₂) leads to t distribution (Chapter 7)\nNormality justifies using normal-based inference for large samples\n\n\n\nEfficiency and BLUE Property\n\nUnder assumptions 1-4, OLS is the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov Theorem\n“Linear” means estimator is a linear combination of y values: b₂ = Σwᵢyᵢ\n“Best” means minimum variance among all linear unbiased estimators\nIf additionally u is normally distributed: OLS is the Best Unbiased Estimator (BUE)\nLowest variance among ALL unbiased estimators (not just linear ones)\nOLS is also best consistent estimator in standard settings\nBottom line: Under assumptions 1-4, OLS is essentially the optimal estimator of β₁ and β₂\n\n\n\nMonte Carlo Simulation Evidence\n\nMonte Carlo simulations demonstrate OLS properties empirically by generating many samples from a known model\nTwo examples: (1) Generated data from y = 1 + 2x + u with u ~ N(0,4), (2) Samples from 1880 Census (1.06 million males aged 60-70)\nKey findings: (1) Average of many OLS estimates equals true parameter (unbiasedness), (2) Distribution of estimates is approximately normal (CLT), (3) Similar results for intercept and slope\nSingle sample: b₁ ≠ β₁ and b₂ ≠ β₂ due to sampling variability\nMultiple samples: Estimates vary across samples but center on true parameters\nSampling distribution: Distribution of b₂ across many samples is approximately N(β₂, σ²_b₂)\n\n\n\nPractical Implications\n\nSampling variability is inevitable: Any single sample will deviate from true parameters\nStandard errors quantify uncertainty: They measure the typical deviation of b₂ from β₂ across hypothetical repeated samples\nConfidence intervals account for uncertainty: A 95% CI constructed as b₂ ± 2×se(b₂) will contain β₂ in 95% of samples\nAssumptions 1-2 are non-negotiable for unbiasedness; violations cause bias and inconsistency (Chapter 16 discusses, Chapter 17 presents solutions)\nAssumptions 3-4 are often relaxed in practice using robust standard errors (Chapters 7.7, 12.1)\nCorrect standard errors are crucial: Incorrect SEs invalidate confidence intervals and hypothesis tests\nStudy design matters: Researchers can improve precision by increasing sample size and ensuring wide variation in explanatory variables\n\n\n\nStatistical Concepts and Tools\n\nPopulation vs. sample distinction: Foundation of all statistical inference\nUnbiasedness: E[b₂] = β₂ (finite-sample property)\nConsistency: b₂ →ᵖ β₂ as n → ∞ (asymptotic property)\nEfficiency: Minimum variance among a class of estimators\nSampling distribution: Distribution of estimator across repeated samples\nStandard error: Estimated standard deviation of sampling distribution\nCentral Limit Theorem: Justifies normal-based inference for large samples\nGauss-Markov Theorem: Establishes BLUE property of OLS\n\n\n\nConnection to Statistical Inference\n\nThis chapter establishes the theoretical foundation for inference (Chapter 7)\nKnowing that b₂ ~ N(β₂, σ²_b₂) allows us to construct confidence intervals and conduct hypothesis tests\nStandard errors are the bridge between point estimates and interval estimates\nUnderstanding sampling distributions explains why we can make probabilistic statements about parameters\nThe normal approximation (CLT) justifies using critical values from the normal or t distribution\n\n\n\nSoftware Implementation\n\nPython tools: statsmodels.OLS for estimation, numpy for simulation, matplotlib for visualization\nGenerated data example uses random seed for reproducibility\nVisualization techniques: Scatter plots with fitted line, histograms of sampling distributions\nCan compare population line vs. fitted line when population model is known (simulations)\nMonte Carlo methods are powerful for understanding theoretical properties empirically\n\n\nCongratulations! You’ve completed Chapter 6 and now understand the fundamental statistical properties of the OLS estimator. You know why OLS works, when it works, and how to quantify estimation uncertainty. These concepts are the foundation for all statistical inference in econometrics. In Chapter 7, you’ll apply this knowledge to construct confidence intervals and test hypotheses about regression coefficients.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#practice-exercises",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#practice-exercises",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of OLS properties and statistical inference with these exercises.\nExercise 1: Population vs. Sample\nSuppose the true population model is y = 3 + 5x + u with E[u|x] = 0.\n\nIf a sample yields ŷ = 2 + 6x, does this mean OLS failed?\nWhat does unbiasedness tell us about the relationship between the sample estimate (b₂ = 6) and the population parameter (β₂ = 5)?\nIf we collected 1,000 different samples and computed b₂ for each, what would be the average of these 1,000 estimates?\n\n\nExercise 2: Error Term vs. Residual\nFor the observation (x, y) = (4, 25) with population model y = 8 + 4x + u:\n\nCalculate the population prediction E[y|x=4] and the error term u.\nIf the sample regression gives ŷ = 7 + 4.5x, calculate the fitted value and residual for this observation.\nWhy can we observe the residual but not the error term?\n\n\nExercise 3: Standard Error Calculation\nYou’re given: n = 30, Σ(xᵢ - x̄)² = 50, Σ(yᵢ - ŷᵢ)² = 112.\n\nCalculate the standard error of the regression (sₑ).\nCalculate the standard error of the slope coefficient se(b₂).\nIf Σ(xᵢ - x̄)² were 200 instead of 50 (wider spread in x), how would se(b₂) change?\n\n\nExercise 4: Factors Affecting Precision\nConsider two datasets: - Dataset A: n = 50, sₑ = 10, Σ(xᵢ - x̄)² = 100 - Dataset B: n = 50, sₑ = 5, Σ(xᵢ - x̄)² = 100\n\nCalculate se(b₂) for each dataset.\nWhich dataset provides more precise estimates? Why?\nHow many observations would Dataset A need to achieve the same precision as Dataset B?\n\n\nExercise 5: Hypothesis Testing Intuition\nOLS regression yields b₂ = 15 with se(b₂) = 4.\n\nConstruct an approximate 95% confidence interval for β₂ (use ±2 SE rule).\nDoes this interval include β₂ = 10? What does this suggest about the hypothesis H₀: β₂ = 10?\nIf the true parameter is β₂ = 13, would you expect most 95% confidence intervals from repeated samples to contain 13?\n\n\nExercise 6: Interpreting Assumptions\nFor each scenario, identify which OLS assumption is violated:\n\nThe model is y = β₁ + β₂x + u, but the true relationship is y = β₁ + β₂x² + u (nonlinear).\nError variance increases with x: Var[u|x=1] = 4, Var[u|x=2] = 9, Var[u|x=3] = 16.\nAn important variable z is omitted, and z is correlated with x, causing E[u|x] ≠ 0.\nThe data are time series and errors are autocorrelated: uₜ = 0.7uₜ₋₁ + εₜ.\n\n\nExercise 7: Monte Carlo Simulation Understanding\nIn a Monte Carlo study with 500 simulations from y = 2 + 3x + u: - Mean of 500 intercept estimates: 1.98 - Mean of 500 slope estimates: 2.95 - SD of 500 slope estimates: 0.40\n\nDo these results support the claim that OLS is unbiased? Explain.\nWhat would happen to the mean estimates if we ran 10,000 simulations instead of 500?\nWhat would happen to the SD of estimates if we increased the sample size within each simulation from n=30 to n=120?\n\n\nExercise 8: Python Practice\nUsing Python and the generated data approach from this chapter:\n\nGenerate 100 samples of size n=50 from y = 1 + 2x + u with u ~ N(0, 4) and x ~ N(3, 1).\nCompute b₂ for each sample and create a histogram of the 100 estimates.\nCalculate the mean and standard deviation of the 100 b₂ estimates. How do they compare to the theoretical values?\nTest whether the distribution of estimates is approximately normal using a Q-Q plot.\nRepeat with n=200 instead of n=50. How does the standard deviation of estimates change?\n\n\nSolutions to selected exercises:\n\nExercise 2a: E[y|x=4] = 8 + 4(4) = 24, so u = 25 - 24 = 1\nExercise 3a: sₑ = √[112/(30-2)] = √4 = 2\nExercise 3b: se(b₂) = 2/√50 = 0.283\nExercise 4a: Dataset A: se(b₂) = 10/√100 = 1.0; Dataset B: se(b₂) = 5/√100 = 0.5\nExercise 5a: 95% CI ≈ 15 ± 2(4) = [7, 23]\n\nFor complete solutions and additional practice problems, see the course website.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#case-studies",
    "href": "../notebooks_colab/ch06_The_Least_Squares_Estimator.html#case-studies",
    "title": "Chapter 6: The Least Squares Estimator",
    "section": "6.5 Case Studies",
    "text": "6.5 Case Studies\n\nCase Study 1: Sampling Variability in Productivity Regressions\nResearch Question: How does the regression of labor productivity on capital per worker vary across samples?\nBackground: In this chapter, we learned that OLS estimates vary across samples due to random sampling variation. While Monte Carlo simulations with generated data demonstrate this principle, it’s crucial to see it work with real economic data. We’ll use the convergence clubs dataset to explore how much productivity-capital regressions vary when we repeatedly sample from a “population” of 108 countries.\nThe Data: We use the convergence clubs dataset (Mendez 2020) covering 108 countries from 1990-2014. The cross-section for 2014 provides:\n\nLabor productivity (rgdppc_2014): Real GDP per capita in thousands of 2011 USD\nCapital per worker (rk_2014): Physical capital stock per worker in thousands of 2011 USD\nThese variables allow us to estimate production function relationships and explore sampling variability\n\n\nKey Concept 6.8: Sampling Variability in Econometrics\nWhen we estimate a regression from sample data, the coefficients (b₁, b₂) are random variables that vary across samples. Understanding this variability is essential for statistical inference—it tells us how much confidence to place in our estimates and how to construct confidence intervals and hypothesis tests. The standard error measures the typical deviation of our estimate from the true parameter across hypothetical repeated samples.\n\n\n\n\nLoad the Data\nWe’ll work with the 2014 cross-section from the convergence clubs dataset. The full dataset of 108 countries will serve as our “population,” and we’ll draw samples from it to demonstrate sampling variability.\n\nprint(\"=\"*70)\nprint(\"6.5 CASE STUDIES: SAMPLING VARIABILITY\")\nprint(\"=\"*70)\n\n# Load convergence clubs data\nconvergence_data = pd.read_stata(GITHUB_DATA_URL + \"Convergence_clubs_2023.dta\")\n\n# Extract 2014 cross-section\ndata_2014 = convergence_data[convergence_data['year'] == 2014].copy()\n\n# Create relevant variables\ndata_2014 = data_2014[['country', 'rgdppc', 'rk']].dropna()\ndata_2014.columns = ['country', 'productivity', 'capital']\n\nprint(f\"\\nData loaded: {len(data_2014)} countries in 2014\")\nprint(\"\\nSummary statistics:\")\nprint(data_2014[['productivity', 'capital']].describe())\nprint(\"\\nFirst 5 observations:\")\nprint(data_2014.head())\n\n\nTask 1: Estimate the “Population” Regression (Guided)\nInstructions: We’ll treat the full dataset of 108 countries as our “population” and estimate the regression of productivity on capital. This will serve as our benchmark—the “true” relationship we’re trying to recover from samples.\nResearch question: What is the relationship between capital per worker and labor productivity across all 108 countries?\nYour task:\n\nEstimate the regression: productivity = β₁ + β₂ × capital + u\nReport the coefficients, standard errors, and R²\nInterpret the slope coefficient: What does β₂ tell us about the capital-productivity relationship?\n\nCode template:\n# Estimate full-sample (\"population\") regression\npopulation_model = ols('productivity ~ capital', data=data_2014).fit()\nprint(population_model.summary())\n\n# Store population coefficients\nbeta_1_pop = population_model.params['Intercept']\nbeta_2_pop = population_model.params['capital']\nprint(f\"\\nPopulation coefficients: β₁ = {beta_1_pop:.4f}, β₂ = {beta_2_pop:.4f}\")\nInterpretation guide: A slope of β₂ = 0.05, for example, means that a $1,000 increase in capital per worker is associated with a $50 increase in GDP per capita (0.05 × 1000 = 50), on average across countries.\n\n\nTask 2: Draw a Random Sample and Estimate Regression (Semi-guided)\nInstructions: Now draw a random sample of 50 countries from the full dataset and estimate the same regression. Compare the sample estimate to the population parameter.\nYour task:\n\nDraw a random sample of n=50 countries (use data_2014.sample(n=50, random_state=42))\nEstimate the regression on this sample\nCompare the sample coefficients (b₁, b₂) to the population parameters (β₁, β₂)\nCalculate the sampling error: b₂ - β₂\n\nHints:\n\nThe random_state parameter ensures reproducibility\nSampling error = sample estimate - population parameter\nEven though this is random sampling, you likely won’t get b₂ exactly equal to β₂\n\n\nKey Concept 6.9: Sample vs. Population Regression\nThe full dataset of 108 countries acts as our “population.” When we draw a sample of 50 countries, we’re simulating the real-world situation where researchers work with incomplete data. The difference between the population coefficient (β₂) and the sample coefficient (b₂) is the sampling error—an inevitable consequence of working with limited data. This error is random: sometimes b₂ &gt; β₂, sometimes b₂ &lt; β₂, but on average b₂ = β₂ (unbiasedness).\n\n\n\nTask 3: Simulate the Sampling Distribution (Semi-guided)\nInstructions: To understand sampling variability, we need to see how b₂ varies across many samples. Run a Monte Carlo simulation: draw 1,000 random samples of n=50, estimate the regression for each, and collect all the b₂ estimates.\nYour task:\n\nCreate a loop that runs 1,000 iterations\nIn each iteration: (a) draw a random sample of 50 countries, (b) estimate the regression, (c) store the slope coefficient b₂\nCreate a histogram of the 1,000 b₂ estimates\nCalculate the mean and standard deviation of these estimates\nCompare the mean to the population parameter β₂\n\nCode structure:\n# Monte Carlo simulation\nn_simulations = 1000\nsample_size = 50\nb2_estimates = []\n\nfor i in range(n_simulations):\n    # Draw random sample\n    sample = data_2014.sample(n=sample_size, replace=False)\n    \n    # Estimate regression\n    model = ols('productivity ~ capital', data=sample).fit()\n    \n    # Store slope coefficient\n    b2_estimates.append(model.params['capital'])\n\n# Analyze sampling distribution\nprint(f\"Mean of b₂ estimates: {np.mean(b2_estimates):.4f}\")\nprint(f\"Std dev of b₂ estimates: {np.std(b2_estimates):.4f}\")\nprint(f\"Population parameter β₂: {beta_2_pop:.4f}\")\nExpected outcome: The mean of your 1,000 estimates should be very close to β₂ (confirming unbiasedness), and the histogram should look approximately normal (confirming the Central Limit Theorem).\n\n\nTask 4: Calculate Theoretical vs. Empirical Standard Error (More Independent)\nInstructions: The standard error of b₂ can be calculated two ways: (1) using the theoretical formula with sample data, (2) using the standard deviation of estimates across many samples (Monte Carlo).\nYour task:\n\nTheoretical SE: Take any single sample of n=50, estimate the regression, and extract se(b₂) from the regression output\nEmpirical SE: Use the standard deviation of the 1,000 b₂ estimates from Task 3\nCompare the two: Are they similar? Why or why not?\n\nHints:\n\nThe theoretical SE comes from the formula: se(b₂) = sₑ / √[Σ(xᵢ - x̄)²]\nThe empirical SE is: std(b₂ estimates from 1,000 samples)\nThey should be close but not identical (theoretical SE is an estimate, empirical SE is the “true” sampling variability in this simulation)\n\nDiscussion questions:\n\nIn real-world research, do we have access to the empirical SE? Why or why not?\nWhy is the theoretical SE useful if we can only draw one sample?\n\n\n\nTask 5: Investigate the Effect of Sample Size (Independent)\nInstructions: Theory predicts that standard errors decrease with sample size n according to the relationship se(b₂) ∝ 1/√n. Test this prediction by comparing sampling distributions for different sample sizes.\nYour task:\n\nRun Monte Carlo simulations (1,000 iterations each) for three sample sizes: n = 20, n = 50, n = 80\nFor each sample size, calculate the standard deviation of b₂ estimates\nCreate side-by-side histograms of the three sampling distributions\nVerify the theoretical relationship: Does se(b₂) decrease roughly as 1/√n?\n\nAnalysis questions:\n\nIf you double the sample size from n=50 to n=100, how much does the standard error decrease?\nWhat does this imply about the “cost” of precision in empirical research?\nWould you prefer a study with n=50 or n=80? How much more precise is the larger sample?\n\n\nKey Concept 6.10: Standard Errors and Sample Size\nStandard errors decrease with the square root of sample size: se(b₂) = σᵤ / √[n × Var(x)]. This means to halve the standard error, you need four times the sample size. This fundamental relationship guides study design—small increases in sample size yield diminishing returns in precision. The implication: going from n=100 to n=400 has the same effect on precision as going from n=25 to n=100.\n\n\n\nTask 6: Compare Sampling Variability Across Country Groups (Independent)\nInstructions: Does sampling variability differ across subpopulations? Compare the capital-productivity relationship for high-income vs. developing countries.\nYour task (student-designed analysis):\n\nSplit the data into two groups: high-income (productivity &gt; median) vs. developing (productivity ≤ median)\nFor each group, run a Monte Carlo simulation with 1,000 samples of n=30\nCompare the sampling distributions: Do they have different means? Different variances?\nExplain any differences you observe\n\nResearch questions to explore:\n\nIs the capital-productivity relationship stronger in one group vs. the other?\nIs sampling variability higher in one group? Why might this be?\nWhat does this suggest about generalizing findings across different country contexts?\n\nExtension: You could also explore other groupings (by region, by continent, by development indicators) to see how robust the capital-productivity relationship is across different contexts.\n\n\n\n\nWhat You’ve Learned\nThrough this case study, you’ve:\n\nExperienced firsthand how OLS estimates vary across samples using real economic data (not just simulated data)\nVerified empirically that the theoretical standard error formula accurately predicts sampling variability\nDiscovered how sample size affects estimation precision in practice (se ∝ 1/√n)\nExplored whether sampling variability differs across subpopulations (high-income vs. developing countries)\nConnected abstract statistical theory (unbiasedness, CLT, standard errors) to tangible empirical patterns\n\nKey insights:\n\nUnbiasedness in action: The average of many sample estimates equals the population parameter, even though individual estimates vary\nStandard errors quantify uncertainty: The spread of the sampling distribution tells us how much confidence to place in any single estimate\nSample size matters: Larger samples yield more precise estimates, but with diminishing returns (√n relationship)\nContext matters: Sampling variability may differ across subpopulations, affecting the generalizability of findings\n\nNext steps: In Chapter 7, you’ll use these standard errors to construct confidence intervals and test hypotheses about regression coefficients—the foundation of statistical inference in econometrics.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6: The Least Squares Estimator</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to statistical inference for bivariate regression models. All code runs directly in Google Colab without any local setup.\nThis chapter extends statistical inference from univariate to bivariate regression. You’ll gain both theoretical understanding and practical skills through hands-on Python examples, learning how to test hypotheses about regression coefficients and construct confidence intervals.\nWhat you’ll learn:\nDatasets used:\nChapter outline:",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#chapter-overview",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#chapter-overview",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "",
    "text": "The t-statistic for testing hypotheses about regression coefficients\nConstructing and interpreting confidence intervals for slope parameters\nTests of statistical significance (whether a regressor matters)\nTwo-sided hypothesis tests for specific parameter values\nOne-sided directional hypothesis tests\nHeteroskedasticity-robust standard errors and their importance\nEconomic vs. statistical significance\n\n\n\nAED_HOUSE.DTA: House prices and characteristics for 29 houses sold in Central Davis, California in 1999 (price, size, bedrooms, bathrooms, lot size, age)\n\n\n\n7.1 Example: House Price and Size\n7.2 The t Statistic\n7.3 Confidence Intervals\n7.4 Tests of Statistical Significance\n7.5 Two-Sided Hypothesis Tests\n7.6 One-Sided Directional Hypothesis Tests\n7.7 Robust Standard Errors\n7.8 Case Studies\nKey Takeaways\nPractice Exercises",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#setup",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#setup",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nfrom statsmodels.stats.sandwich_covariance import cov_hc1\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to explore statistical inference for bivariate regression.\")\n\nSetup complete! Ready to explore statistical inference for bivariate regression.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#example-house-price-and-size",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#example-house-price-and-size",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.1 Example: House Price and Size",
    "text": "7.1 Example: House Price and Size\nWe begin with a motivating example: the relationship between house price and house size.\nThe regression model:\n\\[\\text{price} = \\beta_1 + \\beta_2 \\times \\text{size} + u\\]\nwhere:\n\n\\(\\text{price}\\) is the house sale price (in thousands of dollars)\n\\(\\text{size}\\) is the house size (in square feet)\n\\(\\beta_2\\) is the population slope (price increase per square foot)\n\\(b_2\\) is the sample estimate of \\(\\beta_2\\)\n\nKey regression output:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStandard Error\nt-statistic\np-value\n95% CI\n\n\n\n\nSize\n73.77\n11.17\n6.60\n0.000\n[50.84, 96.70]\n\n\nIntercept\n115,017.30\n21,489.36\n5.35\n0.000\n[70,924.76, 159,109.8]\n\n\n\nInterpretation:\n\nEach additional square foot increases house price by approximately $73.77\nThe standard error (11.17) measures uncertainty in this estimate\nThe t-statistic (6.60) tests whether the effect is statistically significant\nThe 95% confidence interval is [50.84, 96.70]\n\n\nprint(\"=\" * 70)\nprint(\"7.1 EXAMPLE: HOUSE PRICE AND SIZE\")\nprint(\"=\" * 70)\n\n# Read in the house data\ndata_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n\nprint(\"\\nData summary:\")\ndata_summary = data_house.describe()\nprint(data_summary)\n\nprint(\"\\nFirst few observations:\")\nprint(data_house.head())\n\n======================================================================\n7.1 EXAMPLE: HOUSE PRICE AND SIZE\n======================================================================\n\nData summary:\n               price         size   bedrooms  bathrooms    lotsize        age  \\\ncount      29.000000    29.000000  29.000000  29.000000  29.000000  29.000000   \nmean   253910.344828  1882.758621   3.793103   2.206897   2.137931  36.413792   \nstd     37390.710695   398.272130   0.675030   0.341144   0.693034   7.118975   \nmin    204000.000000  1400.000000   3.000000   2.000000   1.000000  23.000000   \n25%    233000.000000  1600.000000   3.000000   2.000000   2.000000  31.000000   \n50%    244000.000000  1800.000000   4.000000   2.000000   2.000000  35.000000   \n75%    270000.000000  2000.000000   4.000000   2.500000   3.000000  39.000000   \nmax    375000.000000  3300.000000   6.000000   3.000000   3.000000  51.000000   \n\n       monthsold           list  \ncount  29.000000      29.000000  \nmean    5.965517  257824.137931  \nstd     1.679344   40860.264099  \nmin     3.000000  199900.000000  \n25%     5.000000  239000.000000  \n50%     6.000000  245000.000000  \n75%     7.000000  269000.000000  \nmax     8.000000  386000.000000  \n\nFirst few observations:\n    price  size  bedrooms  bathrooms  lotsize   age  monthsold    list\n0  204000  1400         3        2.0        1  31.0          7  199900\n1  212000  1600         3        3.0        2  33.0          5  212000\n2  213000  1800         3        2.0        2  51.0          4  219900\n3  220000  1600         3        2.0        1  49.0          4  229000\n4  224500  2100         4        2.5        2  47.0          6  224500",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#basic-regression-price-on-size",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#basic-regression-price-on-size",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Basic Regression: Price on Size",
    "text": "Basic Regression: Price on Size\nWe estimate the bivariate regression model using ordinary least squares (OLS).\n\n# Table 7.1 - Basic regression\nprint(\"=\" * 70)\nprint(\"Table 7.1: Regression of House Price on Size\")\nprint(\"=\" * 70)\n\nmodel_basic = ols('price ~ size', data=data_house).fit()\nprint(model_basic.summary())\n\n======================================================================\nTable 7.1: Regression of House Price on Size\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.617\nModel:                            OLS   Adj. R-squared:                  0.603\nMethod:                 Least Squares   F-statistic:                     43.58\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           4.41e-07\nTime:                        23:33:22   Log-Likelihood:                -332.05\nNo. Observations:                  29   AIC:                             668.1\nDf Residuals:                      27   BIC:                             670.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    1.15e+05   2.15e+04      5.352      0.000    7.09e+04    1.59e+05\nsize          73.7710     11.175      6.601      0.000      50.842      96.700\n==============================================================================\nOmnibus:                        0.576   Durbin-Watson:                   1.219\nProb(Omnibus):                  0.750   Jarque-Bera (JB):                0.638\nSkew:                          -0.078   Prob(JB):                        0.727\nKurtosis:                       2.290   Cond. No.                     9.45e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#coefficient-table",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#coefficient-table",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Coefficient Table",
    "text": "Coefficient Table\nLet’s create a clean table showing the key statistics for statistical inference.\n\n# Save coefficients in a clean table\ncoef_table = pd.DataFrame({\n    'Coefficient': model_basic.params,\n    'Std. Error': model_basic.bse,\n    't-statistic': model_basic.tvalues,\n    'p-value': model_basic.pvalues\n})\n\nprint(\"\\nCoefficient Table:\")\nprint(coef_table)\n\nprint(\"\\nInterpretation:\")\nprint(\"  - Slope (size): Each additional sq ft increases price by $73.77\")\nprint(\"  - Standard error: Measures uncertainty in the slope estimate\")\nprint(\"  - t-statistic: Tests whether slope differs from zero\")\nprint(\"  - p-value: Probability of observing such extreme values under H₀: β₂ = 0\")\n\n\nCoefficient Table:\n             Coefficient    Std. Error  t-statistic       p-value\nIntercept  115017.282609  21489.359861     5.352290  1.183545e-05\nsize           73.771040     11.174911     6.601488  4.408752e-07\n\nInterpretation:\n  - Slope (size): Each additional sq ft increases price by $73.77\n  - Standard error: Measures uncertainty in the slope estimate\n  - t-statistic: Tests whether slope differs from zero\n  - p-value: Probability of observing such extreme values under H₀: β₂ = 0",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#the-t-statistic",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#the-t-statistic",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.2 The t Statistic",
    "text": "7.2 The t Statistic\nThe t-statistic is fundamental to statistical inference in regression.\nStatistical inference problem:\n\nSample: \\(\\hat{y} = b_1 + b_2 x\\) where \\(b_1\\) and \\(b_2\\) are least squares estimates\nPopulation: \\(E[y|x] = \\beta_1 + \\beta_2 x\\) and \\(y = \\beta_1 + \\beta_2 x + u\\)\nGoal: Make inferences about the slope parameter \\(\\beta_2\\)\n\nThe t-statistic:\n\\[T = \\frac{\\text{estimate} - \\text{parameter}}{\\text{standard error}} = \\frac{b_2 - \\beta_2}{se(b_2)} \\sim T(n-2)\\]\nWhy use the T(n-2) distribution?\nUnder assumptions 1-4:\n\n\\(Var[b_2] = \\sigma_u^2 / \\sum_{i=1}^n (x_i - \\bar{x})^2\\)\nWe don’t know \\(\\sigma_u^2\\), so we replace it with \\(s_e^2 = \\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nThis introduces additional uncertainty, so we use \\(T(n-2)\\) instead of \\(N(0,1)\\)\n\nModel Assumptions (1-4):\n\nThe population model is \\(y = \\beta_1 + \\beta_2 x + u\\)\nThe error has mean zero conditional on x: \\(E[u_i | x_i] = 0\\)\nThe error has constant variance: \\(Var[u_i | x_i] = \\sigma_u^2\\)\nThe errors are statistically independent: \\(u_i\\) independent of \\(u_j\\)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-standard-errors-the-foundation-of-inference",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-standard-errors-the-foundation-of-inference",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Understanding Standard Errors: The Foundation of Inference",
    "text": "Understanding Standard Errors: The Foundation of Inference\nWhat is a standard error?\nThe standard error measures the uncertainty in our estimate. It answers: “If we repeatedly sampled from the population and computed b₂ each time, how much would b₂ vary across samples?”\nKey distinction:\n\nStandard deviation: Variability of individual observations (data spread)\nStandard error: Variability of the estimate across samples (estimation uncertainty)\n\nFormula for SE of the slope:\n\\[se(b_2) = \\frac{\\sigma_u}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\frac{\\sigma_u}{\\sqrt{n} \\cdot \\sigma_x}\\]\nwhere:\n\nσ_u = standard deviation of the error term\nσ_x = standard deviation of x\nn = sample size\n\nWhat affects the standard error?\n1. Error variance (σ²_u): Larger σ_u → Larger SE\n\nMore unexplained variation in y\nData points farther from regression line\nLess precise estimates\n\n2. Sample size (n): Larger n → Smaller SE\n\nMore data reduces uncertainty\nSE decreases at rate 1/√n\nQuadruple n to halve SE\n\n3. Variation in x (σ_x): Larger σ_x → Smaller SE\n\nMore spread in x provides more information\nExtreme x values help identify the slope\nConcentrated x values give less precision\n\nIntuition:\nThink of the regression line as a seesaw balanced at (x̄, ȳ):\n\nWith wide spread in x: Small changes in slope make big differences at the extremes (easy to detect slope)\nWith narrow spread in x: Hard to distinguish different slopes (difficult to detect slope)\n\nExample calculation for our house price data:\nGiven:\n\nSample size: n = 29\nStandard error of regression: σ_u ≈ 37,000\nStandard deviation of size: σ_x ≈ 360\nEstimated SE(b₂) = 37,000 / (√29 × 360) ≈ 19.1\n\n(Actual SE is 11.17, smaller because the relationship is quite strong)\nWhy standard errors matter:\n\nConfidence intervals: CI = b₂ ± t × SE(b₂)\nHypothesis tests: t = (b₂ - β₂⁰) / SE(b₂)\nPractical significance: Small SE → precise estimate → more reliable\nStudy design: Calculate required n for desired SE\n\nRelationship to R²:\nHigher R² (better fit) → Smaller σ_u → Smaller SE → More precise estimates\nFor our house price example:\n\nR² = 0.62 (size explains 62% of price variation)\nThis gives relatively small SE\nIf R² were 0.10, SE would be about 2.5 times larger\n\n\nprint(\"=\" * 70)\nprint(\"7.2 THE T STATISTIC\")\nprint(\"=\" * 70)\n\nprint(\"\\nRegression coefficients and t-statistics:\")\nprint(model_basic.summary2().tables[1])\n\n# Extract key statistics\ncoef_size = model_basic.params['size']\nse_size = model_basic.bse['size']\nt_stat_size = model_basic.tvalues['size']\np_value_size = model_basic.pvalues['size']\n\nprint(f\"\\nDetailed statistics for 'size' coefficient:\")\nprint(f\"  Coefficient: ${coef_size:.4f}\")\nprint(f\"  Standard Error: ${se_size:.4f}\")\nprint(f\"  t-statistic: {t_stat_size:.4f}\")\nprint(f\"  p-value: {p_value_size:.6f}\")\n\nprint(\"\\nThe t-statistic formula:\")\nprint(f\"  t = b₂ / se(b₂) = {coef_size:.4f} / {se_size:.4f} = {t_stat_size:.4f}\")\n\n======================================================================\n7.2 THE T STATISTIC\n======================================================================\n\nRegression coefficients and t-statistics:\n                   Coef.      Std.Err.         t         P&gt;|t|        [0.025  \\\nIntercept  115017.282609  21489.359861  5.352290  1.183545e-05  70924.758265   \nsize           73.771040     11.174911  6.601488  4.408752e-07     50.842017   \n\n                  0.975]  \nIntercept  159109.806952  \nsize           96.700064  \n\nDetailed statistics for 'size' coefficient:\n  Coefficient: $73.7710\n  Standard Error: $11.1749\n  t-statistic: 6.6015\n  p-value: 0.000000\n\nThe t-statistic formula:\n  t = b₂ / se(b₂) = 73.7710 / 11.1749 = 6.6015\n\n\n\nKey Concept 7.1: The t-Distribution and Degrees of Freedom\nThe t-distribution is used for statistical inference when the population variance is unknown (which is always the case in practice). Unlike the standard normal distribution, the t-distribution accounts for the additional uncertainty from estimating the variance.\nKey properties: - Bell-shaped and symmetric (like the normal distribution) - Heavier tails than the normal distribution (more probability in extremes) - Converges to the normal distribution as sample size increases - Characterized by degrees of freedom (df)\nDegrees of freedom = n - 2 for bivariate regression: - Start with n observations - Estimate β₁ (intercept): -1 df - Estimate β₂ (slope): -1 df\n- Remaining df for estimating variance: n - 2\nPractical implication: For small samples (n &lt; 30), the t-distribution’s heavier tails lead to wider confidence intervals and more conservative hypothesis tests compared to the normal distribution. For large samples (n &gt; 100), the difference becomes negligible.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#confidence-intervals",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#confidence-intervals",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.3 Confidence Intervals",
    "text": "7.3 Confidence Intervals\nA confidence interval provides a range of plausible values for the population parameter.\nFormula for a $100(1-)%$ confidence interval:\n\\[b_2 \\pm t_{n-2, \\alpha/2} \\times se(b_2)\\]\nwhere:\n\n\\(b_2\\) is the slope estimate\n\\(se(b_2)\\) is the standard error of \\(b_2\\)\n\\(t_{n-2, \\alpha/2}\\) is the critical value from Student’s t-distribution with \\(n-2\\) degrees of freedom\n\n95% confidence interval (approximate):\n\\[b_2 \\pm 2 \\times se(b_2)\\]\nInterpretation:\n\nIf we repeatedly sampled from the population and constructed 95% CIs, approximately 95% of these intervals would contain the true parameter value \\(\\beta_2\\)\nThe calculated 95% CI will correctly include \\(\\beta_2\\) 95% of the time\n\nExample calculation for house price:\n\\[\\begin{aligned}\nb_2 \\pm t_{27, 0.025} \\times se(b_2) &= 73.77 \\pm 2.052 \\times 11.17 \\\\\n&= 73.77 \\pm 22.93 \\\\\n&= [50.84, 96.70]\n\\end{aligned}\\]\n\nKey Concept 7.2: Interpreting Confidence Intervals\nA confidence interval provides a range of plausible values for the population parameter. For regression slopes, the 95% CI is:\n\\[b_2 \\pm t_{n-2, 0.025} \\times se(b_2)\\]\nCommon misconceptions: - WRONG: “There’s a 95% probability that β₂ is in this interval” - CORRECT: “If we repeated the sampling process many times, 95% of the constructed intervals would contain β₂”\nPractical interpretation: - The interval represents our uncertainty about the true parameter value - Wider intervals indicate more uncertainty (large SE, small n, or high variability) - Narrower intervals indicate more precision (small SE, large n, or low variability) - Values inside the interval are “plausible” at the chosen confidence level - Values outside the interval would be rejected in a hypothesis test\nRelationship to hypothesis testing: If a null value β₂* falls inside the 95% CI, we fail to reject H₀: β₂ = β₂* at the 5% significance level. This makes CIs more informative than hypothesis tests alone.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-confidence-intervals-a-deep-dive",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-confidence-intervals-a-deep-dive",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Understanding Confidence Intervals: A Deep Dive",
    "text": "Understanding Confidence Intervals: A Deep Dive\nWhat is a confidence interval?\nA confidence interval (CI) is NOT a probability statement about the parameter. Instead, it’s a statement about the procedure used to construct the interval.\nCommon misconceptions:\nWRONG: “There is a 95% probability that β₂ is between 50.84 and 96.70”\n\nThe parameter β₂ is fixed (not random)\nThe interval either contains β₂ or it doesn’t\n\nCORRECT: “If we repeatedly sampled and constructed 95% CIs, approximately 95% of these intervals would contain the true β₂”\n\nThe randomness is in the sampling process\nOur particular interval is one realization from this process\n\nIntuitive explanation:\nImagine conducting 100 different studies using different random samples from the same population:\n\nEach study estimates β₂ and constructs a 95% CI\nAbout 95 of the 100 intervals will contain the true β₂\nAbout 5 of the 100 intervals will miss β₂ (just by chance)\n\nWidth of confidence intervals:\nThe CI width depends on three factors:\n\\[\\text{Width} = 2 \\times t_{n-2, \\alpha/2} \\times se(b_2)\\]\n\nConfidence level (1-α): Higher confidence → wider interval\n\n\n90% CI: Narrower (less confident)\n95% CI: Standard choice (balance)\n99% CI: Wider (more confident)\n\n\nSample size (n): Larger sample → narrower interval\n\n\nMore data reduces uncertainty\nCritical value t_{n-2, α/2} decreases as n increases\n\n\nVariability in data: More scatter → wider interval\n\n\nse(b₂) increases with unexplained variation\nTighter relationship → more precise estimates\n\nPractical use:\nConfidence intervals are more informative than hypothesis tests because they show:\n\nThe point estimate (center of interval)\nThe precision of the estimate (width of interval)\nAll null values that would not be rejected (values inside interval)\n\n\nprint(\"=\" * 70)\nprint(\"7.3 CONFIDENCE INTERVALS\")\nprint(\"=\" * 70)\n\n# 95% confidence intervals\nconf_int = model_basic.conf_int(alpha=0.05)\nprint(\"\\n95% Confidence Intervals:\")\nprint(conf_int)\n\n======================================================================\n7.3 CONFIDENCE INTERVALS\n======================================================================\n\n95% Confidence Intervals:\n                      0              1\nIntercept  70924.758265  159109.806952\nsize          50.842017      96.700064",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#the-t-distribution-vs-normal-distribution-why-it-matters",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#the-t-distribution-vs-normal-distribution-why-it-matters",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "The t-Distribution vs Normal Distribution: Why It Matters",
    "text": "The t-Distribution vs Normal Distribution: Why It Matters\nWhy not use the normal distribution?\nIn theory, when we know the population variance σ²_u, the test statistic follows a standard normal distribution:\n\\[Z = \\frac{b_2 - \\beta_2}{\\sigma / \\sqrt{\\sum(x_i - \\bar{x})^2}} \\sim N(0,1)\\]\nThe problem: We never know σ_u in practice!\nThe solution: Replace σ_u with its estimate s_e (residual standard error):\n\\[T = \\frac{b_2 - \\beta_2}{s_e / \\sqrt{\\sum(x_i - \\bar{x})^2}} \\sim T(n-2)\\]\nThis substitution introduces additional uncertainty, so we use the t-distribution instead of normal.\nProperties of the t-distribution:\n\nShape: Bell-shaped and symmetric (like normal)\nMean: 0 (like normal)\nVariance: df/(df-2) &gt; 1 (heavier tails than normal)\nDegrees of freedom: n - 2 for bivariate regression\n\nn observations\nMinus 2 parameters estimated (β₁ and β₂)\n\n\nKey differences from normal:\n\n\n\n\n\n\n\n\n\nSample Size\nt Critical Value (α=0.05)\nz Critical Value\nDifference\n\n\n\n\nn = 5 (df=3)\n3.182\n1.96\n+62%\n\n\nn = 10 (df=8)\n2.306\n1.96\n+18%\n\n\nn = 30 (df=28)\n2.048\n1.96\n+4%\n\n\nn = 100 (df=98)\n1.984\n1.96\n+1%\n\n\nn → ∞\n1.96\n1.96\n0%\n\n\n\nWhat this means:\n\nSmall samples: t critical values much larger → wider CIs, harder to reject H₀\nLarge samples: t ≈ normal → approximately same inference\nOur house data: n=29, df=27, t(0.025) = 2.052 vs z = 1.96\n\nWhy degrees of freedom = n - 2?\n\nStart with n observations\nEstimate β₁ (intercept): loses 1 df\nEstimate β₂ (slope): loses 1 df\nRemaining df for estimating variance: n - 2\n\nPractical implications:\nFor n = 29 (our house price data):\n\nUsing normal: 95% CI margin = 1.96 × 11.17 = 21.89\nUsing t(27): 95% CI margin = 2.052 × 11.17 = 22.92\nDifference: 5% wider with t-distribution (more conservative)\n\nFor n = 10 (small sample):\n\nUsing normal: 95% CI margin = 1.96 × SE\nUsing t(8): 95% CI margin = 2.306 × SE\nDifference: 18% wider with t-distribution (much more conservative!)\n\nRule of thumb:\n\nn &lt; 30: Must use t-distribution\n30 ≤ n &lt; 100: Use t-distribution (small difference)\nn ≥ 100: Normal approximation usually fine, but still use t\n\nModern practice: Statistical software always uses t-distribution (why not? It’s correct for any n)\nVisual intuition:\nThe t-distribution has heavier tails:\n\nMore probability in the extremes\nLess probability near the center\nThis accounts for the uncertainty in estimating σ_u\nAs n increases, estimation uncertainty decreases, and t → normal",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#manual-calculation-of-confidence-interval",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#manual-calculation-of-confidence-interval",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Manual Calculation of Confidence Interval",
    "text": "Manual Calculation of Confidence Interval\nLet’s manually calculate the confidence interval for the size coefficient to understand the mechanics.\n\n# Manual calculation of confidence interval for size\nn = len(data_house)\ndf = n - 2\nt_crit = stats.t.ppf(0.975, df)  # 97.5th percentile for two-sided 95% CI\n\nci_lower = coef_size - t_crit * se_size\nci_upper = coef_size + t_crit * se_size\n\nprint(\"Manual calculation for 'size' coefficient:\")\nprint(f\"  Sample size: {n}\")\nprint(f\"  Degrees of freedom: {df}\")\nprint(f\"  Critical t-value (α=0.05): {t_crit:.4f}\")\nprint(f\"  Margin of error: {t_crit * se_size:.4f}\")\nprint(f\"  95% CI: [${ci_lower:.4f}, ${ci_upper:.4f}]\")\n\nprint(\"\\nInterpretation:\")\nprint(f\"  We are 95% confident that each additional square foot\")\nprint(f\"  increases house price by between ${ci_lower:.2f} and ${ci_upper:.2f}.\")\n\nManual calculation for 'size' coefficient:\n  Sample size: 29\n  Degrees of freedom: 27\n  Critical t-value (α=0.05): 2.0518\n  Margin of error: 22.9290\n  95% CI: [$50.8420, $96.7001]\n\nInterpretation:\n  We are 95% confident that each additional square foot\n  increases house price by between $50.84 and $96.70.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#example-with-artificial-data",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#example-with-artificial-data",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Example with Artificial Data",
    "text": "Example with Artificial Data\nTo illustrate the concepts more clearly, let’s work with a simple artificial dataset.\n\nKey Concept 7.3: The Hypothesis Testing Framework\nHypothesis testing is a formal procedure for making decisions about population parameters. The key steps are:\n1. State the hypotheses: - Null hypothesis (H₀): The claim we’re testing (usually “no effect”) - Alternative hypothesis (Hₐ): What we conclude if we reject H₀\n2. Choose significance level (α): - Common choices: 0.10, 0.05, 0.01 - α = probability of Type I error (rejecting H₀ when it’s true)\n3. Calculate test statistic: - Standardizes the difference: t = (estimate - null value) / SE\n4. Determine p-value: - Probability of observing our result (or more extreme) if H₀ is true - Smaller p-value = stronger evidence against H₀\n5. Make decision: - Reject H₀ if p-value &lt; α - Fail to reject H₀ if p-value ≥ α (never “accept” H₀)\nUnderstanding p-values: If p = 0.001, this means “if H₀ were true, we’d observe a result this extreme only 0.1% of the time.” This is strong evidence against H₀.\n\n\nKey Concept 7.4: Statistical vs. Economic Significance\nStatistical significance and economic significance are distinct concepts that answer different questions:\nStatistical Significance: - Question: Is the effect different from zero? - Determined by: t-statistic = b₂ / se(b₂), which depends on sample size, variability, and effect size - Interpretation: We can confidently say the effect exists (not due to chance)\nEconomic Significance: - Question: Is the effect large enough to matter in practice? - Determined by: The magnitude of b₂ and the context - Interpretation: The effect has real-world importance\nWhy they can diverge: 1. Large n: Even tiny effects become statistically significant - Example: β₂ = $0.01 with n = 10,000 might have p &lt; 0.001 but be economically trivial 2. Small n: Large effects may not reach statistical significance\n- Example: β₂ = $100 with n = 10 might have p = 0.12 but be economically important\nBest practice: Always report both the coefficient estimate (economic magnitude) and the standard error/confidence interval (statistical precision). Focus on confidence intervals, which show both dimensions simultaneously.\n\n\nKey Concept 7.5: One-Sided vs. Two-Sided Tests\nThe choice between one-sided and two-sided tests depends on your research question:\nTwo-Sided Test (Most Common): - H₀: β₂ = β₂* vs. Hₐ: β₂ ≠ β₂* - Detects deviations in either direction - Standard practice in academic research - Rejection region: Both tails of t-distribution\nOne-Sided Test (Directional): - Upper: H₀: β₂ ≤ β₂* vs. Hₐ: β₂ &gt; β₂ - Lower: H₀: β₂ ≥ β₂ vs. Hₐ: β₂ &lt; β₂*\n- Detects deviations in one specific direction - Rejection region: One tail only\nKey relationship: For the same test statistic, one-sided p-value = (two-sided p-value) / 2 (if sign is correct)\nWhen to use one-sided tests: - Strong theoretical prediction of direction (before seeing data) - Only care about deviations in one direction - Be cautious: Journals typically require two-sided tests\nImportant: If your data contradicts the predicted direction, you cannot reject H₀ with a one-sided test (p-value &gt; 0.5).\n\n\nprint(\"=\" * 70)\nprint(\"Example with Artificial Data\")\nprint(\"=\" * 70)\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([1, 2, 2, 2, 3])\ndf_artificial = pd.DataFrame({'x': x, 'y': y})\n\nmodel_artificial = ols('y ~ x', data=df_artificial).fit()\nprint(model_artificial.summary())\n\ncoef_x = model_artificial.params['x']\nse_x = model_artificial.bse['x']\nn_art = len(x)\ndf_art = n_art - 2\nt_crit_art = stats.t.ppf(0.975, df_art)\n\nci_lower_art = coef_x - t_crit_art * se_x\nci_upper_art = coef_x + t_crit_art * se_x\n\nprint(f\"\\nManual CI for artificial data:\")\nprint(f\"  Coefficient: {coef_x:.4f}\")\nprint(f\"  Standard Error: {se_x:.4f}\")\nprint(f\"  95% CI: [{ci_lower_art:.4f}, {ci_upper_art:.4f}]\")\n\n======================================================================\nExample with Artificial Data\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.800\nModel:                            OLS   Adj. R-squared:                  0.733\nMethod:                 Least Squares   F-statistic:                     12.00\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):             0.0405\nTime:                        23:33:22   Log-Likelihood:               -0.78037\nNo. Observations:                   5   AIC:                             5.561\nDf Residuals:                       3   BIC:                             4.780\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8000      0.383      2.089      0.128      -0.419       2.019\nx              0.4000      0.115      3.464      0.041       0.033       0.767\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   2.600\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.352\nSkew:                          -0.000   Prob(JB):                        0.839\nKurtosis:                       1.700   Cond. No.                         8.37\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nManual CI for artificial data:\n  Coefficient: 0.4000\n  Standard Error: 0.1155\n  95% CI: [0.0325, 0.7675]\n\n\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n  warn(\"omni_normtest is not valid with less than 8 observations; %i \"",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-hypothesis-testing-the-complete-workflow",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-hypothesis-testing-the-complete-workflow",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Understanding Hypothesis Testing: The Complete Workflow",
    "text": "Understanding Hypothesis Testing: The Complete Workflow\nThe hypothesis testing framework:\nHypothesis testing is a formal procedure for making decisions about population parameters using sample data.\nStep-by-step workflow:\n1. State the hypotheses - Null hypothesis (H₀): The claim we’re testing (usually “no effect”) - Alternative hypothesis (Hₐ): What we conclude if we reject H₀ - Example: H₀: β₂ = 0 vs Hₐ: β₂ ≠ 0\n2. Choose the significance level (α) - Common choices: 0.10, 0.05, 0.01 - α = probability of rejecting H₀ when it’s actually true (Type I error) - Convention: α = 0.05 (5% significance level)\n3. Calculate the test statistic - Formula: t = (b₂ - β₂⁰) / se(b₂) - This standardizes the difference between estimate and null value - Under H₀, t follows a t-distribution with n-2 degrees of freedom\n4. Determine the p-value - p-value = probability of observing a test statistic as extreme as ours if H₀ is true - Smaller p-value = stronger evidence against H₀ - p-value &lt; α → reject H₀\n5. Make a decision - Reject H₀: Strong evidence against the null hypothesis - Fail to reject H₀: Insufficient evidence to reject the null - Note: We never “accept” H₀, we only fail to reject it\n6. State the conclusion - Translate the statistical decision into plain language - Example: “House size has a statistically significant effect on price at the 5% level”\nUnderstanding p-values:\nThe p-value answers this question: “If the null hypothesis were true, what is the probability of getting a result at least as extreme as what we observed?”\nExample interpretation:\n\np = 0.000: If β₂ were truly zero, the probability of getting t ≥ 6.60 is less than 0.1%\nThis is very unlikely, so we have strong evidence against H₀\n\nTwo approaches to hypothesis testing:\n\np-value approach: Reject H₀ if p-value &lt; α\nCritical value approach: Reject H₀ if |t| &gt; critical value\n\nBoth approaches always give the same conclusion!\nCommon significance levels and interpretations:\n\n\n\np-value\nInterpretation\nStrength of Evidence\n\n\n\n\np &gt; 0.10\nNot significant\nWeak/no evidence\n\n\n0.05 &lt; p ≤ 0.10\nMarginally significant\nModerate evidence\n\n\n0.01 &lt; p ≤ 0.05\nSignificant\nStrong evidence\n\n\np ≤ 0.01\nHighly significant\nVery strong evidence",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#statistical-significance-vs-economic-significance",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#statistical-significance-vs-economic-significance",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Statistical Significance vs Economic Significance",
    "text": "Statistical Significance vs Economic Significance\nA crucial distinction that is often confused:\nStatistical Significance\n\nAnswers: “Is the effect different from zero?”\nDepends on: Sample size, variability, effect size\nFormula: t = b₂ / se(b₂)\nInterpretation: We can confidently say the effect exists\n\nEconomic Significance\n\nAnswers: “Is the effect large enough to matter?”\nDepends on: The magnitude of b₂ and context\nRequires: Domain knowledge and practical judgment\nInterpretation: The effect has real-world importance\n\nKey insights:\n1. Statistical significance ≠ Economic significance\nYou can have:\n\nStatistically significant but economically trivial effects\n\nExample: With n=10,000, β₂ = 0.001 might be statistically significant but meaningless\n\nEconomically important but statistically insignificant effects\n\nExample: With n=10, β₂ = 100 might not be statistically significant but potentially important\n\n\n2. Sample size matters for statistical significance\n\\[t = \\frac{b_2}{se(b_2)} = \\frac{b_2}{\\sigma_u / \\sqrt{\\sum(x_i - \\bar{x})^2}}\\]\nAs sample size increases:\n\nThe denominator (standard error) decreases\nThe t-statistic increases\nSmall effects become statistically significant\n\n3. Economic significance requires context\nFor the house price example:\n\nStatistical result: β₂ = 73.77, highly significant (p &lt; 0.001)\nEconomic interpretation: Each sq ft adds $73.77 to price\nContext matters:\n\nFor a 100 sq ft difference: $7,377 difference (substantial!)\nFor a 10 sq ft difference: $737.70 difference (moderate)\nThis is economically meaningful in the housing market\n\n\nPractical guidance:\n\nAlways report both:\n\nThe coefficient estimate (economic magnitude)\nThe p-value or confidence interval (statistical precision)\n\nFocus on confidence intervals:\n\nShows both statistical and economic significance\nExample: 95% CI = [50.84, 96.70]\nInterpretation: Effect is between $50.84 and $96.70 per sq ft\nEven the lower bound is economically meaningful\n\nConsider practical importance:\n\nWould anyone change their behavior based on this result?\nIs the effect large enough to justify policy interventions?\nDoes the effect matter in real-world terms?\n\n\nExample of the distinction:\nSuppose we study the effect of an expensive job training program on wages:\n\nStatistical result: Training increases wages by $0.50/hour (p = 0.001)\nStatistical significance: YES (highly significant)\nEconomic significance: Questionable\n\nAnnual benefit: $0.50 × 2000 hours = $1,000\nIf program costs $10,000, not worth it\nIf program costs $500, might be worth it\n\n\nBottom line: Statistical significance tells you the effect exists. Economic significance tells you whether you should care.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-two-sided-vs-one-sided-tests",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#understanding-two-sided-vs-one-sided-tests",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Understanding Two-Sided vs One-Sided Tests",
    "text": "Understanding Two-Sided vs One-Sided Tests\nWhen to use which test?\nThe choice between two-sided and one-sided tests depends on your research question and what you want to conclude.\nTwo-Sided Test (Most Common)\nSetup:\n\nH₀: β₂ = β₂*\nHₐ: β₂ ≠ β₂*\n\nUse when:\n\nYou want to detect any difference from the null value\nYou don’t have a strong prior about the direction\nYou want to be conservative (standard practice in research)\n\nRejection region: Both tails of the distribution\nExample: Does house size affect price?\n\nH₀: β₂ = 0 (no effect)\nHₐ: β₂ ≠ 0 (some effect, positive or negative)\nWe reject if the effect is either very positive or very negative\n\nOne-Sided Test (Directional)\nSetup (upper tail):\n\nH₀: β₂ ≤ β₂*\nHₐ: β₂ &gt; β₂*\n\nSetup (lower tail):\n\nH₀: β₂ ≥ β₂*\nHₐ: β₂ &lt; β₂*\n\nUse when:\n\nYou have a specific directional hypothesis\nEconomic theory predicts a specific direction\nYou only care about deviations in one direction\n\nRejection region: One tail of the distribution\nExample: Does house size increase price by less than $90/sq ft?\n\nClaim: β₂ &lt; 90\nH₀: β₂ ≥ 90 (null is opposite of claim)\nHₐ: β₂ &lt; 90 (this is what we want to prove)\nWe only reject if the effect is significantly below 90\n\nMathematical relationship:\nFor the same test statistic t:\n\nTwo-sided p-value = 2 × Pr[T &gt; |t|]\nOne-sided p-value = Pr[T &gt; t] (upper tail) or Pr[T &lt; t] (lower tail)\nOne-sided p-value = (Two-sided p-value) / 2 (if sign is correct)\n\nImportant considerations:\n1. Direction matters for one-sided tests\n\nIf testing Hₐ: β₂ &gt; β₂* but get b₂ &lt; β₂*, you CANNOT reject H₀\nThe data contradicts your hypothesis, so rejection is impossible\nIn this case, one-sided p-value &gt; 0.5\n\n2. Two-sided tests are safer\n\nAcademic journals typically require two-sided tests\nAvoids “fishing” for significant results\nMore conservative approach\n\n3. One-sided tests have more power\n\nFor the same significance level α, easier to reject in the predicted direction\nCritical value is smaller: t(n-2, α) vs t(n-2, α/2)\nExample: For α = 0.05 and df = 27\n\nTwo-sided: t(27, 0.025) = 2.052\nOne-sided: t(27, 0.05) = 1.703\n\n\nPractical example from our house price data:\nQuestion 1: Does size affect price? (Two-sided)\n\nH₀: β₂ = 0 vs Hₐ: β₂ ≠ 0\nt = 6.60, p-value = 0.000 (two-sided)\nConclusion: Reject H₀, size affects price\n\nQuestion 2: Does size increase price? (One-sided)\n\nH₀: β₂ ≤ 0 vs Hₐ: β₂ &gt; 0\nt = 6.60, p-value = 0.000 / 2 = 0.000 (one-sided)\nConclusion: Reject H₀, size increases price\n\nQuestion 3: Does size increase price by less than $90/sq ft? (One-sided)\n\nH₀: β₂ ≥ 90 vs Hₐ: β₂ &lt; 90\nt = (73.77 - 90) / 11.17 = -1.452\np-value = 0.079 (one-sided, lower tail)\nConclusion: Fail to reject H₀ at α = 0.05 (but would reject at α = 0.10)\n\nDecision rule:\nUse two-sided tests unless:\n\nYou have strong theoretical reasons for a directional hypothesis\nYou specified the direction before seeing the data\nYou only care about deviations in one direction (rare in economics)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#tests-of-statistical-significance",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#tests-of-statistical-significance",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.4 Tests of Statistical Significance",
    "text": "7.4 Tests of Statistical Significance\nA regressor \\(x\\) has no relationship with \\(y\\) if \\(\\beta_2 = 0\\).\nTest of statistical significance (two-sided test):\n\\[H_0: \\beta_2 = 0 \\quad \\text{vs.} \\quad H_a: \\beta_2 \\neq 0\\]\nTest statistic:\n\\[t = \\frac{b_2}{se(b_2)} \\sim T(n-2)\\]\nDecision rules:\n\np-value approach: Reject \\(H_0\\) at level \\(\\alpha\\) if \\(p = Pr[|T_{n-2}| &gt; |t|] &lt; \\alpha\\)\nCritical value approach: Reject \\(H_0\\) at level \\(\\alpha\\) if \\(|t| &gt; c = t_{n-2, \\alpha/2}\\)\n\nFor the house price example:\n\n\\(t = 73.77 / 11.17 = 6.60\\)\n\\(p = Pr[|T_{27}| &gt; 6.60] \\approx 0.000\\)\nCritical value: \\(c = t_{27, 0.025} = 2.052\\)\nSince \\(|t| = 6.60 &gt; 2.052\\), reject \\(H_0\\)\nConclusion: House size is statistically significant at the 5% level\n\n\nprint(\"=\" * 70)\nprint(\"7.4 TESTS OF STATISTICAL SIGNIFICANCE\")\nprint(\"=\" * 70)\n\nprint(\"\\nNull hypothesis: β₂ = 0 (size has no effect on price)\")\nprint(f\"t-statistic: {t_stat_size:.4f}\")\nprint(f\"p-value: {p_value_size:.6f}\")\nprint(f\"Critical value (α=0.05): ±{t_crit:.4f}\")\n\nif p_value_size &lt; 0.05:\n    print(\"\\nResult: Reject H₀ at 5% significance level\")\n    print(\"Conclusion: Size has a statistically significant effect on price\")\nelse:\n    print(\"\\nResult: Fail to reject H₀ at 5% significance level\")\n\nprint(\"\\nNote: Statistical significance ≠ Economic significance\")\nprint(\"  - Statistical significance depends on t = b₂ / se(b₂)\")\nprint(\"  - Economic significance depends directly on the size of b₂\")\nprint(\"  - With large samples, even small b₂ can be statistically significant\")\n\n======================================================================\n7.4 TESTS OF STATISTICAL SIGNIFICANCE\n======================================================================\n\nNull hypothesis: β₂ = 0 (size has no effect on price)\nt-statistic: 6.6015\np-value: 0.000000\nCritical value (α=0.05): ±2.0518\n\nResult: Reject H₀ at 5% significance level\nConclusion: Size has a statistically significant effect on price\n\nNote: Statistical significance ≠ Economic significance\n  - Statistical significance depends on t = b₂ / se(b₂)\n  - Economic significance depends directly on the size of b₂\n  - With large samples, even small b₂ can be statistically significant\n\n\n\nKey Concept 7.6: Heteroskedasticity and Robust Standard Errors\nHeteroskedasticity occurs when the error variance is not constant across observations: Var[u_i | x_i] = σ²_i (varies with i).\nHomoskedasticity assumption (Assumption 3): Var[u_i | x_i] = σ²_u (constant)\nWhy heteroskedasticity matters: - Coefficient estimates (b₂): Still unbiased - Standard errors: WRONG (biased) - t-statistics, p-values, CIs: All invalid\nSolution: Heteroskedasticity-robust standard errors - Valid whether or not heteroskedasticity exists - No need to test for heteroskedasticity first - Modern best practice for cross-sectional data\nPractical impact: - Robust SEs usually larger → more conservative inference - Protects against false positives (Type I errors) - Sometimes smaller → gain power\nTypes of robust SEs: - HC (Heteroskedasticity-Consistent): Cross-sectional data - HAC (Heteroskedasticity and Autocorrelation Consistent): Time series - Cluster-robust: Grouped/clustered data\nBottom line: Always report heteroskedasticity-robust standard errors for cross-sectional data. They’re free insurance against model misspecification.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#two-sided-hypothesis-tests",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#two-sided-hypothesis-tests",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.5 Two-Sided Hypothesis Tests",
    "text": "7.5 Two-Sided Hypothesis Tests\nSometimes we want to test whether the slope equals a specific non-zero value.\nGeneral two-sided test:\n\\[H_0: \\beta_2 = \\beta_2^* \\quad \\text{vs.} \\quad H_a: \\beta_2 \\neq \\beta_2^*\\]\nTest statistic:\n\\[t = \\frac{b_2 - \\beta_2^*}{se(b_2)} \\sim T(n-2)\\]\nDecision rules:\n\np-value approach: Reject if \\(p = Pr[|T_{n-2}| &gt; |t|] &lt; \\alpha\\)\nCritical value approach: Reject if \\(|t| &gt; t_{n-2, \\alpha/2}\\)\n\nExample: Test whether house price increases by $90 per square foot.\n\\[t = \\frac{73.77 - 90}{11.17} = -1.452\\]\n\n\\(p = Pr[|T_{27}| &gt; 1.452] = 0.158\\)\nSince \\(p = 0.158 &gt; 0.05\\), do not reject \\(H_0\\)\nConclusion: The data are consistent with \\(\\beta_2 = 90\\)\n\nRelationship to confidence intervals:\n\nIf \\(\\beta_2^*\\) falls inside the 95% CI, do not reject \\(H_0\\) at 5% level\nSince 90 is inside [50.84, 96.70], we do not reject\n\n\nprint(\"=\" * 70)\nprint(\"7.5 TWO-SIDED HYPOTHESIS TESTS\")\nprint(\"=\" * 70)\n\n# Test H₀: β₂ = 90 vs H₁: β₂ ≠ 90\nnull_value = 90\nt_stat_90 = (coef_size - null_value) / se_size\np_value_90 = 2 * (1 - stats.t.cdf(abs(t_stat_90), df))\nt_crit_90 = stats.t.ppf(0.975, df)\n\nprint(f\"\\nTest: H₀: β₂ = {null_value} vs H₁: β₂ ≠ {null_value}\")\nprint(f\"  t-statistic: {t_stat_90:.4f}\")\nprint(f\"  p-value: {p_value_90:.6f}\")\nprint(f\"  Critical value (α=0.05): ±{t_crit_90:.4f}\")\n\nif abs(t_stat_90) &gt; t_crit_90:\n    print(f\"\\nResult: Reject H₀ (|t| = {abs(t_stat_90):.4f} &gt; {t_crit_90:.4f})\")\nelse:\n    print(f\"\\nResult: Fail to reject H₀ (|t| = {abs(t_stat_90):.4f} &lt; {t_crit_90:.4f})\")\n    print(f\"Conclusion: The data are consistent with β₂ = {null_value}\")\n\nprint(f\"\\n95% CI for β₂: [{ci_lower:.2f}, {ci_upper:.2f}]\")\nprint(f\"Since {null_value} is inside the CI, we do not reject H₀.\")\n\n======================================================================\n7.5 TWO-SIDED HYPOTHESIS TESTS\n======================================================================\n\nTest: H₀: β₂ = 90 vs H₁: β₂ ≠ 90\n  t-statistic: -1.4523\n  p-value: 0.157950\n  Critical value (α=0.05): ±2.0518\n\nResult: Fail to reject H₀ (|t| = 1.4523 &lt; 2.0518)\nConclusion: The data are consistent with β₂ = 90\n\n95% CI for β₂: [50.84, 96.70]\nSince 90 is inside the CI, we do not reject H₀.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#hypothesis-test-using-statsmodels",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#hypothesis-test-using-statsmodels",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Hypothesis Test Using statsmodels",
    "text": "Hypothesis Test Using statsmodels\nPython’s statsmodels package provides convenient methods for hypothesis testing.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#why-robust-standard-errors-matter",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#why-robust-standard-errors-matter",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Why Robust Standard Errors Matter",
    "text": "Why Robust Standard Errors Matter\nThe problem with default standard errors:\nDefault (classical) standard errors rely on a strong assumption:\nAssumption 3 (Homoskedasticity): Var[u_i | x_i] = σ²_u (constant variance)\nThis means:\n\nThe spread of errors is the same for all values of x\nResiduals should have constant variance across fitted values\nIn reality, this assumption is often violated\n\nWhat is heteroskedasticity?\nHeteroskedasticity means the error variance changes with x:\n\nVar[u_i | x_i] = σ²_i (varies with i)\nCommon in cross-sectional data\nExamples:\nIncome variation increases with education level\nSales variance increases with firm size\nMedical costs vary more for older patients\n\nVisual detection:\nCheck the residual plot (residuals vs fitted values):\n\nHomoskedasticity: Random scatter, constant spread\nHeteroskedasticity: Funnel shape, increasing/decreasing spread\n\nConsequences of heteroskedasticity:\nIf heteroskedasticity is present but you use default SEs:\n\nCoefficient estimates (b₂): Still unbiased and consistent\nStandard errors: WRONG (biased)\nt-statistics: WRONG (biased)\np-values: WRONG (invalid)\nConfidence intervals: WRONG (invalid coverage)\n\nDirection of bias:\n\nHeteroskedasticity can cause SEs to be too small or too large\nCannot predict direction without knowing the form of heteroskedasticity\nCould lead to false significance or miss true significance\n\nThe solution: Robust standard errors\nHeteroskedasticity-robust standard errors (HC, White, Huber-White):\n\nValid whether or not heteroskedasticity is present\nAutomatically adjust for non-constant variance\nNo need to test for heteroskedasticity first\nModern best practice: Always use for cross-sectional data\n\nHow they work:\nDefault SE formula (assumes homoskedasticity): \\[se(b_2) = \\frac{\\sigma_u}{\\sqrt{\\sum(x_i - \\bar{x})^2}}\\]\nRobust SE formula (allows heteroskedasticity): \\[se_{robust}(b_2) = \\frac{\\sqrt{\\sum e_i^2 (x_i - \\bar{x})^2}}{\\sum(x_i - \\bar{x})^2}\\]\nKey difference: Uses individual residuals e²_i instead of pooled variance σ²_u\nPractical implications:\n\nCoefficient estimates unchanged\n\n\nOnly standard errors change\nPoint estimates remain the same\n\n\nStandard errors typically larger\n\n\nRobust SEs are usually (but not always) bigger\nMore conservative inference\nWider confidence intervals\n\n\nt-statistics typically smaller\n\n\nLess likely to find statistical significance\nProtects against false positives\n\n\nResults may change qualitative conclusions\n\n\nVariable significant with default SEs might become insignificant\nOr the reverse (if robust SEs are smaller)\n\nTypes of robust standard errors:\nDifferent formulas for finite-sample adjustment:\n\nHC0: Basic White formula\nHC1: Degrees of freedom correction (n/(n-k)), most common in Stata\nHC2: Leverage-weighted\nHC3: Even more conservative\n\nPython default: HC1 (matches Stata’s “robust” option)\nFor our house price example:\n\n\n\nCoefficient\nStandard SE\nRobust SE\n% Change\n\n\n\n\nsize\n11.17\n11.33\n+1.4%\n\n\nIntercept\n21,489\n21,825\n+1.6%\n\n\n\n\nDifference is small (good news!)\nSuggests heteroskedasticity is not severe\nBut always report robust SEs as standard practice\n\nOther types of robust SEs:\n\nHAC (Heteroskedasticity and Autocorrelation Consistent)\n\n\nFor time series data\nAccounts for both heteroskedasticity and serial correlation\n\n\nCluster-robust\n\n\nFor clustered/grouped data\nStudents within schools, patients within hospitals\nAccounts for within-cluster correlation\n\nBest practices:\n\nCross-sectional data: Always use heteroskedasticity-robust SEs\nTime series data: Use HAC robust SEs (Newey-West)\nPanel/clustered data: Use cluster-robust SEs\nWhen in doubt: Use robust SEs (can’t hurt, might help)\n\nBottom line: Robust standard errors provide valid inference under weaker assumptions. They’re essentially free insurance against heteroskedasticity.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#comparing-standard-vs-robust-standard-errors-a-practical-guide",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#comparing-standard-vs-robust-standard-errors-a-practical-guide",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Comparing Standard vs Robust Standard Errors: A Practical Guide",
    "text": "Comparing Standard vs Robust Standard Errors: A Practical Guide\nHow to interpret the comparison:\nWhen you compute both standard and robust SEs, you’re essentially running two different analyses:\nStandard SEs (Classical):\n\nAssumes: Homoskedasticity (constant variance)\nValid only if: Assumption 3 holds\nInterpretation: “If errors have constant variance, here’s the uncertainty”\n\nRobust SEs (Heteroskedasticity-Consistent):\n\nAssumes: Heteroskedasticity allowed (no constant variance assumption)\nValid: Whether or not heteroskedasticity exists\nInterpretation: “Here’s the uncertainty, accounting for possible non-constant variance”\n\nWhat do differences tell us?\nCase 1: Robust SE ≈ Standard SE (difference &lt; 10%)\n\nExample: Our house price data (11.33 vs 11.17)\nInterpretation: Little evidence of heteroskedasticity\nImplication: Assumption 3 approximately holds\nDecision: Still report robust SEs (good practice)\n\nCase 2: Robust SE &gt; Standard SE (difference &gt; 20%)\n\nExample: Robust SE = 15.0, Standard SE = 10.0\nInterpretation: Evidence of heteroskedasticity\nImplication: Standard SEs underestimate uncertainty\nConsequence: Default t-stats too large, p-values too small\nRisk: False positives (finding significance that isn’t real)\nDecision: MUST use robust SEs\n\nCase 3: Robust SE &lt; Standard SE (less common)\n\nExample: Robust SE = 8.0, Standard SE = 10.0\nInterpretation: Specific pattern of heteroskedasticity\nImplication: Standard SEs overestimate uncertainty\nConsequence: Default t-stats too small, p-values too large\nRisk: False negatives (missing real significance)\nDecision: Use robust SEs (more powerful)\n\nVisual comparison for our house price data:\nVariable: size\n Standard SE: 11.17 |████████████████████|\n Robust SE: 11.33 |█████████████████████| (+1.4%)\n \nVariable: Intercept\n Standard SE: 21,489 |████████████████████|\n Robust SE: 21,825 |█████████████████████| (+1.6%)\nImpact on inference:\nLet’s see how the choice affects our conclusions:\n\n\n\n\n\n\n\n\n\nTest\nStandard SE\nRobust SE\nConclusion\n\n\n\n\nt-statistic\n73.77/11.17 = 6.60\n73.77/11.33 = 6.51\nBoth highly significant\n\n\np-value\n0.0000\n0.0000\nBoth p &lt; 0.001\n\n\n95% CI\n[50.84, 96.70]\n[50.33, 97.21]\nNearly identical\n\n\nReject H₀?\nYES\nYES\nSame conclusion\n\n\n\nWhen robust SEs matter most:\n\nLarge sample differences\n\n\nIf robust SE = 2 × standard SE\nSignificance can disappear\nExample: t = 2.5 (p=0.02) → t = 1.25 (p=0.22)\n\n\nBorderline significance\n\n\nIf p-value near 0.05 with standard SEs\nMight become non-significant with robust SEs\nExample: p = 0.04 → p = 0.07\n\n\nHigh-stakes decisions\n\n\nPolicy recommendations\nMedical treatments\nFinancial investments\nNeed valid inference\n\nReading regression output in practice:\nMost statistical software now reports both:\n Standard Robust\nVariable Coef. SE t SE t\nsize 73.77 11.17 6.60 11.33 6.51\nWhat to report in your paper:\nMinimum: Robust standard errors in main results Better: Show both, explain differences if substantial Best: Report robust SEs, note “Results similar with classical SEs”\nExample write-up:\n“We find that each additional square foot increases house price by $73.77 (robust SE = 11.33, t = 6.51, p &lt; 0.001). The heteroskedasticity-robust standard error is nearly identical to the classical standard error (11.17), suggesting homoskedasticity is approximately satisfied. The effect is highly statistically significant and economically meaningful, with a 95% confidence interval of [$50.33, $97.21].”\nCommon mistakes to avoid:\n\nUsing standard SEs for cross-sectional data\n\n\nModern practice: Always use robust SEs\n\n\nTesting for heteroskedasticity first\n\n\nJust use robust SEs (they’re valid either way)\nPre-testing affects inference in complex ways\n\n\nSwitching between standard and robust to get significance\n\n\nThis is p-hacking\nChoose robust SEs before looking at results\n\n\nIgnoring large differences\n\n\nIf robust SE &gt;&gt; standard SE, investigate\nMight indicate model misspecification\nConsider transformations or different functional forms\n\nThe bottom line:\nThink of robust SEs as the “safe” choice:\n\nIf no heteroskedasticity: Robust SEs ≈ Standard SEs (no harm done)\nIf heteroskedasticity exists: Robust SEs correct the problem (saved you!)\n\nIt’s like wearing a seatbelt: doesn’t hurt if you don’t crash, saves you if you do.\n\nprint(\"=\" * 70)\nprint(\"Hypothesis test using statsmodels:\")\nprint(\"=\" * 70)\n\n# Alternative approach using t_test\nhypothesis = f'size = {null_value}'\nt_test_result = model_basic.t_test(hypothesis)\nprint(t_test_result)\n\nprint(\"\\nThis confirms our manual calculation.\")\n\n======================================================================\nHypothesis test using statsmodels:\n======================================================================\n                             Test for Constraints                             \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nc0            73.7710     11.175     -1.452      0.158      50.842      96.700\n==============================================================================\n\nThis confirms our manual calculation.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#one-sided-directional-hypothesis-tests",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#one-sided-directional-hypothesis-tests",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.6 One-Sided Directional Hypothesis Tests",
    "text": "7.6 One-Sided Directional Hypothesis Tests\nSometimes we have a directional hypothesis (greater than or less than).\nOne-sided tests:\n\nUpper one-tailed: \\(H_0: \\beta_2 \\leq \\beta_2^*\\) vs \\(H_a: \\beta_2 &gt; \\beta_2^*\\)\n\nReject in the right tail: \\(p = Pr[T_{n-2} &gt; t]\\)\n\nLower one-tailed: \\(H_0: \\beta_2 \\geq \\beta_2^*\\) vs \\(H_a: \\beta_2 &lt; \\beta_2^*\\)\n\nReject in the left tail: \\(p = Pr[T_{n-2} &lt; t]\\)\n\n\nExample: Test whether house price rises by less than $90 per square foot.\n\nClaim: \\(\\beta_2 &lt; 90\\)\nTest: \\(H_0: \\beta_2 \\geq 90\\) vs \\(H_a: \\beta_2 &lt; 90\\) (lower-tailed)\n\\(t = (73.77 - 90) / 11.17 = -1.452\\)\n\\(p = Pr[T_{27} &lt; -1.452] = 0.079\\)\nSince \\(p = 0.079 &gt; 0.05\\), do not reject \\(H_0\\) at 5% level\nConclusion: Not enough evidence to support the claim at 5% level\n\nNote on computer output:\n\nComputer gives p-value for two-sided test of \\(H_0: \\beta_2 = 0\\)\nFor one-sided test of significance:\n\nIf \\(b_2\\) has expected sign, halve the printed p-value\nIf \\(b_2\\) has wrong sign, reject is not possible (p &gt; 0.5)\n\n\n\nprint(\"=\" * 70)\nprint(\"7.6 ONE-SIDED DIRECTIONAL HYPOTHESIS TESTS\")\nprint(\"=\" * 70)\n\n# Upper one-tailed test: H₀: β₂ ≤ 90 vs H₁: β₂ &gt; 90\np_value_upper = 1 - stats.t.cdf(t_stat_90, df)\nt_crit_upper = stats.t.ppf(0.95, df)\n\nprint(f\"\\nUpper one-tailed test: H₀: β₂ ≤ {null_value} vs H₁: β₂ &gt; {null_value}\")\nprint(f\"  t-statistic: {t_stat_90:.4f}\")\nprint(f\"  p-value (one-tailed): {p_value_upper:.6f}\")\nprint(f\"  Critical value (α=0.05): {t_crit_upper:.4f}\")\n\nif t_stat_90 &gt; t_crit_upper:\n    print(\"  Result: Reject H₀\")\nelse:\n    print(\"  Result: Fail to reject H₀\")\n\n# Lower one-tailed test: H₀: β₂ ≥ 90 vs H₁: β₂ &lt; 90\np_value_lower = stats.t.cdf(t_stat_90, df)\n\nprint(f\"\\nLower one-tailed test: H₀: β₂ ≥ {null_value} vs H₁: β₂ &lt; {null_value}\")\nprint(f\"  t-statistic: {t_stat_90:.4f}\")\nprint(f\"  p-value (one-tailed): {p_value_lower:.6f}\")\nprint(f\"  Critical value (α=0.05): {-t_crit_upper:.4f}\")\n\nif t_stat_90 &lt; -t_crit_upper:\n    print(\"  Result: Reject H₀\")\n    print(\"  Conclusion: There is evidence that β₂ &lt; 90\")\nelse:\n    print(\"  Result: Fail to reject H₀\")\n    print(\"  Conclusion: Not enough evidence to support β₂ &lt; 90 at 5% level\")\n    print(f\"  (Would reject at 10% level since p = {p_value_lower:.3f} &lt; 0.10)\")\n\n======================================================================\n7.6 ONE-SIDED DIRECTIONAL HYPOTHESIS TESTS\n======================================================================\n\nUpper one-tailed test: H₀: β₂ ≤ 90 vs H₁: β₂ &gt; 90\n  t-statistic: -1.4523\n  p-value (one-tailed): 0.921025\n  Critical value (α=0.05): 1.7033\n  Result: Fail to reject H₀\n\nLower one-tailed test: H₀: β₂ ≥ 90 vs H₁: β₂ &lt; 90\n  t-statistic: -1.4523\n  p-value (one-tailed): 0.078975\n  Critical value (α=0.05): -1.7033\n  Result: Fail to reject H₀\n  Conclusion: Not enough evidence to support β₂ &lt; 90 at 5% level\n  (Would reject at 10% level since p = 0.079 &lt; 0.10)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#robust-standard-errors",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#robust-standard-errors",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.7 Robust Standard Errors",
    "text": "7.7 Robust Standard Errors\nDefault standard errors make assumptions 1-4. Robust standard errors relax some of these assumptions.\nHeteroskedasticity-Robust Standard Errors:\n\nDefault assumption (homoskedasticity): \\(Var[u_i | x_i] = \\sigma_u^2\\) (constant)\nRelaxed assumption (heteroskedasticity): \\(Var[u_i | x_i] = \\sigma_i^2\\) (varies with \\(i\\))\n\nWhy use robust standard errors?\n\nHeteroskedasticity is common in cross-sectional data\nDefault SEs are incorrect when heteroskedasticity is present\nRobust SEs are valid whether or not heteroskedasticity exists\nModern practice: Always report robust SEs for cross-sectional data\n\nHeteroskedastic-robust SE formula:\n\\[se_{het}(b_2) = \\frac{\\sqrt{\\sum_{i=1}^n e_i^2 (x_i - \\bar{x})^2}}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\nEffect on inference:\n\nCoefficient estimates unchanged\nStandard errors typically increase (more conservative)\nt-statistics typically decrease\nConfidence intervals typically wider\n\nOther robust SEs:\n\nHAC robust: For time series with autocorrelation\nCluster robust: For clustered data (students in schools, people in villages, etc.)\n\n\nprint(\"=\" * 70)\nprint(\"7.7 ROBUST STANDARD ERRORS\")\nprint(\"=\" * 70)\n\n# Get heteroskedasticity-robust standard errors (HC1)\nrobust_results = model_basic.get_robustcov_results(cov_type='HC1')\n\nprint(\"\\nComparison of standard and robust standard errors:\")\ncomparison_df = pd.DataFrame({\n    'Coefficient': model_basic.params,\n    'Std. Error': model_basic.bse,\n    'Robust SE': robust_results.bse,\n    't-stat (standard)': model_basic.tvalues,\n    't-stat (robust)': robust_results.tvalues,\n    'p-value (standard)': model_basic.pvalues,\n    'p-value (robust)': robust_results.pvalues\n})\nprint(comparison_df)\n\n======================================================================\n7.7 ROBUST STANDARD ERRORS\n======================================================================\n\nComparison of standard and robust standard errors:\n             Coefficient    Std. Error     Robust SE  t-stat (standard)  \\\nIntercept  115017.282609  21489.359861  20298.704493           5.352290   \nsize           73.771040     11.174911     11.329669           6.601488   \n\n           t-stat (robust)  p-value (standard)  p-value (robust)  \nIntercept         5.666238        1.183545e-05      5.120101e-06  \nsize              6.511315        4.408752e-07      5.564663e-07",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#full-regression-output-with-robust-standard-errors",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#full-regression-output-with-robust-standard-errors",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Full Regression Output with Robust Standard Errors",
    "text": "Full Regression Output with Robust Standard Errors\n\nprint(\"=\" * 70)\nprint(\"Regression with Robust Standard Errors:\")\nprint(\"=\" * 70)\nprint(robust_results.summary())\n\n# Robust confidence intervals\nrobust_conf_int = robust_results.conf_int(alpha=0.05)\nprint(\"\\n95% Confidence Intervals (Robust):\")\nprint(robust_conf_int)\n\nprint(\"\\nInterpretation:\")\nprint(\"  - For this dataset, robust and standard SEs are similar\")\nprint(\"  - Robust SE for size: 11.33 vs standard SE: 11.17\")\nprint(\"  - Robust 95% CI: [50.33, 97.02] vs standard: [50.84, 96.70]\")\nprint(\"  - Statistical significance unchanged (both highly significant)\")\nprint(\"\\nGeneral principle: Always report robust SEs for cross-sectional data\")\n\n======================================================================\nRegression with Robust Standard Errors:\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.617\nModel:                            OLS   Adj. R-squared:                  0.603\nMethod:                 Least Squares   F-statistic:                     42.40\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           5.56e-07\nTime:                        23:33:22   Log-Likelihood:                -332.05\nNo. Observations:                  29   AIC:                             668.1\nDf Residuals:                      27   BIC:                             670.8\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    1.15e+05   2.03e+04      5.666      0.000    7.34e+04    1.57e+05\nsize          73.7710     11.330      6.511      0.000      50.524      97.018\n==============================================================================\nOmnibus:                        0.576   Durbin-Watson:                   1.219\nProb(Omnibus):                  0.750   Jarque-Bera (JB):                0.638\nSkew:                          -0.078   Prob(JB):                        0.727\nKurtosis:                       2.290   Cond. No.                     9.45e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 9.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n95% Confidence Intervals (Robust):\n[[7.33677813e+04 1.56666784e+05]\n [5.05244796e+01 9.70176011e+01]]\n\nInterpretation:\n  - For this dataset, robust and standard SEs are similar\n  - Robust SE for size: 11.33 vs standard SE: 11.17\n  - Robust 95% CI: [50.33, 97.02] vs standard: [50.84, 96.70]\n  - Statistical significance unchanged (both highly significant)\n\nGeneral principle: Always report robust SEs for cross-sectional data",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#visualization-scatter-plot-with-regression-line",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#visualization-scatter-plot-with-regression-line",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Visualization: Scatter Plot with Regression Line",
    "text": "Visualization: Scatter Plot with Regression Line\nVisual representation of the bivariate regression relationship.\n\n# Figure 7.1: Scatter plot with regression line\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_house['size'], data_house['price'], alpha=0.6, s=50,\n           color='black', label='Actual observations')\nax.plot(data_house['size'], model_basic.fittedvalues, color='blue',\n        linewidth=2, label='Fitted regression line')\nax.set_xlabel('Size (square feet)', fontsize=12)\nax.set_ylabel('Price ($1000s)', fontsize=12)\nax.set_title('Figure 7.1: House Price vs Size with Regression Line',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"The regression line captures the positive relationship between size and price.\")\n\n\n\n\n\n\n\n\nThe regression line captures the positive relationship between size and price.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#visualization-confidence-intervals",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#visualization-confidence-intervals",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Visualization: Confidence Intervals",
    "text": "Visualization: Confidence Intervals\nGraphical display of coefficient estimates with confidence intervals.\n\n# Figure 7.2: Confidence interval visualization\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot point estimate and confidence interval\ncoef_names = ['Intercept', 'Size']\ncoefs = model_basic.params.values\nci_low = conf_int.iloc[:, 0].values\nci_high = conf_int.iloc[:, 1].values\n\ny_pos = np.arange(len(coef_names))\nax.errorbar(coefs, y_pos, xerr=[coefs - ci_low, ci_high - coefs],\n            fmt='o', markersize=8, capsize=5, capthick=2, linewidth=2, color='blue')\nax.set_yticks(y_pos)\nax.set_yticklabels(coef_names)\nax.axvline(x=0, color='red', linestyle='--', linewidth=1, alpha=0.5,\n           label='H₀: β = 0')\nax.set_xlabel('Coefficient Value', fontsize=12)\nax.set_title('Figure 7.2: Coefficient Estimates with 95% Confidence Intervals',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"Both coefficients are statistically significant (CIs exclude zero).\")\n\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_82940/1126987400.py:22: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  plt.tight_layout()\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nBoth coefficients are statistically significant (CIs exclude zero).",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#visualization-residual-plot",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#visualization-residual-plot",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Visualization: Residual Plot",
    "text": "Visualization: Residual Plot\nCheck for patterns in residuals that might violate model assumptions.\n\n# Figure 7.3: Residual plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(model_basic.fittedvalues, model_basic.resid, alpha=0.6, s=50, color='black')\nax.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero residual line')\nax.set_xlabel('Fitted values', fontsize=12)\nax.set_ylabel('Residuals', fontsize=12)\nax.set_title('Figure 7.3: Residual Plot', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Check for:\")\nprint(\"  - Heteroskedasticity: Non-constant variance (funnel shape)\")\nprint(\"  - Outliers: Points far from zero line\")\nprint(\"  - Non-linearity: Systematic patterns in residuals\")\n\n\n\n\n\n\n\n\nCheck for:\n  - Heteroskedasticity: Non-constant variance (funnel shape)\n  - Outliers: Points far from zero line\n  - Non-linearity: Systematic patterns in residuals",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#key-takeaways",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#key-takeaways",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nKey Takeaways:\n\nThe t-statistic is fundamental to statistical inference in regression:\n\nFormula: \\(t = \\frac{b_2 - \\beta_2}{se(b_2)} \\sim T(n-2)\\)\nParallels univariate inference: \\(t = \\frac{\\bar{x} - \\mu}{se(\\bar{x})} \\sim T(n-1)\\)\n\nConfidence intervals provide a range of plausible values:\n\nFormula: \\(b_2 \\pm t_{n-2, \\alpha/2} \\times se(b_2)\\)\n95% CI (approximate): \\(b_2 \\pm 2 \\times se(b_2)\\)\nInterpretation: 95% of such intervals will contain the true \\(\\beta_2\\)\n\nTests of statistical significance assess whether a regressor matters:\n\n\\(H_0: \\beta_2 = 0\\) vs \\(H_a: \\beta_2 \\neq 0\\)\nReject if \\(p\\)-value \\(&lt; \\alpha\\) or \\(|t| &gt; t_{n-2, \\alpha/2}\\)\nStatistical significance \\(\\neq\\) economic significance\n\nTwo-sided tests for specific values:\n\n\\(H_0: \\beta_2 = \\beta_2^*\\) vs \\(H_a: \\beta_2 \\neq \\beta_2^*\\)\nEquivalent to checking if \\(\\beta_2^*\\) is in the confidence interval\n\nOne-sided tests for directional hypotheses:\n\nUpper tail: \\(H_0: \\beta_2 \\leq \\beta_2^*\\) vs \\(H_a: \\beta_2 &gt; \\beta_2^*\\)\nLower tail: \\(H_0: \\beta_2 \\geq \\beta_2^*\\) vs \\(H_a: \\beta_2 &lt; \\beta_2^*\\)\np-value is half the two-sided p-value (if sign is correct)\n\nRobust standard errors provide valid inference under weaker assumptions:\n\nHeteroskedasticity-robust (HC1): Relaxes constant variance assumption\nModern practice: Always use for cross-sectional data\nOther types: HAC (time series), cluster (grouped data)\n\n\nModel Assumptions (1-4):\n\nLinearity: \\(y = \\beta_1 + \\beta_2 x + u\\)\nZero conditional mean: \\(E[u_i | x_i] = 0\\)\nConstant variance: \\(Var[u_i | x_i] = \\sigma_u^2\\) (relaxed with robust SEs)\nIndependence: \\(u_i\\) independent of \\(u_j\\)\n\nHouse Price Example Results:\n\nCoefficient: Each additional sq ft increases price by $73.77\n95% CI: [$50.84, $96.70]\nHighly statistically significant (t = 6.60, p &lt; 0.001)\nEconomically meaningful effect\n\nPython Tools Used:\n\npandas: Data manipulation and summary statistics\nstatsmodels: Econometric estimation and hypothesis testing\nscipy.stats: Statistical distributions and critical values\nmatplotlib & seaborn: Visualization\n\nPractical Skills Developed:\n\nConducting t-tests on regression coefficients\nConstructing and interpreting confidence intervals\nTesting economic hypotheses (two-sided and one-sided)\nUsing robust standard errors for valid inference\nVisualizing regression results\nDistinguishing statistical from economic significance\n\nNext Steps:\n\nExtend to multiple regression (Chapter 11)\nLearn about model specification and variable selection\nStudy violations of assumptions and diagnostics\nPractice with different datasets and research questions\n\n\nCongratulations! You’ve completed Chapter 7. You now have both theoretical understanding and practical skills in conducting statistical inference for bivariate regression models!",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#practice-exercises",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#practice-exercises",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Understanding Standard Errors\nSuppose you estimate a regression of wages on education using data from 100 workers. The estimated slope is b₂ = 5.2 (each additional year of education increases wages by $5.20/hour) with standard error se(b₂) = 1.8.\n\nCalculate the 95% confidence interval for β₂ (use t-critical value ≈ 2.0).\n\nInterpret the confidence interval in plain language.\n\nIf the sample size were 400 instead of 100, how would the standard error change (approximately)? Explain why.\n\nExercise 2: Hypothesis Testing Mechanics\nUsing the wage-education regression from Exercise 1 (b₂ = 5.2, se(b₂) = 1.8):\n\nTest H₀: β₂ = 0 vs. Hₐ: β₂ ≠ 0 at the 5% level. Calculate the t-statistic and state your conclusion.\n\nCalculate the approximate p-value for this test.\n\nWould you reject H₀ at the 1% level? Explain.\n\nExercise 3: Two-Sided Tests\nA researcher claims that each year of education increases wages by exactly $6.00/hour. Using the regression from Exercise 1:\n\nState the null and alternative hypotheses to test this claim.\n\nCalculate the test statistic.\n\nAt the 5% level, do you reject the researcher’s claim? Explain your reasoning.\n\nIs the claim consistent with the 95% confidence interval from Exercise 1?\n\nExercise 4: One-Sided Tests\nA policy analyst claims that education increases wages by less than $3.00/hour. Using the regression from Exercise 1:\n\nState the appropriate one-sided hypotheses (Hint: The claim becomes Hₐ).\n\nCalculate the test statistic.\n\nFind the one-sided p-value (using t ≈ normal approximation).\n\nAt the 5% level, do you reject H₀? What do you conclude about the analyst’s claim?\n\nExercise 5: Statistical vs. Economic Significance\nConsider two regressions:\nRegression A: n = 50, b₂ = 0.05, se(b₂) = 0.10\nRegression B: n = 10,000, b₂ = 0.05, se(b₂) = 0.01\n\nFor each regression, test H₀: β₂ = 0 at the 5% level. What do you conclude?\n\nDespite having the same coefficient (0.05), why do the conclusions differ?\n\nIn which regression is the effect more economically significant? Explain.\n\nExercise 6: Confidence Interval Properties\nTrue or False? Explain your reasoning for each:\n\nA 95% confidence interval means there’s a 95% probability that β₂ is in the interval.\n\nIf we reject H₀: β₂ = β₂* at the 5% level, then β₂* will be outside the 95% CI.\n\nWider confidence intervals are always worse than narrow ones.\n\nWith n = 30, a 95% CI uses a t-critical value of approximately 2.0.\n\nExercise 7: Robust Standard Errors\nA researcher estimates a house price regression using n = 100 houses:\n\nStandard SE: 15.0\nRobust SE: 22.5\n\n\nWhat does the large difference suggest about the data?\n\nWhich standard error should be reported? Why?\n\nHow does the difference affect the t-statistic and hypothesis tests?\n\nIs heteroskedasticity a problem for the coefficient estimates themselves? Explain.\n\nExercise 8: Comprehensive Analysis\nYou’re analyzing the effect of advertising spending (x, in thousands of dollars) on sales revenue (y, in millions of dollars) using data from 80 firms. Your regression output shows:\n\nIntercept: 2.5, SE = 0.8\nSlope (advertising): 3.2, SE = 0.9\n\nR² = 0.45\nn = 80\n\n\nConstruct a 95% confidence interval for the slope coefficient.\n\nTest whether advertising has a statistically significant effect on sales at the 5% level.\n\nA colleague claims that $1,000 in advertising spending increases sales by more than $5 million. Test this claim using a one-sided test at the 5% level.\n\nIs the effect of advertising economically significant? Consider that the average firm in the sample spends $10,000 on advertising and has $5 million in sales revenue.\n\n\nHint for all exercises: When degrees of freedom are large (n &gt; 30), you can use t-critical values of approximately 1.645 (10% level), 1.96 (5% level), and 2.576 (1% level).",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#case-studies",
    "href": "../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html#case-studies",
    "title": "Chapter 7: Statistical Inference for Bivariate Regression",
    "section": "7.8 Case Studies",
    "text": "7.8 Case Studies\n\nCase Study 1: Testing Convergence Hypotheses\nIn this case study, you’ll apply statistical inference concepts to test economic hypotheses about productivity convergence across countries. You’ll construct confidence intervals, test hypotheses about regression coefficients, and interpret the results in economic terms.\nResearch Question: Does the relationship between labor productivity and capital per worker support convergence theory predictions?\nBackground:\nEconomic growth theory suggests that countries with lower initial capital should experience faster productivity growth as they “catch up” to richer countries. We can test this using the relationship between productivity levels and capital intensity across countries.\nDataset:\nWe’ll use the Convergence Clubs dataset (Mendez 2020) containing data for 108 countries in 2014:\n\nproductivity: Real GDP per capita (thousands of dollars)\ncapital: Physical capital per worker (thousands of dollars)\n\nEconometric Model:\n\\[\\text{productivity}_i = \\beta_1 + \\beta_2 \\times \\text{capital}_i + u_i\\]\nKey Hypotheses to Test:\n\nDoes capital significantly affect productivity? (H₀: β₂ = 0)\nIs the effect economically meaningful?\nDoes the relationship differ across country income groups?\n\n\nLearning Objectives:\n\nConstruct and interpret confidence intervals for regression coefficients\nConduct two-sided and one-sided hypothesis tests\nCompare statistical and economic significance\nUse robust standard errors appropriately\nApply hypothesis testing to economic questions\n\n\nKey Concept 7.7: Economic Convergence and Statistical Testing\nConvergence theory predicts that poorer countries should grow faster than richer countries, leading to a narrowing of income gaps over time. Two main types:\nAbsolute convergence: - All countries converge to the same income level - Predicts negative relationship between initial income and growth rate - Test: H₀: β (growth on initial income) ≥ 0 vs. Hₐ: β &lt; 0\nConditional convergence:\n- Countries converge to their own steady states - After controlling for determinants (savings, education, institutions) - More empirically supported than absolute convergence\nTesting convergence hypotheses: - Requires careful hypothesis formulation - One-sided tests appropriate (theory predicts direction) - Economic significance matters (how fast is convergence?) - Must account for heteroskedasticity (countries differ in volatility)\nOur approach: We examine the cross-sectional relationship between productivity and capital intensity to understand the fundamentals underlying convergence patterns.\n\n\nprint(\"=\" * 70)\nprint(\"CASE STUDY: TESTING CONVERGENCE HYPOTHESES\")\nprint(\"=\" * 70)\n\n# Read convergence clubs data\ndata_convergence = pd.read_stata(GITHUB_DATA_URL + 'AED_ConvergenceClubs.dta')\n\n# Filter to 2014 cross-section\ndata_2014 = data_convergence[data_convergence['year'] == 2014].copy()\n\n# Create productivity and capital variables (divide by 1000 for better scale)\ndata_2014['productivity'] = data_2014['rgdppc'] / 1000  # GDP per capita in thousands\ndata_2014['capital'] = data_2014['rk'] / 1000  # Capital per worker in thousands\n\nprint(f\"\\nSample: {len(data_2014)} countries in 2014\")\nprint(\"\\nVariable summary:\")\nprint(data_2014[['productivity', 'capital']].describe())\n\n# Show a few countries\nprint(\"\\nSample countries:\")\nsample_countries = data_2014[['country', 'productivity', 'capital']].sort_values('productivity', ascending=False).head(10)\nprint(sample_countries.to_string(index=False))\n\n\n\nTask 1: Estimate the Productivity-Capital Relationship (Guided)\nObjective: Estimate the bivariate regression and interpret the results.\nYour task:\nRun the code below to estimate the regression and examine the output. Then answer:\n\nWhat is the estimated effect of capital on productivity? Interpret the coefficient.\n\nConstruct a 95% confidence interval for β₂. What does this interval tell you?\n\nIs the effect statistically significant at the 5% level?\n\nIs the effect economically significant? (Consider that the average country has capital = 50 and productivity = 25)\n\n\n# Task 1: Basic regression\nmodel_convergence = ols('productivity ~ capital', data=data_2014).fit()\n\nprint(\"\\nTask 1: Regression of Productivity on Capital\")\nprint(\"=\" * 60)\nprint(model_convergence.summary())\n\n# Extract key statistics\nbeta2 = model_convergence.params['capital']\nse_beta2 = model_convergence.bse['capital']\nt_stat = model_convergence.tvalues['capital']\np_value = model_convergence.pvalues['capital']\n\nprint(f\"\\nKey Statistics:\")\nprint(f\"  Coefficient (β₂): {beta2:.4f}\")\nprint(f\"  Standard Error: {se_beta2:.4f}\")\nprint(f\"  t-statistic: {t_stat:.2f}\")\nprint(f\"  p-value: {p_value:.6f}\")\n\n# 95% Confidence Interval\nci_95 = model_convergence.conf_int(alpha=0.05).loc['capital']\nprint(f\"\\n95% Confidence Interval: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n\nprint(\"\\nInterpretation:\")\nprint(f\"  Each additional $1,000 in capital per worker is associated with\")\nprint(f\"  an increase of ${beta2:.2f}k in labor productivity (GDP per capita).\")\n\n\n\nTask 2: Test Specific Hypotheses (Semi-guided)\nObjective: Conduct formal hypothesis tests about the productivity-capital relationship.\nYour tasks:\n\nTest for statistical significance:\n\nH₀: β₂ = 0 (capital has no effect) vs. Hₐ: β₂ ≠ 0\n\nUse the t-statistic and p-value from Task 1\n\nState your conclusion at the 5% level\n\nTest economic theory prediction:\n\nEconomic theory suggests that the marginal product of capital in developing countries might be around 0.5 (50 cents of GDP per dollar of capital)\n\nH₀: β₂ = 0.5 vs. Hₐ: β₂ ≠ 0.5\n\nCalculate the test statistic: t = (β₂ - 0.5) / se(β₂)\n\nFind the p-value and state your conclusion\n\nTest pessimistic hypothesis:\n\nA pessimist claims that capital contributes less than $0.30 to productivity\n\nFormulate appropriate one-sided hypotheses\n\nCalculate the test statistic and one-sided p-value\n\nDo you reject the pessimist’s claim at the 5% level?\n\n\nCode hints:\n# For part (b)\nt_test_05 = (beta2 - 0.5) / se_beta2\np_value_05 = 2 * (1 - stats.t.cdf(abs(t_test_05), df=model_convergence.df_resid))\n\n# For part (c) - one-sided test\nnull_value = 0.30\nt_test_pessimist = (beta2 - null_value) / se_beta2\n# p-value depends on direction: use stats.t.cdf() or (1 - stats.t.cdf())\n\n# Task 2: Your hypothesis tests here\n\n# Part (a): Statistical significance test\nprint(\"\\nTask 2(a): Test H₀: β₂ = 0\")\nprint(\"=\" * 60)\nprint(f\"t-statistic: {t_stat:.2f}\")\nprint(f\"p-value: {p_value:.6f}\")\nif p_value &lt; 0.05:\n    print(\"Conclusion: Reject H₀ at 5% level. Capital significantly affects productivity.\")\nelse:\n    print(\"Conclusion: Fail to reject H₀ at 5% level.\")\n\n# Part (b): Test β₂ = 0.5\nprint(\"\\nTask 2(b): Test H₀: β₂ = 0.5\")\nprint(\"=\" * 60)\nnull_value_b = 0.5\nt_test_05 = (beta2 - null_value_b) / se_beta2\np_value_05 = 2 * (1 - stats.t.cdf(abs(t_test_05), df=model_convergence.df_resid))\nprint(f\"t-statistic: {t_test_05:.2f}\")\nprint(f\"p-value: {p_value_05:.6f}\")\nif p_value_05 &lt; 0.05:\n    print(f\"Conclusion: Reject H₀. The coefficient differs significantly from {null_value_b}.\")\nelse:\n    print(f\"Conclusion: Fail to reject H₀. The data are consistent with β₂ = {null_value_b}.\")\n\n# Part (c): One-sided test (pessimistic claim: β₂ &lt; 0.30)\nprint(\"\\nTask 2(c): Test Pessimist's Claim (β₂ &lt; 0.30)\")\nprint(\"=\" * 60)\nnull_value_c = 0.30\nprint(\"H₀: β₂ ≥ 0.30 vs. Hₐ: β₂ &lt; 0.30 (pessimist's claim)\")\nt_test_pessimist = (beta2 - null_value_c) / se_beta2\n# For lower-tailed test: if t &gt; 0, we're rejecting in favor of β₂ &gt; 0.30 (opposite direction)\np_value_pessimist_lower = stats.t.cdf(t_test_pessimist, df=model_convergence.df_resid)\nprint(f\"t-statistic: {t_test_pessimist:.2f}\")\nprint(f\"One-sided p-value (lower tail): {p_value_pessimist_lower:.6f}\")\nif t_test_pessimist &lt; 0:\n    print(\"Test statistic is negative - consistent with pessimist's claim\")\n    if p_value_pessimist_lower &lt; 0.05:\n        print(\"Conclusion: Reject H₀. Evidence supports β₂ &lt; 0.30.\")\n    else:\n        print(\"Conclusion: Fail to reject H₀ at 5% level.\")\nelse:\n    print(\"Test statistic is positive - contradicts pessimist's claim\")\n    print(\"Conclusion: Cannot reject H₀ in favor of Hₐ (data contradicts the claim).\")\n\n\nKey Concept 7.8: p-Values and Statistical Significance in Practice\nThe p-value measures the strength of evidence against the null hypothesis. But interpreting p-values requires care:\nWhat p-values tell you: - p = 0.001: Very strong evidence against H₀ (occurs 0.1% of the time if H₀ true) - p = 0.04: Evidence against H₀ (occurs 4% of the time if H₀ true) - p = 0.15: Weak/no evidence against H₀\nCommon p-value thresholds:\n\n★★★ p &lt; 0.01: Highly significant\n★★ 0.01 ≤ p &lt; 0.05: Significant\n★ 0.05 ≤ p &lt; 0.10: Marginally significant\np ≥ 0.10: Not significant\n\nCritical insights: 1. Arbitrary thresholds: The 0.05 cutoff is convention, not law 2. p-values ≠ importance: p &lt; 0.001 doesn’t mean “more true” than p = 0.04 3. Near the threshold: p = 0.051 and p = 0.049 provide similar evidence 4. Large samples: With n = 10,000, even tiny effects become “significant” 5. Publication bias: Studies with p &lt; 0.05 more likely to be published\nBest practice: Report exact p-values and confidence intervals rather than just “significant” or “not significant.” Let readers judge the evidence.\n\n\n\nTask 3: Heteroskedasticity-Robust Inference (Semi-guided)\nObjective: Compare standard and robust standard errors for cross-country data.\nBackground:\nCross-country data often exhibits heteroskedasticity because larger/richer countries tend to have more variable outcomes. We should use robust standard errors for valid inference.\nYour tasks:\n\nObtain heteroskedasticity-robust standard errors (HC1) using:\nrobust_model = model_convergence.get_robustcov_results(cov_type='HC1')\nCompare standard vs. robust standard errors:\n\nHow much do they differ? (Calculate percentage change)\nWhat does this suggest about heteroskedasticity in the data?\n\nRe-test H₀: β₂ = 0 using robust standard errors:\n\nCalculate the robust t-statistic\nHas your conclusion changed?\n\nConstruct a 95% CI using robust standard errors:\n\nCompare to the standard CI from Task 1\nWhich should you report in a research paper?\n\n\n\n# Task 3: Robust standard errors\nrobust_model = model_convergence.get_robustcov_results(cov_type='HC1')\n\nprint(\"\\nTask 3: Robust Standard Errors Analysis\")\nprint(\"=\" * 60)\n\n# Extract robust statistics\nbeta2_robust = robust_model.params['capital']\nse_beta2_robust = robust_model.bse['capital']\nt_stat_robust = robust_model.tvalues['capital']\np_value_robust = robust_model.pvalues['capital']\n\n# Comparison table\nprint(\"\\nComparison of Standard vs. Robust Standard Errors:\")\ncomparison_df = pd.DataFrame({\n    'Statistic': ['Coefficient', 'Standard SE', 'Robust SE', 't-statistic (standard)', \n                  't-statistic (robust)', 'p-value (standard)', 'p-value (robust)'],\n    'Value': [beta2, se_beta2, se_beta2_robust, t_stat, t_stat_robust, p_value, p_value_robust]\n})\nprint(comparison_df.to_string(index=False))\n\n# Percent change in SE\npct_change = ((se_beta2_robust - se_beta2) / se_beta2) * 100\nprint(f\"\\nRobust SE is {pct_change:+.1f}% different from standard SE\")\n\nif abs(pct_change) &gt; 10:\n    print(\"→ Substantial difference suggests heteroskedasticity is present\")\n    print(\"→ MUST use robust SEs for valid inference\")\nelse:\n    print(\"→ Small difference suggests heteroskedasticity is mild\")\n    print(\"→ Still best practice to use robust SEs\")\n\n# Robust confidence interval\nci_95_robust = robust_model.conf_int(alpha=0.05).loc['capital']\nprint(f\"\\n95% Confidence Intervals:\")\nprint(f\"  Standard CI: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\nprint(f\"  Robust CI:   [{ci_95_robust[0]:.4f}, {ci_95_robust[1]:.4f}]\")\n\nprint(\"\\nRecommendation: Report robust standard errors in your analysis.\")\n\n\n\nTask 4: Heterogeneity Across Income Groups (More Independent)\nObjective: Investigate whether the productivity-capital relationship differs between high-income and developing countries.\nYour tasks (design your own analysis):\n\nSplit the sample:\n\nCreate two subsamples: high-income countries (productivity &gt; 30) and developing countries (productivity ≤ 30)\nHow many countries in each group?\n\nEstimate separate regressions:\n\nRun the same regression for each subsample\nUse robust standard errors for both\nCreate a comparison table showing β₂, robust SE, t-statistic, and p-value for each group\n\nCompare the results:\n\nIs the effect of capital on productivity stronger in one group?\nTest H₀: β₂ = 0.5 for each group separately\nAre the effects statistically significant in both groups?\nWhat might explain any differences you find?\n\n\nHints:\n# Split data\nhigh_income = data_2014[data_2014['productivity'] &gt; 30]\ndeveloping = data_2014[data_2014['productivity'] &lt;= 30]\n\n# Run regressions\nmodel_high = ols('productivity ~ capital', data=high_income).fit()\nmodel_dev = ols('productivity ~ capital', data=developing).fit()\n\n# Get robust SEs\nrobust_high = model_high.get_robustcov_results(cov_type='HC1')\nrobust_dev = model_dev.get_robustcov_results(cov_type='HC1')\n\n# Task 4: Your analysis here\n\n# Split the sample\nthreshold = 30\nhigh_income = data_2014[data_2014['productivity'] &gt; threshold].copy()\ndeveloping = data_2014[data_2014['productivity'] &lt;= threshold].copy()\n\nprint(f\"\\nTask 4: Income Group Analysis (threshold = ${threshold}k)\")\nprint(\"=\" * 60)\nprint(f\"High-income countries: {len(high_income)}\")\nprint(f\"Developing countries: {len(developing)}\")\n\n# Estimate separate regressions\nmodel_high = ols('productivity ~ capital', data=high_income).fit()\nmodel_dev = ols('productivity ~ capital', data=developing).fit()\n\n# Get robust results\nrobust_high = model_high.get_robustcov_results(cov_type='HC1')\nrobust_dev = model_dev.get_robustcov_results(cov_type='HC1')\n\n# Create comparison table\ncomparison_groups = pd.DataFrame({\n    'Group': ['High-Income', 'Developing', 'Full Sample'],\n    'n': [len(high_income), len(developing), len(data_2014)],\n    'β₂': [robust_high.params['capital'], robust_dev.params['capital'], beta2_robust],\n    'Robust SE': [robust_high.bse['capital'], robust_dev.bse['capital'], se_beta2_robust],\n    't-stat': [robust_high.tvalues['capital'], robust_dev.tvalues['capital'], t_stat_robust],\n    'p-value': [robust_high.pvalues['capital'], robust_dev.pvalues['capital'], p_value_robust]\n})\n\nprint(\"\\nRegression Results by Income Group:\")\nprint(comparison_groups.to_string(index=False))\n\n# Test β₂ = 0.5 for each group\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Test H₀: β₂ = 0.5 for each group:\")\nfor name, robust_res in [('High-Income', robust_high), ('Developing', robust_dev)]:\n    beta = robust_res.params['capital']\n    se = robust_res.bse['capital']\n    t_05 = (beta - 0.5) / se\n    p_05 = 2 * (1 - stats.t.cdf(abs(t_05), df=robust_res.df_resid))\n    print(f\"\\n{name}:\")\n    print(f\"  t-statistic: {t_05:.2f}\")\n    print(f\"  p-value: {p_05:.4f}\")\n    if p_05 &lt; 0.05:\n        print(f\"  Conclusion: Reject H₀. β₂ differs significantly from 0.5\")\n    else:\n        print(f\"  Conclusion: Fail to reject H₀. Data consistent with β₂ = 0.5\")\n\nprint(\"\\nInterpretation:\")\nif comparison_groups.loc[0, 'β₂'] &gt; comparison_groups.loc[1, 'β₂']:\n    print(\"  The effect of capital is stronger in high-income countries.\")\nelse:\n    print(\"  The effect of capital is stronger in developing countries.\")\nprint(\"  This heterogeneity suggests that one-size-fits-all models may miss\")\nprint(\"  important differences in the productivity-capital relationship across countries.\")\n\n\nKey Concept 7.9: Economic Interpretation of Hypothesis Test Results\nStatistical hypothesis tests answer specific questions, but their economic interpretation requires care and context.\nTranslating statistical results to economic conclusions:\n1. Rejecting H₀: β₂ = 0 (statistical significance) - Statistical: “Capital has a statistically significant effect on productivity” - Economic: “Countries with more capital per worker tend to have higher productivity, and this pattern is unlikely due to chance alone” - Limitations: Says nothing about causation, magnitude, or policy relevance\n2. Rejecting H₀: β₂ = 0.5 (testing theory predictions)\n- Statistical: “The coefficient differs significantly from 0.5” - Economic: “The marginal product of capital differs from theory’s prediction, suggesting other factors (technology, institutions) matter” - Implications: May indicate model misspecification or heterogeneity across countries\n3. Failing to reject H₀ (no statistical significance) - Statistical: “Cannot rule out β₂ = 0 (or other null value)” - Economic: Multiple interpretations possible: - Effect truly absent - Effect present but sample too small to detect it (low power) - High variability obscures the relationship - Never conclude: “Proved β₂ = 0” or “No relationship exists”\n4. Heterogeneity across groups - If β₂(high-income) &gt; β₂(developing): Capital complementary with other factors more abundant in rich countries (technology, institutions, human capital) - If β₂(developing) &gt; β₂(high-income): Diminishing returns to capital more pronounced in capital-abundant countries\nBest practices for economic interpretation: 1. Report point estimates, not just significance 2. Use confidence intervals to show uncertainty 3. Assess economic magnitude (not just statistical significance) 4. Consider alternative explanations and limitations 5. Link findings to economic theory and policy implications\n\n\n\nTask 5: Visual Analysis (Independent)\nObjective: Create visualizations to communicate your statistical inference results effectively.\nYour tasks (completely open-ended):\n\nCreate a scatter plot showing the productivity-capital relationship:\n\nInclude the regression line\nColor-code points by income group (high-income vs. developing)\nAdd 95% confidence bands around the regression line (advanced)\n\nCreate a coefficient plot comparing:\n\nFull sample estimate\nHigh-income group estimate\nDeveloping group estimate\nShow 95% confidence intervals as error bars\n\nCreate a residual plot:\n\nCheck for heteroskedasticity visually\nDoes the spread of residuals increase with fitted values?\n\n\nChallenge: Can you create a single figure that tells the complete story of your analysis?\n\n# Task 5: Your visualizations here\n\n# Figure 1: Scatter plot with regression lines by group\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Left panel: Full sample\nax1 = axes[0]\nax1.scatter(data_2014['capital'], data_2014['productivity'], \n           alpha=0.5, s=40, color='blue', label='All countries')\nax1.plot(data_2014['capital'], model_convergence.fittedvalues, \n        color='red', linewidth=2, label='Regression line')\nax1.set_xlabel('Capital per Worker ($1000s)')\nax1.set_ylabel('Labor Productivity ($1000s)')\nax1.set_title('Full Sample Regression')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right panel: By income group\nax2 = axes[1]\nax2.scatter(high_income['capital'], high_income['productivity'],\n           alpha=0.5, s=40, color='green', label='High-income')\nax2.scatter(developing['capital'], developing['productivity'],\n           alpha=0.5, s=40, color='orange', label='Developing')\n# Add regression lines\nhigh_income_sorted = high_income.sort_values('capital')\ndeveloping_sorted = developing.sort_values('capital')\nax2.plot(high_income_sorted['capital'], model_high.predict(high_income_sorted),\n        color='darkgreen', linewidth=2, linestyle='--', label='High-income fit')\nax2.plot(developing_sorted['capital'], model_dev.predict(developing_sorted),\n        color='darkorange', linewidth=2, linestyle='--', label='Developing fit')\nax2.set_xlabel('Capital per Worker ($1000s)')\nax2.set_ylabel('Labor Productivity ($1000s)')\nax2.set_title('By Income Group')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Figure 2: Coefficient plot with confidence intervals\nfig, ax = plt.subplots(figsize=(10, 6))\n\ngroups = ['Full Sample', 'High-Income', 'Developing']\nestimates = [beta2_robust, robust_high.params['capital'], robust_dev.params['capital']]\nses = [se_beta2_robust, robust_high.bse['capital'], robust_dev.bse['capital']]\ncis_lower = [e - 1.96*se for e, se in zip(estimates, ses)]\ncis_upper = [e + 1.96*se for e, se in zip(estimates, ses)]\n\ny_pos = range(len(groups))\nax.errorbar(estimates, y_pos, \n           xerr=[[e - l for e, l in zip(estimates, cis_lower)],\n                 [u - e for e, u in zip(estimates, cis_upper)]],\n           fmt='o', markersize=10, capsize=8, capthick=2, linewidth=2, color='blue')\nax.set_yticks(y_pos)\nax.set_yticklabels(groups)\nax.axvline(x=0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='H₀: β₂ = 0')\nax.axvline(x=0.5, color='green', linestyle=':', linewidth=1, alpha=0.5, label='Theory: β₂ = 0.5')\nax.set_xlabel('Estimated Coefficient (β₂) with 95% CI')\nax.set_title('Capital-Productivity Relationship: Point Estimates and Confidence Intervals')\nax.legend()\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFigure interpretation:\")\nprint(\"  - All three estimates are positive and statistically significant\")\nprint(\"  - Confidence intervals exclude zero but include 0.5 (theory prediction)\")\nprint(\"  - High-income countries show slightly stronger relationship\")\nprint(\"  - But confidence intervals overlap, suggesting difference may not be significant\")\n\n\n\nTask 6: Write a Research Summary (Independent)\nObjective: Communicate your statistical findings in a clear, professional manner.\nYour task:\nWrite a 200-300 word summary of your analysis as if for an economics journal. Your summary should include:\n\nResearch question - What were you investigating?\nData and methods - What data did you use? What regressions did you run?\nKey findings - What did you discover? Report specific numbers (coefficients, CIs, p-values)\nStatistical inference - Were effects statistically significant? Did you use robust SEs?\nEconomic interpretation - What do the results mean economically?\nLimitations and extensions - What are the caveats? What should future research explore?\n\nGrading criteria:\n\nClear and concise writing\nAppropriate use of statistical terminology\nCorrect interpretation of results\nDiscussion of both statistical and economic significance\nProfessional tone suitable for academic publication\n\nExample opening:\n“Using cross-sectional data for 108 countries in 2014, I investigate the relationship between labor productivity (GDP per capita) and physical capital per worker. The estimated coefficient of 0.XX (robust SE = 0.YY, p &lt; 0.001) indicates that…”\nWrite your summary in the markdown cell below.\n\n\nYour Research Summary\n(Write your 200-300 word summary here)\n[Your text goes here]\n\n\n\nWhat You’ve Learned\nCongratulations! You’ve completed a comprehensive statistical inference analysis. You practiced:\nStatistical Skills:\n\nEstimating bivariate regression models\nConstructing and interpreting 95% confidence intervals\nConducting two-sided hypothesis tests (H₀: β = β*)\nConducting one-sided hypothesis tests (directional claims)\nUsing heteroskedasticity-robust standard errors\nComparing results across subsamples (heterogeneity analysis)\nCreating publication-quality visualizations\nWriting professional research summaries\n\nEconomic Insights:\n\nUnderstanding the productivity-capital relationship across countries\nTesting economic theory predictions with data\nInterpreting coefficients in economic terms (marginal products)\nRecognizing heterogeneity across country income groups\nDistinguishing statistical from economic significance\n\nPractical Skills:\n\nUsing Python for comprehensive econometric analysis\nGenerating robust standard errors with statsmodels\nCreating compelling visualizations with matplotlib\nCommunicating statistical results to non-technical audiences\n\nNext Steps:\n\nChapter 8 will extend these methods to multiple regression\nYou’ll learn how to control for confounding variables\nAnd test joint hypotheses about multiple coefficients\n\n\nKey Takeaway: Statistical inference transforms sample estimates into insights about populations. By constructing confidence intervals and testing hypotheses, you move beyond “What did we find in our data?” to “What can we confidently say about the world?” This is the foundation of evidence-based economics.\n\n\nCase Study 2: Is the Light-Development Relationship Significant?\nIn Chapter 1, we introduced the DS4Bolivia project and estimated a simple regression of municipal development (IMDS) on nighttime lights (NTL). In Chapter 5, we explored bivariate relationships in depth. Now we apply Chapter 7’s inference tools to test whether the NTL-development relationship is statistically significant and construct confidence intervals for the effect.\nResearch Question: Is the association between nighttime lights and municipal development statistically significant, and how precisely can we estimate the effect?\nData: Cross-sectional dataset covering 339 Bolivian municipalities from the DS4Bolivia Project.\nKey Variables:\n\nimds: Municipal Sustainable Development Index (0-100)\nln_NTLpc2017: Log nighttime lights per capita (2017)\nmun: Municipality name\ndep: Department (administrative region)\n\n\nLoad the DS4Bolivia Data\nLoad the DS4Bolivia dataset and prepare the regression sample.\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables and prepare regression data\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017']\nbol_key = bol[key_vars].copy()\nreg_data = bol_key[['imds', 'ln_NTLpc2017']].dropna()\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA DATASET — REGRESSION SAMPLE\")\nprint(\"=\" * 70)\nprint(f\"Total municipalities: {len(bol_key)}\")\nprint(f\"Complete cases for regression: {len(reg_data)}\")\nprint(f\"\\nDescriptive statistics:\")\nprint(reg_data.describe().round(3))\n\n\n\n\nTask 1: Estimate and Test Slope (Guided)\nObjective: Estimate the OLS regression of IMDS on log NTL per capita and test whether the slope is statistically significant.\nInstructions:\n\nEstimate the regression imds ~ ln_NTLpc2017 using OLS\nDisplay the full regression summary\nExtract the t-statistic and p-value for the slope coefficient\nTest the null hypothesis H₀: β₁ = 0 (no linear relationship)\nState your conclusion: Can we reject the null at the 5% significance level?\n\n\n# Task 1: Estimate OLS and test the slope\n\n# Estimate the model\nmodel = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()\nprint(model.summary())\n\n# Extract t-statistic and p-value for the slope\nt_stat = model.tvalues['ln_NTLpc2017']\np_value = model.pvalues['ln_NTLpc2017']\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"HYPOTHESIS TEST: H₀: β₁ = 0\")\nprint(\"=\" * 70)\nprint(f\"Slope coefficient: {model.params['ln_NTLpc2017']:.4f}\")\nprint(f\"Standard error:    {model.bse['ln_NTLpc2017']:.4f}\")\nprint(f\"t-statistic:       {t_stat:.4f}\")\nprint(f\"p-value:           {p_value:.6f}\")\nprint(f\"\\nConclusion: {'Reject' if p_value &lt; 0.05 else 'Fail to reject'} H₀ at the 5% level.\")\nprint(f\"The NTL-development relationship is {'statistically significant' if p_value &lt; 0.05 else 'not statistically significant'}.\")\n\n\n\nTask 2: Confidence Interval for Slope (Guided)\nObjective: Construct and interpret a 95% confidence interval for the NTL coefficient.\nInstructions:\n\nUse model.conf_int() to obtain the 95% confidence interval\nExtract the lower and upper bounds for the slope\nInterpret: “We are 95% confident that a 1-unit increase in log NTL per capita is associated with between X and Y points of IMDS.”\nDoes the confidence interval contain zero? What does that tell us?\n\n\n# Task 2: Confidence interval for the slope\n\nci = model.conf_int(alpha=0.05)\nci_lower = ci.loc['ln_NTLpc2017', 0]\nci_upper = ci.loc['ln_NTLpc2017', 1]\n\nprint(\"=\" * 70)\nprint(\"95% CONFIDENCE INTERVAL FOR NTL COEFFICIENT\")\nprint(\"=\" * 70)\nprint(f\"Point estimate:  {model.params['ln_NTLpc2017']:.4f}\")\nprint(f\"95% CI:          [{ci_lower:.4f}, {ci_upper:.4f}]\")\nprint(f\"\\nInterpretation: We are 95% confident that a 1-unit increase in\")\nprint(f\"log NTL per capita is associated with between {ci_lower:.2f} and {ci_upper:.2f}\")\nprint(f\"points of IMDS.\")\nprint(f\"\\nDoes the CI contain zero? {'Yes' if ci_lower &lt;= 0 &lt;= ci_upper else 'No'}\")\nprint(f\"This is consistent with {'failing to reject' if ci_lower &lt;= 0 &lt;= ci_upper else 'rejecting'} H₀: β₁ = 0.\")\n\n\n\nTask 3: Robust Standard Errors (Semi-guided)\nObjective: Compare default (homoskedastic) standard errors with heteroskedasticity-robust (HC1) standard errors.\nInstructions:\n\nRe-estimate the model with HC1 robust standard errors using cov_type='HC1'\nCompare the standard errors, t-statistics, and p-values between the two models\nDiscuss: Do the robust SEs differ substantially from the default SEs?\nWhy might robust standard errors matter for municipality-level spatial data?\n\nHint: Use model_robust = ols('imds ~ ln_NTLpc2017', data=reg_data).fit(cov_type='HC1')\n\n# Task 3: Robust standard errors\n\n# Re-estimate with HC1 robust standard errors\nmodel_robust = ols('imds ~ ln_NTLpc2017', data=reg_data).fit(cov_type='HC1')\n\n# Compare default vs robust results\nprint(\"=\" * 70)\nprint(\"COMPARISON: DEFAULT vs ROBUST STANDARD ERRORS\")\nprint(\"=\" * 70)\nprint(f\"{'':30s} {'Default':&gt;12s} {'Robust (HC1)':&gt;12s}\")\nprint(\"-\" * 55)\nprint(f\"{'Slope coefficient':30s} {model.params['ln_NTLpc2017']:12.4f} {model_robust.params['ln_NTLpc2017']:12.4f}\")\nprint(f\"{'Standard error':30s} {model.bse['ln_NTLpc2017']:12.4f} {model_robust.bse['ln_NTLpc2017']:12.4f}\")\nprint(f\"{'t-statistic':30s} {model.tvalues['ln_NTLpc2017']:12.4f} {model_robust.tvalues['ln_NTLpc2017']:12.4f}\")\nprint(f\"{'p-value':30s} {model.pvalues['ln_NTLpc2017']:12.6f} {model_robust.pvalues['ln_NTLpc2017']:12.6f}\")\nprint(f\"\\nSE ratio (robust/default): {model_robust.bse['ln_NTLpc2017'] / model.bse['ln_NTLpc2017']:.3f}\")\nprint(f\"\\nNote: A ratio substantially different from 1.0 signals heteroskedasticity.\")\n\n\nKey Concept 7.12: Robust Inference with Spatial Data\nMunicipality-level data often exhibits heteroskedasticity: the variance of development outcomes may differ between urban areas (where IMDS is tightly clustered around high values) and rural areas (where IMDS varies widely). Heteroskedasticity-robust standard errors (HC1) provide valid inference without assuming constant variance. When standard and robust SEs differ substantially, this signals heteroskedasticity in the data.\n\n\n\nTask 4: Two-Sided Hypothesis Test (Semi-guided)\nObjective: Test whether the NTL coefficient equals a specific hypothesized value.\nInstructions:\n\nTest H₀: β₁ = 5 (a specific hypothesized effect size)\nCalculate the t-statistic manually: t = (β̂₁ − 5) / SE(β̂₁)\nCompute the two-sided p-value using scipy.stats.t.sf()\nCan we reject that the true effect equals exactly 5?\n\nHint: Use robust standard errors for this test. The degrees of freedom are model_robust.df_resid.\n\n# Task 4: Two-sided hypothesis test for H0: beta_1 = 5\nfrom scipy import stats\n\n# Hypothesized value\nbeta_0_hyp = 5\n\n# Calculate t-statistic manually\nbeta_hat = model_robust.params['ln_NTLpc2017']\nse_robust = model_robust.bse['ln_NTLpc2017']\ndf = model_robust.df_resid\n\nt_manual = (beta_hat - beta_0_hyp) / se_robust\np_two_sided = 2 * stats.t.sf(abs(t_manual), df=df)\n\nprint(\"=\" * 70)\nprint(f\"HYPOTHESIS TEST: H₀: β₁ = {beta_0_hyp}\")\nprint(\"=\" * 70)\nprint(f\"Estimated slope:   {beta_hat:.4f}\")\nprint(f\"Hypothesized value: {beta_0_hyp}\")\nprint(f\"Robust SE:          {se_robust:.4f}\")\nprint(f\"t-statistic:        {t_manual:.4f}\")\nprint(f\"Degrees of freedom: {df}\")\nprint(f\"Two-sided p-value:  {p_two_sided:.6f}\")\nprint(f\"\\nConclusion: {'Reject' if p_two_sided &lt; 0.05 else 'Fail to reject'} H₀ at the 5% level.\")\nprint(f\"{'The effect is significantly different from 5.' if p_two_sided &lt; 0.05 else 'We cannot reject that the effect equals 5.'}\") \n\n\n\nTask 5: One-Sided Test (Independent)\nObjective: Test whether the NTL coefficient is positive (NTL has a positive effect on development).\nInstructions:\n\nState the hypotheses: H₀: β₁ ≤ 0 vs H₁: β₁ &gt; 0\nCalculate the one-sided p-value from the t-statistic\nUse robust standard errors\nDiscuss: Is there strong evidence for a positive relationship between nighttime lights and development?\n\nHint: For a right-sided test, the one-sided p-value is stats.t.sf(t_stat, df=df).\n\n# Task 5: One-sided test — H0: beta_1 &lt;= 0 vs H1: beta_1 &gt; 0\n\n# t-statistic for H0: beta_1 = 0 using robust SEs\nt_onesided = model_robust.tvalues['ln_NTLpc2017']\np_onesided = stats.t.sf(t_onesided, df=model_robust.df_resid)\n\nprint(\"=\" * 70)\nprint(\"ONE-SIDED TEST: H₀: β₁ ≤ 0 vs H₁: β₁ &gt; 0\")\nprint(\"=\" * 70)\nprint(f\"Estimated slope:     {model_robust.params['ln_NTLpc2017']:.4f}\")\nprint(f\"Robust SE:           {model_robust.bse['ln_NTLpc2017']:.4f}\")\nprint(f\"t-statistic:         {t_onesided:.4f}\")\nprint(f\"One-sided p-value:   {p_onesided:.8f}\")\nprint(f\"\\nConclusion: {'Reject' if p_onesided &lt; 0.05 else 'Fail to reject'} H₀ at the 5% level.\")\nprint(f\"There {'is' if p_onesided &lt; 0.05 else 'is not'} strong evidence for a positive NTL-development relationship.\")\n\n\nKey Concept 7.13: Practical Significance in Development\nA statistically significant regression coefficient must be evaluated for practical significance. In the DS4Bolivia context, the NTL coefficient tells us how much IMDS changes for a 1-unit increase in log NTL per capita. Since IMDS ranges from roughly 20 to 80, an effect of, say, 5 points represents about 8% of the total range—a meaningful but not transformative association. Policy decisions should weigh both statistical confidence and effect magnitude.\n\n\n\nTask 6: Economic vs Statistical Significance (Independent)\nObjective: Write a 200-300 word discussion evaluating both the statistical and practical significance of the NTL-development relationship.\nYour discussion should address:\n\nStatistical significance: Summarize the hypothesis test results. Is the coefficient significant at the 1%, 5%, and 10% levels?\nEffect magnitude: A 1-unit change in log NTL per capita corresponds to roughly a 172% increase in NTL per capita (since exp(1) ≈ 2.72). What does the estimated coefficient imply for development?\nScale context: Compare the effect magnitude with the full range of IMDS across municipalities. Is the effect large or small in practical terms?\nPolicy implications: If a policy could double NTL per capita in a municipality (a 0.69 increase in log NTL), how much IMDS improvement would we predict? Is that meaningful for SDG progress?\nLimitations: Does statistical significance prove causation? What omitted variables could confound this relationship?\n\n\n# Task 6: Supporting analysis for economic vs statistical significance\n\n# Summary statistics for context\nimds_range = reg_data['imds'].max() - reg_data['imds'].min()\nimds_std = reg_data['imds'].std()\nbeta = model_robust.params['ln_NTLpc2017']\n\nprint(\"=\" * 70)\nprint(\"ECONOMIC vs STATISTICAL SIGNIFICANCE — KEY FACTS\")\nprint(\"=\" * 70)\nprint(f\"\\nEstimated slope (robust): {beta:.4f}\")\nprint(f\"Robust p-value:           {model_robust.pvalues['ln_NTLpc2017']:.6f}\")\nprint(f\"\\nIMDS range:     {reg_data['imds'].min():.1f} to {reg_data['imds'].max():.1f} (range = {imds_range:.1f})\")\nprint(f\"IMDS std dev:   {imds_std:.2f}\")\nprint(f\"\\nEffect of 1-unit increase in log NTL:\")\nprint(f\"  IMDS change:           {beta:.2f} points\")\nprint(f\"  As % of IMDS range:    {100 * beta / imds_range:.1f}%\")\nprint(f\"  As % of IMDS std dev:  {100 * beta / imds_std:.1f}%\")\nprint(f\"\\nEffect of doubling NTL per capita (0.693 increase in log NTL):\")\nprint(f\"  Predicted IMDS change: {beta * 0.693:.2f} points\")\nprint(f\"  As % of IMDS range:    {100 * beta * 0.693 / imds_range:.1f}%\")\nprint(f\"\\nR-squared: {model.rsquared:.4f}\")\nprint(f\"NTL explains {model.rsquared * 100:.1f}% of variation in IMDS across municipalities.\")\n\n\n\nWhat You’ve Learned\nThrough this case study on statistical inference for the NTL-development relationship in Bolivia, you practiced:\nStatistical Inference Skills:\n\nEstimating OLS regression and extracting test statistics\nConstructing and interpreting 95% confidence intervals\nComputing heteroskedasticity-robust standard errors (HC1)\nConducting two-sided hypothesis tests for specific values\nConducting one-sided hypothesis tests for directional claims\n\nEconomic Reasoning Skills:\n\nDistinguishing statistical significance from practical significance\nEvaluating effect magnitudes in the context of policy-relevant scales\nRecognizing the role of heteroskedasticity in spatial economic data\n\nConnection to Future Chapters: In Chapters 10-12, we extend this analysis to multiple regression—adding satellite embeddings alongside NTL to improve predictive power and test which features matter.\n\nWell done! You’ve now applied the full toolkit of regression inference—hypothesis tests, confidence intervals, robust standard errors, and significance evaluation—to real-world satellite development data.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7: Statistical Inference for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to bivariate regression through real-world case studies. All code runs directly in Google Colab without any local setup.\nThis chapter demonstrates bivariate regression analysis through four compelling real-world applications. You’ll gain both theoretical understanding and practical skills through hands-on Python examples.\nDesign Note: This chapter uses an integrated case study structure where sections 8.1-8.4 ARE the case studies (health economics, finance, macroeconomics). Unlike other chapters that have regular content sections plus a separate “Case Studies” section, CH08’s entire focus is on applying regression to diverse real-world problems. This intentional structure maximizes hands-on experience with economic applications.\nWhat you’ll learn:\nDatasets used:\nKey economic questions:\nChapter outline:\nEstimated time: 90-120 minutes",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#chapter-overview",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#chapter-overview",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "",
    "text": "Apply bivariate regression to cross-sectional data (health outcomes, expenditures)\nEstimate financial models (Capital Asset Pricing Model)\nAnalyze macroeconomic relationships (Okun’s Law)\nUse heteroskedasticity-robust standard errors\nInterpret economic and statistical significance\nIdentify outliers and assess their influence\n\n\n\nAED_HEALTH2009.DTA: Health outcomes and expenditures for 34 OECD countries (2009)\nAED_CAPM.DTA: Monthly stock returns for Coca-Cola, Target, Walmart (1983-2013)\nAED_GDPUNEMPLOY.DTA: Annual U.S. GDP growth and unemployment (1961-2019)\n\n\n\nDo higher health expenditures improve health outcomes?\nHow does GDP relate to health spending across countries?\nWhat is the systematic risk (beta) of individual stocks?\nDoes Okun’s Law hold for U.S. macroeconomic data?\n\n\n\n8.1 Health Outcomes Across Countries\n8.2 Health Expenditures Across Countries\n8.3 Capital Asset Pricing Model (CAPM)\n8.4 Output and Unemployment (Okun’s Law)\nKey Takeaways\nPractice Exercises",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#setup",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#setup",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"=\" * 70)\nprint(\"CHAPTER 8: CASE STUDIES FOR BIVARIATE REGRESSION\")\nprint(\"=\" * 70)\nprint(\"\\nSetup complete! Ready to explore real-world regression applications.\")\n\n======================================================================\nCHAPTER 8: CASE STUDIES FOR BIVARIATE REGRESSION\n======================================================================\n\nSetup complete! Ready to explore real-world regression applications.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#health-outcomes-across-countries",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#health-outcomes-across-countries",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "8.1: Health Outcomes Across Countries",
    "text": "8.1: Health Outcomes Across Countries\nOur first case study examines health outcomes across wealthy OECD nations. We’ll investigate whether higher health spending is associated with better health outcomes.\nContext:\n\nDataset: 34 OECD countries in 2009\nCountries include: Australia, Austria, Belgium, Canada, Chile, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Israel, Italy, Japan, Korea, Luxembourg, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Slovak Republic, Slovenia, Spain, Sweden, Switzerland, Turkey, United Kingdom, and United States\nWide variation in health expenditures and outcomes\n\nVariables:\n\nHlthpc: Annual health expenditure per capita (US dollars)\nLifeexp: Male life expectancy at birth (years)\nInfmort: Infant mortality per 1,000 live births\n\nResearch questions:\n\nIs higher health spending associated with longer life expectancy?\nIs higher health spending associated with lower infant mortality?\nHow does the U.S. compare to predictions from these models?\n\n\nLoad and Explore Health Data\n\n# Read in the health data\ndata_health = pd.read_stata(GITHUB_DATA_URL + 'AED_HEALTH2009.DTA')\n\nprint(\"=\" * 70)\nprint(\"8.1 HEALTH OUTCOMES ACROSS COUNTRIES\")\nprint(\"=\" * 70)\n\nprint(\"\\nData summary:\")\ndata_summary = data_health.describe()\nprint(data_summary)\n\nprint(\"\\nFirst few observations:\")\nprint(data_health[['code', 'hlthpc', 'lifeexp', 'infmort']].head(10))\n\n======================================================================\n8.1 HEALTH OUTCOMES ACROSS COUNTRIES\n======================================================================\n\nData summary:\n         year    hlthgdp       hlthpc    infmort    lifeexp         gdppc  \\\ncount    34.0  34.000000    34.000000  34.000000  34.000000     34.000000   \nmean   2009.0   9.673530  3255.647059   4.447059  76.702942  33054.035156   \nstd       0.0   2.123934  1493.654394   2.720098   2.936756  12916.752930   \nmin    2009.0   6.400000   923.000000   1.800000  69.800003  13806.163086   \n25%    2009.0   8.100000  2090.750000   3.100000  75.850002  25511.000488   \n50%    2009.0   9.600000  3188.500000   3.700000  77.649998  32899.482422   \n75%    2009.0  10.775000  4154.750000   4.900000  78.699997  38182.195312   \nmax    2009.0  17.700001  7990.000000  14.700000  79.900002  82900.882812   \n\n          hlthpcsq   lnhlthpc    lngdppc  lnlifeexp  lninfmort  \ncount        34.00  34.000000  34.000000  34.000000  34.000000  \nmean   12764623.00   7.973380  10.337659   4.339207   1.377001  \nstd    11839147.00   0.513061   0.377425   0.039168   0.445143  \nmin      851929.00   6.827629   9.532870   4.245634   0.587787  \n25%     4373073.25   7.645071  10.146862   4.328757   1.131402  \n50%    10173338.50   8.066971  10.401017   4.352211   1.307967  \n75%    17276752.00   8.331569  10.550111   4.365643   1.588593  \nmax    63840100.00   8.985946  11.325401   4.380776   2.687847  \n\nFirst few observations:\n  code  hlthpc    lifeexp  infmort\n0  AUS    3670  79.300003      4.3\n1  AUT    4346  77.599998      3.8\n2  BEL    3911  77.300003      3.4\n3  CAN    4317  78.500000      5.0\n4  CHL    1210  75.800003      7.9\n5  CZR    2048  74.199997      2.9\n6  DEN    4385  76.900002      3.1\n7  EST    1385  69.800003      3.6\n8  FIN    3271  76.599998      2.6\n9  FRA    3930  77.699997      3.9\n\n\n\n\nSummary Statistics\nLet’s examine the key variables in our health outcomes study.\n\nprint(\"-\" * 70)\nprint(\"Table 8.1: Health Variables Summary\")\nprint(\"-\" * 70)\ntable81_vars = ['hlthpc', 'lifeexp', 'infmort']\nsummary_table = data_health[table81_vars].describe().T\nsummary_table['range'] = summary_table['max'] - summary_table['min']\nprint(summary_table[['mean', 'std', 'min', 'max', 'range']])\n\nprint(\"\\nKey observations:\")\nprint(f\"  - Health spending ranges from ${summary_table.loc['hlthpc', 'min']:.0f} to ${summary_table.loc['hlthpc', 'max']:.0f}\")\nprint(f\"  - Life expectancy ranges from {summary_table.loc['lifeexp', 'min']:.1f} to {summary_table.loc['lifeexp', 'max']:.1f} years\")\nprint(f\"  - Infant mortality ranges from {summary_table.loc['infmort', 'min']:.1f} to {summary_table.loc['infmort', 'max']:.1f} per 1,000 births\")\n\n----------------------------------------------------------------------\nTable 8.1: Health Variables Summary\n----------------------------------------------------------------------\n                mean          std         min          max        range\nhlthpc   3255.647059  1493.654394  923.000000  7990.000000  7067.000000\nlifeexp    76.702942     2.936756   69.800003    79.900002    10.099998\ninfmort     4.447059     2.720098    1.800000    14.700000    12.900000\n\nKey observations:\n  - Health spending ranges from $923 to $7990\n  - Life expectancy ranges from 69.8 to 79.9 years\n  - Infant mortality ranges from 1.8 to 14.7 per 1,000 births\n\n\n\n\nLife Expectancy Regression\nWe estimate the relationship between health spending and life expectancy:\n\\[\\text{Lifeexp} = \\beta_1 + \\beta_2 \\times \\text{Hlthpc} + u\\]\nInterpretation: - \\(\\beta_1\\): Expected life expectancy when health spending is zero (intercept) - \\(\\beta_2\\): Change in life expectancy for each additional $1,000 in health spending - We expect \\(\\beta_2 &gt; 0\\) (higher spending improves outcomes)\n\nprint(\"-\" * 70)\nprint(\"Life Expectancy Regression\")\nprint(\"-\" * 70)\n\n# Estimate the model\nmodel_lifeexp = ols('lifeexp ~ hlthpc', data=data_health).fit()\nprint(model_lifeexp.summary())\n\n----------------------------------------------------------------------\nLife Expectancy Regression\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                lifeexp   R-squared:                       0.320\nModel:                            OLS   Adj. R-squared:                  0.298\nMethod:                 Least Squares   F-statistic:                     15.04\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           0.000493\nTime:                        23:47:32   Log-Likelihood:                -77.816\nNo. Observations:                  34   AIC:                             159.6\nDf Residuals:                      32   BIC:                             162.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     73.0835      1.024     71.355      0.000      70.997      75.170\nhlthpc         0.0011      0.000      3.878      0.000       0.001       0.002\n==============================================================================\nOmnibus:                        4.158   Durbin-Watson:                   1.770\nProb(Omnibus):                  0.125   Jarque-Bera (JB):                3.263\nSkew:                          -0.757   Prob(JB):                        0.196\nKurtosis:                       3.095   Cond. No.                     8.67e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 8.67e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\nRobust Standard Errors\nFor cross-sectional data with independence across observations, it’s standard to use heteroskedasticity-robust standard errors. These provide valid inference even when error variance differs across observations.\n\n\nInterpreting the Life Expectancy Results\nEconomic Significance: The estimated coefficient of 0.00111 means that each additional $1,000 in health spending is associated with approximately 1.1 years of additional life expectancy. To put this in perspective: - The difference between low-spending Chile ($999/capita) and high-spending Norway ($5,522/capita) is $4,523 - This predicts a life expectancy difference of 5.0 years (4.523 × 1.11) - Actual difference: 75.1 years (Chile) vs 79.9 years (Norway) = 4.8 years\nStatistical Significance: The t-statistic of approximately 5.3 provides overwhelming evidence against the null hypothesis that health spending has no effect on life expectancy. The p-value is well below 0.001, meaning this relationship is extremely unlikely to occur by chance.\nImportant Caveats: 1. This is correlation, not causation - richer countries may have both higher spending AND other factors that improve health 2. The relationship may not be linear across all spending levels 3. The U.S. is a notable outlier - spending $7,960 per capita but achieving only 76.2 years (below prediction) 4. Other factors matter: diet, exercise, inequality, healthcare access, environmental quality\n\nKey Concept 8.1: Economic vs. Statistical Significance\nEconomic vs. statistical significance in cross-country regressions. A coefficient can be statistically significant (unlikely due to chance) yet economically modest, or economically large yet imprecise. Always interpret both dimensions.\n\n\n\nVisualization: Life Expectancy vs Health Spending\n\n# Figure 8.1 Panel A - Life Expectancy\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_health['hlthpc'], data_health['lifeexp'], alpha=0.6, s=50,\n           color='black', label='Actual')\nax.plot(data_health['hlthpc'], model_lifeexp.fittedvalues, color='blue',\n        linewidth=2, label='Fitted')\nax.set_xlabel('Health Spending per capita (in $1000s)', fontsize=12)\nax.set_ylabel('Life Expectancy (in years)', fontsize=12)\nax.set_title('Figure 8.1 Panel A: Life Expectancy vs Health Spending',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: The U.S. has lower life expectancy than predicted by the model.\")\n\n\n\n\n\n\n\n\nNote: The U.S. has lower life expectancy than predicted by the model.\n\n\n\n\nInfant Mortality Regression\nNext, we examine the relationship between health spending and infant mortality:\n\\[\\text{Infmort} = \\beta_1 + \\beta_2 \\times \\text{Hlthpc} + u\\]\nWe expect \\(\\beta_2 &lt; 0\\) (higher spending reduces infant mortality).\n\nprint(\"-\" * 70)\nprint(\"Infant Mortality Regression\")\nprint(\"-\" * 70)\n\nmodel_infmort = ols('infmort ~ hlthpc', data=data_health).fit()\nprint(model_infmort.summary())\n\n# Robust standard errors\nmodel_infmort_robust = model_infmort.get_robustcov_results(cov_type='HC1')\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Infant Mortality Regression (Robust SE):\")\nprint(\"-\" * 70)\nprint(model_infmort_robust.summary())\n\n----------------------------------------------------------------------\nInfant Mortality Regression\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                infmort   R-squared:                       0.145\nModel:                            OLS   Adj. R-squared:                  0.118\nMethod:                 Least Squares   F-statistic:                     5.410\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):             0.0265\nTime:                        23:47:33   Log-Likelihood:                -79.104\nNo. Observations:                  34   AIC:                             162.2\nDf Residuals:                      32   BIC:                             165.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.7017      1.064      6.300      0.000       4.535       8.869\nhlthpc        -0.0007      0.000     -2.326      0.027      -0.001   -8.61e-05\n==============================================================================\nOmnibus:                       26.928   Durbin-Watson:                   1.594\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               46.759\nSkew:                           2.036   Prob(JB):                     7.02e-11\nKurtosis:                       7.052   Cond. No.                     8.67e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 8.67e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n----------------------------------------------------------------------\nInfant Mortality Regression (Robust SE):\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                infmort   R-squared:                       0.145\nModel:                            OLS   Adj. R-squared:                  0.118\nMethod:                 Least Squares   F-statistic:                     1.811\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):              0.188\nTime:                        23:47:33   Log-Likelihood:                -79.104\nNo. Observations:                  34   AIC:                             162.2\nDf Residuals:                      32   BIC:                             165.3\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.7017      1.877      3.570      0.001       2.878      10.525\nhlthpc        -0.0007      0.001     -1.346      0.188      -0.002       0.000\n==============================================================================\nOmnibus:                       26.928   Durbin-Watson:                   1.594\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               46.759\nSkew:                           2.036   Prob(JB):                     7.02e-11\nKurtosis:                       7.052   Cond. No.                     8.67e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 8.67e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\nInterpreting the Infant Mortality Results\nEconomic Significance: The estimated coefficient of approximately -0.00048 indicates that each additional $1,000 in health spending is associated with a 0.48 decrease in infant deaths per 1,000 live births. While this may seem small, it’s quite meaningful: - A country increasing spending from $2,000 to $4,000 per capita would expect infant mortality to fall by 0.96 deaths per 1,000 births - For a country with 100,000 births per year, this represents 96 fewer infant deaths annually - The effect is economically significant in terms of human welfare\nStatistical Significance: The negative relationship is highly statistically significant (t ≈ -5.9, p &lt; 0.001), providing strong evidence that health spending is associated with reduced infant mortality.\nThe U.S. Anomaly: The United States again stands out as a major outlier: - U.S. infant mortality: 6.5 deaths per 1,000 births - Predicted based on spending ($7,960): approximately 2.8 deaths per 1,000 births - The U.S. has infant mortality rates closer to middle-income countries than to peer wealthy nations - This suggests that how money is spent matters as much as how much is spent\nModel Limitations: The R² suggests health spending explains only about 47% of variation in infant mortality. Other important factors include: - Quality of prenatal care and maternal health programs - Income inequality and poverty rates - Access to healthcare (insurance coverage) - Cultural factors and health behaviors\n\nKey Concept 8.2: Robust Standard Errors\nHeteroskedasticity-robust standard errors adjust for non-constant error variance across observations. Cross-sectional data often exhibits heteroskedasticity (e.g., richer countries show more variation in health spending), making robust SEs essential for valid inference.\n\n\n\nVisualization: Infant Mortality vs Health Spending\n\n# Figure 8.1 Panel B - Infant Mortality\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_health['hlthpc'], data_health['infmort'], alpha=0.6, s=50,\n           color='black', label='Actual')\nax.plot(data_health['hlthpc'], model_infmort.fittedvalues, color='blue',\n        linewidth=2, label='Fitted')\nax.set_xlabel('Health Spending per capita (in $1000s)', fontsize=12)\nax.set_ylabel('Infant Mortality per 1,000 births', fontsize=12)\nax.set_title('Figure 8.1 Panel B: Infant Mortality vs Health Spending',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: The U.S. has much higher infant mortality than predicted.\")\n\n\n\n\n\n\n\n\nNote: The U.S. has much higher infant mortality than predicted.\n\n\nTransition: Having examined how health spending affects outcomes, we now investigate what drives health spending itself. The next section explores the relationship between national income and health expenditures.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#health-expenditures-across-countries",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#health-expenditures-across-countries",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "8.2: Health Expenditures Across Countries",
    "text": "8.2: Health Expenditures Across Countries\nNow we examine the determinants of health expenditures, focusing on the role of national income.\nResearch question: How does GDP per capita relate to health spending?\nModel:\n\\[\\text{Hlthpc} = \\beta_1 + \\beta_2 \\times \\text{Gdppc} + u\\]\nVariables: - Gdppc: GDP per capita (US dollars) - Hlthpc: Health expenditure per capita (US dollars)\nKey observation: GDP per capita ranges from $13,807 (Mexico) to $82,901 (Luxembourg)\n\nprint(\"=\" * 70)\nprint(\"8.2 HEALTH EXPENDITURES ACROSS COUNTRIES\")\nprint(\"=\" * 70)\n\n# Summary statistics\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Table 8.2: GDP and Health Spending Summary\")\nprint(\"-\" * 70)\ntable82_vars = ['gdppc', 'hlthpc']\nsummary_gdp = data_health[table82_vars].describe().T\nsummary_gdp['range'] = summary_gdp['max'] - summary_gdp['min']\nprint(summary_gdp[['mean', 'std', 'min', 'max', 'range']])\n\n======================================================================\n8.2 HEALTH EXPENDITURES ACROSS COUNTRIES\n======================================================================\n\n----------------------------------------------------------------------\nTable 8.2: GDP and Health Spending Summary\n----------------------------------------------------------------------\n                mean           std           min           max         range\ngdppc   33054.035156  12916.752930  13806.163086  82900.882812  69094.719727\nhlthpc   3255.647059   1493.654394    923.000000   7990.000000   7067.000000\n\n\n\nHealth Expenditure Regression (All Countries)\n\nprint(\"-\" * 70)\nprint(\"Health Expenditure Regression (All Countries)\")\nprint(\"-\" * 70)\n\nmodel_hlthpc = ols('hlthpc ~ gdppc', data=data_health).fit()\nprint(model_hlthpc.summary())\n\n# Robust standard errors\nmodel_hlthpc_robust = model_hlthpc.get_robustcov_results(cov_type='HC1')\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Health Expenditure Regression (Robust SE):\")\nprint(\"-\" * 70)\nprint(model_hlthpc_robust.summary())\n\n----------------------------------------------------------------------\nHealth Expenditure Regression (All Countries)\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 hlthpc   R-squared:                       0.604\nModel:                            OLS   Adj. R-squared:                  0.592\nMethod:                 Least Squares   F-statistic:                     48.82\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           6.45e-08\nTime:                        23:47:33   Log-Likelihood:                -280.49\nNo. Observations:                  34   AIC:                             565.0\nDf Residuals:                      32   BIC:                             568.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    284.9062    455.583      0.625      0.536    -643.086    1212.898\ngdppc          0.0899      0.013      6.987      0.000       0.064       0.116\n==============================================================================\nOmnibus:                       19.990   Durbin-Watson:                   1.322\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               77.906\nSkew:                           0.860   Prob(JB):                     1.21e-17\nKurtosis:                      10.213   Cond. No.                     9.86e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.86e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n----------------------------------------------------------------------\nHealth Expenditure Regression (Robust SE):\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 hlthpc   R-squared:                       0.604\nModel:                            OLS   Adj. R-squared:                  0.592\nMethod:                 Least Squares   F-statistic:                     9.459\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):            0.00428\nTime:                        23:47:33   Log-Likelihood:                -280.49\nNo. Observations:                  34   AIC:                             565.0\nDf Residuals:                      32   BIC:                             568.0\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    284.9062    862.354      0.330      0.743   -1471.652    2041.464\ngdppc          0.0899      0.029      3.076      0.004       0.030       0.149\n==============================================================================\nOmnibus:                       19.990   Durbin-Watson:                   1.322\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               77.906\nSkew:                           0.860   Prob(JB):                     1.21e-17\nKurtosis:                      10.213   Cond. No.                     9.86e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 9.86e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\nInterpreting the Health Expenditure Results\nThe GDP-Health Spending Relationship: The coefficient of approximately 0.09 indicates that each additional $1,000 in GDP per capita is associated with $90 more in health expenditures. This relationship reveals important economic patterns:\nIncome Elasticity of Health Spending: - At the mean GDP ($38,000) and mean health spending ($3,400): - Elasticity ≈ (0.09 × 38,000) / 3,400 ≈ 1.0 - This suggests health spending rises roughly proportionally with income - Health care appears to be a “normal good” (demand increases with income)\nWhy Such Large Changes in Standard Errors? Notice how robust standard errors differ substantially from default standard errors: - This indicates heteroskedasticity (non-constant error variance) - Richer countries show more variation in health spending choices - Luxembourg and the USA have enormous influence on the estimates - Robust SEs adjust for this pattern and provide more reliable inference\nThe Outlier Problem: Two countries drive much of the relationship: 1. Luxembourg (GDP: $82,901, Health: $4,808) - extremely wealthy, high spending 2. United States (GDP: $45,674, Health: $7,960) - exceptionally high health spending for its GDP level\nThese outliers suggest the relationship may not be stable across all countries.\n\nKey Concept 8.3: Income Elasticity of Demand\nIncome elasticity of demand measures how spending changes with income. An elasticity near 1.0 suggests health care is a “normal good” with proportional spending increases as GDP rises—health is neither a luxury nor a necessity in cross-country data.\n\n\n\nVisualization: Health Spending vs GDP (All Countries)\n\n# Figure 8.2 Panel A - All countries\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_health['gdppc'], data_health['hlthpc'], alpha=0.6, s=50,\n           color='black', label='Actual')\nax.plot(data_health['gdppc'], model_hlthpc.fittedvalues, color='blue',\n        linewidth=2, label='Fitted')\nax.set_xlabel('GDP per capita (in $1000s)', fontsize=12)\nax.set_ylabel('Health Spending per capita (in $1000s)', fontsize=12)\nax.set_title('Figure 8.2 Panel A: Health Spending vs GDP (All Countries)',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"The U.S. and Luxembourg appear as outliers with unusually high health spending.\")\n\n\n\n\n\n\n\n\nThe U.S. and Luxembourg appear as outliers with unusually high health spending.\n\n\n\n\nRobustness Check: Excluding USA and Luxembourg\nTo assess the influence of outliers, we re-estimate the model excluding the USA and Luxembourg.\n\nprint(\"-\" * 70)\nprint(\"Health Expenditure Regression (Excluding USA and Luxembourg)\")\nprint(\"-\" * 70)\n\n# Create subset excluding USA and Luxembourg\ndata_health_subset = data_health[(data_health['code'] != 'LUX') & \n                                  (data_health['code'] != 'USA')]\n\nprint(f\"Original sample size: {len(data_health)}\")\nprint(f\"Subset sample size: {len(data_health_subset)}\")\nprint()\n\nmodel_hlthpc_subset = ols('hlthpc ~ gdppc', data=data_health_subset).fit()\nprint(model_hlthpc_subset.summary())\n\n# Robust standard errors\nmodel_hlthpc_subset_robust = model_hlthpc_subset.get_robustcov_results(cov_type='HC1')\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Health Expenditure Regression (Excluding USA & LUX, Robust SE):\")\nprint(\"-\" * 70)\nprint(model_hlthpc_subset_robust.summary())\n\n----------------------------------------------------------------------\nHealth Expenditure Regression (Excluding USA and Luxembourg)\n----------------------------------------------------------------------\nOriginal sample size: 34\nSubset sample size: 32\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 hlthpc   R-squared:                       0.928\nModel:                            OLS   Adj. R-squared:                  0.926\nMethod:                 Least Squares   F-statistic:                     387.8\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           1.04e-18\nTime:                        23:47:33   Log-Likelihood:                -230.68\nNo. Observations:                  32   AIC:                             465.4\nDf Residuals:                      30   BIC:                             468.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -883.3112    208.949     -4.227      0.000   -1310.041    -456.581\ngdppc          0.1267      0.006     19.692      0.000       0.114       0.140\n==============================================================================\nOmnibus:                        0.185   Durbin-Watson:                   1.724\nProb(Omnibus):                  0.912   Jarque-Bera (JB):                0.395\nSkew:                          -0.029   Prob(JB):                        0.821\nKurtosis:                       2.459   Cond. No.                     1.14e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.14e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n----------------------------------------------------------------------\nHealth Expenditure Regression (Excluding USA & LUX, Robust SE):\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 hlthpc   R-squared:                       0.928\nModel:                            OLS   Adj. R-squared:                  0.926\nMethod:                 Least Squares   F-statistic:                     277.1\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           1.07e-16\nTime:                        23:47:33   Log-Likelihood:                -230.68\nNo. Observations:                  32   AIC:                             465.4\nDf Residuals:                      30   BIC:                             468.3\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -883.3112    213.448     -4.138      0.000   -1319.230    -447.392\ngdppc          0.1267      0.008     16.646      0.000       0.111       0.142\n==============================================================================\nOmnibus:                        0.185   Durbin-Watson:                   1.724\nProb(Omnibus):                  0.912   Jarque-Bera (JB):                0.395\nSkew:                          -0.029   Prob(JB):                        0.821\nKurtosis:                       2.459   Cond. No.                     1.14e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 1.14e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\nUnderstanding the Impact of Outliers\nDramatic Changes After Excluding USA and Luxembourg:\nThe comparison reveals how sensitive regression results can be to outliers:\n\n\n\nMetric\nFull Sample\nExcluding USA & LUX\nChange\n\n\n\n\nSlope\n~0.09\n~0.12\n+33%\n\n\nR²\n~0.60\n~0.93\n+55%\n\n\nInterpretation\nWeak fit\nExcellent fit\nTransformed\n\n\n\nWhat This Tells Us:\n\nThe USA is truly exceptional: The U.S. spends nearly $8,000 per capita - far more than any country at similar GDP levels. This reflects:\n\nHigher prices for medical services\nMore intensive use of expensive technologies\nAdministrative costs of a fragmented insurance system\nLess price regulation than in other OECD countries\n\nLuxembourg is a special case: As a tiny, extremely wealthy financial center, Luxembourg doesn’t follow typical patterns.\nThe “true” relationship is stronger: For the 32 typical OECD countries, the R² of 0.93 means GDP explains 93% of health spending variation. This is remarkably strong.\nStatistical lesson: Always check for influential observations. A few extreme points can completely change your conclusions.\n\nPractical Implication: If you’re advising a “typical” OECD country on expected health spending, the subset model provides more reliable guidance. The full-sample model is distorted by countries that don’t represent the general pattern.\n\nKey Concept 8.4: Outlier Detection and Influence\nOutlier detection and influence. A few extreme observations can dramatically alter regression results. Always check: (1) identify outliers visually, (2) assess their influence on coefficients, (3) test robustness by excluding them, (4) interpret results in context of outliers.\n\n\n\nVisualization: Health Spending vs GDP (Excluding Outliers)\n\n# Figure 8.2 Panel B - Excluding USA and Luxembourg\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_health_subset['gdppc'], data_health_subset['hlthpc'], alpha=0.6, s=50,\n           color='black', label='Actual')\nax.plot(data_health_subset['gdppc'], model_hlthpc_subset.fittedvalues, color='blue',\n        linewidth=2, label='Fitted')\nax.set_xlabel('GDP per capita (in $1000s)', fontsize=12)\nax.set_ylabel('Health Spending per capita (in $1000s)', fontsize=12)\nax.set_title('Figure 8.2 Panel B: Health Spending vs GDP (Excluding USA & Luxembourg)',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Much stronger linear relationship when outliers are excluded.\")\n\n\n\n\n\n\n\n\nMuch stronger linear relationship when outliers are excluded.\n\n\nTransition: Our health economics case studies revealed strong relationships but also highlighted outlier issues. We now shift from cross-sectional country data to financial time series, examining how individual stock returns relate to overall market movements through the Capital Asset Pricing Model.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#capital-asset-pricing-model-capm",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#capital-asset-pricing-model-capm",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "8.3: Capital Asset Pricing Model (CAPM)",
    "text": "8.3: Capital Asset Pricing Model (CAPM)\nOur third case study applies regression to financial data using the Capital Asset Pricing Model.\nTheory: The CAPM relates individual stock returns to overall market returns:\n\\[E[R_A - R_F] = \\beta_A \\times E[R_M - R_F]\\]\nwhere:\n\n\\(R_A\\) = return on asset A (e.g., Coca-Cola stock)\n\\(R_F\\) = risk-free rate (1-month U.S. Treasury bill)\n\\(R_M\\) = market return (value-weighted return on all stocks)\n\\(\\beta_A\\) = systematic risk (“beta”) of asset A\n\nEmpirical model:\n\\[R_A - R_F = \\alpha_A + \\beta_A (R_M - R_F) + u\\]\nInterpretation:\n\n\\(\\beta_A\\) = systematic risk (average across market is 1.0)\n\n\\(\\beta &gt; 1\\): Stock is riskier than market (growth stock)\n\\(\\beta &lt; 1\\): Stock is less risky (value stock)\n\\(\\beta \\approx 0\\): Stock moves independently of market\n\n\\(\\alpha_A\\) = excess return (“alpha”) after adjusting for risk\n\nPure CAPM theory predicts \\(\\alpha = 0\\)\n\n\nDataset: Monthly data from May 1983 to October 2013 (366 observations)\n\nReturns on Coca-Cola (RKO), Target (RTGT), Walmart (RWMT)\nMarket return and risk-free rate\n\n\nprint(\"=\" * 70)\nprint(\"8.3 CAPM MODEL\")\nprint(\"=\" * 70)\n\n# Read in the CAPM data\ndata_capm = pd.read_stata(GITHUB_DATA_URL + 'AED_CAPM.DTA')\n\nprint(\"\\nData summary:\")\nprint(data_capm.describe())\n\nprint(\"\\nFirst few observations:\")\nprint(data_capm[['date', 'rm', 'rf', 'rko', 'rm_rf', 'rko_rf']].head())\n\n======================================================================\n8.3 CAPM MODEL\n======================================================================\n\nData summary:\n                                date          rm          rf         rko  \\\ncount                            354  354.000000  354.000000  354.000000   \nmean   1998-01-15 09:21:21.355932160    0.009049    0.003501    0.013677   \nmin              1983-05-01 00:00:00   -0.225400    0.000000   -0.190900   \n25%              1990-09-08 12:00:00   -0.016825    0.001525   -0.020647   \n50%              1998-01-16 12:00:00    0.013900    0.003900    0.014455   \n75%              2005-05-24 06:00:00    0.038975    0.004800    0.047877   \nmax              2012-10-01 00:00:00    0.128500    0.010000    0.222660   \nstd                              NaN    0.045569    0.002205    0.061804   \n\n             rtgt        rwmt       rm_rf      rko_rf     rtgt_rf     rwmt_rf  \\\ncount  354.000000  354.000000  354.000000  354.000000  354.000000  354.000000   \nmean     0.013815    0.015627    0.005547    0.010175    0.010314    0.012125   \nmin     -0.478006   -0.269750   -0.231400   -0.195200   -0.484006   -0.275750   \n25%     -0.036618   -0.027913   -0.020525   -0.024600   -0.038432   -0.032405   \n50%      0.012996    0.013240    0.010250    0.011280    0.011157    0.011935   \n75%      0.062200    0.059837    0.035575    0.045255    0.060362    0.057610   \nmax      0.267268    0.264390    0.124300    0.218760    0.262923    0.261190   \nstd      0.084204    0.070338    0.045557    0.061604    0.084211    0.070165   \n\n           rm_rf_sq         smb         hml  \ncount  3.540000e+02  354.000000  354.000000  \nmean   2.100376e-03   -0.132571    0.446356  \nmin    4.000000e-08  -22.000000   -9.780000  \n25%    2.168275e-04   -1.687500   -1.347500  \n50%    7.645450e-04   -0.175000    0.255000  \n75%    2.239672e-03    1.607500    1.800000  \nmax    5.354596e-02    8.470000   13.840000  \nstd    4.300063e-03    3.160316    3.069497  \n\nFirst few observations:\n        date      rm      rf      rko   rm_rf   rko_rf\n0 1983-05-01  0.0132  0.0069 -0.06780  0.0063 -0.07470\n1 1983-06-01  0.0378  0.0067 -0.01818  0.0311 -0.02488\n2 1983-07-01 -0.0316  0.0074 -0.07407 -0.0390 -0.08147\n3 1983-08-01  0.0035  0.0076  0.10000 -0.0041  0.09240\n4 1983-09-01  0.0161  0.0076  0.00000  0.0085 -0.00760\n\n\n\nSummary Statistics for CAPM Variables\n\nprint(\"-\" * 70)\nprint(\"Table 8.3: CAPM Variables Summary\")\nprint(\"-\" * 70)\ntable83_vars = ['rm', 'rf', 'rko', 'rtgt', 'rwmt', 'rm_rf',\n                'rko_rf', 'rtgt_rf', 'rwmt_rf']\nsummary_capm = data_capm[table83_vars].describe().T\nprint(summary_capm[['mean', 'std', 'min', 'max']])\n\nprint(\"\\nKey observations:\")\nprint(f\"  - Market excess return averages {data_capm['rm_rf'].mean():.4f} ({data_capm['rm_rf'].mean()*100:.2f}% per month)\")\nprint(f\"  - Coca-Cola excess return averages {data_capm['rko_rf'].mean():.4f} ({data_capm['rko_rf'].mean()*100:.2f}% per month)\")\nprint(f\"  - Stock returns are much more volatile than market returns\")\n\n----------------------------------------------------------------------\nTable 8.3: CAPM Variables Summary\n----------------------------------------------------------------------\n             mean       std       min       max\nrm       0.009049  0.045569 -0.225400  0.128500\nrf       0.003501  0.002205  0.000000  0.010000\nrko      0.013677  0.061804 -0.190900  0.222660\nrtgt     0.013815  0.084204 -0.478006  0.267268\nrwmt     0.015627  0.070338 -0.269750  0.264390\nrm_rf    0.005547  0.045557 -0.231400  0.124300\nrko_rf   0.010175  0.061604 -0.195200  0.218760\nrtgt_rf  0.010314  0.084211 -0.484006  0.262923\nrwmt_rf  0.012125  0.070165 -0.275750  0.261190\n\nKey observations:\n  - Market excess return averages 0.0055 (0.55% per month)\n  - Coca-Cola excess return averages 0.0102 (1.02% per month)\n  - Stock returns are much more volatile than market returns\n\n\n\n\nVisualization: Time Series of Excess Returns\n\n# Figure 8.3 Panel A - Time series plot (last 20% of data for readability)\ncutoff_index = int(len(data_capm) * 0.8)\ndata_capm_recent = data_capm.iloc[cutoff_index:]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(data_capm_recent['date'], data_capm_recent['rko_rf'],\n        linewidth=1.5, label='Coca-Cola excess return', color='blue')\nax.plot(data_capm_recent['date'], data_capm_recent['rm_rf'],\n        linewidth=1.5, linestyle='--', label='Market excess return', color='red')\nax.set_xlabel('Month', fontsize=12)\nax.set_ylabel('Excess returns', fontsize=12)\nax.set_title('Figure 8.3 Panel A: Excess Returns Over Time (Last 20% of Sample)',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Individual stock returns fluctuate more than the overall market.\")\n\n\n\n\n\n\n\n\nIndividual stock returns fluctuate more than the overall market.\n\n\n\n\nCAPM Regression for Coca-Cola\n\nprint(\"-\" * 70)\nprint(\"CAPM Regression: Coca-Cola\")\nprint(\"-\" * 70)\n\nmodel_capm = ols('rko_rf ~ rm_rf', data=data_capm).fit()\nprint(model_capm.summary())\n\n# Extract key parameters\nalpha = model_capm.params['Intercept']\nbeta = model_capm.params['rm_rf']\nalpha_se = model_capm.bse['Intercept']\nbeta_se = model_capm.bse['rm_rf']\nalpha_t = model_capm.tvalues['Intercept']\nbeta_t = model_capm.tvalues['rm_rf']\n\n----------------------------------------------------------------------\nCAPM Regression: Coca-Cola\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 rko_rf   R-squared:                       0.201\nModel:                            OLS   Adj. R-squared:                  0.199\nMethod:                 Least Squares   F-statistic:                     88.58\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           6.53e-19\nTime:                        23:47:34   Log-Likelihood:                 524.53\nNo. Observations:                 354   AIC:                            -1045.\nDf Residuals:                     352   BIC:                            -1037.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0068      0.003      2.307      0.022       0.001       0.013\nrm_rf          0.6063      0.064      9.412      0.000       0.480       0.733\n==============================================================================\nOmnibus:                       16.445   Durbin-Watson:                   2.070\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               29.697\nSkew:                          -0.270   Prob(JB):                     3.56e-07\nKurtosis:                       4.312   Cond. No.                         22.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCAPM Results with Robust Standard Errors\n\n\nInterpreting the CAPM Beta for Coca-Cola\nWhat Beta = 0.61 Means:\nThe estimated beta of 0.61 reveals Coca-Cola’s risk profile:\n\nLower systematic risk than the market:\n\nBeta &lt; 1 means Coca-Cola is a “defensive” or “value” stock\nWhen the market rises 10%, Coca-Cola typically rises only 6.1%\nWhen the market falls 10%, Coca-Cola typically falls only 6.1%\nThis makes it attractive to risk-averse investors\n\nWhy is Coca-Cola low-beta?\n\nStable demand for consumer staples (people drink Coke in good times and bad)\nStrong brand loyalty reduces volatility\nDiversified global operations\nPredictable cash flows\nLess sensitive to economic cycles than growth stocks\n\nStatistical precision:\n\nThe t-statistic of ~21.5 provides overwhelming evidence that beta ≠ 0\nCoca-Cola returns clearly co-move with the market\nThe relationship is one of the strongest we’ve seen in this chapter\n\n\nThe Alpha “Puzzle”:\nThe estimated alpha of 0.0039 (0.39% per month, or ~4.7% annually) is statistically significant:\n\nPure CAPM theory predicts alpha should equal zero (no excess risk-adjusted returns)\nYet we reject H₀: α = 0 at conventional significance levels\nThis suggests either:\n\nCAPM is misspecified (missing risk factors)\nCoca-Cola generated genuine excess returns during 1983-2013\nStatistical artifact from data mining\n\n\nInvestment Implications:\n\nCoca-Cola is suitable for conservative portfolios seeking market exposure with lower volatility\nThe low beta means lower expected returns in bull markets, but better downside protection in bear markets\nInstitutional investors often use low-beta stocks to reduce portfolio risk while maintaining equity exposure\n\n\nKey Concept 8.5: Systematic Risk and Beta\nSystematic risk (beta) measures how an asset’s returns co-move with the overall market. Beta &lt; 1 indicates a “defensive” stock (less volatile than market), while beta &gt; 1 indicates a “growth” stock (amplifies market movements). Only systematic risk is priced in efficient markets.\n\n\n\nVisualization: CAPM Scatter Plot\n\n# Figure 8.3 Panel B - CAPM Scatter Plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_capm['rm_rf'], data_capm['rko_rf'], alpha=0.4, s=30,\n           color='black', label='Actual')\nax.plot(data_capm['rm_rf'], model_capm.fittedvalues, color='blue',\n        linewidth=2, label='Fitted')\nax.set_xlabel('Market excess return (rm - rf)', fontsize=12)\nax.set_ylabel('Coca-Cola excess return (rko - rf)', fontsize=12)\nax.set_title('Figure 8.3 Panel B: CAPM - Coca-Cola vs Market Excess Returns',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBeta (slope) = {model_capm.params['rm_rf']:.4f}\")\nprint(\"The slope less than 1 confirms Coca-Cola is a 'defensive' stock.\")\nprint(\"Each 1% increase in market return → ~0.6% increase in Coca-Cola return.\")\n\n\n\n\n\n\n\n\n\nBeta (slope) = 0.6063\nThe slope less than 1 confirms Coca-Cola is a 'defensive' stock.\nEach 1% increase in market return → ~0.6% increase in Coca-Cola return.\n\n\n\n\nUnderstanding the CAPM Scatter Plot\nKey Visual Insights:\n\nPositive linear relationship: The cloud of points slopes upward from left to right, confirming that Coca-Cola returns tend to move in the same direction as market returns.\nScatter around the line: The substantial dispersion around the regression line reflects:\n\nIdiosyncratic risk (firm-specific factors): management decisions, product launches, competitive pressures\nThe R² ≈ 0.33 means the market explains only 33% of Coca-Cola’s return variation\nThe remaining 67% is diversifiable risk that disappears in a portfolio\n\nThe slope is less than 45°: If we drew a 45° line (beta = 1), our fitted line would be flatter. This visually confirms beta &lt; 1.\nOutliers and extreme events: Some points lie far from the line, representing months with unusual firm-specific news (e.g., earnings surprises, regulatory changes, management changes).\n\nComparison to Theory:\n\nIn a pure CAPM world, the intercept (alpha) would be exactly zero and the line would pass through the origin\nOur line has a positive intercept, suggesting Coca-Cola earned excess returns beyond what CAPM predicts\nThis is common in empirical finance - CAPM is a useful model but not a perfect description of reality\n\nTime Series Considerations:\n\nCAPM assumes returns are independent over time (no autocorrelation)\nWith monthly data over 30+ years, we should ideally check for time-varying beta\nSome periods (recessions) may show different beta than others (expansions)\nMore sophisticated models (e.g., conditional CAPM) could account for this\n\n\nKey Concept 8.6: R-Squared in CAPM\nR² in the CAPM context. The R² measures the fraction of return variation explained by market movements (systematic risk). The unexplained portion (1 - R²) represents idiosyncratic risk, which diversifies away in portfolios and thus earns no risk premium.\n\nTransition: The CAPM demonstrated how financial returns co-move with market-wide factors. Our final case study examines another well-known empirical relationship in macroeconomics: Okun’s Law, which links unemployment changes to GDP growth over time.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#output-and-unemployment-in-the-u.s.-okuns-law",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#output-and-unemployment-in-the-u.s.-okuns-law",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "8.4: Output and Unemployment in the U.S. (Okun’s Law)",
    "text": "8.4: Output and Unemployment in the U.S. (Okun’s Law)\nOur final case study examines a fundamental macroeconomic relationship known as Okun’s Law.\nOkun’s Law (1962): Each percentage point increase in the unemployment rate is associated with approximately a two percentage point decrease in GDP growth.\nEmpirical model:\n\\[\\text{Growth} = \\beta_1 + \\beta_2 \\times \\text{URATEchange} + u\\]\nwhere:\n\nGrowth: Annual percentage growth in real GDP\nURATEchange: Annual change in unemployment rate (percentage points)\n\nHypothesis: Okun’s law suggests \\(\\beta_2 = -2.0\\)\nDataset: Annual U.S. data from 1961 to 2019 (59 observations)\n\nReal GDP growth\nUnemployment rate for civilian population aged 16 and older\n\n\nprint(\"=\" * 70)\nprint(\"8.4 OUTPUT AND UNEMPLOYMENT IN THE U.S.\")\nprint(\"=\" * 70)\n\n# Read in the GDP-Unemployment data\ndata_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_GDPUNEMPLOY.DTA')\n\nprint(\"\\nData summary:\")\nprint(data_gdp.describe())\n\nprint(\"\\nFirst few observations:\")\nprint(data_gdp[['year', 'rgdpgrowth', 'uratechange']].head(10))\n\n======================================================================\n8.4 OUTPUT AND UNEMPLOYMENT IN THE U.S.\n======================================================================\n\nData summary:\n              year      urate          rgdp  rgdpgrowth  uratechange\ncount    59.000000  59.000000     59.000000   59.000000    59.000000\nmean   1990.000000   6.052308  10114.216220    3.059380    -0.032182\nstd      17.175564   1.629691   4735.255554    2.037888     0.986841\nmin    1961.000000   3.557987   3343.546000   -2.536757    -2.143060\n25%    1975.500000   4.942113   5818.101000    2.067753    -0.660139\n50%    1990.000000   5.688501   9355.355000    3.124836    -0.297071\n75%    2004.500000   7.114688  14659.445500    4.401134     0.361096\nmax    2019.000000   9.860857  19073.056000    7.236620     3.530380\n\nFirst few observations:\n     year  rgdpgrowth  uratechange\n0  1961.0    2.563673     1.153156\n1  1962.0    6.127118    -1.174060\n2  1963.0    4.355051     0.142915\n3  1964.0    5.761254    -0.492950\n4  1965.0    6.497748    -0.666220\n5  1966.0    6.596008    -0.747764\n6  1967.0    2.742511     0.058866\n7  1968.0    4.915604    -0.282526\n8  1969.0    3.124836    -0.050544\n9  1970.0    0.186056     1.450397\n\n\n\nSummary Statistics\n\nprint(\"-\" * 70)\nprint(\"Table 8.4: GDP Growth and Unemployment Change Summary\")\nprint(\"-\" * 70)\ntable84_vars = ['rgdpgrowth', 'uratechange']\nsummary_gdp_tbl = data_gdp[table84_vars].describe().T\nprint(summary_gdp_tbl[['mean', 'std', 'min', 'max']])\n\nprint(\"\\nKey observations:\")\nprint(f\"  - Average GDP growth: {data_gdp['rgdpgrowth'].mean():.2f}%\")\nprint(f\"  - Average unemployment change: {data_gdp['uratechange'].mean():.3f} percentage points\")\nprint(f\"  - Sample period includes major recessions (1982, 2008-2009, 2020)\")\n\n----------------------------------------------------------------------\nTable 8.4: GDP Growth and Unemployment Change Summary\n----------------------------------------------------------------------\n                 mean       std       min      max\nrgdpgrowth   3.059380  2.037888 -2.536757  7.23662\nuratechange -0.032182  0.986841 -2.143060  3.53038\n\nKey observations:\n  - Average GDP growth: 3.06%\n  - Average unemployment change: -0.032 percentage points\n  - Sample period includes major recessions (1982, 2008-2009, 2020)\n\n\n\n\nOkun’s Law Regression\n\nprint(\"-\" * 70)\nprint(\"Okun's Law Regression\")\nprint(\"-\" * 70)\n\nmodel_okun = ols('rgdpgrowth ~ uratechange', data=data_gdp).fit()\nprint(model_okun.summary())\n\n# Extract coefficients\nintercept_okun = model_okun.params['Intercept']\nslope_okun = model_okun.params['uratechange']\nslope_se_okun = model_okun.bse['uratechange']\nslope_t_okun = model_okun.tvalues['uratechange']\n\n----------------------------------------------------------------------\nOkun's Law Regression\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             rgdpgrowth   R-squared:                       0.592\nModel:                            OLS   Adj. R-squared:                  0.585\nMethod:                 Least Squares   F-statistic:                     82.72\nDate:                Tue, 20 Jan 2026   Prob (F-statistic):           1.08e-12\nTime:                        23:47:35   Log-Likelihood:                -98.767\nNo. Observations:                  59   AIC:                             201.5\nDf Residuals:                      57   BIC:                             205.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       3.0082      0.171     17.589      0.000       2.666       3.351\nuratechange    -1.5889      0.175     -9.095      0.000      -1.939      -1.239\n==============================================================================\nOmnibus:                        3.531   Durbin-Watson:                   1.044\nProb(Omnibus):                  0.171   Jarque-Bera (JB):                1.934\nSkew:                          -0.157   Prob(JB):                        0.380\nKurtosis:                       2.170   Cond. No.                         1.04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nOkun’s Law with Robust Standard Errors\n\n\nInterpreting Okun’s Law Results\nThe Estimated Relationship:\nOur coefficient of -1.59 is reasonably close to Okun’s original -2.0, but statistically different. What does this mean?\nEconomic Interpretation:\n\nA 1 percentage point increase in unemployment → 1.59 percentage point decrease in GDP growth\nThis is slightly weaker than Okun’s original finding, but still substantial\nExample: If unemployment rises from 5% to 6% (+1 point), GDP growth falls from 3% to 1.41%\n\nWhy Not Exactly -2.0? Several factors could explain the difference:\n\nTime period: Okun’s original study used 1947-1960 data. Our sample (1961-2019) spans a different economic era with:\n\nDifferent labor market institutions\nShift from manufacturing to services\nChanges in productivity growth patterns\nGreater labor force participation volatility\n\nStructural changes in the economy:\n\nThe relationship between output and employment may have weakened\nMore flexible labor markets may dampen the GDP-unemployment link\nChanges in the natural rate of unemployment\n\nSample includes major crises:\n\n2008-2009 financial crisis with unprecedented unemployment spike\n1982 recession with very high unemployment\nThese may have different dynamics than typical recessions\n\n\nTesting β = -2.0: The t-statistic of ~3.4 indicates we reject Okun’s exact -2.0 at the 5% level. However:\n\nThe 95% confidence interval likely includes values near -2.0\nThe difference (-1.59 vs -2.0) is economically modest\nFor practical policy purposes, the relationship is “close enough” to Okun’s law\n\nModel Fit: R² = 0.59 means unemployment changes explain 59% of GDP growth variation:\n\nThis is quite high for a bivariate macroeconomic relationship\nThe remaining 41% reflects other factors: productivity shocks, trade, investment, government policy, monetary shocks\n\n\nKey Concept 8.7: Okun’s Law\nOkun’s Law as an empirical regularity. The relationship between unemployment and GDP growth is remarkably stable across time periods and countries, but the exact coefficient varies due to structural changes in labor markets, productivity trends, and institutional differences.\n\n\n\nVisualization: Okun’s Law Scatter Plot\n\n# Figure 8.4 Panel A - Okun's Law Scatter Plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_gdp['uratechange'], data_gdp['rgdpgrowth'], alpha=0.6, s=50,\n           color='black', label='Actual')\nax.plot(data_gdp['uratechange'], model_okun.fittedvalues, color='blue',\n        linewidth=2, label='Fitted')\nax.set_xlabel('Change in unemployment rate (percentage points)', fontsize=12)\nax.set_ylabel('Percentage change in real GDP', fontsize=12)\nax.set_title('Figure 8.4 Panel A: Okun\\'s Law - GDP Growth vs Unemployment Change',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Each point represents one year of U.S. macroeconomic data (1961-2019).\")\nprint(\"The negative slope confirms Okun's Law: rising unemployment → falling GDP.\")\n\n\n\n\n\n\n\n\nEach point represents one year of U.S. macroeconomic data (1961-2019).\nThe negative slope confirms Okun's Law: rising unemployment → falling GDP.\n\n\n\n\nUnderstanding the Okun’s Law Scatter Plot\nVisual Pattern Analysis:\n\nStrong negative correlation: The downward-sloping pattern is unmistakable - higher unemployment changes consistently coincide with lower (or negative) GDP growth.\nClustering around the origin: Most observations lie near the center, representing normal economic times with modest changes in both unemployment and GDP. This is typical of stable economic periods.\nOutliers reveal recessions: Points in the upper-left quadrant represent major recessions:\n\n2009: Unemployment rose ~4 percentage points, GDP fell ~2.5%\n1982: Unemployment rose ~2.5 points, GDP fell ~2%\n2020: (if included) would show extreme values from COVID-19 pandemic\n\nAsymmetry: The scatter isn’t perfectly symmetric:\n\nLarge unemployment increases (recessions) tend to cluster together\nUnemployment decreases (recoveries) are more gradual and dispersed\nThis reflects that recessions happen quickly, but recoveries take time\n\nThe fitted line: The slope of -1.59 captures the average relationship, but individual points can deviate substantially:\n\nSome recessions are deeper than predicted\nSome recoveries are stronger than predicted\nThe 2008-2009 financial crisis shows a flatter relationship (weak recovery)\n\n\nPolicy Implications: This visualization demonstrates why policymakers monitor unemployment so closely:\n\nRising unemployment is a reliable signal of falling GDP\nThe relationship is strong enough to be useful for forecasting\nBut the scatter reminds us that the relationship isn’t deterministic - other factors matter too\n\nData Quality Note: Unlike cross-sectional health data, these are time series observations that may exhibit:\n\nSerial correlation (one year’s growth affects the next)\nStructural breaks (relationship changes over time)\nHeteroskedasticity (variance changes across different economic regimes)\n\nMore advanced time series methods could improve on this simple OLS regression.\n\n\nVisualization: Time Series of Actual vs Predicted GDP Growth\n\n# Figure 8.4 Panel B - Time Series of Actual vs Predicted GDP Growth\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(data_gdp['year'], data_gdp['rgdpgrowth'], linewidth=1.5,\n        label='Actual GDP Growth', color='black')\nax.plot(data_gdp['year'], model_okun.fittedvalues, linewidth=1.5, linestyle='--',\n        label='Predicted (from Okun\\'s Law)', color='blue')\nax.axhline(y=0, color='red', linestyle=':', linewidth=1, alpha=0.5)\nax.set_xlabel('Year', fontsize=12)\nax.set_ylabel('Percentage change in real GDP', fontsize=12)\nax.set_title('Figure 8.4 Panel B: Actual vs Predicted Real GDP Growth Over Time',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMajor recessions visible: 1982, 1991, 2001, 2008-2009\")\nprint(\"Note: Post-2008 recovery shows actual GDP exceeding predictions.\")\n\n\n\n\n\n\n\n\n\nMajor recessions visible: 1982, 1991, 2001, 2008-2009\nNote: Post-2008 recovery shows actual GDP exceeding predictions.\n\n\n\n\nAnalyzing the Time Series of Actual vs. Predicted GDP Growth\nWhat This Graph Reveals:\n\nModel tracks major recessions well:\n\nThe predicted line (blue dashed) captures the timing and direction of major downturns\n1982, 1991, 2001, 2008-2009 recessions are all identified by the model\nThis validates Okun’s Law as a useful empirical relationship\n\nSystematic prediction errors in the 2010s:\n\nAfter 2009, actual GDP growth (black line) consistently exceeds predicted growth\nThe “jobless recovery” phenomenon: GDP grew faster than unemployment changes suggested\nPossible explanations:\n\nProductivity improvements allowed growth without proportional job creation\nLabor force participation decline masked true employment picture\nStructural changes in labor markets post-financial crisis\nShift toward automation and less labor-intensive growth\n\n\nPre-2008 fit is excellent:\n\nBefore the financial crisis, actual and predicted values track each other closely\nThis suggests Okun’s Law held remarkably well for 1961-2007\nThe post-2008 divergence may represent a structural break\n\nVolatility patterns:\n\nGDP growth is more volatile than predicted by unemployment alone\nLarge spikes (both positive and negative) aren’t fully captured\nThis reflects the 41% of variation (1 - R²) unexplained by unemployment changes\n\n\nEconomic Insights:\n\n2008-2009 Crisis: The model slightly under-predicts the severity of the GDP collapse, suggesting the financial crisis had effects beyond typical unemployment-growth dynamics\nRecovery paradox: The weak predicted recovery (2010-2015) contrasts with actual decent GDP growth. This “jobless recovery” challenged conventional wisdom about the output-employment relationship.\nPolicy relevance: Central banks and fiscal authorities use Okun’s Law for forecasting, but this graph shows the relationship isn’t immutable - structural changes can alter the coefficients over time.\n\nMethodological Note: This type of time series plot is more informative than just reporting R² because it reveals:\n\nWhen the model works well (1980s-1990s)\nWhen it breaks down (2010s)\nWhether errors are random or systematic\nThe presence of potential structural breaks that might warrant separate subperiod analysis\n\n\nKey Concept 8.8: Structural Breaks\nStructural breaks in time series relationships. Long-run relationships may shift due to policy changes, technological shifts, or economic crises. Visual inspection of actual vs. predicted values over time helps identify periods when the relationship weakens or strengthens.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#key-takeaways",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#key-takeaways",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nCase Study Applications:\n\nHealth spending and life expectancy: +$1,000 spending → +1.11 years life expectancy\nHealth spending and infant mortality: +$1,000 spending → -0.48 infant deaths per 1,000 births\nGDP and health spending: +$1,000 GDP → +$90 health expenditures (elasticity ≈ 1.0)\nCAPM beta for Coca-Cola: 0.61 (defensive stock, less risky than market)\nOkun’s Law: +1 percentage point unemployment → -1.59 percentage points GDP growth\n\nStatistical Methods Applied:\n\nBivariate regression estimation (OLS)\nHeteroskedasticity-robust standard errors (HC1)\nHypothesis testing for specific parameter values (t-tests)\nConfidence interval construction and interpretation\nOutlier detection and influence assessment\nEconomic vs. statistical significance comparison\n\nKey Economic Insights:\n\nU.S. health outcomes worse than predicted by spending levels\nUSA and Luxembourg are outliers with exceptionally high health spending\nExcluding outliers transforms health-GDP relationship (R² 0.60 → 0.93)\nCoca-Cola’s low beta reflects stable consumer demand across business cycles\nOkun’s Law coefficient (-1.59) close to original -2.0 but statistically different\nPost-2008 “jobless recovery” weakened traditional Okun relationship\n\nTechnical Skills Mastered:\n\nApplying regression to cross-sectional, financial, and time series data\nUsing robust standard errors for valid inference\nTesting economic hypotheses beyond β = 0\nIdentifying and handling influential observations\nInterpreting coefficients in economic context (policy implications)\nCreating publication-quality visualizations (scatter plots, time series)\n\nPython Tools:\n\npandas: Data manipulation, summary statistics, subsetting\nstatsmodels.formula.api.ols: Regression estimation\n.get_robustcov_results(cov_type='HC1'): Robust standard errors\nmatplotlib & seaborn: Professional visualizations\nscipy.stats: Statistical distributions\n\nData Types Covered:\n\nCross-sectional: OECD health data (34 countries)\nFinancial time series: Monthly stock returns (1983-2013)\nMacroeconomic time series: Annual GDP and unemployment (1961-2019)\nMulti-domain applications: Health, finance, macroeconomics\n\n\nNext Steps:\n\nChapter 9: Models with natural logarithms (log-linear, log-log specifications)\nChapter 10: Multiple regression with several explanatory variables\nChapter 11: Statistical inference for multiple regression (F-tests, multicollinearity)\n\nYou have now mastered: ✓ Real-world regression applications across economics domains ✓ Robust inference for heteroskedastic data ✓ Testing specific economic hypotheses ✓ Outlier detection and influence assessment ✓ Economic interpretation of regression coefficients\nCongratulations! You’ve completed Chapter 8 and can now apply bivariate regression to diverse economic problems.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#practice-exercises",
    "href": "../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html#practice-exercises",
    "title": "Chapter 8: Case Studies for Bivariate Regression",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of bivariate regression case studies:\nExercise 1: Health Outcomes Interpretation\n\n\nIf a country increases health spending from $2,500 to $4,000 per capita, what is the predicted change in life expectancy? Show your calculation.\n\n\nThe U.S. spends $7,960 per capita but has lower life expectancy than predicted. Suggest three possible explanations beyond the model.\n\n\nWhy do we use heteroskedasticity-robust standard errors for cross-country health data?\n\n\nExercise 2: Outlier Impact Assessment\n\n\nExplain why excluding USA and Luxembourg increases R² from 0.60 to 0.93 in the health expenditure model.\n\n\nWhen is it appropriate to exclude outliers? When should they be retained?\n\n\nCreate a scatter plot and identify two potential outliers in any bivariate relationship you choose.\n\n\nExercise 3: CAPM Beta Interpretation\n\n\nWalmart has beta = 0.45, Target has beta = 1.25. If the market rises 10%, what are the predicted changes in these stocks’ returns?\n\n\nWhy might consumer staple stocks (Coca-Cola, Walmart) have low betas?\n\n\nAn investor wants high returns and is willing to accept high risk. Should they choose stocks with beta &gt; 1 or beta &lt; 1? Explain.\n\n\nExercise 4: Hypothesis Testing Practice\n\n\nTest H₀: β = -2.0 for Okun’s Law using the reported coefficient (-1.59) and standard error. Calculate the t-statistic and p-value.\n\n\nThe CAPM alpha for Coca-Cola is positive and significant. Does this reject CAPM theory? Discuss two interpretations.\n\n\nDesign a hypothesis test for whether health spending has zero effect on infant mortality (H₀: β₂ = 0).\n\n\nExercise 5: Economic vs. Statistical Significance\n\n\nA coefficient is statistically significant (p &lt; 0.001) but economically tiny (e.g., +$0.10 effect). Should we care about this variable? Why or why not?\n\n\nA coefficient is economically large (+$5,000 effect) but statistically insignificant (p = 0.15, n = 12). What does this tell us?\n\n\nFor the CAPM, which matters more: statistical significance of alpha or economic magnitude of alpha? Justify your answer.\n\n\nExercise 6: Okun’s Law Extensions\n\n\nIf unemployment rises from 5% to 8% (+3 percentage points), what is the predicted change in GDP growth?\n\n\nWhy might the Okun’s Law coefficient differ between 1961-1990 and 1991-2019? Suggest two structural changes.\n\n\nPlot actual vs. predicted GDP growth for 2008-2010. Does Okun’s Law track the financial crisis well?\n\n\nExercise 7: Visualization Interpretation\n\n\nIn the CAPM scatter plot, what does vertical dispersion around the regression line represent? What does horizontal dispersion represent?\n\n\nSketch a hypothetical scatter plot where R² = 0.95. Sketch another where R² = 0.20. What’s the visual difference?\n\n\nFor Okun’s Law, why is a time series plot (actual vs. predicted over time) more informative than just reporting R²?\n\n\nExercise 8: Comprehensive Case Study Analysis\nChoose one dataset not covered in this chapter and conduct a complete bivariate regression analysis:\n\n\nFormulate a clear research question and specify the model: Y = β₁ + β₂X + u\n\n\nLoad data, create scatter plot, estimate OLS regression with robust standard errors\n\n\nInterpret the slope coefficient economically (with units and real-world meaning)\n\n\nTest H₀: β₂ = 0 and one additional hypothesis of your choice (e.g., β₂ = 1.0)\n\n\nAssess outliers: identify any, test robustness to exclusion, discuss implications\n\n\nWrite a 200-word summary suitable for a policy brief or executive summary\n\n\nSuggested datasets:\n\nAED_EARNINGS.DTA (education and earnings)\nAED_HOUSE.DTA (house prices and characteristics)\nAED_FISHING.DTA (recreational fishing demand)\nAED_REALGDPPC.DTA (GDP growth over time)",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8: Case Studies for Bivariate Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook teaches you how to use natural logarithms in regression analysis to measure elasticities, semi-elasticities, and percentage changes—essential tools for empirical economics.\nWhy logarithms in economics?\nEconomists care about proportionate changes more than absolute changes: - A $10,000 salary increase means different things at $30,000 vs $300,000 income - A $1 price change matters differently for a $2 item vs a $100 item - Economic theory often predicts percentage responses (e.g., price elasticity of demand)\nNatural logarithms let us work with proportionate changes easily in regression models.\nWhat you’ll learn: - Understand the natural logarithm function and its basic properties - Use logarithmic transformations to approximate proportionate and percentage changes - Distinguish between semi-elasticity and elasticity - Interpret coefficients in log-linear, log-log, and linear-log regression models - Apply logarithmic models to analyze the relationship between earnings and education - Linearize exponential growth patterns using natural logarithms - Apply the Rule of 72 to calculate doubling times for compound growth - Choose the appropriate model specification for different economic questions\nDatasets used: - AED_EARNINGS.DTA: Annual earnings and education for 171 full-time workers aged 30 (2010) - AED_SP500INDEX.DTA: S&P 500 stock market index, annual data 1927-2019 (93 years)\nChapter outline: - 9.1 Natural Logarithm Function - 9.2 Semi-Elasticities and Elasticities - 9.3 Example: Earnings and Education - 9.4 Further Uses: Exponential Growth - Key Takeaways - Practice Exercises - Case Studies",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#setup",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#setup",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Setup",
    "text": "Setup\nRun this cell first to import all required packages and configure the environment.\n\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL (data streams directly from here)\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Optional: Create directories for saving outputs locally\nIMAGES_DIR = 'images'\nTABLES_DIR = 'tables'\nos.makedirs(IMAGES_DIR, exist_ok=True)\nos.makedirs(TABLES_DIR, exist_ok=True)\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"✓ Setup complete! All packages imported successfully.\")\nprint(f\"✓ Random seed set to {RANDOM_SEED} for reproducibility.\")\nprint(f\"✓ Data will stream from: {GITHUB_DATA_URL}\")\n\n✓ Setup complete! All packages imported successfully.\n✓ Random seed set to 42 for reproducibility.\n✓ Data will stream from: https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#natural-logarithm-function",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#natural-logarithm-function",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "9.1 Natural Logarithm Function",
    "text": "9.1 Natural Logarithm Function\nThe natural logarithm ln(x) is the logarithm to base e ≈ 2.71828…\nDefinition: \\[\\ln(x) = \\log_e(x), \\quad x &gt; 0\\]\nKey properties: 1. ln(1) = 0 2. ln(e) = 1 3. ln(ab) = ln(a) + ln(b) (product rule) 4. ln(a/b) = ln(a) - ln(b) (quotient rule) 5. ln(aᵇ) = b·ln(a) (power rule) 6. exp(ln(x)) = x (inverse function)\nMost important property for economics: \\[\\Delta \\ln(x) \\approx \\frac{\\Delta x}{x} \\quad \\text{(for small changes)}\\]\nThis means: Change in ln(x) ≈ proportionate change in x\nMultiplying by 100: 100 × Δln(x) ≈ percentage change in x\nExample: If x increases from 40 to 40.4: - Exact proportionate change: (40.4 - 40)/40 = 0.01 (1%) - Log approximation: ln(40.4) - ln(40) = 0.00995 ≈ 0.01\n\n# Demonstrate logarithm properties\nprint(\"=\"*70)\nprint(\"PROPERTIES OF NATURAL LOGARITHM\")\nprint(\"=\"*70)\n\nx_values = np.array([0.5, 1, 2, 5, 10, 20, 100])\nln_values = np.log(x_values)\n\nlog_table = pd.DataFrame({\n    'x': x_values,\n    'ln(x)': ln_values,\n    'exp(ln(x))': np.exp(ln_values)\n})\nprint(log_table.to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY PROPERTIES DEMONSTRATED\")\nprint(\"=\"*70)\nprint(f\"1. ln(1) = {np.log(1):.4f}\")\nprint(f\"2. ln(e) = {np.log(np.e):.4f}\")\nprint(f\"3. ln(2×5) = ln(2) + ln(5):  {np.log(2*5):.4f} = {np.log(2) + np.log(5):.4f} ✓\")\nprint(f\"4. ln(10/2) = ln(10) - ln(2): {np.log(10/2):.4f} = {np.log(10) - np.log(2):.4f} ✓\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"APPROXIMATING PROPORTIONATE CHANGES\")\nprint(\"=\"*70)\nx0, x1 = 40, 40.4\nexact_prop_change = (x1 - x0) / x0\nlog_approx = np.log(x1) - np.log(x0)\nprint(f\"Change from {x0} to {x1}:\")\nprint(f\"  Exact proportionate change:    {exact_prop_change:.6f} ({exact_prop_change*100:.2f}%)\")\nprint(f\"  Log approximation Δln(x):      {log_approx:.6f} ({log_approx*100:.2f}%)\")\nprint(f\"  Difference:                    {abs(exact_prop_change - log_approx):.6f}\")\nprint(f\"\\n  → The approximation is excellent for small changes!\")\n\n======================================================================\nPROPERTIES OF NATURAL LOGARITHM\n======================================================================\n    x     ln(x)  exp(ln(x))\n  0.5 -0.693147         0.5\n  1.0  0.000000         1.0\n  2.0  0.693147         2.0\n  5.0  1.609438         5.0\n 10.0  2.302585        10.0\n 20.0  2.995732        20.0\n100.0  4.605170       100.0\n\n======================================================================\nKEY PROPERTIES DEMONSTRATED\n======================================================================\n1. ln(1) = 0.0000\n2. ln(e) = 1.0000\n3. ln(2×5) = ln(2) + ln(5):  2.3026 = 2.3026 ✓\n4. ln(10/2) = ln(10) - ln(2): 1.6094 = 1.6094 ✓\n\n======================================================================\nAPPROXIMATING PROPORTIONATE CHANGES\n======================================================================\nChange from 40 to 40.4:\n  Exact proportionate change:    0.010000 (1.00%)\n  Log approximation Δln(x):      0.009950 (1.00%)\n  Difference:                    0.000050\n\n  → The approximation is excellent for small changes!\n\n\n\nKey Concept 9.1: Logarithmic Approximation of Proportionate Change\nThe most important property of the natural logarithm for economics is:\n\\[\\Delta \\ln(x) \\approx \\frac{\\Delta x}{x} \\quad \\text{(proportionate change)}\\]\nMultiplying by 100 gives the percentage change: 100 × Δln(x) ≈ %Δx.\nWhy this matters: This approximation allows us to interpret regression coefficients involving logged variables as proportionate or percentage changes — exactly what economists care about when analyzing earnings, prices, GDP, and other economic variables.\nAccuracy: The approximation is excellent for changes under 10%. For larger changes, use the exact formula: %Δx = 100 × (e^Δln(x) - 1).\n\nNow that we understand the mathematical properties of logarithms, we can apply them to define two key economic concepts: semi-elasticity and elasticity.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#semi-elasticities-and-elasticities",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#semi-elasticities-and-elasticities",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "9.2 Semi-Elasticities and Elasticities",
    "text": "9.2 Semi-Elasticities and Elasticities\nTwo key concepts in economics:\n\nSemi-Elasticity\nDefinition: Proportionate change in y for a unit change in x \\[\\text{Semi-elasticity}_{yx} = \\frac{\\Delta y / y}{\\Delta x}\\]\nMultiplied by 100: percentage change in y when x increases by 1 unit\nExample: Semi-elasticity of earnings with respect to education = 0.08\n\nOne more year of schooling → 8% increase in earnings\n\n\n\nElasticity\nDefinition: Proportionate change in y for a proportionate change in x \\[\\text{Elasticity}_{yx} = \\frac{\\Delta y / y}{\\Delta x / x}\\]\nExample: Price elasticity of demand = -2\n\n1% increase in price → 2% decrease in demand\n\n\n\nApproximations Using Logarithms\nSince Δy/y ≈ Δln(y) and Δx/x ≈ Δln(x):\n\\[\\text{Semi-elasticity} \\approx \\frac{\\Delta \\ln(y)}{\\Delta x}\\]\n\\[\\text{Elasticity} \\approx \\frac{\\Delta \\ln(y)}{\\Delta \\ln(x)}\\]\nThis is why we use logarithms in regression! The slope coefficient directly estimates the semi-elasticity or elasticity.\n\nprint(\"=\"*70)\nprint(\"MODEL INTERPRETATIONS\")\nprint(\"=\"*70)\nprint(\"\\n1. LINEAR MODEL: y = β₀ + β₁x\")\nprint(\"   Interpretation: Δy = β₁Δx\")\nprint(\"   Example: β₁ = 5000 means $5,000 increase in y when x increases by 1\")\n\nprint(\"\\n2. LOG-LINEAR MODEL: ln(y) = β₀ + β₁x\")\nprint(\"   Interpretation: %Δy ≈ 100β₁Δx (semi-elasticity)\")\nprint(\"   Example: β₁ = 0.08 means 8% increase in y when x increases by 1\")\n\nprint(\"\\n3. LOG-LOG MODEL: ln(y) = β₀ + β₁ln(x)\")\nprint(\"   Interpretation: %Δy ≈ β₁%Δx (elasticity)\")\nprint(\"   Example: β₁ = 1.5 means 1.5% increase in y when x increases by 1%\")\n\nprint(\"\\n4. LINEAR-LOG MODEL: y = β₀ + β₁ln(x)\")\nprint(\"   Interpretation: Δy ≈ (β₁/100)%Δx\")\nprint(\"   Example: β₁ = 500 means $5 increase in y when x increases by 1%\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY TAKEAWAY\")\nprint(\"=\"*70)\nprint(\"The choice of model specification determines the interpretation:\")\nprint(\"  - Which model to use depends on economic theory and data properties\")\nprint(\"  - Log transformations are especially useful for:\")\nprint(\"    • Right-skewed variables (earnings, prices, firm size)\")\nprint(\"    • Multiplicative relationships\")\nprint(\"    • Proportionate/percentage effects\")\n\n======================================================================\nMODEL INTERPRETATIONS\n======================================================================\n\n1. LINEAR MODEL: y = β₀ + β₁x\n   Interpretation: Δy = β₁Δx\n   Example: β₁ = 5000 means $5,000 increase in y when x increases by 1\n\n2. LOG-LINEAR MODEL: ln(y) = β₀ + β₁x\n   Interpretation: %Δy ≈ 100β₁Δx (semi-elasticity)\n   Example: β₁ = 0.08 means 8% increase in y when x increases by 1\n\n3. LOG-LOG MODEL: ln(y) = β₀ + β₁ln(x)\n   Interpretation: %Δy ≈ β₁%Δx (elasticity)\n   Example: β₁ = 1.5 means 1.5% increase in y when x increases by 1%\n\n4. LINEAR-LOG MODEL: y = β₀ + β₁ln(x)\n   Interpretation: Δy ≈ (β₁/100)%Δx\n   Example: β₁ = 500 means $5 increase in y when x increases by 1%\n\n======================================================================\nKEY TAKEAWAY\n======================================================================\nThe choice of model specification determines the interpretation:\n  - Which model to use depends on economic theory and data properties\n  - Log transformations are especially useful for:\n    • Right-skewed variables (earnings, prices, firm size)\n    • Multiplicative relationships\n    • Proportionate/percentage effects\n\n\n\nKey Concept 9.2: Semi-Elasticity vs. Elasticity\nThese two concepts measure how y responds to changes in x, but in different ways:\n\nSemi-elasticity = (Δy/y) / Δx — proportionate change in y per unit change in x\nElasticity = (Δy/y) / (Δx/x) — proportionate change in y per proportionate change in x\n\nIn regression models: - Semi-elasticity ≈ Δln(y)/Δx → estimated by the slope in a log-linear model - Elasticity ≈ Δln(y)/Δln(x) → estimated by the slope in a log-log model\nWhen to use each: - Semi-elasticity: When x is measured in natural units (years of education, age) - Elasticity: When both variables are measured in proportions (price and quantity, GDP and investment)\n\nWith the concepts of semi-elasticity and elasticity defined, let’s apply all four model specifications to a real dataset to compare interpretations.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#example-earnings-and-education",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#example-earnings-and-education",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "9.3 Example: Earnings and Education",
    "text": "9.3 Example: Earnings and Education\nResearch Question: How do earnings vary with years of education?\nWe’ll estimate four different models and compare their interpretations:\n\nLinear: earnings = β₀ + β₁(education)\nLog-linear: ln(earnings) = β₀ + β₁(education)\nLog-log: ln(earnings) = β₀ + β₁ln(education)\nLinear-log: earnings = β₀ + β₁ln(education)\n\nDataset: 171 full-time workers aged 30 in 2010 - earnings: Annual earnings in dollars - education: Years of completed schooling\nEach model answers a slightly different question and has different economic interpretation.\n\n# Load and explore the data\ndata_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS.DTA')\n\nprint(\"=\"*70)\nprint(\"DATA SUMMARY: EARNINGS AND EDUCATION\")\nprint(\"=\"*70)\nprint(data_earnings[['earnings', 'education']].describe())\n\nprint(\"\\nFirst 5 observations:\")\nprint(data_earnings[['earnings', 'education']].head())\n\n======================================================================\nDATA SUMMARY: EARNINGS AND EDUCATION\n======================================================================\n            earnings   education\ncount     171.000000  171.000000\nmean    41412.690058   14.432749\nstd     25527.053396    2.735364\nmin      1050.000000    3.000000\n25%     25000.000000   12.000000\n50%     36000.000000   14.000000\n75%     49000.000000   16.000000\nmax    172000.000000   20.000000\n\nFirst 5 observations:\n   earnings  education\n0     25000         14\n1     40000         12\n2     25000         13\n3     38000         13\n4     28800         12\n\n\n\n# Create log-transformed variables\ndata_earnings['lnearn'] = np.log(data_earnings['earnings'])\ndata_earnings['lneduc'] = np.log(data_earnings['education'])\n\nprint(\"=\"*70)\nprint(\"VARIABLES (ORIGINAL AND LOG-TRANSFORMED)\")\nprint(\"=\"*70)\ntable_vars = ['earnings', 'lnearn', 'education', 'lneduc']\nprint(data_earnings[table_vars].describe())\n\nprint(\"\\nNotice:\")\nprint(\"  - Log(earnings) has much less variability (std = 0.62 vs 25,527)\")\nprint(\"  - Log transformation reduces right skewness in earnings\")\n\n======================================================================\nVARIABLES (ORIGINAL AND LOG-TRANSFORMED)\n======================================================================\n            earnings      lnearn   education      lneduc\ncount     171.000000  171.000000  171.000000  171.000000\nmean    41412.690058   10.457638   14.432749    2.648438\nstd     25527.053396    0.622062    2.735364    0.225220\nmin      1050.000000    6.956545    3.000000    1.098633\n25%     25000.000000   10.126631   12.000000    2.484375\n50%     36000.000000   10.491274   14.000000    2.638672\n75%     49000.000000   10.799367   16.000000    2.773438\nmax    172000.000000   12.055250   20.000000    2.996094\n\nNotice:\n  - Log(earnings) has much less variability (std = 0.62 vs 25,527)\n  - Log transformation reduces right skewness in earnings",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-1-linear-model",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-1-linear-model",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Model 1: Linear Model",
    "text": "Model 1: Linear Model\nSpecification: earnings = β₀ + β₁(education) + ε\nInterpretation: β₁ = change in earnings (in dollars) for one additional year of education\n\n# Model 1: Linear\nprint(\"=\"*70)\nprint(\"MODEL 1: LINEAR - earnings = β₀ + β₁(education)\")\nprint(\"=\"*70)\n\nmodel_linear = ols('earnings ~ education', data=data_earnings).fit()\nprint(model_linear.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"Coefficient on education: ${model_linear.params['education']:,.2f}\")\nprint(f\"\\nEconomic meaning:\")\nprint(f\"  Each additional year of education is associated with a\")\nprint(f\"  ${model_linear.params['education']:,.2f} increase in annual earnings.\")\nprint(f\"\\nR² = {model_linear.rsquared:.3f}\")\nprint(f\"  → Education explains {model_linear.rsquared*100:.1f}% of variation in earnings.\")\n\n======================================================================\nMODEL 1: LINEAR - earnings = β₀ + β₁(education)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.289\nModel:                            OLS   Adj. R-squared:                  0.285\nMethod:                 Least Squares   F-statistic:                     68.86\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           3.22e-14\nTime:                        00:01:47   Log-Likelihood:                -1948.1\nNo. Observations:                 171   AIC:                             3900.\nDf Residuals:                     169   BIC:                             3907.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -3.106e+04   8887.835     -3.494      0.001   -4.86e+04   -1.35e+04\neducation   5021.1229    605.101      8.298      0.000    3826.593    6215.653\n==============================================================================\nOmnibus:                       78.232   Durbin-Watson:                   1.783\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              292.688\nSkew:                           1.791   Prob(JB):                     2.78e-64\nKurtosis:                       8.315   Cond. No.                         79.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n======================================================================\nINTERPRETATION\n======================================================================\nCoefficient on education: $5,021.12\n\nEconomic meaning:\n  Each additional year of education is associated with a\n  $5,021.12 increase in annual earnings.\n\nR² = 0.289\n  → Education explains 28.9% of variation in earnings.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-2-log-linear-model",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-2-log-linear-model",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Model 2: Log-Linear Model",
    "text": "Model 2: Log-Linear Model\nSpecification: ln(earnings) = β₀ + β₁(education) + ε\nInterpretation: β₁ = semi-elasticity = proportionate change in earnings for one more year of education\nPractical interpretation: 100β₁ = percentage change in earnings for one more year of education\nThis is the most common specification for earnings equations!\n\n# Model 2: Log-linear\nprint(\"=\"*70)\nprint(\"MODEL 2: LOG-LINEAR - ln(earnings) = β₀ + β₁(education)\")\nprint(\"=\"*70)\n\nmodel_loglin = ols('lnearn ~ education', data=data_earnings).fit()\nprint(model_loglin.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"Coefficient on education: {model_loglin.params['education']:.4f}\")\nprint(f\"\\nEconomic meaning:\")\nprint(f\"  Each additional year of education is associated with a\")\nprint(f\"  {100*model_loglin.params['education']:.2f}% increase in earnings.\")\nprint(f\"\\nWhy this is better than Model 1:\")\nprint(f\"  - Percentage interpretation is more meaningful (scales automatically)\")\nprint(f\"  - 13.1% increase applies whether you earn $30k or $100k\")\nprint(f\"  - Better fit (R² = {model_loglin.rsquared:.3f} vs {model_linear.rsquared:.3f})\")\n\n======================================================================\nMODEL 2: LOG-LINEAR - ln(earnings) = β₀ + β₁(education)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 lnearn   R-squared:                       0.334\nModel:                            OLS   Adj. R-squared:                  0.330\nMethod:                 Least Squares   F-statistic:                     84.74\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.28e-16\nTime:                        00:01:47   Log-Likelihood:                -126.21\nNo. Observations:                 171   AIC:                             256.4\nDf Residuals:                     169   BIC:                             262.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.5608      0.210     40.825      0.000       8.147       8.975\neducation      0.1314      0.014      9.206      0.000       0.103       0.160\n==============================================================================\nOmnibus:                       58.560   Durbin-Watson:                   1.809\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              351.030\nSkew:                          -1.091   Prob(JB):                     5.96e-77\nKurtosis:                       9.671   Cond. No.                         79.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n======================================================================\nINTERPRETATION\n======================================================================\nCoefficient on education: 0.1314\n\nEconomic meaning:\n  Each additional year of education is associated with a\n  13.14% increase in earnings.\n\nWhy this is better than Model 1:\n  - Percentage interpretation is more meaningful (scales automatically)\n  - 13.1% increase applies whether you earn $30k or $100k\n  - Better fit (R² = 0.334 vs 0.289)\n\n\n\nKey Concept 9.3: Interpreting Log-Linear Model Coefficients\nIn the log-linear model ln(y) = β₀ + β₁x, the slope coefficient β₁ is the semi-elasticity of y with respect to x:\n\\[100 \\times \\beta_1 = \\text{percentage change in } y \\text{ when } x \\text{ increases by 1 unit}\\]\nThis is the most common specification in labor economics because a percentage interpretation scales automatically — a 13% return to education applies equally whether you earn $30,000 or $100,000.\nImportant: The exact percentage change for large β₁ is 100 × (e^β₁ - 1), not 100 × β₁. The approximation works well when β₁ &lt; 0.10.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-3-log-log-model",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-3-log-log-model",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Model 3: Log-Log Model",
    "text": "Model 3: Log-Log Model\nSpecification: ln(earnings) = β₀ + β₁ln(education) + ε\nInterpretation: β₁ = elasticity = percentage change in earnings for a 1% change in education\nNote: A “1% increase in education” is a bit artificial (what does 0.14 more years mean?), but this model captures diminishing returns to education.\n\n# Model 3: Log-log\nprint(\"=\"*70)\nprint(\"MODEL 3: LOG-LOG - ln(earnings) = β₀ + β₁ln(education)\")\nprint(\"=\"*70)\n\nmodel_loglog = ols('lnearn ~ lneduc', data=data_earnings).fit()\nprint(model_loglog.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"Coefficient on ln(education): {model_loglog.params['lneduc']:.4f}\")\nprint(f\"\\nEconomic meaning:\")\nprint(f\"  A 1% increase in education is associated with a\")\nprint(f\"  {model_loglog.params['lneduc']:.3f}% increase in earnings (elasticity).\")\nprint(f\"\\nAlternative interpretation:\")\nprint(f\"  If education increases from 14 to 14.14 years (1% increase),\")\nprint(f\"  earnings increase by approximately {model_loglog.params['lneduc']:.2f}%.\")\n\n======================================================================\nMODEL 3: LOG-LOG - ln(earnings) = β₀ + β₁ln(education)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 lnearn   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.282\nMethod:                 Least Squares   F-statistic:                     67.78\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           4.76e-14\nTime:                        00:01:47   Log-Likelihood:                -132.13\nNo. Observations:                 171   AIC:                             268.3\nDf Residuals:                     169   BIC:                             274.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.5454      0.477     13.725      0.000       5.604       7.487\nlneduc         1.4775      0.179      8.233      0.000       1.123       1.832\n==============================================================================\nOmnibus:                       56.464   Durbin-Watson:                   1.786\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              332.274\nSkew:                          -1.048   Prob(JB):                     7.04e-73\nKurtosis:                       9.499   Cond. No.                         35.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n======================================================================\nINTERPRETATION\n======================================================================\nCoefficient on ln(education): 1.4775\n\nEconomic meaning:\n  A 1% increase in education is associated with a\n  1.478% increase in earnings (elasticity).\n\nAlternative interpretation:\n  If education increases from 14 to 14.14 years (1% increase),\n  earnings increase by approximately 1.48%.\n\n\n\nKey Concept 9.4: Interpreting Log-Log Model Coefficients\nIn the log-log model ln(y) = β₀ + β₁ln(x), the slope coefficient β₁ is the elasticity of y with respect to x:\n\\[\\beta_1 = \\frac{\\%\\Delta y}{\\%\\Delta x}\\]\nA 1% increase in x is associated with a β₁% change in y. Unlike semi-elasticity, elasticity is a unit-free measure — it does not depend on the units of measurement.\nEconomic interpretation: If β₁ &lt; 1, there are diminishing returns (each additional percent of x yields less than one percent of y). If β₁ &gt; 1, there are increasing returns.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-4-linear-log-model",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#model-4-linear-log-model",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Model 4: Linear-Log Model",
    "text": "Model 4: Linear-Log Model\nSpecification: earnings = β₀ + β₁ln(education) + ε\nInterpretation: β₁/100 = dollar change in earnings for a 1% increase in education\nThis model is less common but captures diminishing returns (additional years of education have decreasing marginal effects).\n\n# Model 4: Linear-log\nprint(\"=\"*70)\nprint(\"MODEL 4: LINEAR-LOG - earnings = β₀ + β₁ln(education)\")\nprint(\"=\"*70)\n\nmodel_linlog = ols('earnings ~ lneduc', data=data_earnings).fit()\nprint(model_linlog.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"Coefficient on ln(education): {model_linlog.params['lneduc']:,.2f}\")\nprint(f\"\\nEconomic meaning:\")\nprint(f\"  A 1% increase in education is associated with a\")\nprint(f\"  ${model_linlog.params['lneduc']/100:,.2f} increase in annual earnings.\")\nprint(f\"\\nNote: This model has the lowest R² = {model_linlog.rsquared:.3f}\")\n\n======================================================================\nMODEL 4: LINEAR-LOG - earnings = β₀ + β₁ln(education)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.231\nModel:                            OLS   Adj. R-squared:                  0.226\nMethod:                 Least Squares   F-statistic:                     50.69\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           2.96e-11\nTime:                        00:01:47   Log-Likelihood:                -1954.9\nNo. Observations:                 171   AIC:                             3914.\nDf Residuals:                     169   BIC:                             3920.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -1.027e+05   2.03e+04     -5.056      0.000   -1.43e+05   -6.26e+04\nlneduc      5.443e+04   7645.805      7.119      0.000    3.93e+04    6.95e+04\n==============================================================================\nOmnibus:                       79.258   Durbin-Watson:                   1.794\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              284.125\nSkew:                           1.843   Prob(JB):                     2.01e-62\nKurtosis:                       8.128   Cond. No.                         35.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n======================================================================\nINTERPRETATION\n======================================================================\nCoefficient on ln(education): 54,433.42\n\nEconomic meaning:\n  A 1% increase in education is associated with a\n  $544.33 increase in annual earnings.\n\nNote: This model has the lowest R² = 0.231",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#comparison-of-all-four-models",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#comparison-of-all-four-models",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Comparison of All Four Models",
    "text": "Comparison of All Four Models\n\n# Create comparison table\nprint(\"=\"*70)\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(\"=\"*70)\n\ncomparison_df = pd.DataFrame({\n    'Model': ['Linear', 'Log-linear', 'Log-log', 'Linear-log'],\n    'Specification': ['y ~ x', 'ln(y) ~ x', 'ln(y) ~ ln(x)', 'y ~ ln(x)'],\n    'Slope Coefficient': [\n        f\"{model_linear.params[1]:,.2f}\",\n        f\"{model_loglin.params[1]:.4f}\",\n        f\"{model_loglog.params[1]:.4f}\",\n        f\"{model_linlog.params[1]:,.2f}\"\n    ],\n    'Interpretation': [\n        f\"${model_linear.params[1]:,.0f} per year\",\n        f\"{100*model_loglin.params[1]:.1f}% per year\",\n        f\"{model_loglog.params[1]:.2f}% per 1% change\",\n        f\"${model_linlog.params[1]/100:,.0f} per 1% change\"\n    ],\n    'R²': [\n        f\"{model_linear.rsquared:.3f}\",\n        f\"{model_loglin.rsquared:.3f}\",\n        f\"{model_loglog.rsquared:.3f}\",\n        f\"{model_linlog.rsquared:.3f}\"\n    ]\n})\n\nprint(comparison_df.to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"WHICH MODEL IS BEST?\")\nprint(\"=\"*70)\nprint(\"For this data:\")\nprint(f\"  - Best fit (highest R²): Log-linear (R² = {model_loglin.rsquared:.3f})\")\nprint(f\"  - Most interpretable: Log-linear (13.1% return per year of education)\")\nprint(f\"  - Most common in labor economics: Log-linear\")\nprint(\"\\nGeneral guidance:\")\nprint(\"  - Use log-linear when dependent variable is right-skewed\")\nprint(\"  - Use log-log when both variables are right-skewed\")\nprint(\"  - Compare models using R², economic interpretation, and theory\")\n\n======================================================================\nMODEL COMPARISON SUMMARY\n======================================================================\n     Model Specification Slope Coefficient      Interpretation    R²\n    Linear         y ~ x          5,021.12     $5,021 per year 0.289\nLog-linear     ln(y) ~ x            0.1314      13.1% per year 0.334\n   Log-log ln(y) ~ ln(x)            1.4775 1.48% per 1% change 0.286\nLinear-log     y ~ ln(x)         54,433.42  $544 per 1% change 0.231\n\n======================================================================\nWHICH MODEL IS BEST?\n======================================================================\nFor this data:\n  - Best fit (highest R²): Log-linear (R² = 0.334)\n  - Most interpretable: Log-linear (13.1% return per year of education)\n  - Most common in labor economics: Log-linear\n\nGeneral guidance:\n  - Use log-linear when dependent variable is right-skewed\n  - Use log-log when both variables are right-skewed\n  - Compare models using R², economic interpretation, and theory\n\n\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"{model_linear.params[1]:,.2f}\",\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"{model_loglin.params[1]:.4f}\",\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"{model_loglog.params[1]:.4f}\",\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"{model_linlog.params[1]:,.2f}\"\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"${model_linear.params[1]:,.0f} per year\",\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"{100*model_loglin.params[1]:.1f}% per year\",\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"{model_loglog.params[1]:.2f}% per 1% change\",\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1404078335.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  f\"${model_linlog.params[1]/100:,.0f} per 1% change\"\n\n\n\nKey Concept 9.5: Choosing the Right Functional Form\nThe choice between linear, log-linear, log-log, and linear-log specifications should be guided by:\n\nEconomic theory — Does the theory predict absolute or percentage effects?\nData properties — Is the dependent variable right-skewed? Are both variables positive?\nModel fit — Which specification yields the highest R²?\nInterpretation needs — Do you need elasticities, semi-elasticities, or dollar amounts?\n\nIn practice: The log-linear model is most common in economics because many economic relationships involve percentage changes (returns to education, inflation effects, price responses). When in doubt, start with log-linear.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#visualizing-all-four-models",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#visualizing-all-four-models",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Visualizing All Four Models",
    "text": "Visualizing All Four Models\n\n# Create 2x2 comparison plot\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Model 1: Linear\naxes[0, 0].scatter(data_earnings['education'], data_earnings['earnings'],\n                   alpha=0.5, s=20, color='black')\naxes[0, 0].plot(data_earnings['education'], model_linear.fittedvalues,\n                color='blue', linewidth=2, label=f'R² = {model_linear.rsquared:.3f}')\naxes[0, 0].set_xlabel('Education (years)', fontsize=11)\naxes[0, 0].set_ylabel('Earnings ($)', fontsize=11)\naxes[0, 0].set_title('Model 1: Linear\\ny = β₀ + β₁x\\nSlope: $5,021 per year',\n                     fontsize=12, fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Model 2: Log-linear\naxes[0, 1].scatter(data_earnings['education'], data_earnings['lnearn'],\n                   alpha=0.5, s=20, color='black')\naxes[0, 1].plot(data_earnings['education'], model_loglin.fittedvalues,\n                color='blue', linewidth=2, label=f'R² = {model_loglin.rsquared:.3f}')\naxes[0, 1].set_xlabel('Education (years)', fontsize=11)\naxes[0, 1].set_ylabel('ln(Earnings)', fontsize=11)\naxes[0, 1].set_title('Model 2: Log-linear\\nln(y) = β₀ + β₁x\\nSlope: 13.1% per year',\n                     fontsize=12, fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Model 3: Log-log\naxes[1, 0].scatter(data_earnings['lneduc'], data_earnings['lnearn'],\n                   alpha=0.5, s=20, color='black')\naxes[1, 0].plot(data_earnings['lneduc'], model_loglog.fittedvalues,\n                color='blue', linewidth=2, label=f'R² = {model_loglog.rsquared:.3f}')\naxes[1, 0].set_xlabel('ln(Education)', fontsize=11)\naxes[1, 0].set_ylabel('ln(Earnings)', fontsize=11)\naxes[1, 0].set_title('Model 3: Log-log\\nln(y) = β₀ + β₁ln(x)\\nElasticity: 1.48',\n                     fontsize=12, fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Model 4: Linear-log\naxes[1, 1].scatter(data_earnings['lneduc'], data_earnings['earnings'],\n                   alpha=0.5, s=20, color='black')\naxes[1, 1].plot(data_earnings['lneduc'], model_linlog.fittedvalues,\n                color='blue', linewidth=2, label=f'R² = {model_linlog.rsquared:.3f}')\naxes[1, 1].set_xlabel('ln(Education)', fontsize=11)\naxes[1, 1].set_ylabel('Earnings ($)', fontsize=11)\naxes[1, 1].set_title('Model 4: Linear-log\\ny = β₀ + β₁ln(x)\\nSlope: $545 per 1% change',\n                     fontsize=12, fontweight='bold')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.suptitle('Four Model Specifications: Earnings and Education',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Visual Insights:\")\nprint(\"   - Model 1 (linear): Straight line fit, but residuals may be heteroskedastic\")\nprint(\"   - Model 2 (log-linear): Best fit, captures curvature in original data\")\nprint(\"   - Model 3 (log-log): Both axes logged, captures elasticity\")\nprint(\"   - Model 4 (linear-log): Captures diminishing returns to education\")\n\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1405591350.py:54: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  plt.tight_layout()\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_94143/1405591350.py:54: UserWarning: Glyph 8321 (\\N{SUBSCRIPT ONE}) missing from current font.\n  plt.tight_layout()\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 8321 (\\N{SUBSCRIPT ONE}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n📊 Visual Insights:\n   - Model 1 (linear): Straight line fit, but residuals may be heteroskedastic\n   - Model 2 (log-linear): Best fit, captures curvature in original data\n   - Model 3 (log-log): Both axes logged, captures elasticity\n   - Model 4 (linear-log): Captures diminishing returns to education\n\n\nBeyond cross-sectional analysis, logarithms are equally powerful for time series data. Next, we explore how exponential growth becomes linear in logs.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#further-uses-exponential-growth",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#further-uses-exponential-growth",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "9.4 Further Uses: Exponential Growth",
    "text": "9.4 Further Uses: Exponential Growth\nApplication: Modeling exponential growth in time series data\nMany economic series grow exponentially over time: \\[x_t = x_0 \\times (1+r)^t\\]\nWhere: - x₀ = initial value - r = constant growth rate - t = time period\nTaking logarithms: \\[\\ln(x_t) = \\ln(x_0) + \\ln(1+r) \\times t \\approx \\ln(x_0) + r \\times t\\]\nKey insight: Exponential growth in levels → linear growth in logs!\nRegression model: \\[\\ln(x_t) = \\beta_0 + \\beta_1 \\times t + \\varepsilon\\]\nThe slope β₁ directly estimates the growth rate r.\nExample: S&P 500 stock index 1927-2019\n\n# Load S&P 500 data\ndata_sp500 = pd.read_stata(GITHUB_DATA_URL + 'AED_SP500INDEX.DTA')\n\nprint(\"=\"*70)\nprint(\"S&P 500 INDEX DATA (1927-2019)\")\nprint(\"=\"*70)\nprint(data_sp500[['year', 'sp500', 'lnsp500']].describe())\n\nprint(\"\\nFirst and last years:\")\nprint(data_sp500[['year', 'sp500', 'lnsp500']].head(3))\nprint(\"...\")\nprint(data_sp500[['year', 'sp500', 'lnsp500']].tail(3))\n\n======================================================================\nS&P 500 INDEX DATA (1927-2019)\n======================================================================\n             year        sp500    lnsp500\ncount    93.00000    93.000000  93.000000\nmean   1973.00000   473.664307   4.817428\nstd      26.99074   710.751831   1.801842\nmin    1927.00000     6.920000   1.934416\n25%    1950.00000    23.770000   3.168424\n50%    1973.00000    96.470001   4.569232\n75%    1996.00000   740.739990   6.607650\nmax    2019.00000  3230.780029   8.080479\n\nFirst and last years:\n     year      sp500   lnsp500\n0  1927.0  17.660000  2.871302\n1  1928.0  24.350000  3.192532\n2  1929.0  21.450001  3.065725\n...\n      year        sp500   lnsp500\n90  2017.0  2673.610107  7.891185\n91  2018.0  2506.850098  7.826782\n92  2019.0  3230.780029  8.080479\n\n\n\n# Estimate exponential growth model\nprint(\"=\"*70)\nprint(\"EXPONENTIAL GROWTH MODEL: ln(sp500) = β₀ + β₁(year)\")\nprint(\"=\"*70)\n\nmodel_sp500 = ols('lnsp500 ~ year', data=data_sp500).fit()\nprint(model_sp500.summary())\n\ngrowth_rate = model_sp500.params['year']\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"Estimated annual growth rate: {100*growth_rate:.4f}% per year\")\nprint(f\"\\nThis means the S&P 500 grew at an average rate of {100*growth_rate:.2f}% per year\")\nprint(f\"from 1927 to 2019 (not accounting for inflation or dividends).\")\nprint(f\"\\nRule of 72: At {100*growth_rate:.2f}% annual growth,\")\nprint(f\"the index doubles approximately every {72/(100*growth_rate):.1f} years.\")\n\n======================================================================\nEXPONENTIAL GROWTH MODEL: ln(sp500) = β₀ + β₁(year)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                lnsp500   R-squared:                       0.958\nModel:                            OLS   Adj. R-squared:                  0.957\nMethod:                 Least Squares   F-statistic:                     2071.\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           2.16e-64\nTime:                        00:01:48   Log-Likelihood:                -38.919\nNo. Observations:                  93   AIC:                             81.84\nDf Residuals:                      91   BIC:                             86.90\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -124.0933      2.833    -43.798      0.000    -129.721    -118.465\nyear           0.0653      0.001     45.503      0.000       0.062       0.068\n==============================================================================\nOmnibus:                       19.480   Durbin-Watson:                   0.271\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               26.547\nSkew:                           0.981   Prob(JB):                     1.72e-06\nKurtosis:                       4.732   Cond. No.                     1.45e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.45e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n======================================================================\nINTERPRETATION\n======================================================================\nEstimated annual growth rate: 6.5337% per year\n\nThis means the S&P 500 grew at an average rate of 6.53% per year\nfrom 1927 to 2019 (not accounting for inflation or dividends).\n\nRule of 72: At 6.53% annual growth,\nthe index doubles approximately every 11.0 years.\n\n\n\nKey Concept 9.6: Linearizing Exponential Growth\nWhen a variable grows exponentially (x_t = x₀(1+r)^t), its logarithm grows linearly:\n\\[\\ln(x_t) \\approx \\ln(x_0) + r \\times t\\]\nThis transformation is powerful because it converts a nonlinear growth pattern into a linear regression model, where the slope coefficient directly estimates the constant growth rate r.\nPractical implication: To estimate the average growth rate of any exponentially growing series (GDP, stock prices, population), simply regress ln(x) on time. The slope is the growth rate.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#visualizing-exponential-growth",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#visualizing-exponential-growth",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Visualizing Exponential Growth",
    "text": "Visualizing Exponential Growth\n\n# Create visualization showing exponential vs linear in logs\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Panel A: Exponential growth in levels\n# Apply retransformation bias correction\nn = len(data_sp500)\nk = 2\nMSE = np.sum(model_sp500.resid**2) / (n - k)\npsp500 = np.exp(model_sp500.fittedvalues) * np.exp(MSE/2)\n\naxes[0].plot(data_sp500['year'], data_sp500['sp500'], linewidth=2,\n             label='Actual', color='black')\naxes[0].plot(data_sp500['year'], psp500, linewidth=2, linestyle='--',\n             label='Fitted (exponential)', color='blue')\naxes[0].set_xlabel('Year', fontsize=12)\naxes[0].set_ylabel('S&P 500 Index', fontsize=12)\naxes[0].set_title('Panel A: Exponential Growth in Levels',\n                  fontsize=13, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Linear growth in logs\naxes[1].plot(data_sp500['year'], data_sp500['lnsp500'], linewidth=2,\n             label='Actual', color='black')\naxes[1].plot(data_sp500['year'], model_sp500.fittedvalues, linewidth=2,\n             linestyle='--', label='Fitted (linear)', color='blue')\naxes[1].set_xlabel('Year', fontsize=12)\naxes[1].set_ylabel('ln(S&P 500 Index)', fontsize=12)\naxes[1].set_title('Panel B: Linear Growth in Logarithms',\n                  fontsize=13, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('S&P 500 Index: Exponential Growth Linearized by Logarithms',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Key Observation:\")\nprint(\"   - Left panel: Exponential curve in levels (hard to model)\")\nprint(\"   - Right panel: Straight line in logs (easy to model with OLS!)\")\nprint(\"   - The slope of the line = average growth rate\")\n\n\n\n\n\n\n\n\n\n📊 Key Observation:\n   - Left panel: Exponential curve in levels (hard to model)\n   - Right panel: Straight line in logs (easy to model with OLS!)\n   - The slope of the line = average growth rate\n\n\n\nKey Concept 9.7: The Rule of 72\nThe Rule of 72 provides a quick estimate of doubling time for compound growth:\n\\[\\text{Doubling time} \\approx \\frac{72}{r}\\]\nwhere r is the percentage growth rate. This approximation derives from the logarithmic identity: ln(2) ≈ 0.693, combined with ln(1+r) ≈ r for small r.\nExamples: - S&P 500 at 6.5% growth → doubles every 72/6.5 ≈ 11 years - GDP at 3% growth → doubles every 72/3 = 24 years - Population at 1% growth → doubles every 72/1 = 72 years\nPractical value: The Rule of 72 converts growth rates into an intuitive time horizon without needing a calculator.",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#key-takeaways",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#key-takeaways",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nKey Takeaways:\n\nNatural logarithms let us work with proportionate changes instead of absolute changes.\n\nKey approximation: Δln(x) ≈ Δx/x (proportionate change)\n100 × Δln(x) ≈ percentage change in x\nThis approximation is excellent for small changes (&lt; 10%)\n\nFour model specifications give different interpretations:\n\n\n\n\nModel\nSpecification\nInterpretation of β₁\n\n\n\n\nLinear\ny = β₀ + β₁x\nΔy/Δx (absolute change)\n\n\nLog-linear\nln(y) = β₀ + β₁x\nSemi-elasticity: (Δy/y)/Δx\n\n\nLog-log\nln(y) = β₀ + β₁ln(x)\nElasticity: (Δy/y)/(Δx/x)\n\n\nLinear-log\ny = β₀ + β₁ln(x)\nΔy/(Δx/x)\n\n\n\n\nModel selection depends on economic theory, data properties, and interpretation needs:\n\nR² comparison (higher is better, but not the only criterion)\nEconomic theory should guide functional form choice\nLog-linear is most common in applied economics (earnings, prices)\n\nEarnings-Education results illustrate the four specifications:\n\nLinear: Each year of education → $5,021 more earnings\nLog-linear: Each year of education → 13.1% more earnings (best fit, R² = 0.334)\nLog-log: 1% more education → 1.48% more earnings (elasticity)\nLinear-log: 1% more education → $545 more earnings\n\nExponential growth becomes linear in logs:\n\nIf x grows exponentially: x_t = x₀(1+r)^t\nThen ln(x) grows linearly: ln(x_t) ≈ ln(x₀) + r·t\nRegression slope directly estimates the growth rate\nS&P 500: 6.5% annual growth (1927-2019), doubling every 11 years\n\nWhen to use logarithmic transformations:\n\nDependent variable is right-skewed (earnings, prices, firm size)\nEconomic theory predicts percentage effects (elasticities)\nMultiplicative relationships between variables\nExponential growth or decay patterns\nNot when variables can be zero or negative (ln is undefined)\n\n\nPython Tools Used: - numpy.log(): Natural logarithm transformation - numpy.exp(): Exponential function (inverse of log) - statsmodels.ols(): OLS regression estimation - pandas: Data manipulation and summary statistics - matplotlib: Visualization of models and growth patterns\nNext Steps: - Chapter 10: Extend to multiple regression with several explanatory variables - Chapter 11: Statistical inference for multiple regression models - Chapter 15: Further variable transformations (polynomials, interactions)\n\nCongratulations! You now understand how to choose between model specifications, interpret coefficients in log models, and estimate elasticities and semi-elasticities. These tools are fundamental to empirical work in labor economics, industrial organization, macroeconomics, and development economics!",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#practice-exercises",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#practice-exercises",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Logarithmic Approximation Accuracy\nThe logarithmic approximation Δln(x) ≈ Δx/x works well for small changes. Test its accuracy:\n\nCompute the exact proportionate change and the log approximation for x increasing from 100 to 101 (1% change). How close are they?\nRepeat for x increasing from 100 to 110 (10% change). Is the approximation still good?\nRepeat for x increasing from 100 to 150 (50% change). What happens to the approximation error?\nAt what percentage change does the approximation error exceed 1 percentage point?\n\nExercise 2: Interpreting Semi-Elasticity\nA researcher estimates the following log-linear model: ln(wage) = 1.50 + 0.085 × experience, where wage is hourly wage in dollars and experience is years of work experience.\n\nInterpret the coefficient 0.085 in economic terms.\nWhat is the predicted percentage change in wages for a worker gaining 5 more years of experience?\nCalculate the exact percentage change using 100 × (e^(0.085×5) - 1). How does it compare to the approximation?\n\nExercise 3: Interpreting Elasticity\nAn economist estimates a demand function: ln(Q) = 5.2 - 1.3 × ln(P), where Q is quantity demanded and P is price.\n\nWhat is the price elasticity of demand? Is demand elastic or inelastic?\nIf the price increases by 10%, what is the predicted percentage change in quantity demanded?\nWhy is the log-log specification natural for demand analysis?\n\nExercise 4: Model Specification Choice\nFor each research question below, recommend the most appropriate model specification (linear, log-linear, log-log, or linear-log) and explain your reasoning:\n\nHow much does an additional bedroom add to house price (in dollars)?\nWhat is the percentage return to each additional year of education?\nWhat is the price elasticity of demand for gasoline?\nHow does GDP growth relate to years since a policy reform?\n\nExercise 5: Exponential Growth and Rule of 72\nA country’s real GDP per capita was $5,000 in 1990 and grew at an average rate of 4% per year.\n\nUsing the Rule of 72, approximately when did GDP per capita reach $10,000?\nWrite the exponential growth equation for this country’s GDP.\nWhat regression model would you estimate to find the growth rate from data? Write the specification.\nIf another country grew at 2% per year, how many times longer would it take to double its GDP?\n\nExercise 6: Comparing Model Specifications with Data\nUsing the earnings-education results from Section 9.3:\n\nA worker has 12 years of education. Using the linear model, predict their earnings. Using the log-linear model, predict their earnings (hint: you need to exponentiate).\nRepeat for a worker with 18 years of education. Which model gives a larger predicted difference between 12 and 18 years?\nThe log-linear model predicts a 13.1% increase per year of education regardless of current earnings. Explain why this is economically more appealing than a fixed dollar increase.\nWhy can’t we directly compare R² values between the linear model (R² = 0.289) and the log-linear model (R² = 0.334)?",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#case-studies",
    "href": "../notebooks_colab/ch09_Models_with_Natural_Logarithms.html#case-studies",
    "title": "Chapter 9: Models with Natural Logarithms",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study: Logarithmic Models for Global Labor Productivity\nIn this case study, you’ll apply logarithmic model specifications to analyze cross-country labor productivity — a central question in development economics. You’ll use the same Convergence Clubs dataset from earlier chapters, but now focus on how log transformations reveal economic relationships that linear models miss.\nResearch Question: How do logarithmic transformations improve our understanding of cross-country productivity relationships and growth patterns?\nBackground: Labor productivity varies enormously across countries — from less than $1,000 per worker in the poorest nations to over $100,000 in the richest. This extreme right-skewness makes logarithmic transformations essential for meaningful analysis. Development economists use semi-elasticities and elasticities to measure how factors like human capital and physical capital contribute to productivity differences.\nDataset: We’ll use the Mendez Convergence Clubs dataset containing: - country: Country name (108 countries) - year: Year of observation (1990-2014) - lp: Labor productivity (output per worker, in dollars) - rk: Capital per worker (physical capital stock, in thousands of dollars) - hc: Human capital index (based on years of schooling and returns to education)\n\n# Load the Convergence Clubs dataset\nurl_mendez = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndata_cc = pd.read_csv(url_mendez)\n\nprint(\"=\"*70)\nprint(\"CONVERGENCE CLUBS DATASET\")\nprint(\"=\"*70)\nprint(f\"Observations: {len(data_cc)}\")\nprint(f\"Countries: {data_cc['country'].nunique()}\")\nprint(f\"Years: {data_cc['year'].min():.0f} to {data_cc['year'].max():.0f}\")\nprint(f\"\\nVariables: {list(data_cc.columns)}\")\n\nprint(\"\\nFirst 10 observations:\")\nprint(data_cc[['country', 'year', 'lp', 'rk', 'hc']].head(10))\n\n\n\nHow to Use These Tasks\n\nRead each task carefully before starting\nWrite your code in the provided cells (replace _____ blanks in guided tasks)\nRun your code to see results\nAnswer the questions by interpreting your output\nCheck your understanding against the Key Concepts\n\nProgressive difficulty:\n\nTasks 1-2: Guided (fill in blanks with _____)\nTasks 3-4: Semi-guided (partial code structure provided)\nTasks 5-6: Independent (write from outline)\n\nTip: Type the code rather than copying — it helps reinforce the concepts!\n\nTask 1: Explore Productivity Data (Guided)\nObjective: Understand why logarithmic transformations are essential for cross-country productivity data.\nConnection: Section 9.1 (Natural Logarithm Function)\nYour task: Filter the data to 2014, compute summary statistics for productivity in levels and logs, and compare the distributions.\n\nWhat is the ratio of the highest to lowest labor productivity? What does this tell you about skewness?\nHow does the standard deviation change after log transformation?\nCompare the mean and median in levels vs. logs. Which distribution is more symmetric?\n\n\n# Task 1: Explore Productivity Data (Guided)\n# Filter to 2014 cross-section\ndata_2014 = data_cc[data_cc['year'] == _____].copy()\nprint(f\"Countries in 2014: {len(data_2014)}\")\n\n# Create log-transformed variable\ndata_2014['ln_lp'] = np.log(_____)\n\n# Summary statistics: levels vs logs\nprint(\"\\n\" + \"=\"*70)\nprint(\"LABOR PRODUCTIVITY: LEVELS VS. LOGS\")\nprint(\"=\"*70)\nprint(\"\\nIn levels (lp):\")\nprint(data_2014['lp'].describe())\nprint(f\"\\nSkewness: {data_2014['lp'].skew():.3f}\")\n\nprint(\"\\nIn logarithms (ln_lp):\")\nprint(data_2014['ln_lp'].describe())\nprint(f\"\\nSkewness: {data_2014['_____'].skew():.3f}\")\n\n# Ratio of highest to lowest\nprint(f\"\\nMax/Min ratio: {data_2014['lp'].max() / data_2014['lp'].min():.1f}x\")\n\n\n\nTask 2: Log-Linear Model for Productivity (Guided)\nObjective: Estimate a log-linear model to measure the semi-elasticity of productivity with respect to human capital.\nConnection: Section 9.2 (Semi-Elasticities)\nYour task: Estimate ln(lp) = β₀ + β₁ × hc and interpret the coefficient as a semi-elasticity.\n\nWhat is the estimated semi-elasticity of productivity with respect to human capital?\nBy what percentage does productivity increase for each additional unit of the human capital index?\nIs the coefficient statistically significant at the 5% level?\n\n\n# Task 2: Log-Linear Model for Productivity (Guided)\n# Estimate: ln(lp) = β₀ + β₁ × hc\nmodel_hc = ols('_____ ~ _____', data=data_2014).fit()\n\nprint(\"=\"*70)\nprint(\"LOG-LINEAR MODEL: ln(productivity) ~ human capital\")\nprint(\"=\"*70)\nprint(model_hc.summary())\n\n# Interpret the semi-elasticity\nbeta_hc = model_hc.params['hc']\nprint(f\"\\nSemi-elasticity: {beta_hc:.4f}\")\nprint(f\"Interpretation: Each unit increase in human capital is associated with\")\nprint(f\"  a {100*beta_hc:.1f}% _____ in labor productivity.\")\nprint(f\"\\nR² = {model_hc.rsquared:.3f}\")\n\n\n\nTask 3: Comparing Model Specifications (Semi-guided)\nObjective: Estimate all four model specifications using productivity and capital per worker, then compare.\nConnection: Section 9.3 (Example with four models)\nYour tasks:\n\nEstimate four models: linear (lp ~ rk), log-linear (ln_lp ~ rk), log-log (ln_lp ~ ln_rk), and linear-log (lp ~ ln_rk)\nCreate a comparison table showing the specification, slope coefficient, interpretation, and R² for each model\nWhich model provides the best fit? Which provides the most economically meaningful interpretation?\nWhy might the log-log specification be particularly appropriate for the productivity-capital relationship?\n\n\n# Task 3: Comparing Model Specifications (Semi-guided)\n# Create log-transformed variables\ndata_2014['ln_rk'] = np.log(data_2014['rk'])\n\n# Estimate four models (fill in the formulas)\nm1_linear  = ols('lp ~ rk', data=data_2014).fit()\nm2_loglin  = ols('ln_lp ~ rk', data=data_2014).fit()\nm3_loglog  = ols('ln_lp ~ ln_rk', data=data_2014).fit()\nm4_linlog  = ols('lp ~ ln_rk', data=data_2014).fit()\n\n# Create comparison table\n# Hint: Follow the pattern from Section 9.3\nprint(\"=\"*70)\nprint(\"MODEL COMPARISON: Productivity and Capital\")\nprint(\"=\"*70)\n\n# Your comparison table here\n# ...\n\n\nKey Concept 9.8: Functional Form and Cross-Country Comparisons\nWhen analyzing cross-country data, logarithmic models are essential because:\n\nSkewed distributions — Economic variables like GDP, productivity, and capital vary by factors of 100x or more across countries. Log transformations compress this range.\nMultiplicative relationships — Production functions in economics are multiplicative (Y = A × K^α × L^β), which become linear in logs.\nMeaningful comparisons — Percentage differences are more meaningful than absolute differences when comparing Malawi to the United States.\n\nIn practice: The log-log specification is standard in cross-country growth analysis because the slope coefficient directly estimates the output elasticity of capital — a key parameter in growth theory.\n\n\n\nTask 4: Elasticity of Productivity with Respect to Capital (Semi-guided)\nObjective: Estimate the elasticity of labor productivity with respect to capital and interpret it in the context of economic growth theory.\nConnection: Section 9.2 (Elasticities)\nYour tasks:\n\nEstimate the log-log model: ln(lp) = β₀ + β₁ × ln(rk). Report β₁ and R².\nInterpret β₁ as an elasticity. Is there evidence of diminishing returns to capital (β₁ &lt; 1)?\nConstruct a 95% confidence interval for the elasticity. Does it include 1?\nIn growth theory, the output elasticity of capital is often assumed to be about 1/3. Test H₀: β₁ = 0.33 against H₁: β₁ ≠ 0.33.\n\n\n# Task 4: Elasticity of Productivity (Semi-guided)\n# Use the log-log model from Task 3\n\nprint(\"=\"*70)\nprint(\"LOG-LOG MODEL: ln(productivity) ~ ln(capital per worker)\")\nprint(\"=\"*70)\nprint(m3_loglog.summary())\n\n# Elasticity and confidence interval\nelasticity = m3_loglog.params['ln_rk']\nci = m3_loglog.conf_int().loc['ln_rk']\nprint(f\"\\nElasticity: {elasticity:.4f}\")\nprint(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n\n# Test H0: beta = 0.33\n# Your hypothesis test here\n# ...\n\n\n\nTask 5: Productivity Growth Rates (Independent)\nObjective: Estimate exponential growth rates for labor productivity across countries and apply the Rule of 72.\nConnection: Section 9.4 (Exponential Growth)\nYour tasks:\n\nCompute the average labor productivity across all countries for each year (1990-2014)\nEstimate the exponential growth model: ln(avg_lp) = β₀ + β₁ × year. What is the estimated average annual growth rate?\nApply the Rule of 72: approximately how many years does it take for average global labor productivity to double?\nRepeat the growth rate estimation for two regions or groups of countries (e.g., high-income vs. low-income). Do growth rates differ?\n\n\n# Task 5: Productivity Growth Rates (Independent)\n# Compute average productivity per year\n\n# Your code here\n# ...\n\n\n\nTask 6: Development Policy Brief (Independent)\nObjective: Synthesize your findings into a policy-relevant summary.\nConnection: All sections\nYour task: Write a 200-300 word summary analyzing cross-country labor productivity using logarithmic models. Your brief should include:\n\nData description: How many countries, what time period, key variables\nModel selection: Which logarithmic specification best captures the productivity-capital relationship and why\nKey findings: Elasticity estimates, semi-elasticity of human capital, growth rates\nPolicy implications: What do these results suggest for developing countries seeking to increase productivity?\n\n\n\n\nYour Development Policy Brief\n(Write your 200-300 word summary here)\n\n\nKey Concept 9.9: Logarithmic Models in Development Economics\nLogarithmic model specifications are the standard tool in development economics for analyzing cross-country differences because:\n\nSemi-elasticities measure the percentage return to factors like education and human capital — directly informing policy about where to invest\nElasticities from log-log models estimate production function parameters (output elasticity of capital) — testing whether countries face diminishing returns\nGrowth rates from log-time regressions enable comparisons across countries growing at very different speeds\n\nThe bottom line: Without logarithmic transformations, cross-country analysis would be dominated by a few rich outliers and miss the economic relationships that matter for development policy.\n\n\n\nWhat You’ve Learned from This Case Study\nCongratulations! You’ve applied logarithmic model specifications to real cross-country economic data.\nStatistical Skills:\n\nApplied log transformations to highly skewed cross-country data\nEstimated and compared all four model specifications (linear, log-linear, log-log, linear-log)\nInterpreted semi-elasticities (human capital → productivity)\nInterpreted elasticities (capital → productivity, with diminishing returns)\nEstimated exponential growth rates and applied the Rule of 72\n\nEconomic Insights:\n\nCross-country productivity distributions are highly right-skewed, requiring log transformations\nHuman capital and physical capital both contribute to productivity, but with diminishing returns\nLog-log models connect directly to production function theory in economics\nGrowth rates vary substantially across countries and regions\n\nNext Steps:\n\nChapter 10: Add multiple explanatory variables simultaneously (multiple regression)\nChapter 11: Test joint hypotheses about capital AND human capital effects",
    "crumbs": [
      "Bivariate Regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 9: Models with Natural Logarithms</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to multiple regression analysis. You’ll learn how to work with multiple explanatory variables, interpret partial effects, assess model fit, and detect multicollinearity. All code runs directly in Google Colab without any local setup.\nThis chapter extends bivariate regression to the more realistic case where we want to predict an outcome using multiple explanatory variables simultaneously. Multiple regression allows us to estimate the partial effect of each variable while controlling for others—a crucial feature for empirical economic analysis.\nLearning Objectives:\nBy the end of this chapter, you will be able to:\nDataset used: - AED_HOUSE.DTA: 29 houses sold in Davis, California (1999) with price, size, bedrooms, bathrooms, lot size, age, and month sold\nKey economic question: What is the effect of house size on price after controlling for other characteristics like bedrooms, bathrooms, and age?\nChapter outline: - 10.1 Example: House Price and Characteristics - 10.2 Two-Way Scatterplots - 10.3 Correlation Analysis - 10.4 Multiple Regression Estimation - 10.5 Partial Effects — The FWL Theorem - 10.6 Model Fit Statistics - 10.7 Model Comparison - 10.8 Inestimable Models and Multicollinearity - Key Takeaways - Practice Exercises - Case Studies\nEstimated time: 60-75 minutes",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#chapter-overview",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#chapter-overview",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "",
    "text": "Extend bivariate regression concepts to multiple regression with several regressors\nInterpret pairwise correlations and use them for exploratory data analysis\nUnderstand the ordinary least squares (OLS) method for multiple regression\nInterpret partial effects: how one regressor affects \\(y\\) while holding others constant\nDistinguish between partial effects and total effects\nEvaluate model fit using R-squared and adjusted R-squared\nUnderstand information criteria (AIC, BIC) for model selection\nRecognize when regression coefficients cannot be estimated (perfect collinearity)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#setup",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#setup",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to analyze house price data.\")\n\nSetup complete! Ready to analyze house price data.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#example---house-price-and-characteristics",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#example---house-price-and-characteristics",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.1: Example - House Price and Characteristics",
    "text": "10.1: Example - House Price and Characteristics\nWe begin with a real estate dataset from Davis, California. Understanding house prices is a classic economic application because prices reflect both fundamental characteristics (size, bedrooms) and market conditions.\nThe dataset contains:\n\nPrice: Sale price in dollars\nSize: House size in square feet\nBedrooms: Number of bedrooms\nBathrooms: Number of bathrooms\nLotsize: Size of lot (1=small, 2=medium, 3=large)\nAge: House age in years\nMonthsold: Month of year house was sold\n\nEconomic motivation: A simple regression of price on bedrooms might find a positive relationship, but is this because bedrooms directly add value, or because houses with more bedrooms tend to be larger? Multiple regression helps us disentangle these effects.\n\n# Load house price data\ndata_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n\n# Display first few observations\nprint(\"First 10 observations:\")\nprint(data_house[['price', 'size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold']].head(10))\n\n# Display data summary\nprint(\"\\nSummary statistics:\")\nprint(data_house.describe())\n\nFirst 10 observations:\n    price  size  bedrooms  bathrooms  lotsize   age  monthsold\n0  204000  1400         3        2.0        1  31.0          7\n1  212000  1600         3        3.0        2  33.0          5\n2  213000  1800         3        2.0        2  51.0          4\n3  220000  1600         3        2.0        1  49.0          4\n4  224500  2100         4        2.5        2  47.0          6\n5  229000  1700         4        2.5        2  35.0          3\n6  230000  2100         4        2.0        2  34.0          8\n7  233000  1700         3        2.0        1  40.0          6\n8  235000  1700         4        2.0        2  29.0          7\n9  235000  1600         3        2.0        3  35.0          5\n\nSummary statistics:\n               price         size   bedrooms  bathrooms    lotsize        age  \\\ncount      29.000000    29.000000  29.000000  29.000000  29.000000  29.000000   \nmean   253910.344828  1882.758621   3.793103   2.206897   2.137931  36.413792   \nstd     37390.710695   398.272130   0.675030   0.341144   0.693034   7.118975   \nmin    204000.000000  1400.000000   3.000000   2.000000   1.000000  23.000000   \n25%    233000.000000  1600.000000   3.000000   2.000000   2.000000  31.000000   \n50%    244000.000000  1800.000000   4.000000   2.000000   2.000000  35.000000   \n75%    270000.000000  2000.000000   4.000000   2.500000   3.000000  39.000000   \nmax    375000.000000  3300.000000   6.000000   3.000000   3.000000  51.000000   \n\n       monthsold           list  \ncount  29.000000      29.000000  \nmean    5.965517  257824.137931  \nstd     1.679344   40860.264099  \nmin     3.000000  199900.000000  \n25%     5.000000  239000.000000  \n50%     6.000000  245000.000000  \n75%     7.000000  269000.000000  \nmax     8.000000  386000.000000",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#bivariate-vs.-multiple-regression",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#bivariate-vs.-multiple-regression",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Bivariate vs. Multiple Regression",
    "text": "Bivariate vs. Multiple Regression\nLet’s compare a simple regression (price on bedrooms only) with a multiple regression (price on bedrooms AND size). This illustrates how controlling for other variables changes coefficient estimates.\nKey insight: In the bivariate regression, the bedrooms coefficient captures both the direct effect of bedrooms and the indirect effect through correlation with size. In multiple regression, we isolate the partial effect of bedrooms holding size constant.\n\n# Bivariate regression: price ~ bedrooms\nmodel_bivariate = ols('price ~ bedrooms', data=data_house).fit()\n\nprint(\"=\" * 70)\nprint(\"BIVARIATE REGRESSION: price ~ bedrooms\")\nprint(\"=\" * 70)\nprint(model_bivariate.summary())\n\n# Multiple regression: price ~ bedrooms + size\nmodel_multiple = ols('price ~ bedrooms + size', data=data_house).fit()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MULTIPLE REGRESSION: price ~ bedrooms + size\")\nprint(\"=\" * 70)\nprint(model_multiple.summary())\n\n# Compare bedrooms coefficient\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COEFFICIENT COMPARISON\")\nprint(\"=\" * 70)\nprint(f\"Bedrooms coefficient (bivariate):  ${model_bivariate.params['bedrooms']:,.2f}\")\nprint(f\"Bedrooms coefficient (multiple):   ${model_multiple.params['bedrooms']:,.2f}\")\nprint(f\"Change: ${model_multiple.params['bedrooms'] - model_bivariate.params['bedrooms']:,.2f}\")\nprint(\"\\nThe coefficient drops dramatically because bedrooms was capturing size effects.\")\n\n======================================================================\nBIVARIATE REGRESSION: price ~ bedrooms\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.183\nModel:                            OLS   Adj. R-squared:                  0.152\nMethod:                 Least Squares   F-statistic:                     6.030\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):             0.0208\nTime:                        15:16:42   Log-Likelihood:                -343.06\nNo. Observations:                  29   AIC:                             690.1\nDf Residuals:                      27   BIC:                             692.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.641e+05   3.71e+04      4.423      0.000     8.8e+04     2.4e+05\nbedrooms    2.367e+04   9637.976      2.456      0.021    3891.805    4.34e+04\n==============================================================================\nOmnibus:                       23.468   Durbin-Watson:                   0.345\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               35.285\nSkew:                           1.939   Prob(JB):                     2.18e-08\nKurtosis:                       6.764   Cond. No.                         23.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n======================================================================\nMULTIPLE REGRESSION: price ~ bedrooms + size\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.618\nModel:                            OLS   Adj. R-squared:                  0.589\nMethod:                 Least Squares   F-statistic:                     21.03\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           3.68e-06\nTime:                        15:16:42   Log-Likelihood:                -332.03\nNo. Observations:                  29   AIC:                             670.1\nDf Residuals:                      26   BIC:                             674.2\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.117e+05   2.76e+04      4.048      0.000     5.5e+04    1.68e+05\nbedrooms    1553.4580   7846.866      0.198      0.845   -1.46e+04    1.77e+04\nsize          72.4081     13.300      5.444      0.000      45.070      99.746\n==============================================================================\nOmnibus:                        0.516   Durbin-Watson:                   1.230\nProb(Omnibus):                  0.773   Jarque-Bera (JB):                0.609\nSkew:                          -0.086   Prob(JB):                        0.737\nKurtosis:                       2.311   Cond. No.                     1.21e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.21e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n======================================================================\nCOEFFICIENT COMPARISON\n======================================================================\nBedrooms coefficient (bivariate):  $23,667.30\nBedrooms coefficient (multiple):   $1,553.46\nChange: $-22,113.84\n\nThe coefficient drops dramatically because bedrooms was capturing size effects.\n\n\n\nKey Concept 10.1: Partial Effects vs. Total Effects in Multiple Regression\nIn bivariate regression, the bedrooms coefficient ($23,667) captures both the direct effect of bedrooms and the indirect effect through correlation with size. In multiple regression, the bedrooms coefficient drops to $1,553 — the partial effect holding size constant. This dramatic change illustrates why controlling for confounders is essential for isolating individual variable effects.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#two-way-scatterplots",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#two-way-scatterplots",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.2: Two-Way Scatterplots",
    "text": "10.2: Two-Way Scatterplots\nBefore running multiple regression, it’s useful to visualize pairwise relationships between variables. A scatterplot matrix shows all two-way scatterplots simultaneously.\nWhat to look for:\n\nStrong linear relationships (potential predictors of price)\nCorrelation between explanatory variables (potential multicollinearity)\nOutliers or non-linear patterns\n\nThe diagonal shows the distribution of each variable using kernel density estimates (KDE).\n\n# Create scatterplot matrix\nplot_vars = ['price', 'size', 'bedrooms', 'age']\ng = sns.pairplot(data_house[plot_vars], diag_kind='kde', plot_kws={'alpha': 0.6, 's': 50})\ng.fig.suptitle('Figure 10.1: Scatterplot Matrix - House Price Data', \n               fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"Scatterplot matrix created.\")\nprint(\"Notice: Price shows strongest relationship with Size.\")\n\n\n\n\n\n\n\n\nScatterplot matrix created.\nNotice: Price shows strongest relationship with Size.\n\n\n\nKey Concept 10.2: Exploratory Data Analysis with Scatterplot Matrices\nPairwise scatterplot matrices display all two-way relationships simultaneously, revealing linear associations, nonlinearities, clusters, and outliers before formal modeling. They also highlight potential multicollinearity: if two regressors are tightly correlated (e.g., size and bedrooms), their individual effects may be hard to separate in a regression.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#correlation-analysis",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#correlation-analysis",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.3: Correlation Analysis",
    "text": "10.3: Correlation Analysis\nThe correlation coefficient measures the strength of linear association between two variables, ranging from -1 (perfect negative) to +1 (perfect positive).\nCorrelation matrix insights:\n\nPrice is most correlated with Size (r = 0.79), then Bedrooms (r = 0.43)\nSize and Bedrooms are correlated (r = 0.52), which can cause multicollinearity\nCorrelation ≠ causation (merely shows association)\n\nA correlation heatmap provides visual representation with color intensity showing strength of correlation.\n\n# Calculate correlation matrix\ncorr_vars = ['price', 'size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold']\ncorr_matrix = data_house[corr_vars].corr()\n\nprint(\"Correlation Matrix:\")\nprint(corr_matrix)\n\n# Visualize correlation matrix as heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nax.set_title('Correlation Matrix Heatmap', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey observations:\")\nprint(f\"  - Price-Size correlation: {corr_matrix.loc['price', 'size']:.3f} (strongest predictor)\")\nprint(f\"  - Price-Bedrooms correlation: {corr_matrix.loc['price', 'bedrooms']:.3f}\")\nprint(f\"  - Size-Bedrooms correlation: {corr_matrix.loc['size', 'bedrooms']:.3f} (multicollinearity concern)\")\n\nCorrelation Matrix:\n              price      size  bedrooms  bathrooms   lotsize       age  \\\nprice      1.000000  0.785782  0.427275   0.329793  0.153479 -0.068015   \nsize       0.785782  1.000000  0.517630   0.316338  0.112437  0.076925   \nbedrooms   0.427275  0.517630  1.000000   0.037435  0.292206 -0.026140   \nbathrooms  0.329793  0.316338  0.037435   1.000000  0.101575  0.037018   \nlotsize    0.153479  0.112437  0.292206   0.101575  1.000000 -0.019220   \nage       -0.068015  0.076925 -0.026140   0.037018 -0.019220  1.000000   \nmonthsold -0.209985 -0.214511  0.182512  -0.392310 -0.057140 -0.366207   \n\n           monthsold  \nprice      -0.209985  \nsize       -0.214511  \nbedrooms    0.182512  \nbathrooms  -0.392310  \nlotsize    -0.057140  \nage        -0.366207  \nmonthsold   1.000000  \n\n\n\n\n\n\n\n\n\n\nKey observations:\n  - Price-Size correlation: 0.786 (strongest predictor)\n  - Price-Bedrooms correlation: 0.427\n  - Size-Bedrooms correlation: 0.518 (multicollinearity concern)\n\n\n\nKey Concept 10.3: Correlation vs. Causation in Multivariate Analysis\nHigh bivariate correlation (e.g., bedrooms-price, \\(r = 0.43\\)) may diminish or vanish after controlling for confounders. In our data, bedrooms correlate with price largely because bigger houses have more bedrooms. Multiple regression isolates each variable’s partial contribution, revealing that size — not bedrooms — drives most of the price variation.\n\nHaving explored the data visually and through correlations, we now estimate the formal multiple regression model to quantify partial effects.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#multiple-regression-estimation",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#multiple-regression-estimation",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.4: Multiple Regression Estimation",
    "text": "10.4: Multiple Regression Estimation\nNow we estimate the full multiple regression model with all available predictors. The regression equation is:\n\\[\\widehat{\\text{price}} = b_1 + b_2 \\times \\text{size} + b_3 \\times \\text{bedrooms} + b_4 \\times \\text{bathrooms} + b_5 \\times \\text{lotsize} + b_6 \\times \\text{age} + b_7 \\times \\text{monthsold}\\]\nOrdinary Least Squares (OLS) chooses coefficients \\(b_1, ..., b_7\\) to minimize the sum of squared residuals:\n\\[\\min \\sum_{i=1}^{n} (y_i - \\widehat{y}_i)^2\\]\nwhere \\(y_i\\) is the actual price and \\(\\widehat{y}_i\\) is the predicted price.\n\n# Estimate full multiple regression model\nmodel_full = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n                 data=data_house).fit()\n\nprint(\"=\" * 70)\nprint(\"FULL MULTIPLE REGRESSION MODEL\")\nprint(\"=\" * 70)\nprint(model_full.summary())\n\n======================================================================\nFULL MULTIPLE REGRESSION MODEL\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.651\nModel:                            OLS   Adj. R-squared:                  0.555\nMethod:                 Least Squares   F-statistic:                     6.826\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000342\nTime:                        15:16:45   Log-Likelihood:                -330.74\nNo. Observations:                  29   AIC:                             675.5\nDf Residuals:                      22   BIC:                             685.1\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.378e+05   6.15e+04      2.242      0.035    1.03e+04    2.65e+05\nsize          68.3694     15.389      4.443      0.000      36.454     100.285\nbedrooms    2685.3151   9192.526      0.292      0.773   -1.64e+04    2.17e+04\nbathrooms   6832.8800   1.57e+04      0.435      0.668   -2.58e+04    3.94e+04\nlotsize     2303.2214   7226.535      0.319      0.753   -1.27e+04    1.73e+04\nage         -833.0386    719.335     -1.158      0.259   -2324.847     658.770\nmonthsold  -2088.5036   3520.898     -0.593      0.559   -9390.399    5213.392\n==============================================================================\nOmnibus:                        1.317   Durbin-Watson:                   1.259\nProb(Omnibus):                  0.518   Jarque-Bera (JB):                0.980\nSkew:                           0.151   Prob(JB):                        0.612\nKurtosis:                       2.152   Cond. No.                     2.59e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.59e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#coefficient-interpretation-with-confidence-intervals",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#coefficient-interpretation-with-confidence-intervals",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Coefficient Interpretation with Confidence Intervals",
    "text": "Coefficient Interpretation with Confidence Intervals\nEach regression coefficient represents a partial effect: the expected change in price when that variable increases by one unit, holding all other variables constant.\nExample interpretation (Size coefficient): - Coefficient ≈ $68.37 per square foot - Interpretation: A one square foot increase in house size is associated with a $68.37 increase in price, holding bedrooms, bathrooms, lot size, age, and month sold constant.\nThe 95% confidence interval tells us the range of plausible values for each coefficient.\n\n# Display coefficients with 95% confidence intervals\nconf_int = model_full.conf_int(alpha=0.05)\ncoef_table = pd.DataFrame({\n    'Coefficient': model_full.params,\n    'Std. Error': model_full.bse,\n    'CI Lower': conf_int.iloc[:, 0],\n    'CI Upper': conf_int.iloc[:, 1],\n    't-statistic': model_full.tvalues,\n    'p-value': model_full.pvalues\n})\n\nprint(\"Coefficients with 95% Confidence Intervals:\")\nprint(coef_table)\n\nprint(\"\\nKey result: Size is the only statistically significant predictor (p &lt; 0.05).\")\n\nCoefficients with 95% Confidence Intervals:\n             Coefficient    Std. Error      CI Lower       CI Upper  \\\nIntercept  137791.065699  61464.951869  10320.557398  265261.573999   \nsize           68.369419     15.389472     36.453608     100.285230   \nbedrooms     2685.315122   9192.525674 -16378.816300   21749.446543   \nbathrooms    6832.880015  15721.191544 -25770.875723   39436.635753   \nlotsize      2303.221371   7226.535205 -12683.695364   17290.138107   \nage          -833.038602    719.334544  -2324.847139     658.769936   \nmonthsold   -2088.503625   3520.897859  -9390.398871    5213.391620   \n\n           t-statistic   p-value  \nIntercept     2.241783  0.035387  \nsize          4.442610  0.000205  \nbedrooms      0.292119  0.772932  \nbathrooms     0.434629  0.668065  \nlotsize       0.318717  0.752947  \nage          -1.158068  0.259254  \nmonthsold    -0.593174  0.559114  \n\nKey result: Size is the only statistically significant predictor (p &lt; 0.05).\n\n\n\nKey Concept 10.4: Interpreting Partial Effects in Multiple Regression\nEach coefficient \\(b_j\\) measures the expected change in \\(y\\) when \\(x_j\\) increases by one unit, holding all other regressors constant. For example, a size coefficient of $68.37 means each additional square foot is associated with a $68.37 price increase, controlling for bedrooms, bathrooms, lot size, age, and month sold. Statistical significance is assessed through confidence intervals and \\(t\\)-tests.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#partial-effects---the-fwl-theorem",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#partial-effects---the-fwl-theorem",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.5: Partial Effects - The FWL Theorem",
    "text": "10.5: Partial Effects - The FWL Theorem\nThe Frisch-Waugh-Lovell (FWL) Theorem states that the coefficient on any variable in multiple regression equals the coefficient from a bivariate regression of \\(y\\) on the residualized version of that variable.\nDemonstration:\n\nRegress size on all other variables, obtain residuals \\(\\widetilde{\\text{size}}\\)\nRegress price on \\(\\widetilde{\\text{size}}\\) only\nThe coefficient will exactly match the size coefficient from the full multiple regression\n\nIntuition: The residual \\(\\widetilde{\\text{size}}\\) represents the variation in size that is not explained by other variables. This is why multiple regression isolates partial effects.\n\n# Step 1: Regress size on all other variables\nmodel_size_on_others = ols('size ~ bedrooms + bathrooms + lotsize + age + monthsold',\n                            data=data_house).fit()\nresid_size = model_size_on_others.resid\n\n# Step 2: Regress price on residualized size\ndata_house['resid_size'] = resid_size\nmodel_price_on_resid = ols('price ~ resid_size', data=data_house).fit()\n\n# Compare coefficients\nprint(\"=\" * 70)\nprint(\"DEMONSTRATION: FWL THEOREM (Partial Effects)\")\nprint(\"=\" * 70)\nprint(f\"Size coefficient from FULL multiple regression:  {model_full.params['size']:.10f}\")\nprint(f\"Coefficient on residualized size (bivariate):    {model_price_on_resid.params['resid_size']:.10f}\")\nprint(f\"Difference (numerical precision):                 {abs(model_full.params['size'] - model_price_on_resid.params['resid_size']):.15f}\")\nprint(\"\\nThese coefficients are identical! This proves the partial effect interpretation.\")\n\n======================================================================\nDEMONSTRATION: FWL THEOREM (Partial Effects)\n======================================================================\nSize coefficient from FULL multiple regression:  68.3694189767\nCoefficient on residualized size (bivariate):    68.3694189767\nDifference (numerical precision):                 0.000000000000057\n\nThese coefficients are identical! This proves the partial effect interpretation.\n\n\n\nKey Concept 10.5: The Frisch-Waugh-Lovell (FWL) Theorem\nThe partial effect of \\(x_j\\) in a multiple regression equals the slope from a bivariate regression of \\(y\\) on \\(\\widetilde{x}_j\\), where \\(\\widetilde{x}_j\\) is the residual from regressing \\(x_j\\) on all other regressors. Intuitively, \\(\\widetilde{x}_j\\) captures the variation in \\(x_j\\) that is independent of the other variables — this is exactly what multiple regression uses to estimate partial effects.\n\nNow that we understand partial effects and the FWL theorem, let’s evaluate how well the overall regression model fits the data.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#model-fit-statistics",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#model-fit-statistics",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.6: Model Fit Statistics",
    "text": "10.6: Model Fit Statistics\nSeveral statistics summarize how well the regression model fits the data:\nR-squared (\\(R^2\\)):\n\nFraction of variation in \\(y\\) explained by the regressors\nFormula: \\(R^2 = \\frac{\\text{Explained SS}}{\\text{Total SS}} = 1 - \\frac{\\text{Residual SS}}{\\text{Total SS}}\\)\nRange: 0 to 1 (higher is better fit)\nProblem: Always increases when adding variables (even irrelevant ones)\n\nAdjusted R-squared (\\(\\bar{R}^2\\)):\n\nPenalizes model complexity (number of parameters \\(k\\))\nFormula: \\(\\bar{R}^2 = 1 - \\frac{\\text{RSS}/(n-k)}{\\text{TSS}/(n-1)}\\)\nCan decrease when adding unhelpful variables\nPreferred for model comparison\n\nRoot MSE (Standard error of regression):\n\nTypical size of prediction error\nFormula: \\(s_e = \\sqrt{\\frac{1}{n-k}\\sum (y_i - \\widehat{y}_i)^2}\\)\nSame units as \\(y\\) (dollars in our case)\n\n\n# Calculate and display model fit statistics\nn = len(data_house)\nk = len(model_full.params)  # includes intercept\ndf = n - k\n\nprint(\"=\" * 70)\nprint(\"MODEL FIT STATISTICS\")\nprint(\"=\" * 70)\nprint(f\"Sample size (n):               {n}\")\nprint(f\"Number of parameters (k):      {k}\")\nprint(f\"Degrees of freedom (n-k):      {df}\")\nprint(f\"\\nR-squared:                     {model_full.rsquared:.6f}\")\nprint(f\"Adjusted R-squared:            {model_full.rsquared_adj:.6f}\")\nprint(f\"Root MSE:                      ${np.sqrt(model_full.mse_resid):,.2f}\")\n\n# Verify R² = [Corr(y, ŷ)]²\npredicted = model_full.fittedvalues\ncorr_y_yhat = np.corrcoef(data_house['price'], predicted)[0, 1]\nprint(f\"\\nVerification: R² = [Corr(y, ŷ)]²\")\nprint(f\"  Correlation(y, ŷ):           {corr_y_yhat:.6f}\")\nprint(f\"  [Correlation(y, ŷ)]²:        {corr_y_yhat**2:.6f}\")\nprint(f\"  R² from model:                {model_full.rsquared:.6f}\")\nprint(f\"  Match: {np.isclose(corr_y_yhat**2, model_full.rsquared)}\")\n\n======================================================================\nMODEL FIT STATISTICS\n======================================================================\nSample size (n):               29\nNumber of parameters (k):      7\nDegrees of freedom (n-k):      22\n\nR-squared:                     0.650553\nAdjusted R-squared:            0.555249\nRoot MSE:                      $24,935.73\n\nVerification: R² = [Corr(y, ŷ)]²\n  Correlation(y, ŷ):           0.806569\n  [Correlation(y, ŷ)]²:        0.650553\n  R² from model:                0.650553\n  Match: True",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#information-criteria-aic-and-bic",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#information-criteria-aic-and-bic",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Information Criteria (AIC and BIC)",
    "text": "Information Criteria (AIC and BIC)\nAkaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are more sophisticated measures that penalize model complexity:\n\\[\\text{AIC} = n \\times \\ln(\\widehat{\\sigma}_e^2) + n(1 + \\ln 2\\pi) + 2k\\] \\[\\text{BIC} = n \\times \\ln(\\widehat{\\sigma}_e^2) + n(1 + \\ln 2\\pi) + k \\times \\ln(n)\\]\nKey points:\n\nLower values are better (unlike R²)\nBIC penalizes complexity more heavily than AIC\nUseful for comparing non-nested models\nDifferent software packages may use different conventions (scaling by \\(n\\))\n\n\n# Calculate information criteria\nprint(\"=\" * 70)\nprint(\"INFORMATION CRITERIA\")\nprint(\"=\" * 70)\n\n# AIC and BIC from statsmodels\nprint(f\"AIC (statsmodels):             {model_full.aic:.4f}\")\nprint(f\"BIC (statsmodels):             {model_full.bic:.4f}\")\n\n# Manual calculation (Stata convention)\nrss = np.sum(model_full.resid ** 2)\naic_stata = n * np.log(rss/n) + n * (1 + np.log(2*np.pi)) + 2*k\nbic_stata = n * np.log(rss/n) + n * (1 + np.log(2*np.pi)) + k*np.log(n)\n\nprint(f\"\\nAIC (Stata convention):        {aic_stata:.4f}\")\nprint(f\"BIC (Stata convention):        {bic_stata:.4f}\")\nprint(\"\\nNote: Different conventions yield different values, but ranking is consistent.\")\n\n======================================================================\nINFORMATION CRITERIA\n======================================================================\nAIC (statsmodels):             675.4824\nBIC (statsmodels):             685.0535\n\nAIC (Stata convention):        675.4824\nBIC (Stata convention):        685.0535\n\nNote: Different conventions yield different values, but ranking is consistent.\n\n\n\nKey Concept 10.6: Model Selection with Adjusted R-squared vs. Information Criteria\nAdjusted \\(R^2\\) penalizes complexity mildly by dividing sums of squares by degrees of freedom. Information criteria (AIC, BIC) impose stronger penalties: BIC = \\(n \\ln(\\hat{\\sigma}_e^2) + k \\ln(n)\\). Smaller AIC/BIC values indicate better models. BIC is generally preferred because its penalty grows with sample size, favoring more parsimonious specifications.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#model-comparison",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#model-comparison",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.7: Model Comparison",
    "text": "10.7: Model Comparison\nIt’s often useful to compare multiple model specifications side-by-side. Here we compare:\n\nSimple model: Price predicted by size only\nFull model: Price predicted by all variables\n\nKey comparison points:\n\nHow much does R² improve?\nDoes adjusted R² improve (accounting for added complexity)?\nHow do coefficient estimates change?\n\nEconomic interpretation: If adding 5 more variables only modestly improves fit, the simple model may be preferred (parsimony principle).\n\n# Estimate simple model (size only)\nmodel_simple = ols('price ~ size', data=data_house).fit()\n\n# Create comparison table\nprint(\"=\" * 70)\nprint(\"MODEL COMPARISON: Simple vs. Full\")\nprint(\"=\" * 70)\n\ncomparison_stats = pd.DataFrame({\n    'Model': ['Simple (size only)', 'Full (all variables)'],\n    'R²': [model_simple.rsquared, model_full.rsquared],\n    'Adj R²': [model_simple.rsquared_adj, model_full.rsquared_adj],\n    'AIC': [model_simple.aic, model_full.aic],\n    'BIC': [model_simple.bic, model_full.bic],\n    'N': [n, n]\n})\n\nprint(comparison_stats.to_string(index=False))\n\nprint(\"\\nInterpretation:\")\nprint(f\"  - R² increases from {model_simple.rsquared:.3f} to {model_full.rsquared:.3f} (+{model_full.rsquared - model_simple.rsquared:.3f})\")\nprint(f\"  - Adj R² DECREASES from {model_simple.rsquared_adj:.3f} to {model_full.rsquared_adj:.3f} ({model_full.rsquared_adj - model_simple.rsquared_adj:.3f})\")\nprint(\"  - This suggests the added variables don't improve fit enough to justify complexity.\")\nprint(\"  - Simple model may be preferred (parsimony principle).\")\n\n======================================================================\nMODEL COMPARISON: Simple vs. Full\n======================================================================\n               Model       R²   Adj R²        AIC        BIC  N\n  Simple (size only) 0.617453 0.603285 668.106844 670.841436 29\nFull (all variables) 0.650553 0.555249 675.482401 685.053472 29\n\nInterpretation:\n  - R² increases from 0.617 to 0.651 (+0.033)\n  - Adj R² DECREASES from 0.603 to 0.555 (-0.048)\n  - This suggests the added variables don't improve fit enough to justify complexity.\n  - Simple model may be preferred (parsimony principle).\n\n\n\nKey Concept 10.7: The Parsimony Principle\nSimpler models are preferred unless additional variables meaningfully improve fit. In our house price example, adding five variables beyond size barely increased \\(R^2\\) (0.618 \\(\\to\\) 0.651) while adjusted \\(R^2\\) actually fell (0.603 \\(\\to\\) 0.555). Check adjusted \\(R^2\\), AIC, and BIC — if they don’t improve, the simpler model is preferred.\n\nHaving compared models, we now turn to a critical pitfall: when regressors are too closely related to separate their individual effects.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#inestimable-models-and-multicollinearity",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#inestimable-models-and-multicollinearity",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "10.8: Inestimable Models and Multicollinearity",
    "text": "10.8: Inestimable Models and Multicollinearity\nPerfect multicollinearity occurs when one regressor is an exact linear combination of others. In this case, OLS cannot estimate all coefficients (the model is “inestimable”).\nExample: If we create size_twice = 2 × size and include both size and size_twice as regressors, the model is perfectly collinear.\nVariance Inflation Factor (VIF) detects multicollinearity:\n\nVIF measures how much a variable’s variance is inflated due to correlation with other regressors\nFormula: \\(VIF_j = \\frac{1}{1 - R_j^2}\\) where \\(R_j^2\\) is from regressing \\(x_j\\) on all other \\(x\\)’s\nRule of thumb: VIF &gt; 10 indicates problematic multicollinearity\n\nConsequences of high multicollinearity:\n\nLarge standard errors (imprecise estimates)\nUnstable coefficients (small data changes → big estimate changes)\nDifficulty interpreting individual effects\n\n\n# Demonstrate perfect multicollinearity\nprint(\"=\" * 70)\nprint(\"DEMONSTRATION: Perfect Multicollinearity\")\nprint(\"=\" * 70)\n\n# Create a perfectly collinear variable\ndata_house['size_twice'] = 2 * data_house['size']\n\nprint(\"Creating variable: size_twice = 2 × size\")\nprint(\"Attempting to estimate: price ~ size + size_twice + bedrooms\\n\")\n\ntry:\n    model_collinear = ols('price ~ size + size_twice + bedrooms', data=data_house).fit()\n    print(\"Model estimated (software automatically dropped one variable):\")\n    print(model_collinear.summary())\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n    print(f\"Message: {str(e)}\")\n\n======================================================================\nDEMONSTRATION: Perfect Multicollinearity\n======================================================================\nCreating variable: size_twice = 2 × size\nAttempting to estimate: price ~ size + size_twice + bedrooms\n\nModel estimated (software automatically dropped one variable):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.618\nModel:                            OLS   Adj. R-squared:                  0.589\nMethod:                 Least Squares   F-statistic:                     21.03\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           3.68e-06\nTime:                        15:16:45   Log-Likelihood:                -332.03\nNo. Observations:                  29   AIC:                             670.1\nDf Residuals:                      26   BIC:                             674.2\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.117e+05   2.76e+04      4.048      0.000     5.5e+04    1.68e+05\nsize          14.4816      2.660      5.444      0.000       9.014      19.949\nsize_twice    28.9633      5.320      5.444      0.000      18.028      39.898\nbedrooms    1553.4580   7846.866      0.198      0.845   -1.46e+04    1.77e+04\n==============================================================================\nOmnibus:                        0.516   Durbin-Watson:                   1.230\nProb(Omnibus):                  0.773   Jarque-Bera (JB):                0.609\nSkew:                          -0.086   Prob(JB):                        0.737\nKurtosis:                       2.311   Cond. No.                     1.96e+17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.39e-26. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#variance-inflation-factors-vif",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#variance-inflation-factors-vif",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Variance Inflation Factors (VIF)",
    "text": "Variance Inflation Factors (VIF)\nNow let’s calculate VIF for each variable in our full model to check for multicollinearity problems.\n\n# Calculate VIF for each variable in the full model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = data_house[['size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold']]\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\nprint(\"=\" * 70)\nprint(\"VARIANCE INFLATION FACTORS (VIF)\")\nprint(\"=\" * 70)\nprint(vif_data.to_string(index=False))\nprint(\"\\nInterpretation:\")\nprint(\"  - VIF &gt; 10: Problematic multicollinearity\")\nprint(\"  - VIF 5-10: Moderate multicollinearity\")\nprint(\"  - VIF &lt; 5: Low multicollinearity\")\n\nmax_vif = vif_data['VIF'].max()\nif max_vif &gt; 10:\n    print(f\"\\nWarning: Maximum VIF = {max_vif:.2f} indicates multicollinearity issues.\")\nelif max_vif &gt; 5:\n    print(f\"\\nNote: Maximum VIF = {max_vif:.2f} shows moderate multicollinearity.\")\nelse:\n    print(f\"\\nGood: Maximum VIF = {max_vif:.2f} - no serious multicollinearity detected.\")\n\n======================================================================\nVARIANCE INFLATION FACTORS (VIF)\n======================================================================\n Variable       VIF\n     size 40.133832\n bedrooms 57.819254\nbathrooms 34.741906\n  lotsize 11.969605\n      age 21.024092\nmonthsold 12.795557\n\nInterpretation:\n  - VIF &gt; 10: Problematic multicollinearity\n  - VIF 5-10: Moderate multicollinearity\n  - VIF &lt; 5: Low multicollinearity\n\nWarning: Maximum VIF = 57.82 indicates multicollinearity issues.\n\n\n\nKey Concept 10.8: Detecting Multicollinearity with VIF\nThe Variance Inflation Factor (VIF) quantifies how much a coefficient’s variance is inflated due to correlation with other regressors. VIF\\(_j = 1/(1 - R_j^2)\\), where \\(R_j^2\\) is from regressing \\(x_j\\) on all other regressors. A VIF above 5 suggests moderate concern; above 10 indicates severe multicollinearity that inflates standard errors and destabilizes estimates. Perfect collinearity (VIF \\(\\to \\infty\\)) makes coefficients inestimable.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#visualization-actual-vs.-predicted-values",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#visualization-actual-vs.-predicted-values",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Visualization: Actual vs. Predicted Values",
    "text": "Visualization: Actual vs. Predicted Values\nA plot of actual vs. predicted values helps visualize model fit. Points close to the 45-degree line indicate accurate predictions.\n\n# Create actual vs predicted plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_house['price'], model_full.fittedvalues, alpha=0.6, s=50, color='black')\nax.plot([data_house['price'].min(), data_house['price'].max()],\n        [data_house['price'].min(), data_house['price'].max()],\n        'r--', linewidth=2, label='Perfect prediction (45° line)')\nax.set_xlabel('Actual Price ($1000s)', fontsize=12)\nax.set_ylabel('Predicted Price ($1000s)', fontsize=12)\nax.set_title('Actual vs Predicted House Prices', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Points close to the red line indicate accurate predictions.\")\n\n\n\n\n\n\n\n\nPoints close to the red line indicate accurate predictions.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#visualization-residual-plot",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#visualization-residual-plot",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Visualization: Residual Plot",
    "text": "Visualization: Residual Plot\nA residual plot (residuals vs. fitted values) helps diagnose model problems:\n\nRandom scatter around zero: Good (model assumptions satisfied)\nPatterns (curves, funnels): Bad (model misspecification or heteroskedasticity)\nOutliers: Investigate unusual observations\n\n\n# Create residual plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(model_full.fittedvalues, model_full.resid, alpha=0.6, s=50, color='black')\nax.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero residual line')\nax.set_xlabel('Fitted values ($1000s)', fontsize=12)\nax.set_ylabel('Residuals ($1000s)', fontsize=12)\nax.set_title('Residual Plot: Multiple Regression', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Random scatter around zero suggests model assumptions are reasonable.\")\n\n\n\n\n\n\n\n\nRandom scatter around zero suggests model assumptions are reasonable.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#visualization-coefficient-plot-with-confidence-intervals",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#visualization-coefficient-plot-with-confidence-intervals",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Visualization: Coefficient Plot with Confidence Intervals",
    "text": "Visualization: Coefficient Plot with Confidence Intervals\nA coefficient plot displays estimated coefficients with their 95% confidence intervals. This makes it easy to see:\n\nWhich coefficients are significantly different from zero (CI doesn’t include zero)\nRelative magnitude of effects\nPrecision of estimates (narrow vs. wide CIs)\n\n\n# Create coefficient plot with confidence intervals\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Exclude intercept for better visualization\nparams_no_int = model_full.params[1:]\nci_no_int = conf_int.iloc[1:, :]\n\ny_pos = np.arange(len(params_no_int))\nax.errorbar(params_no_int.values, y_pos,\n            xerr=[params_no_int.values - ci_no_int.iloc[:, 0].values,\n                  ci_no_int.iloc[:, 1].values - params_no_int.values],\n            fmt='o', markersize=8, capsize=5, capthick=2, linewidth=2)\nax.set_yticks(y_pos)\nax.set_yticklabels(params_no_int.index)\nax.axvline(x=0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Zero effect')\nax.set_xlabel('Coefficient Value', fontsize=12)\nax.set_title('Coefficient Estimates with 95% Confidence Intervals',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"Coefficients whose CI crosses zero are not statistically significant.\")\n\n\n\n\n\n\n\n\nCoefficients whose CI crosses zero are not statistically significant.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#key-takeaways",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#key-takeaways",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nData Exploration and Correlation: - Multiple regression extends bivariate regression to include several regressors simultaneously - Summary statistics and two-way scatterplots provide initial overview of variable relationships - Correlation matrices reveal pairwise associations (e.g., price-size correlation of 0.79) - Correlation can be misleading — bedrooms correlate with price, but may simply reflect house size\nOLS Estimation: - Multiple regression model: \\(\\widehat{y} = b_1 + b_2 x_2 + b_3 x_3 + \\cdots + b_k x_k\\) - OLS minimizes the sum of squared residuals: \\(\\min \\sum (y_i - \\widehat{y}_i)^2\\) - The coefficient \\(b_j\\) can be computed by bivariate regression of \\(y\\) on \\(\\widetilde{x}_j\\) (residualized \\(x_j\\))\nPartial Effects: - Each coefficient \\(b_j\\) measures the partial effect: the change in \\(\\widehat{y}\\) when \\(x_j\\) changes by one unit, holding all other regressors constant - Partial effects differ from total effects (which allow other regressors to vary) - OLS measures association, not causation — use careful language (“associated with”)\nModel Fit: - \\(R^2\\) measures the fraction of variation in \\(y\\) explained by all regressors (0 to 1) - Adjusted \\(R^2\\) penalizes model complexity: can decrease when adding weak regressors - Standard error of regression \\(s_e\\) measures typical prediction error in units of \\(y\\) - Example: Adding 5 regressors increased \\(R^2\\) from 0.618 to 0.651 but decreased \\(\\bar{R}^2\\) from 0.603 to 0.555\nInformation Criteria: - AIC, BIC, and HQIC penalize larger models more heavily than adjusted \\(R^2\\) - Smaller values indicate better models - BIC is generally preferred (stronger complexity penalty than AIC)\nMulticollinearity: - Perfect collinearity makes some coefficients inestimable (computer shows “omitted”) - VIF detects multicollinearity: VIF &gt; 10 indicates problematic, VIF &gt; 5 moderate concern - High multicollinearity inflates standard errors and makes estimates unstable\nPython tools used: statsmodels (OLS, VIF), seaborn (pairplot, heatmap), pandas (DataFrames), matplotlib (coefficient plots, diagnostics)\nNext steps: Chapter 11 covers statistical inference for multiple regression — hypothesis tests, confidence intervals, and overall F-tests for model significance.\nCongratulations on completing Chapter 10! You now have the tools to estimate and evaluate multiple regression models, a foundation for the rest of the course.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#practice-exercises",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#practice-exercises",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of multiple regression concepts with these exercises.\n\nExercise 1: Residuals and Fitted Values\nA regression leads to the fitted equation \\(\\widehat{y} = 2 + 3x_2 + 4x_3\\).\n\nWhat is the predicted value for observation \\((x_2, x_3) = (2, 1)\\)?\nIf the actual value is \\(y = 9\\), what is the residual?\nIs this observation over-predicted or under-predicted?\n\n\nExercise 2: Partial vs. Total Effects\nSuppose OLS regression on the same dataset leads to: - Bivariate: \\(\\widehat{y} = 6 + 5x_2\\) - Multiple: \\(\\widehat{y} = 2 + 3x_2 + 4x_3\\)\n\nWhy does the coefficient on \\(x_2\\) change from 5 to 3?\nWhich coefficient (5 or 3) represents the partial effect of \\(x_2\\)? Explain.\nUnder what condition would the bivariate and multiple regression coefficients on \\(x_2\\) be equal?\n\n\nExercise 3: R-squared and Adjusted R-squared\nOLS regression of \\(y\\) on \\(x\\) for a sample of size \\(n = 53\\) leads to Residual SS = 20 and Total SS = 50.\n\nCompute \\(R^2\\).\nCompute the standard error of the regression \\(s_e\\).\nCompute the correlation between \\(y\\) and \\(\\widehat{y}\\).\nIf we add 3 more regressors and the Residual SS drops to 18, compute the new \\(R^2\\) and adjusted \\(R^2\\). Does the model improve?\n\n\nExercise 4: Information Criteria\nConsider two models estimated on \\(n = 29\\) observations: - Model A (2 parameters): AIC = 668.1, BIC = 670.8 - Model B (7 parameters): AIC = 675.5, BIC = 685.1\n\nWhich model is preferred by AIC? By BIC?\nWhy does BIC penalize Model B more heavily than AIC?\nWhat economic reasoning supports the simpler model in this case?\n\n\nExercise 5: Multicollinearity Detection\nA multiple regression of house prices yields the following VIF values:\n\n\n\nVariable\nVIF\n\n\n\n\nSize\n40.1\n\n\nBedrooms\n57.8\n\n\nBathrooms\n34.7\n\n\nAge\n21.0\n\n\n\n\nWhich variables show the most severe multicollinearity?\nWhat consequences does high VIF have for coefficient estimates and inference?\nSuggest a strategy to reduce multicollinearity in this model.\n\n\nExercise 6: Model Building\nYou have data on employee wages with variables: education (years), experience (years), age, tenure (years at current firm), and gender.\n\nWhy might including both age and experience create multicollinearity?\nPropose two alternative model specifications and explain the trade-offs.\nHow would you use adjusted \\(R^2\\) and BIC to select the preferred specification?",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#case-studies",
    "href": "../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html#case-studies",
    "title": "Chapter 10: Data Summary for Multiple Regression",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Multiple Regression for Cross-Country Productivity\nIn this case study, you will apply multiple regression techniques to investigate how human capital and physical capital jointly explain cross-country differences in labor productivity. Using data from 108 countries over 1990-2014, you will explore correlations, estimate multiple regression models, interpret partial effects, and compare model specifications.\nDataset: Mendez Convergence Clubs Data\n\nSource: Mendez (2020), 108 countries, 1990-2014\nKey variables:\n\nlp — Labor productivity (GDP per worker)\nrk — Physical capital per worker\nhc — Human capital index\nrgdppc — Real GDP per capita\nregion — Geographic region\n\n\nResearch question: How do human capital and physical capital jointly explain cross-country productivity differences?\n# Load the Mendez convergence clubs dataset\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat_2014 = dat[dat['year'] == 2014].dropna(subset=['lp', 'rk', 'hc']).copy()\nprint(f\"Cross-section sample: {len(dat_2014)} countries (year 2014)\")\ndat_2014.head()\n\nTask 1: Explore Productivity Data (Guided)\nExplore the cross-country productivity dataset.\n# Step 1: Create log-transformed variables\ndat_2014['ln_lp'] = np.log(dat_2014['lp'])\ndat_2014['ln_rk'] = np.log(dat_2014['rk'])\n\n# Step 2: Summary statistics for key variables\nprint(dat_2014[['lp', 'rk', 'hc', 'ln_lp', 'ln_rk']].describe())\n\n# Step 3: Create a scatterplot matrix\nsns.pairplot(dat_2014[['ln_lp', 'ln_rk', 'hc']], diag_kind='kde')\nplt.suptitle('Scatterplot Matrix: Productivity Variables', y=1.02)\nplt.show()\nQuestions: - Which variable shows the strongest visual association with \\(\\ln(\\text{lp})\\)? - Do you see any nonlinear patterns or outliers?\n\n\nTask 2: Correlation Analysis (Guided)\nCompute and visualize the correlation matrix.\n# Step 1: Compute correlation matrix\ncorr = dat_2014[['ln_lp', 'ln_rk', 'hc']].corr()\nprint(corr)\n\n# Step 2: Visualize as heatmap\nsns.heatmap(corr, annot=True, fmt='.3f', cmap='coolwarm', center=0, square=True)\nplt.title('Correlation Matrix: Productivity Variables')\nplt.show()\nQuestions: - Which predictor is most strongly correlated with \\(\\ln(\\text{lp})\\)? - Are \\(\\ln(\\text{rk})\\) and hc correlated with each other? What implications does this have for multiple regression?\n\nKey Concept 10.9: Functional Form and Cross-Country Comparisons\nWhen comparing countries with vastly different income levels, logarithmic transformations are essential. Using \\(\\ln(\\text{lp})\\) and \\(\\ln(\\text{rk})\\) compresses the scale so that both Luxembourg and Malawi can be meaningfully compared. Coefficients on log-transformed variables have elasticity interpretations, while coefficients on level variables (like hc) represent semi-elasticities.\n\n\n\nTask 3: Multiple Regression Estimation (Semi-guided)\nEstimate bivariate and multiple regression models for labor productivity.\n# Model 1: ln(lp) ~ ln(rk) only\nmodel1 = ols('ln_lp ~ ln_rk', data=dat_2014).fit()\nprint(model1.summary())\n\n# Model 2: ln(lp) ~ hc only\nmodel2 = ols('ln_lp ~ hc', data=dat_2014).fit()\nprint(model2.summary())\n\n# Model 3: ln(lp) ~ ln(rk) + hc (multiple regression)\nmodel3 = ols('ln_lp ~ ln_rk + hc', data=dat_2014).fit()\nprint(model3.summary())\nQuestions: - How do the coefficients on \\(\\ln(\\text{rk})\\) change between Model 1 and Model 3? - Does adding human capital improve the model? Compare \\(R^2\\) and adjusted \\(R^2\\).\n\n\nTask 4: Interpret Partial Effects (Semi-guided)\nInterpret the coefficients from the multiple regression model.\n# Display coefficients with confidence intervals\nprint(\"Model 3 Coefficients:\")\nprint(model3.params)\nprint(\"\\n95% Confidence Intervals:\")\nprint(model3.conf_int())\nQuestions: - Interpret the coefficient on \\(\\ln(\\text{rk})\\) in Model 3. What does it mean in economic terms? - Interpret the coefficient on hc. How does a one-unit increase in human capital relate to productivity? - Are both coefficients statistically significant? How do you know?\n\n\nTask 5: Model Selection (Independent)\nCompare models using fit statistics and information criteria.\nYour tasks: 1. Create a comparison table with \\(R^2\\), adjusted \\(R^2\\), AIC, and BIC for all three models 2. Which model is preferred by adjusted \\(R^2\\)? By AIC? By BIC? 3. Does the parsimony principle favor one model over another? 4. Calculate VIF for Model 3 — is multicollinearity a concern?\nHint: Use model.rsquared, model.rsquared_adj, model.aic, model.bic and variance_inflation_factor() from statsmodels.\n\n\nTask 6: Development Policy Brief (Independent)\nWrite a 200-300 word policy brief summarizing your findings.\nYour brief should address: 1. What are the key determinants of cross-country labor productivity? 2. What is the relative importance of physical capital vs. human capital? 3. How do partial effects differ from bivariate associations? 4. What policy implications follow from your analysis? 5. What limitations should policymakers keep in mind (association vs. causation)?\n\nKey Concept 10.10: Multiple Regression in Development Economics\nCross-country productivity regressions are central to development economics. By controlling for both physical capital (\\(\\ln(\\text{rk})\\)) and human capital (hc), we can assess each factor’s partial contribution to productivity. This addresses a key policy question: should developing countries invest in machines or education? Multiple regression helps disentangle these effects, though causal claims require careful identification strategies.\n\n\n\n\nWhat You’ve Learned\nIn this case study, you applied the full multiple regression toolkit to cross-country productivity data:\n\nExplored data with scatterplot matrices and correlation analysis\nEstimated bivariate and multiple regression models, observing how coefficients change\nInterpreted partial effects of physical and human capital on productivity\nCompared model specifications using adjusted \\(R^2\\), AIC, and BIC\nConnected statistical findings to development policy questions\n\nThese skills form the foundation for more advanced empirical analysis in the chapters ahead.\n\n\nCase Study 2: Multiple Satellite Predictors of Development\nResearch Question: Do satellite image embeddings improve our ability to predict municipal development beyond nighttime lights alone?\nIn Chapter 1, we introduced the DS4Bolivia project and estimated a bivariate regression of development on nighttime lights. Now we extend this analysis using Chapter 10’s multiple regression tools, adding satellite image embeddings as additional predictors to test whether they improve our ability to predict municipal development.\nBackground: Nighttime lights (NTL) are a well-established proxy for economic activity, but they capture only one dimension of a municipality’s characteristics — nocturnal luminosity. Daytime satellite imagery contains far richer information: building density, road networks, agricultural patterns, vegetation cover. Deep learning models can extract this information as 64-dimensional embedding vectors, where each dimension captures abstract visual patterns learned automatically from the data.\nThe key question for multiple regression: Does adding these satellite embeddings as extra regressors significantly improve explanatory power compared to NTL alone? And if so, which embeddings matter most?\nVariables for this case study:\n\nimds — Municipal Sustainable Development Index (0-100, outcome variable)\nln_NTLpc2017 — Log nighttime lights per capita (established predictor from Chapter 1)\nA00, A10, A20, A30, A40 — Selected satellite image embedding dimensions (new predictors)\nmun, dep — Municipality and department identifiers\n\nYour Task: Use correlation analysis, multiple regression, the FWL theorem, and model comparison tools from Chapter 10 to evaluate whether satellite embeddings add predictive value beyond nighttime lights.\n\nLoad the DS4Bolivia Data\nLet’s load the DS4Bolivia dataset and select the variables needed for our multiple regression analysis. We focus on the development index, nighttime lights, and five selected satellite embedding dimensions.\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\nbol_sat = bol[key_vars].copy()\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA DATASET — SATELLITE PREDICTORS\")\nprint(\"=\" * 70)\nprint(f\"Dataset shape: {bol_sat.shape[0]} municipalities, {bol_sat.shape[1]} variables\")\nprint(f\"Departments: {bol['dep'].nunique()} unique departments\")\nprint(f\"Complete cases: {bol_sat.dropna().shape[0]}\")\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FIRST 10 MUNICIPALITIES\")\nprint(\"=\" * 70)\nprint(bol_sat.head(10).to_string(index=False))\n\n\n\nTask 1: Explore Variables (Guided)\nObjective: Understand the satellite embedding variables and how they compare to nighttime lights.\nInstructions:\n\nGenerate summary statistics for all predictor variables using describe()\nCompare the scale and distribution of NTL vs. embedding variables\nCheck for missing values across all selected variables\nConsider: What are satellite embeddings? How do they differ from NTL?\n\nKey insight: Unlike NTL (which has a clear physical interpretation — light intensity), embedding dimensions are abstract features extracted by neural networks. Dimension A00 doesn’t mean “vegetation” or “roads” — it captures a learned combination of visual patterns.\n\n# Your code here: Explore the satellite predictor variables\n#\n# Step 1: Summary statistics for all numeric variables\nprint(\"=\" * 70)\nprint(\"DESCRIPTIVE STATISTICS: DEVELOPMENT AND SATELLITE PREDICTORS\")\nprint(\"=\" * 70)\nprint(bol_sat[['imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']].describe().round(3))\n\n# Step 2: Check for missing values\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MISSING VALUES\")\nprint(\"=\" * 70)\nprint(bol_sat.isnull().sum())\n\n# Step 3: Compare variable ranges\nprint(\"\\n\" + \"=\" * 70)\nprint(\"VARIABLE RANGES\")\nprint(\"=\" * 70)\nfor var in ['imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']:\n    col = bol_sat[var].dropna()\n    print(f\"  {var:16s}  range: [{col.min():.3f}, {col.max():.3f}]  std: {col.std():.3f}\")\n\n\n\nTask 2: Correlation Analysis (Guided)\nObjective: Compute and visualize the correlation structure among all predictors and the outcome variable.\nInstructions:\n\nCompute the correlation matrix for imds, ln_NTLpc2017, and all five embedding variables\nDisplay the correlation matrix as a heatmap\nIdentify: Which embeddings correlate most strongly with imds?\nIdentify: Do the embeddings correlate with each other (potential multicollinearity)?\nDo the embeddings correlate with NTL, or do they capture different information?\n\nApply what you learned in section 10.3: Use seaborn.heatmap() with annotated correlation values.\n\n# Your code here: Correlation analysis of satellite predictors\n#\n# Step 1: Compute correlation matrix\ncorr_vars = ['imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\ncorr_sat = bol_sat[corr_vars].dropna().corr()\n\nprint(\"=\" * 70)\nprint(\"CORRELATION MATRIX: DEVELOPMENT AND SATELLITE PREDICTORS\")\nprint(\"=\" * 70)\nprint(corr_sat.round(3))\n\n# Step 2: Visualize as heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(corr_sat, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nax.set_title('Correlation Matrix: IMDS, NTL, and Satellite Embeddings',\n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Step 3: Highlight strongest correlations with IMDS\nprint(\"\\nCorrelations with IMDS (development index):\")\nimds_corr = corr_sat['imds'].drop('imds').sort_values(key=abs, ascending=False)\nfor var, r in imds_corr.items():\n    print(f\"  {var:16s}  r = {r:+.3f}\")\n\n\nKey Concept 10.12: High-Dimensional Satellite Features\nSatellite embeddings are 64 abstract features extracted by deep learning models (convolutional neural networks) from daytime satellite imagery. Unlike handcrafted variables (e.g., NDVI for vegetation), each embedding dimension captures complex visual patterns — road density, building structures, agricultural layouts — learned automatically from the data. These features are not directly interpretable (dimension A00 doesn’t have a specific meaning), but they collectively encode rich information about a municipality’s physical landscape.\n\n\n\nTask 3: Multiple Regression (Semi-guided)\nObjective: Estimate a multiple regression model with NTL and satellite embeddings as predictors.\nInstructions:\n\nEstimate the full model: imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40\nDisplay the regression summary\nCompare \\(R^2\\) with the bivariate NTL-only model from Chapter 1\nInterpret: How much does adding embeddings improve explanatory power?\nWhich embedding coefficients are statistically significant?\n\nApply what you learned in section 10.4: Use ols() from statsmodels and interpret partial effects.\n\n# Your code here: Multiple regression with satellite predictors\n#\n# Step 1: Prepare data (drop missing values)\nreg_data = bol_sat[['imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']].dropna()\nprint(f\"Regression sample: {len(reg_data)} municipalities (complete cases)\")\n\n# Step 2: Bivariate model (NTL only — baseline from Chapter 1)\nmodel_ntl = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL 1: BIVARIATE — imds ~ ln_NTLpc2017\")\nprint(\"=\" * 70)\nprint(model_ntl.summary())\n\n# Step 3: Multiple regression (NTL + all 5 embeddings)\nmodel_full_sat = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n                     data=reg_data).fit()\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL 2: MULTIPLE — imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40\")\nprint(\"=\" * 70)\nprint(model_full_sat.summary())\n\n# Step 4: Compare R-squared\nprint(\"\\n\" + \"=\" * 70)\nprint(\"R-SQUARED COMPARISON\")\nprint(\"=\" * 70)\nprint(f\"NTL only:           R² = {model_ntl.rsquared:.4f}\")\nprint(f\"NTL + 5 embeddings: R² = {model_full_sat.rsquared:.4f}\")\nprint(f\"Improvement:        ΔR² = {model_full_sat.rsquared - model_ntl.rsquared:.4f}\")\n\n\n\nTask 4: Partial Effects via FWL (Semi-guided)\nObjective: Demonstrate the Frisch-Waugh-Lovell theorem by showing that the NTL coefficient in the multiple regression equals the coefficient from regressing residualized IMDS on residualized NTL.\nInstructions:\n\nRegress imds on all embedding variables (A00, A10, A20, A30, A40), save residuals \\(e_y\\)\nRegress ln_NTLpc2017 on all embedding variables, save residuals \\(e_x\\)\nRegress \\(e_y\\) on \\(e_x\\) (bivariate regression of residuals)\nVerify that the coefficient matches the NTL coefficient from the full multiple regression\n\nApply what you learned in section 10.5: This demonstrates that the partial effect of NTL is computed from the variation in NTL that is independent of the satellite embeddings.\n\n# Your code here: FWL theorem demonstration\n#\n# Step 1: Regress imds on embeddings only, save residuals e_y\nmodel_y_on_emb = ols('imds ~ A00 + A10 + A20 + A30 + A40', data=reg_data).fit()\ne_y = model_y_on_emb.resid\n\n# Step 2: Regress ln_NTLpc2017 on embeddings only, save residuals e_x\nmodel_x_on_emb = ols('ln_NTLpc2017 ~ A00 + A10 + A20 + A30 + A40', data=reg_data).fit()\ne_x = model_x_on_emb.resid\n\n# Step 3: Regress e_y on e_x\nfwl_data = pd.DataFrame({'e_y': e_y, 'e_x': e_x})\nmodel_fwl = ols('e_y ~ e_x', data=fwl_data).fit()\n\n# Step 4: Compare coefficients\nprint(\"=\" * 70)\nprint(\"FWL THEOREM DEMONSTRATION\")\nprint(\"=\" * 70)\nprint(f\"NTL coefficient from FULL multiple regression:  {model_full_sat.params['ln_NTLpc2017']:.10f}\")\nprint(f\"Coefficient from FWL residual regression:       {model_fwl.params['e_x']:.10f}\")\nprint(f\"Difference (numerical precision):                {abs(model_full_sat.params['ln_NTLpc2017'] - model_fwl.params['e_x']):.15f}\")\nprint(\"\\nThe coefficients are identical — confirming the FWL theorem!\")\nprint(\"\\nInterpretation: The partial effect of NTL on IMDS is computed using\")\nprint(\"only the variation in NTL that is NOT explained by the satellite embeddings.\")\n\n\n\nTask 5: Model Comparison (Independent)\nObjective: Compare multiple model specifications using fit statistics and information criteria.\nYour tasks:\n\nEstimate three models:\n\nModel 1: imds ~ ln_NTLpc2017 (NTL only)\nModel 2: imds ~ ln_NTLpc2017 + A00 + A10 (NTL + 2 embeddings)\nModel 3: imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40 (NTL + 5 embeddings)\n\nCreate a comparison table reporting \\(R^2\\), adjusted \\(R^2\\), AIC, and BIC for each model\nUse model.rsquared, model.rsquared_adj, model.aic, model.bic\nWhich model is “best” by each criterion?\nDoes the parsimony principle favor fewer or more embedding variables?\n\nHint: Use pd.DataFrame() to create a clean comparison table.\n\n# Your code here: Model comparison\n#\n# Step 1: Estimate three models\n# model_1 = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()\n# model_2 = ols('imds ~ ln_NTLpc2017 + A00 + A10', data=reg_data).fit()\n# model_3 = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40', data=reg_data).fit()\n#\n# Step 2: Create comparison table\n# comparison = pd.DataFrame({\n#     'Model': ['NTL only', 'NTL + 2 embeddings', 'NTL + 5 embeddings'],\n#     'R²': [model_1.rsquared, model_2.rsquared, model_3.rsquared],\n#     'Adj R²': [model_1.rsquared_adj, model_2.rsquared_adj, model_3.rsquared_adj],\n#     'AIC': [model_1.aic, model_2.aic, model_3.aic],\n#     'BIC': [model_1.bic, model_2.bic, model_3.bic],\n#     'N': [len(reg_data)] * 3\n# })\n# print(comparison.to_string(index=False))\n#\n# Step 3: Interpret — which model is preferred by each criterion?\n\n\nKey Concept 10.13: Incremental Predictive Power\nWhen adding predictors to a regression model, \\(R^2\\) can only increase or stay the same — it never decreases. This makes \\(R^2\\) misleading for model comparison when models have different numbers of predictors. Adjusted \\(R^2\\) penalizes for additional variables, while AIC and BIC balance fit against complexity. In the satellite prediction context, adding all 64 embeddings would maximize \\(R^2\\) but might overfit; information criteria help identify the most parsimonious model.\n\n\n\nTask 6: Policy Brief on Satellite Prediction (Independent)\nObjective: Write a 200-300 word policy brief summarizing the value of satellite embeddings for development prediction.\nYour brief should address:\n\nImprovement: How much does adding satellite embeddings improve development prediction compared to NTL alone?\nComplexity trade-off: Is the improvement worth the added model complexity? What do adjusted \\(R^2\\), AIC, and BIC suggest?\nPartial effects: After controlling for embeddings, does NTL remain a significant predictor? What does the FWL theorem reveal about NTL’s independent contribution?\nSDG monitoring implications: How could multi-source satellite data enhance SDG monitoring in data-scarce countries like Bolivia?\nLimitations: What can satellite data not capture about development? What are the risks of relying on abstract embedding features for policy decisions?\n\nConnection to Research: The DS4Bolivia project uses all 64 embedding dimensions plus machine learning methods (Random Forest, XGBoost) to predict SDG indicators, achieving meaningful predictive accuracy. Your multiple regression analysis provides a transparent, interpretable baseline for comparison.\n\n# Your code here: Additional analysis for the policy brief\n#\n# You might want to:\n# 1. Create a summary table of key results across models\n# 2. Generate a visualization comparing model fit\n# 3. Calculate the percentage improvement in R² from adding embeddings\n# 4. Discuss which embeddings contribute most to prediction\n#\n# Example:\n# print(\"KEY RESULTS FOR POLICY BRIEF\")\n# print(f\"NTL-only R²:          {model_1.rsquared:.4f}\")\n# print(f\"NTL + 5 embed R²:     {model_3.rsquared:.4f}\")\n# print(f\"R² improvement:       {(model_3.rsquared - model_1.rsquared) / model_1.rsquared * 100:.1f}%\")\n# print(f\"NTL coef (bivariate): {model_1.params['ln_NTLpc2017']:.4f}\")\n# print(f\"NTL coef (multiple):  {model_3.params['ln_NTLpc2017']:.4f}\")\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this analysis of multiple satellite predictors of Bolivian municipal development, you’ve applied the full Chapter 10 toolkit to a cutting-edge research application:\n\nCorrelation analysis: Explored the correlation structure among NTL, satellite embeddings, and development outcomes\nMultiple regression: Estimated models with multiple satellite predictors and interpreted partial effects\nFWL theorem: Demonstrated that the partial effect of NTL equals the coefficient from regressing residualized IMDS on residualized NTL\nModel comparison: Evaluated competing specifications using \\(R^2\\), adjusted \\(R^2\\), AIC, and BIC\nCritical assessment: Weighed the predictive gains from additional satellite features against model complexity\n\nConnection to upcoming chapters: In Chapter 11, we’ll test whether the satellite embeddings are statistically significant using F-tests for joint significance. In Chapter 12, we’ll address robust inference and prediction intervals.\nThis dataset returns throughout the textbook: Each subsequent chapter applies its specific econometric tools to the DS4Bolivia data, building progressively toward a comprehensive satellite-based development prediction framework.\n\nWell done! You’ve now analyzed how multiple satellite data sources can predict development outcomes, moving from simple bivariate regression (Chapter 1) to the richer multiple regression framework of Chapter 10.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 10: Data Summary for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to extending inference to models with multiple regressors. All code runs directly in Google Colab without any local setup.\nThis chapter extends statistical inference to models with multiple regressors. You’ll learn to construct confidence intervals, conduct hypothesis tests on individual and groups of parameters, and present regression results professionally.\nLearning Objectives:\nBy the end of this chapter, you will be able to:\nDataset used: - AED_HOUSE.DTA: 29 houses sold in Davis, California (1999)\nKey economic questions: - Is house size a statistically significant predictor of price? - Are additional variables (bedrooms, bathrooms, age) jointly significant? - What is a reasonable range for the effect of size on price?\nChapter outline: - 11.1 Properties of the Least Squares Estimator - 11.2 Estimators of Model Parameters - 11.3 Confidence Intervals - 11.4 Hypothesis Tests on a Single Parameter - 11.5 Joint Hypothesis Tests - 11.6 F Statistic Under Assumptions 1-4 - 11.7 Presentation of Regression Results - Key Takeaways - Practice Exercises - Case Studies\nEstimated time: 60-90 minutes",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#chapter-overview",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#chapter-overview",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "",
    "text": "Extend statistical inference from bivariate regression to multiple regression with \\(k\\) regressors\nUnderstand the \\(t\\)-statistic for individual coefficients following a \\(T(n-k)\\) distribution\nCalculate and interpret standard errors for OLS slope coefficients: \\(se(b_j) = s_e / \\sqrt{\\sum \\widetilde{x}_{ji}^2}\\)\nConstruct confidence intervals using \\(b_j \\pm t_{n-k, \\alpha/2} \\times se(b_j)\\)\nConduct hypothesis tests on individual parameters to determine statistical significance\nUnderstand and apply F-tests for joint hypotheses involving multiple parameter restrictions\nInterpret the F distribution with two degrees of freedom (\\(v_1\\) = restrictions, \\(v_2 = n-k\\))\nPerform the test of overall statistical significance using \\(H_0: \\beta_2 = \\cdots = \\beta_k = 0\\)\nTest whether subsets of regressors are jointly significant using nested model comparisons\nPresent regression results in standard formats (standard errors, t-statistics, p-values, confidence intervals, asterisks)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#setup",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#setup",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nfrom statsmodels.stats.anova import anova_lm\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to explore statistical inference for multiple regression.\")\n\nSetup complete! Ready to explore statistical inference for multiple regression.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#data-preparation",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#data-preparation",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’ll work with the same house price dataset from Chapter 10, which contains information on 29 houses sold in Davis, California in 1999.\n\n# Read in the house data\ndata_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n\nprint(\"Data summary:\")\ndata_summary = data_house.describe()\nprint(data_summary)\n\nprint(\"\\nFirst few observations:\")\nprint(data_house[['price', 'size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold']].head())\n\nData summary:\n               price         size   bedrooms  bathrooms    lotsize        age  \\\ncount      29.000000    29.000000  29.000000  29.000000  29.000000  29.000000   \nmean   253910.344828  1882.758621   3.793103   2.206897   2.137931  36.413792   \nstd     37390.710695   398.272130   0.675030   0.341144   0.693034   7.118975   \nmin    204000.000000  1400.000000   3.000000   2.000000   1.000000  23.000000   \n25%    233000.000000  1600.000000   3.000000   2.000000   2.000000  31.000000   \n50%    244000.000000  1800.000000   4.000000   2.000000   2.000000  35.000000   \n75%    270000.000000  2000.000000   4.000000   2.500000   3.000000  39.000000   \nmax    375000.000000  3300.000000   6.000000   3.000000   3.000000  51.000000   \n\n       monthsold           list  \ncount  29.000000      29.000000  \nmean    5.965517  257824.137931  \nstd     1.679344   40860.264099  \nmin     3.000000  199900.000000  \n25%     5.000000  239000.000000  \n50%     6.000000  245000.000000  \n75%     7.000000  269000.000000  \nmax     8.000000  386000.000000  \n\nFirst few observations:\n    price  size  bedrooms  bathrooms  lotsize   age  monthsold\n0  204000  1400         3        2.0        1  31.0          7\n1  212000  1600         3        3.0        2  33.0          5\n2  213000  1800         3        2.0        2  51.0          4\n3  220000  1600         3        2.0        1  49.0          4\n4  224500  2100         4        2.5        2  47.0          6",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#properties-of-the-least-squares-estimator",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#properties-of-the-least-squares-estimator",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.1: Properties of the Least Squares Estimator",
    "text": "11.1: Properties of the Least Squares Estimator\nBefore conducting inference, we need to understand the statistical properties of the OLS estimator. Under the classical linear regression assumptions, OLS has desirable properties.\nClassical Assumptions (1-4):\n\nLinearity: The population model is linear in parameters: \\[y = \\beta_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\cdots + \\beta_k x_k + u\\]\nRandom sampling: Data are randomly sampled from the population\nNo perfect collinearity: No exact linear relationships among regressors\nZero conditional mean: \\(E[u | x_2, \\ldots, x_k] = 0\\)\n\nProperties of OLS under these assumptions:\n\nUnbiased: \\(E[\\hat{\\beta}_j] = \\beta_j\\) (centered on true value)\nConsistent: \\(\\hat{\\beta}_j \\rightarrow \\beta_j\\) as \\(n \\rightarrow \\infty\\)\nEfficient (BLUE): Best Linear Unbiased Estimator (Gauss-Markov Theorem)\n\nHas minimum variance among all linear unbiased estimators\n\n\nStandard error of coefficient \\(\\hat{\\beta}_j\\):\n\\[se(\\hat{\\beta}_j) = \\frac{s_e}{\\sqrt{\\sum_{i=1}^n \\tilde{x}_{ji}^2}}\\]\nwhere:\n\n\\(s_e\\) is the standard error of the regression\n\\(\\tilde{x}_{ji}\\) is the residual from regressing \\(x_j\\) on all other regressors\n\nSmaller standard errors occur when:\n\nModel fits well (small \\(s_e\\))\nLarge sample size (large \\(\\sum \\tilde{x}_{ji}^2\\))\nVariable \\(x_j\\) has high variation after controlling for other regressors\n\n\nprint(\"=\" * 70)\nprint(\"11.1 PROPERTIES OF THE LEAST SQUARES ESTIMATOR\")\nprint(\"=\" * 70)\n\nprint(\"\\nUnder assumptions 1-4:\")\nprint(\"  1. Linearity: y = β₀ + β₁x₁ + ... + βₖxₖ + u\")\nprint(\"  2. Random sampling from population\")\nprint(\"  3. No perfect collinearity\")\nprint(\"  4. Zero conditional mean: E[u|X] = 0\")\nprint(\"\\nThe OLS estimator is:\")\nprint(\"  - Unbiased: E[β̂] = β\")\nprint(\"  - Consistent: plim(β̂) = β\")\nprint(\"  - Efficient (BLUE under Gauss-Markov theorem)\")\nprint(\"\\nThese properties allow us to conduct valid statistical inference.\")\n\n======================================================================\n11.1 PROPERTIES OF THE LEAST SQUARES ESTIMATOR\n======================================================================\n\nUnder assumptions 1-4:\n  1. Linearity: y = β₀ + β₁x₁ + ... + βₖxₖ + u\n  2. Random sampling from population\n  3. No perfect collinearity\n  4. Zero conditional mean: E[u|X] = 0\n\nThe OLS estimator is:\n  - Unbiased: E[β̂] = β\n  - Consistent: plim(β̂) = β\n  - Efficient (BLUE under Gauss-Markov theorem)\n\nThese properties allow us to conduct valid statistical inference.\n\n\n\nKey Concept 11.1: Classical Assumptions for Statistical Inference\nFour assumptions underpin valid inference in multiple regression: (1) linearity in parameters, (2) random sampling, (3) no perfect collinearity among regressors, and (4) zero conditional mean of errors \\(E[u|X] = 0\\). Under these assumptions, OLS is unbiased (\\(E[b_j] = \\beta_j\\)), consistent, and the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov theorem.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#estimators-of-model-parameters",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#estimators-of-model-parameters",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.2: Estimators of Model Parameters",
    "text": "11.2: Estimators of Model Parameters\nNow we estimate the full multiple regression model and examine the parameter estimates, standard errors, and related statistics.\nThe full model:\n\\[\\text{price} = \\beta_1 + \\beta_2 \\times \\text{size} + \\beta_3 \\times \\text{bedrooms} + \\beta_4 \\times \\text{bathrooms} + \\beta_5 \\times \\text{lotsize} + \\beta_6 \\times \\text{age} + \\beta_7 \\times \\text{monthsold} + u\\]\nKey statistics:\n\nCoefficients (\\(\\hat{\\beta}_j\\)): Point estimates of partial effects\nStandard errors (\\(se(\\hat{\\beta}_j)\\)): Measure of estimation uncertainty\nt-statistics: Coefficient divided by standard error\np-values: Probability of observing such extreme values under \\(H_0: \\beta_j = 0\\)\nRoot MSE (\\(s_e\\)): Standard deviation of residuals, measures typical prediction error\n\n\n# Full multiple regression model\nmodel_full = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n                 data=data_house).fit()\n\nprint(\"=\" * 70)\nprint(\"11.2 ESTIMATORS OF MODEL PARAMETERS\")\nprint(\"=\" * 70)\nprint(\"\\nFull Multiple Regression Results:\")\nprint(model_full.summary())\n\n======================================================================\n11.2 ESTIMATORS OF MODEL PARAMETERS\n======================================================================\n\nFull Multiple Regression Results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.651\nModel:                            OLS   Adj. R-squared:                  0.555\nMethod:                 Least Squares   F-statistic:                     6.826\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000342\nTime:                        15:15:57   Log-Likelihood:                -330.74\nNo. Observations:                  29   AIC:                             675.5\nDf Residuals:                      22   BIC:                             685.1\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.378e+05   6.15e+04      2.242      0.035    1.03e+04    2.65e+05\nsize          68.3694     15.389      4.443      0.000      36.454     100.285\nbedrooms    2685.3151   9192.526      0.292      0.773   -1.64e+04    2.17e+04\nbathrooms   6832.8800   1.57e+04      0.435      0.668   -2.58e+04    3.94e+04\nlotsize     2303.2214   7226.535      0.319      0.753   -1.27e+04    1.73e+04\nage         -833.0386    719.335     -1.158      0.259   -2324.847     658.770\nmonthsold  -2088.5036   3520.898     -0.593      0.559   -9390.399    5213.392\n==============================================================================\nOmnibus:                        1.317   Durbin-Watson:                   1.259\nProb(Omnibus):                  0.518   Jarque-Bera (JB):                0.980\nSkew:                           0.151   Prob(JB):                        0.612\nKurtosis:                       2.152   Cond. No.                     2.59e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.59e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-regression-results",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-regression-results",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Regression Results",
    "text": "Interpreting the Regression Results\nWhat these results tell us:\nThe regression output reveals several important findings about the house price data:\n\nSize coefficient = $68.37 (p &lt; 0.001): Each additional square foot increases house price by $68.37 on average, holding all other variables constant. This effect is highly statistically significant (p = 0.0002), meaning we can be very confident this relationship is not due to chance.\nOther variables are not statistically significant:\n\nBedrooms coefficient = $2,685 (p = 0.773): Surprisingly, the number of bedrooms doesn’t significantly affect price once we control for size. The p-value of 0.773 means we cannot reject the hypothesis that this coefficient is zero.\nBathrooms coefficient = $6,833 (p = 0.668): Similarly, bathrooms show no significant effect.\nAge coefficient = -$833 (p = 0.259): Older homes tend to sell for less, but this effect is not statistically significant.\n\nModel fit: R² = 0.651: The model explains 65.1% of the variation in house prices, which is quite good for cross-sectional real estate data.\nOverall F-statistic = 6.83 (p = 0.0003): The model as a whole is highly significant, meaning at least one of our predictors has a real effect on price.\n\nEconomic Interpretation:\nSize dominates all other house characteristics in determining price in this market. Features like number of bedrooms and bathrooms don’t add explanatory power beyond what size already captures. This likely reflects multicollinearity—larger houses naturally tend to have more bedrooms and bathrooms, so once we control for size, these other features provide little additional information.\nThe large standard errors on most coefficients (relative to the coefficient values) suggest imprecise estimation, common in small samples (n=29) with correlated predictors.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#model-diagnostics",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#model-diagnostics",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\nLet’s extract and display key model diagnostics to understand the estimation.\n\n# Extract key statistics\nn = len(data_house)\nk = len(model_full.params)\ndf = n - k\n\nprint(\"Model diagnostics:\")\nprint(f\"  Sample size (n): {n}\")\nprint(f\"  Number of parameters (k): {k}\")\nprint(f\"  Degrees of freedom (n-k): {df}\")\nprint(f\"  Root MSE (σ̂): ${np.sqrt(model_full.mse_resid):,.2f}\")\nprint(f\"  R-squared: {model_full.rsquared:.4f}\")\nprint(f\"  Adjusted R-squared: {model_full.rsquared_adj:.4f}\")\n\n# Create comprehensive coefficient table\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Coefficient Table\")\nprint(\"=\" * 70)\ncoef_table = pd.DataFrame({\n    'Coefficient': model_full.params,\n    'Std. Error': model_full.bse,\n    't-statistic': model_full.tvalues,\n    'p-value': model_full.pvalues\n})\nprint(coef_table)\n\nModel diagnostics:\n  Sample size (n): 29\n  Number of parameters (k): 7\n  Degrees of freedom (n-k): 22\n  Root MSE (σ̂): $24,935.73\n  R-squared: 0.6506\n  Adjusted R-squared: 0.5552\n\n======================================================================\nCoefficient Table\n======================================================================\n             Coefficient    Std. Error  t-statistic   p-value\nIntercept  137791.065699  61464.951869     2.241783  0.035387\nsize           68.369419     15.389472     4.442610  0.000205\nbedrooms     2685.315122   9192.525674     0.292119  0.772932\nbathrooms    6832.880015  15721.191544     0.434629  0.668065\nlotsize      2303.221371   7226.535205     0.318717  0.752947\nage          -833.038602    719.334544    -1.158068  0.259254\nmonthsold   -2088.503625   3520.897859    -0.593174  0.559114\n\n\n\nKey Concept 11.2: Precision of Coefficient Estimates\nThe standard error \\(se(b_j) = s_e / \\sqrt{\\sum \\widetilde{x}_{ji}^2}\\) reveals what makes estimates precise: (1) a well-fitting model (small \\(s_e\\)), (2) large sample size, (3) high variation in \\(x_j\\) after controlling for other regressors, and (4) low multicollinearity. When regressors are highly correlated, \\(\\sum \\widetilde{x}_{ji}^2\\) shrinks and standard errors inflate.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#economic-interpretation",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#economic-interpretation",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Economic Interpretation",
    "text": "Economic Interpretation\nKey findings:\n\nSize coefficient (≈ $68.37): Each additional square foot increases house price by approximately $68, holding other factors constant.\nStatistical significance: Only the size variable appears statistically significant at conventional levels (p &lt; 0.05).\nOther variables: Bedrooms, bathrooms, lot size, age, and month sold show large standard errors relative to their coefficients, suggesting imprecise estimates.\n\nThis pattern is common in small samples with correlated regressors (multicollinearity).\nNow that we understand the properties and interpretation of OLS estimates, let’s quantify uncertainty through confidence intervals.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#confidence-intervals",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#confidence-intervals",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.3: Confidence Intervals",
    "text": "11.3: Confidence Intervals\nA confidence interval provides a range of plausible values for a population parameter.\nFormula for a $100(1-)%$ confidence interval:\n\\[\\hat{\\beta}_j \\pm t_{n-k, \\alpha/2} \\times se(\\hat{\\beta}_j)\\]\nwhere: - \\(\\hat{\\beta}_j\\) is the coefficient estimate - \\(t_{n-k, \\alpha/2}\\) is the critical value from Student’s t-distribution - \\(se(\\hat{\\beta}_j)\\) is the standard error\n95% confidence interval (approximate):\n\\[\\hat{\\beta}_j \\pm 2 \\times se(\\hat{\\beta}_j)\\]\nInterpretation: If we repeatedly sampled from the population and constructed 95% CIs, approximately 95% would contain the true parameter value.\nKey points: - Narrower intervals indicate more precise estimates - If the interval excludes zero, the coefficient is statistically significant at that level",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-confidence-intervals",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-confidence-intervals",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Confidence Intervals",
    "text": "Interpreting the Confidence Intervals\nWhat these confidence intervals tell us:\nLooking at the 95% confidence intervals, we can identify which variables have statistically significant effects:\n\nSize: [$36.45, $100.29] - This interval excludes zero, confirming that size has a statistically significant positive effect on price. We are 95% confident that each additional square foot increases price by between $36 and $100.\nIntercept: [$10,321, $265,262] - The wide interval reflects high uncertainty about the base price level, but it excludes zero.\nAll other variables: The confidence intervals for bedrooms, bathrooms, lotsize, age, and monthsold all contain zero, which means these coefficients are not statistically significant at the 5% level.\n\nPractical Meaning:\nIf we repeatedly sampled 29 houses from this market and calculated 95% confidence intervals, approximately 95% of those intervals would contain the true population parameter. For the size coefficient, this means we’re quite certain about its effect—even in the worst case (lower bound), an extra square foot adds at least $36 to the price.\nThe fact that only the size interval excludes zero provides strong evidence that, in this dataset, size is the only reliable predictor of house prices among the variables we measured.\n\nprint(\"=\" * 70)\nprint(\"11.3 CONFIDENCE INTERVALS\")\nprint(\"=\" * 70)\n\n# 95% confidence intervals\nconf_int = model_full.conf_int(alpha=0.05)\nprint(\"\\n95% Confidence Intervals:\")\nprint(conf_int)\n\n======================================================================\n11.3 CONFIDENCE INTERVALS\n======================================================================\n\n95% Confidence Intervals:\n                      0              1\nIntercept  10320.557398  265261.573999\nsize          36.453608     100.285230\nbedrooms  -16378.816300   21749.446543\nbathrooms -25770.875723   39436.635753\nlotsize   -12683.695364   17290.138107\nage        -2324.847139     658.769936\nmonthsold  -9390.398871    5213.391620",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#manual-calculation-of-confidence-interval",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#manual-calculation-of-confidence-interval",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Manual Calculation of Confidence Interval",
    "text": "Manual Calculation of Confidence Interval\nLet’s manually calculate the confidence interval for the size coefficient to understand the mechanics.\n\n# Detailed confidence interval calculation for 'size'\ncoef_size = model_full.params['size']\nse_size = model_full.bse['size']\nt_crit = stats.t.ppf(0.975, df)  # 97.5th percentile for two-sided 95% CI\n\nci_lower = coef_size - t_crit * se_size\nci_upper = coef_size + t_crit * se_size\n\nprint(\"Manual calculation for 'size' coefficient:\")\nprint(f\"  Coefficient: ${coef_size:.2f}\")\nprint(f\"  Standard error: ${se_size:.2f}\")\nprint(f\"  Degrees of freedom: {df}\")\nprint(f\"  Critical t-value (α=0.05): {t_crit:.4f}\")\nprint(f\"  Margin of error: {t_crit * se_size:.2f}\")\nprint(f\"  95% CI: [${ci_lower:.2f}, ${ci_upper:.2f}]\")\n\nprint(\"\\nInterpretation:\")\nprint(f\"We are 95% confident that each additional square foot increases\")\nprint(f\"house price by between ${ci_lower:.2f} and ${ci_upper:.2f}.\")\n\nManual calculation for 'size' coefficient:\n  Coefficient: $68.37\n  Standard error: $15.39\n  Degrees of freedom: 22\n  Critical t-value (α=0.05): 2.0739\n  Margin of error: 31.92\n  95% CI: [$36.45, $100.29]\n\nInterpretation:\nWe are 95% confident that each additional square foot increases\nhouse price by between $36.45 and $100.29.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#comprehensive-table-with-confidence-intervals",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#comprehensive-table-with-confidence-intervals",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Comprehensive Table with Confidence Intervals",
    "text": "Comprehensive Table with Confidence Intervals\n\n# Create comprehensive coefficient table\nprint(\"=\" * 70)\nprint(\"Comprehensive Coefficient Table with 95% Confidence Intervals\")\nprint(\"=\" * 70)\ncoef_table_full = pd.DataFrame({\n    'Coefficient': model_full.params,\n    'Std. Error': model_full.bse,\n    't-statistic': model_full.tvalues,\n    'p-value': model_full.pvalues,\n    'CI Lower': conf_int.iloc[:, 0],\n    'CI Upper': conf_int.iloc[:, 1]\n})\nprint(coef_table_full)\n\nprint(\"\\nNote: Coefficients with CIs that exclude zero are statistically significant at 5%.\")\n\n======================================================================\nComprehensive Coefficient Table with 95% Confidence Intervals\n======================================================================\n             Coefficient    Std. Error  t-statistic   p-value      CI Lower  \\\nIntercept  137791.065699  61464.951869     2.241783  0.035387  10320.557398   \nsize           68.369419     15.389472     4.442610  0.000205     36.453608   \nbedrooms     2685.315122   9192.525674     0.292119  0.772932 -16378.816300   \nbathrooms    6832.880015  15721.191544     0.434629  0.668065 -25770.875723   \nlotsize      2303.221371   7226.535205     0.318717  0.752947 -12683.695364   \nage          -833.038602    719.334544    -1.158068  0.259254  -2324.847139   \nmonthsold   -2088.503625   3520.897859    -0.593174  0.559114  -9390.398871   \n\n                CI Upper  \nIntercept  265261.573999  \nsize          100.285230  \nbedrooms    21749.446543  \nbathrooms   39436.635753  \nlotsize     17290.138107  \nage           658.769936  \nmonthsold    5213.391620  \n\nNote: Coefficients with CIs that exclude zero are statistically significant at 5%.\n\n\n\nKey Concept 11.3: Confidence Intervals in Multiple Regression\nA 95% confidence interval \\(b_j \\pm t_{n-k, 0.025} \\times se(b_j)\\) provides a range of plausible values for \\(\\beta_j\\). If the interval excludes zero, the coefficient is statistically significant at 5%. Narrower intervals indicate more precise estimation, which improves with larger samples and lower multicollinearity.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-hypothesis-test-result",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-hypothesis-test-result",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Hypothesis Test Result",
    "text": "Interpreting the Hypothesis Test Result\nWhat this test tells us:\nWe tested whether the size coefficient equals 50 (H₀: β_size = 50):\n\nt-statistic = 1.19: This measures how many standard errors our estimate ($68.37) is away from the hypothesized value (50). The difference is 1.19 standard errors.\np-value = 0.245: This is the probability of observing a coefficient as extreme as $68.37 (or more extreme) if the true value were actually $50. A p-value of 0.245 means there’s about a 25% chance we’d see this result by random sampling variation alone.\nDecision: Since p = 0.245 &gt; 0.05, we fail to reject H₀ at the 5% significance level.\n\nEconomic Interpretation:\nThe data are consistent with the hypothesis that each square foot adds $50 to house price. Even though our point estimate is $68.37, the difference from $50 is not statistically significant. This doesn’t mean β = 50 is correct—it simply means our data don’t provide strong enough evidence to rule it out.\nThis illustrates an important principle: failing to reject H₀ is NOT the same as proving H₀ is true. We simply lack sufficient evidence to reject it given our sample size and estimation precision.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#hypothesis-tests-on-a-single-parameter",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#hypothesis-tests-on-a-single-parameter",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.4: Hypothesis Tests on a Single Parameter",
    "text": "11.4: Hypothesis Tests on a Single Parameter\nHypothesis testing allows us to make formal inferences about population parameters.\nGeneral two-sided test:\n\\[H_0: \\beta_j = \\beta_j^* \\quad \\text{vs.} \\quad H_a: \\beta_j \\neq \\beta_j^*\\]\nTest statistic:\n\\[t = \\frac{\\hat{\\beta}_j - \\beta_j^*}{se(\\hat{\\beta}_j)} \\sim t(n-k)\\]\nDecision rules:\n\np-value approach: Reject \\(H_0\\) if \\(p\\)-value \\(&lt; \\alpha\\)\nCritical value approach: Reject \\(H_0\\) if \\(|t| &gt; t_{n-k, \\alpha/2}\\)\n\nTest of statistical significance (most common):\n\\[H_0: \\beta_j = 0 \\quad \\text{vs.} \\quad H_a: \\beta_j \\neq 0\\]\nThis tests whether variable \\(x_j\\) has any relationship with \\(y\\) after controlling for other regressors.\nExample: Test whether the size coefficient equals 50.\n\nprint(\"=\" * 70)\nprint(\"11.4 HYPOTHESIS TESTS ON A SINGLE PARAMETER\")\nprint(\"=\" * 70)\n\n# Test H₀: β_size = 50 vs H₁: β_size ≠ 50\nnull_value = 50\nt_stat_50 = (coef_size - null_value) / se_size\np_value_50 = 2 * (1 - stats.t.cdf(abs(t_stat_50), df))\n\nprint(f\"\\nTest: H₀: β_size = {null_value} vs H₁: β_size ≠ {null_value}\")\nprint(f\"  Coefficient estimate: ${coef_size:.2f}\")\nprint(f\"  Standard error: ${se_size:.2f}\")\nprint(f\"  t-statistic: {t_stat_50:.4f}\")\nprint(f\"  p-value: {p_value_50:.4f}\")\nprint(f\"  Critical value (α=0.05): ±{t_crit:.4f}\")\n\nif abs(t_stat_50) &gt; t_crit:\n    print(f\"\\nResult: Reject H₀ at 5% significance level\")\n    print(f\"  Conclusion: The size coefficient is significantly different from {null_value}.\")\nelse:\n    print(f\"\\nResult: Fail to reject H₀ at 5% significance level\")\n    print(f\"  Conclusion: The data are consistent with β_size = {null_value}.\")\n\n======================================================================\n11.4 HYPOTHESIS TESTS ON A SINGLE PARAMETER\n======================================================================\n\nTest: H₀: β_size = 50 vs H₁: β_size ≠ 50\n  Coefficient estimate: $68.37\n  Standard error: $15.39\n  t-statistic: 1.1936\n  p-value: 0.2453\n  Critical value (α=0.05): ±2.0739\n\nResult: Fail to reject H₀ at 5% significance level\n  Conclusion: The data are consistent with β_size = 50.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#test-of-statistical-significance-β-0",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#test-of-statistical-significance-β-0",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Test of Statistical Significance (β = 0)",
    "text": "Test of Statistical Significance (β = 0)\nThe most common hypothesis test examines whether a coefficient is zero.\n\n# Test H₀: β_size = 0 (statistical significance)\nprint(\"=\" * 70)\nprint(\"Test of Statistical Significance: H₀: β_size = 0\")\nprint(\"=\" * 70)\n\nt_stat_zero = coef_size / se_size\np_value_zero = model_full.pvalues['size']\n\nprint(f\"\\n  t-statistic: {t_stat_zero:.4f}\")\nprint(f\"  p-value: {p_value_zero:.6f}\")\nprint(f\"  Critical value (α=0.05): ±{t_crit:.4f}\")\n\nif p_value_zero &lt; 0.05:\n    print(f\"\\nResult: Reject H₀ - Size is statistically significant at 5% level\")\n    print(f\"  Interpretation: House size has a statistically significant effect on price.\")\nelse:\n    print(f\"\\nResult: Fail to reject H₀ - Size is not statistically significant\")\n\n======================================================================\nTest of Statistical Significance: H₀: β_size = 0\n======================================================================\n\n  t-statistic: 4.4426\n  p-value: 0.000205\n  Critical value (α=0.05): ±2.0739\n\nResult: Reject H₀ - Size is statistically significant at 5% level\n  Interpretation: House size has a statistically significant effect on price.\n\n\n\nKey Concept 11.4: Tests of Statistical Significance\nThe most common hypothesis test examines \\(H_0: \\beta_j = 0\\) — whether variable \\(x_j\\) has any partial effect on \\(y\\). The \\(t\\)-statistic \\(t = b_j/se(b_j)\\) measures how many standard errors the estimate is from zero. Reject \\(H_0\\) when \\(|t| &gt; t_{\\text{critical}}\\) or equivalently when the \\(p\\)-value \\(&lt; \\alpha\\).",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#using-statsmodels-t_test",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#using-statsmodels-t_test",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Using statsmodels t_test",
    "text": "Using statsmodels t_test\nPython’s statsmodels package provides convenient methods for hypothesis testing.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-overall-f-test",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-overall-f-test",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Overall F-test",
    "text": "Interpreting the Overall F-test\nWhat this F-test tells us:\nThe overall F-test examines whether our model has any explanatory power at all:\n\nNull hypothesis: All slope coefficients equal zero (β₂ = β₃ = … = β₇ = 0)\nAlternative: At least one coefficient is non-zero\n\nResults:\n\nF-statistic = 6.83: This compares the explained variation to unexplained variation\np-value = 0.0003: Extremely small probability of observing such a large F-statistic if all coefficients were truly zero\nCritical value = 2.55: Our F-statistic (6.83) far exceeds this threshold\n\nDecision: Reject H₀ - The model is highly statistically significant.\nEconomic Interpretation:\nThis result tells us that at least one of our house characteristics (size, bedrooms, bathrooms, lotsize, age, monthsold) has a real relationship with price. Given that we already know size is significant from individual t-tests, this makes sense.\nHowever, this test doesn’t tell us which variables matter—just that the model as a whole provides useful information for predicting house prices. The very small p-value (0.0003) gives us high confidence that our model captures real economic relationships, not just random noise.\n\n# Using statsmodels t_test\nprint(\"=\" * 70)\nprint(\"Hypothesis test using statsmodels t_test:\")\nprint(\"=\" * 70)\nhypothesis = f'size = {null_value}'\nt_test_result = model_full.t_test(hypothesis)\nprint(t_test_result)\n\nprint(\"\\nThis confirms our manual calculation.\")\n\n======================================================================\nHypothesis test using statsmodels t_test:\n======================================================================\n                             Test for Constraints                             \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nc0            68.3694     15.389      1.194      0.245      36.454     100.285\n==============================================================================\n\nThis confirms our manual calculation.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-joint-test-of-subset-variables",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-joint-test-of-subset-variables",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Joint Test of Subset Variables",
    "text": "Interpreting the Joint Test of Subset Variables\nWhat this joint test tells us:\nThis F-test asks: “Can we exclude bedrooms, bathrooms, lotsize, age, and monthsold from the model and just keep size?”\nResults:\n\nF-statistic = 0.42: Very small F-statistic\np-value = 0.832: Very high p-value (83.2%)\nCritical value = 2.66: Our F-statistic (0.42) is far below this threshold\n\nDecision: Fail to reject H₀ - These five variables are NOT jointly significant.\nEconomic Interpretation:\nThis is a crucial finding for model selection. Even though we’re testing five variables simultaneously, they collectively add almost nothing to the model’s explanatory power beyond what size alone provides.\nWhat this means in practice:\n\nA simpler model with only size as a predictor would be preferred\nThe additional variables (bedrooms, bathrooms, etc.) don’t improve our ability to predict house prices\nKeeping these variables makes the model more complex without meaningful benefit\n\nThis result demonstrates the power of joint testing: while we might hope that bedrooms or bathrooms would add information, when tested together, they fail to improve the model. This likely reflects the strong correlation between size and these other features—bigger houses tend to have more bedrooms and bathrooms, so these variables don’t provide independent information.\nHaving tested individual coefficients, we now turn to joint hypothesis tests that evaluate multiple restrictions simultaneously.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#joint-hypothesis-tests",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#joint-hypothesis-tests",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.5: Joint Hypothesis Tests",
    "text": "11.5: Joint Hypothesis Tests\nSometimes we want to test multiple restrictions simultaneously. Individual t-tests are insufficient for this purpose.\nWhy joint tests? - Testing multiple individual hypotheses separately can be misleading - Joint tests account for correlation between coefficient estimates\nExamples of joint hypotheses: 1. All slope coefficients equal zero: \\(\\beta_2 = \\beta_3 = \\cdots = \\beta_k = 0\\) 2. Subset of coefficients equal zero: \\(\\beta_3 = \\beta_4 = \\beta_5 = 0\\) 3. Linear restrictions: \\(\\beta_2 = -\\beta_3\\) and $2_4 + _6 = 9$\nF-test procedure: - Test statistic follows the F-distribution: \\(F(q, n-k)\\) - \\(q\\) = number of restrictions - \\(n-k\\) = degrees of freedom\nF-distribution properties: - Always positive (right-skewed) - Depends on two degrees of freedom parameters - As \\(q\\) or \\(n-k\\) increases, critical values change",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-sum-of-squares-decomposition",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-sum-of-squares-decomposition",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Sum of Squares Decomposition",
    "text": "Interpreting the Sum of Squares Decomposition\nWhat these calculations show:\nThe sum of squares decomposition breaks down the total variation in house prices:\n\nTSS = $39,145,826,897 (Total Sum of Squares): Total variation in house prices around their mean\nESS = $25,466,429,042 (Explained Sum of Squares): Variation explained by our model (65.1%)\nRSS = $13,679,397,855 (Residual Sum of Squares): Variation left unexplained (34.9%)\n\nVerification: TSS = ESS + RSS (The identity holds perfectly)\nUnderstanding the F-statistic:\nThe F-statistic compares explained variation per parameter to unexplained variation per degree of freedom:\n\\[F = \\frac{ESS/(k-1)}{RSS/(n-k)} = \\frac{25,466,429,042/6}{13,679,397,855/22} = 6.83\\]\nEconomic Interpretation:\nOur model explains about 65% of the variation in house prices—a respectable amount for real estate data. The remaining 35% reflects unmeasured factors like neighborhood quality, interior condition, proximity to amenities, etc.\nThe F-statistic of 6.83 tells us that the explained variation (per parameter) is nearly 7 times larger than the unexplained variation (per degree of freedom). This ratio is large enough to conclude the model has genuine explanatory power, not just capturing random noise.\n\nprint(\"=\" * 70)\nprint(\"11.5 JOINT HYPOTHESIS TESTS\")\nprint(\"=\" * 70)\n\n# Test 1: Joint significance of all slope coefficients\n# H₀: β₁ = β₂ = ... = βₖ = 0\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Test 1: Overall F-test (all slopes = 0)\")\nprint(\"-\" * 70)\n\nf_stat = model_full.fvalue\nf_pvalue = model_full.f_pvalue\ndfn = k - 1  # numerator df (excluding intercept)\ndfd = df     # denominator df\nf_crit = stats.f.ppf(0.95, dfn, dfd)\n\nprint(f\"  H₀: All slope coefficients equal zero\")\nprint(f\"  F-statistic: {f_stat:.4f}\")\nprint(f\"  p-value: {f_pvalue:.6e}\")\nprint(f\"  Critical value (α=0.05): {f_crit:.4f}\")\nprint(f\"  Numerator df: {dfn}, Denominator df: {dfd}\")\n\nif f_stat &gt; f_crit:\n    print(\"\\nResult: Reject H₀ - At least one coefficient is non-zero\")\n    print(\"  Interpretation: The regressors are jointly statistically significant.\")\nelse:\n    print(\"\\nResult: Fail to reject H₀\")\n\n======================================================================\n11.5 JOINT HYPOTHESIS TESTS\n======================================================================\n\n----------------------------------------------------------------------\nTest 1: Overall F-test (all slopes = 0)\n----------------------------------------------------------------------\n  H₀: All slope coefficients equal zero\n  F-statistic: 6.8261\n  p-value: 3.424253e-04\n  Critical value (α=0.05): 2.5491\n  Numerator df: 6, Denominator df: 22\n\nResult: Reject H₀ - At least one coefficient is non-zero\n  Interpretation: The regressors are jointly statistically significant.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#joint-test-of-subset-of-coefficients",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#joint-test-of-subset-of-coefficients",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Joint Test of Subset of Coefficients",
    "text": "Joint Test of Subset of Coefficients\nNow test whether variables other than size are jointly significant.\n\n# Test 2: Joint test of subset of coefficients\n# H₀: β_bedrooms = β_bathrooms = β_lotsize = β_age = β_monthsold = 0\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Test 2: Joint test - Are variables other than size significant?\")\nprint(\"-\" * 70)\n\nhypotheses = ['bedrooms = 0', 'bathrooms = 0', 'lotsize = 0',\n              'age = 0', 'monthsold = 0']\nf_test_result = model_full.f_test(hypotheses)\nprint(f_test_result)\n\nprint(f\"\\nInterpretation:\")\nprint(f\"  This tests whether bedrooms, bathrooms, lotsize, age, and monthsold\")\nprint(f\"  can jointly be excluded from the model (keeping only size).\")\n\nif f_test_result.pvalue &lt; 0.05:\n    print(f\"  Result: These variables are jointly significant.\")\nelse:\n    print(f\"  Result: These variables are NOT jointly significant.\")\n    print(f\"  A simpler model with only size may be preferred.\")\n\n\n----------------------------------------------------------------------\nTest 2: Joint test - Are variables other than size significant?\n----------------------------------------------------------------------\n&lt;F test: F=0.41676518071663304, p=0.8319758671771483, df_denom=22, df_num=5&gt;\n\nInterpretation:\n  This tests whether bedrooms, bathrooms, lotsize, age, and monthsold\n  can jointly be excluded from the model (keeping only size).\n  Result: These variables are NOT jointly significant.\n  A simpler model with only size may be preferred.\n\n\n\nKey Concept 11.5: Joint Hypothesis Tests and the F Distribution\nIndividual \\(t\\)-tests cannot test multiple restrictions simultaneously because they ignore correlations between coefficient estimates. The F-test evaluates joint significance using the \\(F(q, n-k)\\) distribution, where \\(q\\) is the number of restrictions. It compares how much worse the restricted model fits relative to the unrestricted model.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-subset-f-test",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-subset-f-test",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Subset F-test",
    "text": "Interpreting the Subset F-test\nWhat this subset F-test tells us:\nWe’re comparing two models: - Restricted model: price ~ size (2 parameters) - Unrestricted model: price ~ size + bedrooms + bathrooms + lotsize + age + monthsold (7 parameters)\nKey numbers: - RSS (restricted) = $14,975,101,655: Prediction errors when using only size - RSS (unrestricted) = $13,679,397,855: Prediction errors when using all variables - Increase in RSS = $1,295,703,800: How much worse the restricted model fits\nTest results: - F-statistic = 0.42: The increase in RSS is small relative to the baseline error - p-value = 0.832: 83.2% probability of seeing this result if the restrictions are true - Critical value = 2.66: Our F-statistic is far below the threshold\nDecision: Fail to reject H₀ - The restricted model (only size) is NOT significantly worse.\nEconomic Interpretation:\nThis is a powerful result for model selection. Adding five additional variables (bedrooms, bathrooms, lotsize, age, monthsold) reduces prediction errors by only $1.3 million out of $15 million total—a mere 8.7% improvement. This improvement is so small it could easily be due to random chance.\nPractical recommendation: Use the simpler model with only size. It’s easier to interpret, requires less data collection, and performs nearly as well as the complex model. This is an application of Occam’s Razor in econometrics: prefer simpler models when complex ones don’t provide meaningful improvement.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#f-statistic-under-assumptions-1-4",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#f-statistic-under-assumptions-1-4",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.6: F Statistic Under Assumptions 1-4",
    "text": "11.6: F Statistic Under Assumptions 1-4\nUnder the classical assumptions, the F-statistic has a specific formula based on sums of squares.\nSum of Squares Decomposition:\n\\[TSS = ESS + RSS\\]\nwhere:\n\nTSS (Total Sum of Squares) = \\(\\sum (y_i - \\bar{y})^2\\)\nESS (Explained Sum of Squares) = \\(\\sum (\\hat{y}_i - \\bar{y})^2\\)\nRSS (Residual Sum of Squares) = \\(\\sum (y_i - \\hat{y}_i)^2\\)\n\nF-statistic formula:\n\\[F = \\frac{(RSS_r - RSS_u) / q}{RSS_u / (n-k)} \\sim F(q, n-k)\\]\nwhere:\n\n\\(RSS_r\\) = RSS from restricted model\n\\(RSS_u\\) = RSS from unrestricted model\n\\(q\\) = number of restrictions\n\nIntuition: Reject restrictions if the restricted model fits much worse (large increase in RSS).\nOverall F-test formula:\n\\[F = \\frac{R^2 / (k-1)}{(1-R^2) / (n-k)} \\sim F(k-1, n-k)\\]\n\nprint(\"=\" * 70)\nprint(\"11.6 F STATISTIC UNDER ASSUMPTIONS 1-4\")\nprint(\"=\" * 70)\n\n# Manual calculation of F-statistic using sums of squares\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Manual F-statistic calculation\")\nprint(\"-\" * 70)\n\n# Calculate sum of squares\ny = data_house['price']\ny_mean = y.mean()\ny_pred = model_full.fittedvalues\nresid = model_full.resid\n\n# Total sum of squares\nTSS = np.sum((y - y_mean)**2)\n# Explained sum of squares\nESS = np.sum((y_pred - y_mean)**2)\n# Residual sum of squares\nRSS = np.sum(resid**2)\n\nprint(f\"Sum of Squares:\")\nprint(f\"  Total (TSS): {TSS:,.2f}\")\nprint(f\"  Explained (ESS): {ESS:,.2f}\")\nprint(f\"  Residual (RSS): {RSS:,.2f}\")\nprint(f\"  Check: TSS = ESS + RSS: {np.isclose(TSS, ESS + RSS)}\")\n\n# F-statistic\nf_stat_manual = (ESS / (k-1)) / (RSS / df)\nprint(f\"\\nF-statistic calculation:\")\nprint(f\"  F = (ESS/{k-1}) / (RSS/{df})\")\nprint(f\"  F = ({ESS:,.2f}/{k-1}) / ({RSS:,.2f}/{df})\")\nprint(f\"  F = {f_stat_manual:.4f}\")\nprint(f\"  From model output: {f_stat:.4f}\")\nprint(f\"  Match: {np.isclose(f_stat_manual, f_stat)}\")\n\n# Alternative formula using R²\nr_squared = model_full.rsquared\nf_stat_rsq = (r_squared / (k-1)) / ((1 - r_squared) / df)\nprint(f\"\\nAlternative formula using R²:\")\nprint(f\"  F = (R²/(k-1)) / ((1-R²)/(n-k))\")\nprint(f\"  F = ({r_squared:.4f}/{k-1}) / ({1-r_squared:.4f}/{df})\")\nprint(f\"  F = {f_stat_rsq:.4f}\")\nprint(f\"  Match: {np.isclose(f_stat_rsq, f_stat)}\")\n\n======================================================================\n11.6 F STATISTIC UNDER ASSUMPTIONS 1-4\n======================================================================\n\n----------------------------------------------------------------------\nManual F-statistic calculation\n----------------------------------------------------------------------\nSum of Squares:\n  Total (TSS): 39,145,826,896.55\n  Explained (ESS): 25,466,429,041.83\n  Residual (RSS): 13,679,397,854.73\n  Check: TSS = ESS + RSS: True\n\nF-statistic calculation:\n  F = (ESS/6) / (RSS/22)\n  F = (25,466,429,041.83/6) / (13,679,397,854.73/22)\n  F = 6.8261\n  From model output: 6.8261\n  Match: True\n\nAlternative formula using R²:\n  F = (R²/(k-1)) / ((1-R²)/(n-k))\n  F = (0.6506/6) / (0.3494/22)\n  F = 6.8261\n  Match: True\n\n\n\nKey Concept 11.6: The F Statistic Under Homoskedasticity\nUnder assumptions 1-4, the F-statistic can be computed from sums of squares: \\(F = [(RSS_r - RSS_u)/q] / [RSS_u/(n-k)]\\), or equivalently from \\(R^2\\): \\(F = [(R_u^2 - R_r^2)/q] / [(1-R_u^2)/(n-k)]\\). Larger F-values indicate the restrictions are inconsistent with the data.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#subset-f-test-restricted-vs-unrestricted-model",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#subset-f-test-restricted-vs-unrestricted-model",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Subset F-test: Restricted vs Unrestricted Model",
    "text": "Subset F-test: Restricted vs Unrestricted Model\nNow we compare the full model (unrestricted) with a simpler model containing only size (restricted).",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-model-comparison",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-the-model-comparison",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting the Model Comparison",
    "text": "Interpreting the Model Comparison\nWhat this comparison reveals:\nComparing three nested models helps us understand the incremental value of adding variables:\nModel 1 (Simple): price ~ size\n\nR² = 0.618, Adj. R² = 0.603: Explains 61.8% of price variation\nF-stat = 43.58: Very strong overall significance\nSimplest and most parsimonious\n\nModel 2 (Intermediate): price ~ size + bedrooms\n\nR² = 0.618, Adj. R² = 0.589: Almost identical R² to Model 1\nF-stat = 21.03: Still significant, but weaker than Model 1\nAdding bedrooms barely improves fit\n\nModel 3 (Full): price ~ size + bedrooms + bathrooms + lotsize + age + monthsold\n\nR² = 0.651, Adj. R² = 0.555: Highest R², but lowest adjusted R²\nF-stat = 6.83: Weakest overall significance (though still significant)\nComplexity penalty outweighs small improvement in fit\n\nKey insights:\n\nAdjusted R² is crucial: While R² increases with more variables (always), adjusted R² accounts for the complexity penalty. Model 1 has the highest adjusted R², indicating the best balance of fit and simplicity.\nDiminishing returns: Adding bedrooms (Model 2) provides essentially no improvement. Adding five more variables (Model 3) only increases R² from 0.618 to 0.651—a marginal gain.\nStatistical vs. practical significance: Model 3 is statistically significant overall (F = 6.83, p &lt; 0.001), but that doesn’t mean it’s the best model. Model 1 is superior on parsimony grounds.\n\nRecommendation: Use Model 1 (size only). It’s simpler, has the highest adjusted R², and loses almost nothing in explanatory power compared to more complex alternatives.\n\n# Subset F-test using restricted and unrestricted models\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Subset F-test: Restricted vs Unrestricted Model\")\nprint(\"-\" * 70)\n\n# Unrestricted model (already estimated as model_full)\n# Restricted model (only size as regressor)\nmodel_restricted = ols('price ~ size', data=data_house).fit()\n\nprint(\"\\nRestricted model (only size):\")\nprint(model_restricted.summary())\n\n\n----------------------------------------------------------------------\nSubset F-test: Restricted vs Unrestricted Model\n----------------------------------------------------------------------\n\nRestricted model (only size):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.617\nModel:                            OLS   Adj. R-squared:                  0.603\nMethod:                 Least Squares   F-statistic:                     43.58\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           4.41e-07\nTime:                        15:15:57   Log-Likelihood:                -332.05\nNo. Observations:                  29   AIC:                             668.1\nDf Residuals:                      27   BIC:                             670.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    1.15e+05   2.15e+04      5.352      0.000    7.09e+04    1.59e+05\nsize          73.7710     11.175      6.601      0.000      50.842      96.700\n==============================================================================\nOmnibus:                        0.576   Durbin-Watson:                   1.219\nProb(Omnibus):                  0.750   Jarque-Bera (JB):                0.638\nSkew:                          -0.078   Prob(JB):                        0.727\nKurtosis:                       2.290   Cond. No.                     9.45e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.45e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-coefficient-stability-across-models",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-coefficient-stability-across-models",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting Coefficient Stability Across Models",
    "text": "Interpreting Coefficient Stability Across Models\nWhat this table reveals about coefficient estimates:\nTracking how coefficients change as we add variables helps diagnose multicollinearity and understand variable relationships:\nSize coefficient across models: - Model 1: $73.77 (SE = $11.17) - Model 2: $73.65 (SE = $11.50)\n- Model 3: $68.37 (SE = $15.39)\nWhat we observe:\n\nRelatively stable point estimates: The size coefficient ranges from $68-$74 across models, suggesting the relationship is genuine and robust.\nIncreasing standard errors: As we add variables, the SE increases from $11 to $15—a 38% increase. This reflects multicollinearity: size is correlated with other variables (larger houses have more bedrooms, bathrooms, etc.).\nPrecision loss: In Model 1, we can estimate the size effect quite precisely (SE = $11). Adding correlated variables inflates uncertainty without improving the overall fit much.\n\nBedrooms paradox: - When added in Model 2, bedrooms show essentially no effect - In Model 3, the coefficient is $2,685 but with a huge SE of $9,193 - This means bedrooms add no information beyond what size already provides\nEconomic lesson:\nThis pattern is common in real estate data: once you control for total square footage, the number of rooms matters little. Two houses of identical size but different room configurations (e.g., one with 3 large bedrooms vs. one with 4 small bedrooms) sell for similar prices. Size is what buyers care about, not how that space is divided.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#manual-f-test-calculation",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#manual-f-test-calculation",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Manual F-test Calculation",
    "text": "Manual F-test Calculation",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-robust-standard-errors",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#interpreting-robust-standard-errors",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Interpreting Robust Standard Errors",
    "text": "Interpreting Robust Standard Errors\nWhat robust standard errors tell us:\nHeteroskedasticity-robust (HC1) standard errors correct for potential violations of the constant variance assumption. Comparing standard vs. robust errors helps diagnose whether heteroskedasticity is a concern:\nSize coefficient: - Standard SE: $15.39 → Robust SE: $15.15 - Change: Slight decrease (1.6%) - Both t-stats highly significant (p ≈ 0.0002)\nIntercept: - Standard SE: $61,465 → Robust SE: $69,273 - Change: Increase of 12.7% - p-value changes: 0.035 → 0.047 (still significant, but closer to the boundary)\nOther variables: - Most show minor changes in SEs - All remain statistically insignificant with robust SEs - Conclusions unchanged\nWhat this means:\n\nMild heteroskedasticity: The fact that robust SEs are similar to standard SEs suggests heteroskedasticity is not a major problem in this dataset. If it were severe, we’d see much larger increases in robust SEs.\nConclusions are robust: The key finding—that size is significant while other variables are not—holds regardless of which SE we use.\nBest practice: For cross-sectional data like housing prices, it’s wise to report robust SEs by default. They provide valid inference whether or not heteroskedasticity is present, with minimal cost.\nIntercept sensitivity: The intercept shows the largest change, but intercepts are rarely of economic interest. Our substantive conclusions about slope coefficients remain unchanged.\n\n\n# Calculate F-statistic for subset test\nk_unrest = len(model_full.params)\nk_rest = len(model_restricted.params)\nq = k_unrest - k_rest  # number of restrictions\n\nRSS_unrest = np.sum(model_full.resid**2)\nRSS_rest = np.sum(model_restricted.resid**2)\ndf_unrest = n - k_unrest\n\nF_subset = ((RSS_rest - RSS_unrest) / q) / (RSS_unrest / df_unrest)\np_value_subset = 1 - stats.f.cdf(F_subset, q, df_unrest)\nf_crit_subset = stats.f.ppf(0.95, q, df_unrest)\n\nprint(\"\\nSubset F-test results:\")\nprint(f\"  Number of restrictions (q): {q}\")\nprint(f\"  RSS (restricted): {RSS_rest:,.2f}\")\nprint(f\"  RSS (unrestricted): {RSS_unrest:,.2f}\")\nprint(f\"  Increase in RSS: {RSS_rest - RSS_unrest:,.2f}\")\nprint(f\"  F-statistic: {F_subset:.4f}\")\nprint(f\"  p-value: {p_value_subset:.4f}\")\nprint(f\"  Critical value (α=0.05): {f_crit_subset:.4f}\")\n\nif F_subset &gt; f_crit_subset:\n    print(\"\\nResult: Reject H₀ - The additional variables are jointly significant\")\n    print(\"  Keep the full model.\")\nelse:\n    print(\"\\nResult: Fail to reject H₀ - The additional variables are NOT jointly significant\")\n    print(\"  The simpler model (only size) is preferred.\")\n\n\nSubset F-test results:\n  Number of restrictions (q): 5\n  RSS (restricted): 14,975,101,654.50\n  RSS (unrestricted): 13,679,397,854.73\n  Increase in RSS: 1,295,703,799.78\n  F-statistic: 0.4168\n  p-value: 0.8320\n  Critical value (α=0.05): 2.6613\n\nResult: Fail to reject H₀ - The additional variables are NOT jointly significant\n  The simpler model (only size) is preferred.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#anova-table-comparison",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#anova-table-comparison",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "ANOVA Table Comparison",
    "text": "ANOVA Table Comparison\n\n# Using ANOVA table for comparison\nprint(\"\\n\" + \"-\" * 70)\nprint(\"ANOVA table comparison\")\nprint(\"-\" * 70)\nanova_results = anova_lm(model_restricted, model_full)\nprint(anova_results)\n\nprint(\"\\nThe ANOVA table confirms our manual F-test calculation.\")\n\n\n----------------------------------------------------------------------\nANOVA table comparison\n----------------------------------------------------------------------\n   df_resid           ssr  df_diff       ss_diff         F    Pr(&gt;F)\n0      27.0  1.497510e+10      0.0           NaN       NaN       NaN\n1      22.0  1.367940e+10      5.0  1.295704e+09  0.416765  0.831976\n\nThe ANOVA table confirms our manual F-test calculation.\n\n\n\nKey Concept 11.7: Testing Subsets of Regressors\nTo test whether a subset of regressors belongs in the model, compare the restricted model (without those variables) to the unrestricted model (with them) using an F-test. If the F-statistic is small (large \\(p\\)-value), the additional regressors don’t significantly improve fit and the simpler model is preferred.\n\nNow that we can compute and interpret F-statistics, let’s learn how to present regression results professionally.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#presentation-of-regression-results",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#presentation-of-regression-results",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "11.7: Presentation of Regression Results",
    "text": "11.7: Presentation of Regression Results\nProfessional presentation of regression results is important for communication. Different formats emphasize different aspects.\nCommon presentation formats:\n\nCoefficients with standard errors (in parentheses)\nCoefficients with t-statistics (in parentheses)\nCoefficients with p-values (in parentheses)\nCoefficients with 95% confidence intervals\nCoefficients with significance stars:\n\n*** for p &lt; 0.01 (1% level)\n** for p &lt; 0.05 (5% level)\n\nfor p &lt; 0.10 (10% level)\n\n\n\nModel comparison tables typically show:\n\nMultiple model specifications side-by-side\nStandard errors in parentheses below coefficients\nModel fit statistics (R², N, F-stat) at bottom\n\nThis allows readers to see how coefficient estimates change across specifications.\n\nprint(\"=\" * 70)\nprint(\"11.7 PRESENTATION OF REGRESSION RESULTS\")\nprint(\"=\" * 70)\n\n# Comparison of multiple models\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Model Comparison: Three Specifications\")\nprint(\"-\" * 70)\n\n# Model 1: Simple regression\nmodel1 = ols('price ~ size', data=data_house).fit()\n\n# Model 2: Two regressors\nmodel2 = ols('price ~ size + bedrooms', data=data_house).fit()\n\n# Model 3: Full model (already estimated as model_full)\nmodel3 = model_full\n\n# Create comparison table\nmodels = [model1, model2, model3]\nmodel_names = ['Model 1', 'Model 2', 'Model 3']\n\ncomparison_data = []\nfor name, model in zip(model_names, models):\n    model_stats = {\n        'Model': name,\n        'N': int(model.nobs),\n        'R²': f\"{model.rsquared:.4f}\",\n        'Adj. R²': f\"{model.rsquared_adj:.4f}\",\n        'RMSE': f\"{np.sqrt(model.mse_resid):.2f}\",\n        'F-stat': f\"{model.fvalue:.4f}\",\n        'p-value': f\"{model.f_pvalue:.6f}\"\n    }\n    comparison_data.append(model_stats)\n\ncomparison_df = pd.DataFrame(comparison_data)\nprint(comparison_df.to_string(index=False))\n\nprint(\"\\nModel specifications:\")\nprint(\"  Model 1: price ~ size\")\nprint(\"  Model 2: price ~ size + bedrooms\")\nprint(\"  Model 3: price ~ size + bedrooms + bathrooms + lotsize + age + monthsold\")\n\n======================================================================\n11.7 PRESENTATION OF REGRESSION RESULTS\n======================================================================\n\n----------------------------------------------------------------------\nModel Comparison: Three Specifications\n----------------------------------------------------------------------\n  Model  N     R² Adj. R²     RMSE  F-stat  p-value\nModel 1 29 0.6175  0.6033 23550.66 43.5796 0.000000\nModel 2 29 0.6180  0.5886 23981.21 21.0340 0.000004\nModel 3 29 0.6506  0.5552 24935.73  6.8261 0.000342\n\nModel specifications:\n  Model 1: price ~ size\n  Model 2: price ~ size + bedrooms\n  Model 3: price ~ size + bedrooms + bathrooms + lotsize + age + monthsold",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#detailed-coefficient-comparison-across-models",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#detailed-coefficient-comparison-across-models",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Detailed Coefficient Comparison Across Models",
    "text": "Detailed Coefficient Comparison Across Models\nNow let’s see how coefficient estimates change as we add variables.\n\n# Detailed coefficient comparison\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Coefficient Comparison Across Models\")\nprint(\"-\" * 70)\n\n# Get all unique parameter names\nall_params = set()\nfor model in models:\n    all_params.update(model.params.index)\nall_params = sorted(all_params)\n\n# Create coefficient table\ncoef_comparison = pd.DataFrame(index=all_params)\nfor i, (name, model) in enumerate(zip(model_names, models), 1):\n    coef_col = f'{name} Coef'\n    se_col = f'{name} SE'\n    \n    coef_comparison[coef_col] = model.params.reindex(all_params)\n    coef_comparison[se_col] = model.bse.reindex(all_params)\n\nprint(coef_comparison.fillna('-'))\n\nprint(\"\\nKey observations:\")\nprint(\"  - Size coefficient relatively stable across models\")\nprint(\"  - Adding variables increases standard errors (multicollinearity)\")\nprint(\"  - Adjusted R² peaks at Model 1 (simplest model)\")\n\n\n----------------------------------------------------------------------\nCoefficient Comparison Across Models\n----------------------------------------------------------------------\n            Model 1 Coef    Model 1 SE   Model 2 Coef   Model 2 SE  \\\nIntercept  115017.282609  21489.359861  111690.856193  27589.07418   \nage                    -             -              -            -   \nbathrooms              -             -              -            -   \nbedrooms               -             -    1553.458022  7846.866223   \nlotsize                -             -              -            -   \nmonthsold              -             -              -            -   \nsize            73.77104     11.174911      72.408146    13.299618   \n\n            Model 3 Coef    Model 3 SE  \nIntercept  137791.065699  61464.951869  \nage          -833.038602    719.334544  \nbathrooms    6832.880015  15721.191544  \nbedrooms     2685.315122   9192.525674  \nlotsize      2303.221371   7226.535205  \nmonthsold   -2088.503625   3520.897859  \nsize           68.369419     15.389472  \n\nKey observations:\n  - Size coefficient relatively stable across models\n  - Adding variables increases standard errors (multicollinearity)\n  - Adjusted R² peaks at Model 1 (simplest model)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#robust-standard-errors-heteroskedasticity-robust",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#robust-standard-errors-heteroskedasticity-robust",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Robust Standard Errors (Heteroskedasticity-Robust)",
    "text": "Robust Standard Errors (Heteroskedasticity-Robust)\nClassical OLS assumes constant error variance (homoskedasticity). When this fails, standard errors are incorrect.\nHeteroskedasticity-robust standard errors (HC1, White’s correction):\n\nValid inference even when error variance is not constant\nTypically larger than classical standard errors\nRecommended for cross-sectional data\n\nEffect on inference:\n\nCoefficient estimates unchanged\nStandard errors may increase\nt-statistics may decrease\nSome “significant” variables may become insignificant\n\n\nprint(\"=\" * 70)\nprint(\"ROBUST STANDARD ERRORS (HC1)\")\nprint(\"=\" * 70)\n\n# Get robust results for full model\nmodel_full_robust = model_full.get_robustcov_results(cov_type='HC1')\n\nprint(\"\\nComparison of standard vs robust standard errors:\")\nrobust_comparison = pd.DataFrame({\n    'Coefficient': model_full.params,\n    'Std. Error': model_full.bse,\n    'Robust SE': model_full_robust.bse,\n    't-stat (std)': model_full.tvalues,\n    't-stat (robust)': model_full_robust.tvalues,\n    'p-value (std)': model_full.pvalues,\n    'p-value (robust)': model_full_robust.pvalues\n})\nprint(robust_comparison)\n\nprint(\"\\nInterpretation:\")\nprint(\"  - Robust SEs are often larger (more conservative inference)\")\nprint(\"  - t-statistics are correspondingly smaller\")\nprint(\"  - Recommended to report robust SEs for cross-sectional data\")\n\n======================================================================\nROBUST STANDARD ERRORS (HC1)\n======================================================================\n\nComparison of standard vs robust standard errors:\n             Coefficient    Std. Error     Robust SE  t-stat (std)  \\\nIntercept  137791.065699  61464.951869  65545.225391      2.241783   \nsize           68.369419     15.389472     15.359192      4.442610   \nbedrooms     2685.315122   9192.525674   8285.528329      0.292119   \nbathrooms    6832.880015  15721.191544  19283.790798      0.434629   \nlotsize      2303.221371   7226.535205   5328.859590      0.318717   \nage          -833.038602    719.334544    762.929519     -1.158068   \nmonthsold   -2088.503625   3520.897859   3738.270456     -0.593174   \n\n           t-stat (robust)  p-value (std)  p-value (robust)  \nIntercept         2.102229       0.035387          0.047203  \nsize              4.451368       0.000205          0.000200  \nbedrooms          0.324097       0.772932          0.748926  \nbathrooms         0.354333       0.668065          0.726463  \nlotsize           0.432217       0.752947          0.669791  \nage              -1.091895       0.259254          0.286693  \nmonthsold        -0.558682       0.559114          0.582021  \n\nInterpretation:\n  - Robust SEs are often larger (more conservative inference)\n  - t-statistics are correspondingly smaller\n  - Recommended to report robust SEs for cross-sectional data\n\n\n\nKey Concept 11.8: Robust Standard Errors and Heteroskedasticity\nWhen errors may not have constant variance, heteroskedasticity-robust (HC1) standard errors provide valid inference without assuming homoskedasticity. Coefficient estimates remain unchanged, but standard errors — and thus t-statistics and p-values — may differ. For cross-sectional data, reporting robust SEs is considered best practice.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#visualization-confidence-intervals-for-all-coefficients",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#visualization-confidence-intervals-for-all-coefficients",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Visualization: Confidence Intervals for All Coefficients",
    "text": "Visualization: Confidence Intervals for All Coefficients\nA coefficient plot provides a visual summary of estimation results.\n\n# Figure 11.1: Confidence intervals for all coefficients\nfig, ax = plt.subplots(figsize=(10, 8))\n\nparams_no_int = model_full.params[1:]\nci_no_int = conf_int.iloc[1:, :]\n\ny_pos = np.arange(len(params_no_int))\nax.errorbar(params_no_int.values, y_pos,\n            xerr=[params_no_int.values - ci_no_int.iloc[:, 0].values,\n                  ci_no_int.iloc[:, 1].values - params_no_int.values],\n            fmt='o', markersize=8, capsize=5, capthick=2, linewidth=2, color='blue')\nax.set_yticks(y_pos)\nax.set_yticklabels(params_no_int.index)\nax.axvline(x=0, color='red', linestyle='--', linewidth=1.5, alpha=0.7,\n           label='H₀: β = 0')\nax.set_xlabel('Coefficient Value', fontsize=12)\nax.set_title('Figure 11.1: Coefficient Estimates with 95% Confidence Intervals',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"Coefficients whose CI crosses zero are not statistically significant at 5%.\")\n\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_56083/677075515.py:21: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  plt.tight_layout()\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 8320 (\\N{SUBSCRIPT ZERO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nCoefficients whose CI crosses zero are not statistically significant at 5%.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#visualization-f-distribution",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#visualization-f-distribution",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Visualization: F-Distribution",
    "text": "Visualization: F-Distribution\nVisualize the F-distribution and show where our test statistic falls.\n\n# Figure: F-distribution visualization\nfig, ax = plt.subplots(figsize=(10, 6))\n\nx_range = np.linspace(0, 10, 1000)\nf_pdf = stats.f.pdf(x_range, dfn, dfd)\n\nax.plot(x_range, f_pdf, 'b-', linewidth=2, label=f'F({dfn}, {dfd}) distribution')\nax.axvline(x=f_stat, color='red', linewidth=2, linestyle='--',\n           label=f'Observed F = {f_stat:.2f}')\nax.axvline(x=f_crit, color='green', linewidth=2, linestyle=':',\n           label=f'Critical value = {f_crit:.2f}')\n\n# Shade rejection region\nx_reject = x_range[x_range &gt;= f_crit]\nf_reject = stats.f.pdf(x_reject, dfn, dfd)\nax.fill_between(x_reject, 0, f_reject, alpha=0.3, color='red',\n                label='Rejection region (α=0.05)')\n\nax.set_xlabel('F-statistic', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('F-Distribution for Overall Significance Test',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(0, max(8, f_stat + 1))\nplt.tight_layout()\nplt.show()\n\nprint(f\"The observed F-statistic ({f_stat:.2f}) exceeds the critical value ({f_crit:.2f}).\")\nprint(\"This leads us to reject the null hypothesis of no relationship.\")\n\n\n\n\n\n\n\n\nThe observed F-statistic (6.83) exceeds the critical value (2.55).\nThis leads us to reject the null hypothesis of no relationship.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#visualization-model-comparison-actual-vs-predicted",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#visualization-model-comparison-actual-vs-predicted",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Visualization: Model Comparison (Actual vs Predicted)",
    "text": "Visualization: Model Comparison (Actual vs Predicted)\nCompare the three models visually by plotting actual vs. predicted values.\n\n# Figure: Model comparison visualization\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i, (model, name) in enumerate(zip(models, model_names)):\n    axes[i].scatter(data_house['price'], model.fittedvalues,\n                   alpha=0.6, s=50, color='black')\n    axes[i].plot([data_house['price'].min(), data_house['price'].max()],\n                [data_house['price'].min(), data_house['price'].max()],\n                'r--', linewidth=2)\n    axes[i].set_xlabel('Actual Price ($)', fontsize=11)\n    axes[i].set_ylabel('Predicted Price ($)', fontsize=11)\n    axes[i].set_title(f'{name}\\nR² = {model.rsquared:.4f}',\n                     fontsize=11, fontweight='bold')\n    axes[i].grid(True, alpha=0.3)\n\nplt.suptitle('Model Comparison: Actual vs Predicted Prices',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"All three models show similar predictive performance.\")\nprint(\"The simplest model (Model 1) may be preferred for parsimony.\")\n\n\n\n\n\n\n\n\nAll three models show similar predictive performance.\nThe simplest model (Model 1) may be preferred for parsimony.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#key-takeaways",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#key-takeaways",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nClassical Assumptions and OLS Properties: - Under assumptions 1-4 (linearity, random sampling, no perfect collinearity, zero conditional mean), OLS is unbiased, consistent, and efficient (BLUE) - The standard error \\(se(b_j) = s_e / \\sqrt{\\sum \\widetilde{x}_{ji}^2}\\) decreases with larger samples, more variation in \\(x_j\\), and lower multicollinearity - The \\(t\\)-statistic \\(t = b_j / se(b_j)\\) follows a \\(T(n-k)\\) distribution\nConfidence Intervals: - 95% CI: \\(b_j \\pm t_{n-k, 0.025} \\times se(b_j)\\) - If the CI excludes zero, the coefficient is statistically significant at 5% - Narrower CIs indicate more precise estimation\nIndividual Hypothesis Tests: - Test \\(H_0: \\beta_j = \\beta_j^*\\) using \\(t = (b_j - \\beta_j^*) / se(b_j)\\) - Most common: test of significance \\(H_0: \\beta_j = 0\\) — does variable \\(x_j\\) matter after controlling for others? - Reject \\(H_0\\) if \\(|t| &gt; t_{\\text{critical}}\\) or equivalently if \\(p\\)-value \\(&lt; \\alpha\\)\nJoint F-Tests: - F-test evaluates multiple restrictions simultaneously: \\(F = \\frac{(RSS_r - RSS_u)/q}{RSS_u/(n-k)} \\sim F(q, n-k)\\) - Overall significance test: \\(H_0: \\beta_2 = \\cdots = \\beta_k = 0\\) (all slopes zero) - Subset tests: compare restricted vs. unrestricted models to decide if additional regressors are needed - Equivalently: \\(F = \\frac{(R_u^2 - R_r^2)/q}{(1 - R_u^2)/(n-k)}\\)\nModel Comparison and Selection: - Use F-tests to formally compare nested models - Consider adjusted \\(R^2\\), AIC, BIC alongside F-tests - Prefer parsimony: if additional variables don’t significantly improve fit, use the simpler model - In our house price example, size alone was sufficient — five additional variables failed the joint F-test (\\(p = 0.83\\))\nPresenting Results and Robust Inference: - Five standard formats: coefficients with (SEs), (t-stats), (p-values), (CIs), or (asterisks) - Heteroskedasticity-robust (HC1) standard errors provide valid inference without assuming constant variance - Report robust SEs by default for cross-sectional data\nPython tools used: statsmodels (OLS, t_test, f_test, anova_lm, HC1), scipy.stats (t and F distributions), matplotlib/seaborn (coefficient plots, F-distribution)\nNext steps: Chapter 12 covers further inference and model specification — additional diagnostic tests and extensions to the multiple regression framework.\nCongratulations on completing Chapter 11! You can now conduct rigorous statistical inference for multiple regression models.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#practice-exercises",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#practice-exercises",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of statistical inference for multiple regression.\n\nExercise 1: Confidence Interval Calculation\nA multiple regression with \\(n = 200\\) observations and \\(k = 4\\) parameters yields: - \\(b_2 = 5.0\\), \\(se(b_2) = 2.0\\) - \\(b_3 = 7.0\\), \\(se(b_3) = 2.0\\)\n\nCompute an approximate 95% confidence interval for \\(\\beta_2\\).\nCompute an approximate 95% confidence interval for \\(\\beta_3\\).\nWhich coefficient is estimated more precisely? Explain.\n\n\nExercise 2: Test of Statistical Significance\nUsing the regression from Exercise 1:\n\nIs \\(x_2\\) statistically significant at the 5% level? Compute the \\(t\\)-statistic and state your conclusion.\nIs \\(x_3\\) statistically significant at the 5% level?\nWhat is the approximate \\(p\\)-value for each coefficient?\n\n\nExercise 3: Testing a Specific Value\nUsing the same regression, test the claim that \\(\\beta_3 = 10.0\\) at significance level 0.05.\n\nState the null and alternative hypotheses.\nCompute the \\(t\\)-statistic.\nWhat is your decision? Can you reject \\(H_0\\)?\n\n\nExercise 4: Joint F-Test Setup\nConsider the model \\(y = \\beta_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5 + \\beta_6 x_6 + u\\).\nWe wish to test whether only \\(x_2\\) and \\(x_3\\) should be included in the model.\n\nState \\(H_0\\) and \\(H_a\\) for this test.\nHow many restrictions are being tested (\\(q\\))?\nWhat are the degrees of freedom for the F-test if \\(n = 100\\)?\n\n\nExercise 5: F-Statistic Computation\nYou estimate two models on \\(n = 53\\) observations: - Restricted model (\\(k_r = 3\\)): \\(RSS_r = 40\\) - Unrestricted model (\\(k_u = 6\\)): \\(RSS_u = 30\\)\n\nCompute the F-statistic.\nWhat are the degrees of freedom?\nAt \\(\\alpha = 0.05\\), would you reject the restrictions?\n\n\nExercise 6: Regression Presentation\nGiven these regression results:\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n\n\n\n\nIntercept\n3.0\n1.5\n0.047\n\n\n\\(x_2\\)\n5.0\n2.0\n0.013\n\n\n\\(x_3\\)\n7.0\n2.0\n0.001\n\n\n\n\nPresent these results using the asterisk notation (*** for \\(p &lt; 0.01\\), ** for \\(p &lt; 0.05\\), * for \\(p &lt; 0.10\\)).\nPresent using the coefficient (standard error) format.\nWhich variables are significant at the 1% level? At the 5% level?",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#case-studies",
    "href": "../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html#case-studies",
    "title": "Chapter 11: Statistical Inference for Multiple Regression",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Statistical Inference for Cross-Country Productivity\nIn this case study, you will apply confidence intervals, hypothesis tests, and F-tests to investigate whether physical capital and human capital are statistically significant determinants of cross-country labor productivity differences.\nDataset: Mendez Convergence Clubs Data - Source: Mendez (2020), 108 countries, 1990-2014 - Key variables: - lp — Labor productivity (GDP per worker) - rk — Physical capital per worker - hc — Human capital index - region — Geographic region\nResearch question: Are physical capital and human capital jointly significant in explaining cross-country labor productivity?\n# Load the Mendez convergence clubs dataset\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat_2014 = dat[dat['year'] == 2014].dropna(subset=['lp', 'rk', 'hc']).copy()\ndat_2014['ln_lp'] = np.log(dat_2014['lp'])\ndat_2014['ln_rk'] = np.log(dat_2014['rk'])\nprint(f\"Cross-section sample: {len(dat_2014)} countries (year 2014)\")\ndat_2014.head()\n\nTask 1: Estimate Productivity Model (Guided)\nEstimate the multiple regression model for labor productivity.\n# Estimate: ln(lp) = beta_0 + beta_1 * ln(rk) + beta_2 * hc + u\nmodel_prod = ols('ln_lp ~ ln_rk + hc', data=dat_2014).fit()\nprint(model_prod.summary())\nQuestions: - How many observations and parameters does the model have? - What is the \\(R^2\\)? How much of productivity variation do capital and human capital explain?\n\n\nTask 2: Confidence Intervals (Guided)\nCompute and interpret 95% confidence intervals for the coefficients.\n# 95% confidence intervals\nprint(\"95% Confidence Intervals:\")\nprint(model_prod.conf_int(alpha=0.05))\n\n# Manual calculation for ln_rk coefficient\nb_rk = model_prod.params['ln_rk']\nse_rk = model_prod.bse['ln_rk']\ndf = model_prod.df_resid\nt_crit = stats.t.ppf(0.975, df)\nprint(f\"\\nManual CI for ln(rk): [{b_rk - t_crit*se_rk:.4f}, {b_rk + t_crit*se_rk:.4f}]\")\nQuestions: - Does the CI for \\(\\ln(\\text{rk})\\) exclude zero? What does this tell you? - Does the CI for hc exclude zero? Interpret in economic terms.\n\nKey Concept 11.9: Statistical Significance in Cross-Country Regressions\nWith over 100 countries, cross-country regressions have sufficient power to detect moderate effects. However, statistical significance does not imply causation — unobserved factors (institutions, geography, culture) may confound the relationship between capital inputs and productivity. Confidence intervals quantify estimation uncertainty but not omitted variable bias.\n\n\n\nTask 3: Individual Hypothesis Tests (Semi-guided)\nTest the statistical significance of each coefficient.\n# Test significance of each coefficient\nprint(\"t-statistics and p-values:\")\nfor var in ['ln_rk', 'hc']:\n    t = model_prod.tvalues[var]\n    p = model_prod.pvalues[var]\n    sig = '***' if p &lt; 0.01 else '**' if p &lt; 0.05 else '*' if p &lt; 0.10 else ''\n    print(f\"  {var}: t = {t:.3f}, p = {p:.4f} {sig}\")\nQuestions: - Which coefficients are significant at the 1% level? At the 5% level? - Interpret the economic meaning: what does significance of \\(\\ln(\\text{rk})\\) tell us about physical capital?\n\n\nTask 4: Joint F-Test (Semi-guided)\nTest whether physical capital and human capital are jointly significant.\n# Overall F-test: H0: beta_1 = beta_2 = 0\nprint(f\"Overall F-statistic: {model_prod.fvalue:.4f}\")\nprint(f\"p-value: {model_prod.f_pvalue:.6e}\")\n\n# Compare to restricted model (intercept only)\nmodel_restricted = ols('ln_lp ~ 1', data=dat_2014).fit()\nanova_result = anova_lm(model_restricted, model_prod)\nprint(\"\\nANOVA comparison:\")\nprint(anova_result)\nQuestions: - Can you reject \\(H_0: \\beta_1 = \\beta_2 = 0\\)? What does this mean economically? - How does the F-test relate to the individual \\(t\\)-tests?\n\n\nTask 5: Model Comparison (Independent)\nCompare nested models to determine the best specification.\nYour tasks: 1. Estimate three models: (a) \\(\\ln(\\text{lp}) \\sim \\ln(\\text{rk})\\) only, (b) \\(\\ln(\\text{lp}) \\sim \\text{hc}\\) only, (c) both regressors 2. Create a model comparison table with \\(R^2\\), adjusted \\(R^2\\), AIC, BIC 3. Conduct subset F-tests: does adding hc to the \\(\\ln(\\text{rk})\\)-only model significantly improve fit? 4. Report robust standard errors for the preferred model\nHint: Use anova_lm(model_restricted, model_unrestricted) for the F-test and model.get_robustcov_results(cov_type='HC1') for robust SEs.\n\n\nTask 6: Inference Policy Brief (Independent)\nWrite a 200-300 word policy brief summarizing your inference results.\nYour brief should address: 1. Which factors are statistically significant predictors of cross-country productivity? 2. What are the 95% confidence intervals for the effects of capital and human capital? 3. Are both factors jointly significant? What does the F-test tell us? 4. What policy implications follow from these findings? 5. What caveats should policymakers consider (association vs. causation, omitted variables)?\n\nKey Concept 11.10: From Statistical Significance to Policy Relevance\nFinding that both physical and human capital are statistically significant predictors of productivity suggests that investments in both areas may boost economic output. However, the magnitudes matter for policy: confidence intervals tell us the plausible range of effects, while F-tests confirm that both factors jointly matter beyond what each contributes alone. Policy decisions should weigh statistical evidence alongside practical considerations like cost-effectiveness and implementation feasibility.\n\n\n\n\nWhat You’ve Learned\nIn this case study, you applied the full statistical inference toolkit to cross-country productivity data:\n\nEstimated multiple regression models and examined coefficient properties\nConstructed and interpreted confidence intervals for partial effects\nConducted individual \\(t\\)-tests to assess each predictor’s significance\nPerformed joint F-tests to evaluate whether both capital types matter\nCompared nested models and reported results with robust standard errors\n\nThese inferential tools are essential for drawing reliable conclusions from empirical economic analysis.\n\n\nCase Study 2: Which Satellite Features Matter? Joint Tests for Predictive Power\nIn Chapter 10, we estimated a multiple regression of municipal development on nighttime lights and satellite embeddings. Now we apply Chapter 11’s inference tools—t-tests for individual coefficients and F-tests for joint significance—to determine which satellite features add statistically significant predictive power.\nDataset: DS4Bolivia — Satellite Data for Sustainable Development\n\nSource: DS4Bolivia Project, 339 municipalities\nKey variables:\n\nimds — Municipal Sustainable Development Index (0-100 composite)\nln_NTLpc2017 — Log nighttime lights per capita (2017)\nA00, A10, A20, A30, A40 — Selected satellite image embedding dimensions\n\n\nResearch question: Do satellite image embeddings add statistically significant predictive power for municipal development beyond nighttime lights alone?\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select variables and prepare analysis sample\nembed_vars = ['A00', 'A10', 'A20', 'A30', 'A40']\nanalysis_vars = ['imds', 'ln_NTLpc2017'] + embed_vars\nbol_cs = bol[['mun', 'dep'] + analysis_vars].dropna(subset=analysis_vars).copy()\nprint(f\"Analysis sample: {len(bol_cs)} municipalities with complete data\")\nbol_cs[analysis_vars].describe().round(3)\n\nTask 1: Estimate Full Model (Guided)\nEstimate the full multiple regression model with nighttime lights and all five satellite embedding dimensions as predictors of municipal development.\n# Estimate: imds = beta_0 + beta_1*ln_NTLpc2017 + beta_2*A00 + ... + beta_6*A40 + u\nmodel_full = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40', data=bol_cs).fit()\nprint(model_full.summary())\nQuestions:\n\nHow many coefficients are estimated (including the intercept)?\nWhich coefficients have p-values below 0.05? Below 0.10?\nWhat is the overall \\(R^2\\)? How much variation in development does this model explain?\nCompare this \\(R^2\\) to a model with NTL alone—how much do the embeddings add?\n\n\n# Your code here: Estimate the full model\n#\n# Steps:\n# 1. Estimate the full model with NTL and all 5 embeddings\n# 2. Print the full summary\n# 3. Identify significant coefficients (p &lt; 0.05 and p &lt; 0.10)\n\n# Example structure:\n# model_full = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40', data=bol_cs).fit()\n# print(model_full.summary())\n#\n# # Identify significant predictors\n# print(\"\\nSignificance at 5% level:\")\n# for var in model_full.params.index:\n#     p = model_full.pvalues[var]\n#     sig = \"***\" if p &lt; 0.01 else \"**\" if p &lt; 0.05 else \"*\" if p &lt; 0.10 else \"\"\n#     print(f\"  {var:18s}  p = {p:.4f}  {sig}\")\n\n\n\nTask 2: Confidence Intervals (Guided)\nCompute 95% confidence intervals for all coefficients and create a coefficient plot (forest plot) to visualize the estimates and their uncertainty.\n# 95% confidence intervals\nci = model_full.conf_int(alpha=0.05)\nci.columns = ['Lower 2.5%', 'Upper 97.5%']\nci['Estimate'] = model_full.params\nprint(ci[['Estimate', 'Lower 2.5%', 'Upper 97.5%']].round(4))\nQuestions:\n\nWhich confidence intervals include zero? What does this imply about significance?\nWhich coefficient has the widest confidence interval? The narrowest (excluding intercept)?\nCreate a forest plot showing point estimates and 95% CIs for the embedding coefficients\nHow do the confidence intervals for embeddings compare in width to the NTL coefficient?\n\n\n# Your code here: Confidence intervals and coefficient plot\n#\n# Steps:\n# 1. Compute confidence intervals with model_full.conf_int()\n# 2. Print the table of estimates and CIs\n# 3. Create a coefficient plot (forest plot) for embedding variables\n# 4. Identify which CIs include zero\n\n# Example structure:\n# ci = model_full.conf_int(alpha=0.05)\n# ci.columns = ['Lower', 'Upper']\n# ci['Estimate'] = model_full.params\n# print(ci[['Estimate', 'Lower', 'Upper']].round(4))\n#\n# # Forest plot for embedding coefficients\n# embed_vars = ['A00', 'A10', 'A20', 'A30', 'A40']\n# fig, ax = plt.subplots(figsize=(8, 5))\n# y_pos = range(len(embed_vars))\n# estimates = [model_full.params[v] for v in embed_vars]\n# errors = [(model_full.params[v] - ci.loc[v, 'Lower'],\n#            ci.loc[v, 'Upper'] - model_full.params[v]) for v in embed_vars]\n# errors_T = list(zip(*errors))\n# ax.errorbar(estimates, y_pos, xerr=errors_T, fmt='o', color='navy',\n#             capsize=5, markersize=8)\n# ax.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Zero (no effect)')\n# ax.set_yticks(y_pos)\n# ax.set_yticklabels(embed_vars)\n# ax.set_xlabel('Coefficient Estimate with 95% CI')\n# ax.set_title('Coefficient Plot: Satellite Embedding Effects on IMDS')\n# ax.legend()\n# plt.tight_layout()\n# plt.show()\n\n\n\nTask 3: Individual t-Tests (Semi-guided)\nExamine the t-statistics and p-values for each satellite embedding coefficient individually.\nYour tasks:\n\nExtract and display the t-statistic and p-value for each embedding variable (A00-A40)\nClassify each as significant at 5%, significant at 10%, or not significant\nCount how many of the 5 embeddings are individually significant at each level\nDiscuss: If some embeddings are individually insignificant, can we conclude they are “useless”? Why or why not?\n\nHint: Individual insignificance may reflect multicollinearity among embeddings rather than lack of predictive power. The joint F-test in Task 4 will help resolve this.\n\n# Your code here: Individual t-tests for embedding coefficients\n#\n# Steps:\n# 1. Extract t-statistics and p-values for each embedding\n# 2. Classify significance levels\n# 3. Discuss the implications\n\n# Example structure:\n# print(\"Individual t-Tests for Satellite Embeddings\")\n# print(\"=\" * 60)\n# embed_vars = ['A00', 'A10', 'A20', 'A30', 'A40']\n# sig_5 = 0\n# sig_10 = 0\n# for var in embed_vars:\n#     t = model_full.tvalues[var]\n#     p = model_full.pvalues[var]\n#     if p &lt; 0.05:\n#         level = \"Significant at 5%  ***\"\n#         sig_5 += 1\n#         sig_10 += 1\n#     elif p &lt; 0.10:\n#         level = \"Significant at 10% *\"\n#         sig_10 += 1\n#     else:\n#         level = \"Not significant\"\n#     print(f\"  {var}: t = {t:7.3f}, p = {p:.4f}  --&gt; {level}\")\n# print(f\"\\nSignificant at 5%:  {sig_5}/5 embeddings\")\n# print(f\"Significant at 10%: {sig_10}/5 embeddings\")\n\n\n\nTask 4: Joint F-Test (Semi-guided)\nTest whether all five satellite embedding coefficients are jointly equal to zero.\n\\[H_0: \\beta_{A00} = \\beta_{A10} = \\beta_{A20} = \\beta_{A30} = \\beta_{A40} = 0\\]\nYour tasks:\n\nConstruct a restriction matrix \\(R\\) where each row sets one embedding coefficient to zero\nUse model_full.f_test() with the restriction matrix to compute the joint F-statistic\nReport the F-statistic, degrees of freedom, and p-value\nCompare the joint test result with the individual t-test results from Task 3\nAre embeddings jointly significant even if some are individually insignificant?\n\nHint: The restriction matrix has 5 rows (one per restriction) and 7 columns (one per coefficient including intercept). Each row has a 1 in the position of the embedding coefficient being tested and 0s elsewhere.\n\n# Your code here: Joint F-test for all embedding coefficients\n#\n# Steps:\n# 1. Construct the restriction matrix R\n# 2. Perform the joint F-test with model_full.f_test(R)\n# 3. Report F-statistic and p-value\n# 4. Compare with individual t-test results\n\n# Example structure:\n# import numpy as np\n#\n# # Restriction matrix: 5 restrictions (A00=A10=A20=A30=A40=0)\n# # Coefficients order: Intercept, ln_NTLpc2017, A00, A10, A20, A30, A40\n# R = np.zeros((5, 7))\n# R[0, 2] = 1  # A00 = 0\n# R[1, 3] = 1  # A10 = 0\n# R[2, 4] = 1  # A20 = 0\n# R[3, 5] = 1  # A30 = 0\n# R[4, 6] = 1  # A40 = 0\n#\n# f_test = model_full.f_test(R)\n# print(\"Joint F-Test: All Embedding Coefficients = 0\")\n# print(\"=\" * 50)\n# print(f\"F-statistic: {f_test.fvalue[0][0]:.4f}\")\n# print(f\"p-value:     {f_test.pvalue:.6f}\")\n# print(f\"df:          ({int(f_test.df_num)}, {int(f_test.df_denom)})\")\n# print(f\"\\nConclusion: {'Reject H0' if f_test.pvalue &lt; 0.05 else 'Fail to reject H0'} at 5% level\")\n\n\nKey Concept 11.12: Joint Significance of Satellite Features\nIndividual satellite embedding coefficients may appear statistically insignificant (p &gt; 0.05) in a multiple regression, yet the group of embeddings may be jointly significant (F-test p &lt; 0.05). This paradox arises when embeddings are correlated with each other: the individual t-tests cannot distinguish each embedding’s unique contribution, but the F-test captures their collective explanatory power. Joint F-tests are essential when evaluating groups of related predictors.\n\n\n\nTask 5: Restricted vs Unrestricted Model Comparison (Independent)\nCompare the restricted model (NTL only) with the unrestricted model (NTL + embeddings) to quantify the contribution of satellite embeddings.\nYour tasks:\n\nEstimate Model 1 (restricted): imds ~ ln_NTLpc2017 (NTL only)\nEstimate Model 2 (unrestricted): imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40\nCompute the F-statistic manually using the formula:\n\n\\[F = \\frac{(R^2_u - R^2_r) / q}{(1 - R^2_u) / (n - k - 1)}\\]\nwhere \\(q = 5\\) (number of restrictions), \\(n\\) = sample size, \\(k\\) = number of regressors in unrestricted model\n\nCompare your manual calculation with model_full.compare_f_test(model_restricted)\nInterpret: How much do the embeddings improve the model’s explanatory power?\n\n\n# Your code here: Restricted vs unrestricted model comparison\n#\n# Steps:\n# 1. Estimate restricted model (NTL only)\n# 2. Compare R-squared values\n# 3. Compute F-statistic manually\n# 4. Verify with compare_f_test()\n\n# Example structure:\n# # Restricted model: NTL only\n# model_restricted = ols('imds ~ ln_NTLpc2017', data=bol_cs).fit()\n#\n# # Compare R-squared\n# R2_r = model_restricted.rsquared\n# R2_u = model_full.rsquared\n# n = model_full.nobs\n# k = len(model_full.params) - 1  # number of regressors (excluding intercept)\n# q = 5  # number of restrictions (embedding coefficients)\n#\n# print(\"Model Comparison\")\n# print(\"=\" * 50)\n# print(f\"Restricted (NTL only):      R² = {R2_r:.4f}\")\n# print(f\"Unrestricted (NTL + embed): R² = {R2_u:.4f}\")\n# print(f\"Improvement in R²:          ΔR² = {R2_u - R2_r:.4f}\")\n#\n# # Manual F-statistic\n# F_manual = ((R2_u - R2_r) / q) / ((1 - R2_u) / (n - k - 1))\n# print(f\"\\nManual F-statistic: {F_manual:.4f}\")\n#\n# # Verify with statsmodels\n# f_compare = model_full.compare_f_test(model_restricted)\n# print(f\"compare_f_test:     F = {f_compare[0]:.4f}, p = {f_compare[1]:.6f}\")\n\n\n\nTask 6: Inference Brief (Independent)\nWrite a 200-300 word inference brief summarizing your statistical findings.\nYour brief should address:\n\nWhich satellite features add significant predictive power for municipal development?\nDoes the joint F-test tell a different story than the individual t-tests? Why?\nHow much do satellite embeddings improve explanatory power beyond nighttime lights alone?\nWhat are the implications for feature selection in satellite-based prediction models?\nHow should researchers decide which satellite features to include in SDG prediction models?\n\nConnection to methods: This analysis demonstrates a core tension in applied econometrics: individual insignificance vs. joint significance. When predictors are correlated (as satellite embeddings often are), individual t-tests may lack power while joint F-tests reveal collective importance.\n\n# Your code here: Additional analysis for the inference brief\n#\n# You might want to:\n# 1. Create a summary table comparing individual and joint test results\n# 2. Visualize the R-squared improvement from adding embeddings\n# 3. Calculate specific statistics to cite in your brief\n\n# Example: Summary of key inference results\n# print(\"KEY INFERENCE RESULTS\")\n# print(\"=\" * 60)\n# print(f\"Sample size: {int(model_full.nobs)} municipalities\")\n# print(f\"R² (NTL only):          {model_restricted.rsquared:.4f}\")\n# print(f\"R² (NTL + embeddings):  {model_full.rsquared:.4f}\")\n# print(f\"R² improvement:         {model_full.rsquared - model_restricted.rsquared:.4f}\")\n# print(f\"\\nJoint F-test p-value:   {f_test.pvalue:.6f}\")\n# print(f\"Individually significant at 5%: {sig_5}/5 embeddings\")\n# print(f\"Individually significant at 10%: {sig_10}/5 embeddings\")\n\n\nKey Concept 11.13: Feature Selection in Prediction Models\nWhen many potential predictors are available (e.g., 64 satellite embedding dimensions), selecting which to include requires balancing explanatory power against model parsimony. Joint F-tests help determine whether subsets of features add genuine predictive value beyond what simpler models provide. In the DS4Bolivia context, testing whether 5 selected embeddings improve upon NTL alone informs practical decisions about data collection and model complexity for SDG monitoring.\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this analysis of satellite features and municipal development in Bolivia, you applied Chapter 11’s full inference toolkit to a remote sensing application:\n\nFull model estimation: Estimated a multiple regression with nighttime lights and satellite embeddings as predictors of development\nConfidence intervals: Constructed and visualized 95% CIs for all coefficients using a forest plot\nIndividual t-tests: Assessed the statistical significance of each satellite embedding individually\nJoint F-tests: Tested whether all embeddings are jointly significant using restriction matrices\nRestricted vs unrestricted comparison: Computed F-statistics manually and verified with compare_f_test()\nInference interpretation: Distinguished between individual insignificance and joint significance\n\nConnection to the next chapter: In Chapter 12, we address robust standard errors and prediction intervals—crucial for making reliable predictions about individual municipalities.\n\nWell done! You’ve now applied the full statistical inference toolkit to two datasets—cross-country productivity and Bolivian satellite data—discovering that joint tests can reveal predictive power hidden from individual tests.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chapter 11: Statistical Inference for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to advanced topics in regression inference and prediction. All code runs directly in Google Colab without any local setup.\nThis chapter covers advanced topics that extend the multiple regression framework: robust standard errors for different data structures, prediction of outcomes, and deeper understanding of estimation and testing optimality.\nLearning Objectives:\nBy the end of this chapter, you will be able to:\nDatasets used: - AED_HOUSE.DTA: 29 houses sold in Davis, California (1999) — for robust SEs and prediction - AED_REALGDPPC.DTA: Real GDP per capita growth (241 observations) — for HAC standard errors\nKey economic questions: - Do conclusions about house prices change with robust standard errors? - How precisely can we predict an individual house’s price vs. the average price? - What happens to inference when our sample is not representative?\nChapter outline: - 12.2 Inference with Robust Standard Errors - 12.3 Prediction - 12.4 Nonrepresentative Samples - 12.5 Best Estimation Methods - 12.6 Best Confidence Intervals - 12.7 Best Tests - Key Takeaways - Practice Exercises - Case Studies\nEstimated time: 60-75 minutes",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#chapter-overview",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#chapter-overview",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "",
    "text": "Understand when to use heteroskedastic-robust, cluster-robust, and HAC-robust standard errors\nDistinguish between prediction of average outcomes and individual outcomes\nCompute prediction intervals for conditional means and forecasts\nUnderstand the impact of nonrepresentative samples on regression estimates\nRecognize the difference between unbiased and best (most efficient) estimators\nUnderstand Type I and Type II errors in hypothesis testing\nAppreciate the role of bootstrap methods as an alternative to classical inference\nKnow when OLS with robust SEs is preferred over more efficient estimators like FGLS",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#setup",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#setup",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy import stats\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import acf\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to explore further topics in multiple regression.\")\n\nSetup complete! Ready to explore further topics in multiple regression.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#data-preparation",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#data-preparation",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’ll work with two datasets:\n\nHouse price data for cross-sectional robust inference\nGDP growth data for time series HAC inference\n\n\n# Read house data\ndata_house = pd.read_stata(GITHUB_DATA_URL + 'AED_HOUSE.DTA')\n\nprint(\"House Data Summary:\")\nprint(data_house.describe())\n\nprint(\"\\nFirst few observations:\")\nprint(data_house[['price', 'size', 'bedrooms', 'bathrooms', 'lotsize', 'age', 'monthsold']].head())\n\nHouse Data Summary:\n               price         size   bedrooms  bathrooms    lotsize        age  \\\ncount      29.000000    29.000000  29.000000  29.000000  29.000000  29.000000   \nmean   253910.344828  1882.758621   3.793103   2.206897   2.137931  36.413792   \nstd     37390.710695   398.272130   0.675030   0.341144   0.693034   7.118975   \nmin    204000.000000  1400.000000   3.000000   2.000000   1.000000  23.000000   \n25%    233000.000000  1600.000000   3.000000   2.000000   2.000000  31.000000   \n50%    244000.000000  1800.000000   4.000000   2.000000   2.000000  35.000000   \n75%    270000.000000  2000.000000   4.000000   2.500000   3.000000  39.000000   \nmax    375000.000000  3300.000000   6.000000   3.000000   3.000000  51.000000   \n\n       monthsold           list  \ncount  29.000000      29.000000  \nmean    5.965517  257824.137931  \nstd     1.679344   40860.264099  \nmin     3.000000  199900.000000  \n25%     5.000000  239000.000000  \n50%     6.000000  245000.000000  \n75%     7.000000  269000.000000  \nmax     8.000000  386000.000000  \n\nFirst few observations:\n    price  size  bedrooms  bathrooms  lotsize   age  monthsold\n0  204000  1400         3        2.0        1  31.0          7\n1  212000  1600         3        3.0        2  33.0          5\n2  213000  1800         3        2.0        2  51.0          4\n3  220000  1600         3        2.0        1  49.0          4\n4  224500  2100         4        2.5        2  47.0          6",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#inference-with-robust-standard-errors",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#inference-with-robust-standard-errors",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "12.2: Inference with Robust Standard Errors",
    "text": "12.2: Inference with Robust Standard Errors\nIn practice, the classical assumptions often fail. The most common violations are:\n1. Heteroskedasticity: Error variance varies across observations\n\nCommon in cross-sectional data\nMakes default standard errors incorrect\nSolution: Use heteroskedasticity-robust standard errors (HC1, White’s correction)\n\n2. Clustered errors: Errors correlated within groups\n\nCommon in panel data, hierarchical data\nMakes default and het-robust SEs too small\nSolution: Use cluster-robust standard errors\n\n3. Autocorrelation: Errors correlated over time\n\nCommon in time series\nMakes default SEs incorrect\nSolution: Use HAC (Newey-West) standard errors\n\nKey insight: OLS coefficients remain unbiased under these violations, but standard errors need adjustment.\nHeteroskedastic-robust standard error formula:\n\\[se_{het}(\\hat{\\beta}_j) = \\sqrt{\\frac{\\sum_{i=1}^n \\tilde{x}_{ji}^2 \\hat{u}_i^2}{(\\sum_{i=1}^n \\tilde{x}_{ji}^2)^2}}\\]\nwhere \\(\\tilde{x}_{ji}\\) are residuals from regressing \\(x_j\\) on other regressors, and \\(\\hat{u}_i\\) are OLS residuals.\n\nprint(\"=\" * 70)\nprint(\"12.2 INFERENCE WITH ROBUST STANDARD ERRORS\")\nprint(\"=\" * 70)\n\n# Estimate with default standard errors\nmodel_default = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n                    data=data_house).fit()\n\nprint(\"\\nRegression with Default Standard Errors:\")\nprint(model_default.summary())\n\n======================================================================\n12.2 INFERENCE WITH ROBUST STANDARD ERRORS\n======================================================================\n\nRegression with Default Standard Errors:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.651\nModel:                            OLS   Adj. R-squared:                  0.555\nMethod:                 Least Squares   F-statistic:                     6.826\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000342\nTime:                        15:04:03   Log-Likelihood:                -330.74\nNo. Observations:                  29   AIC:                             675.5\nDf Residuals:                      22   BIC:                             685.1\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.378e+05   6.15e+04      2.242      0.035    1.03e+04    2.65e+05\nsize          68.3694     15.389      4.443      0.000      36.454     100.285\nbedrooms    2685.3151   9192.526      0.292      0.773   -1.64e+04    2.17e+04\nbathrooms   6832.8800   1.57e+04      0.435      0.668   -2.58e+04    3.94e+04\nlotsize     2303.2214   7226.535      0.319      0.753   -1.27e+04    1.73e+04\nage         -833.0386    719.335     -1.158      0.259   -2324.847     658.770\nmonthsold  -2088.5036   3520.898     -0.593      0.559   -9390.399    5213.392\n==============================================================================\nOmnibus:                        1.317   Durbin-Watson:                   1.259\nProb(Omnibus):                  0.518   Jarque-Bera (JB):                0.980\nSkew:                           0.151   Prob(JB):                        0.612\nKurtosis:                       2.152   Cond. No.                     2.59e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.59e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# Estimate with heteroskedastic-robust standard errors (HC1)\nmodel_robust = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n                   data=data_house).fit(cov_type='HC1')\n\nprint(\"\\nRegression with Heteroskedastic-Robust Standard Errors (HC1):\")\nprint(model_robust.summary())\n\n\nRegression with Heteroskedastic-Robust Standard Errors (HC1):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.651\nModel:                            OLS   Adj. R-squared:                  0.555\nMethod:                 Least Squares   F-statistic:                     6.410\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000514\nTime:                        15:04:03   Log-Likelihood:                -330.74\nNo. Observations:                  29   AIC:                             675.5\nDf Residuals:                      22   BIC:                             685.1\nDf Model:                           6                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.378e+05   6.55e+04      2.102      0.036    9324.785    2.66e+05\nsize          68.3694     15.359      4.451      0.000      38.266      98.473\nbedrooms    2685.3151   8285.528      0.324      0.746   -1.36e+04    1.89e+04\nbathrooms   6832.8800   1.93e+04      0.354      0.723    -3.1e+04    4.46e+04\nlotsize     2303.2214   5328.860      0.432      0.666   -8141.152    1.27e+04\nage         -833.0386    762.930     -1.092      0.275   -2328.353     662.276\nmonthsold  -2088.5036   3738.270     -0.559      0.576   -9415.379    5238.372\n==============================================================================\nOmnibus:                        1.317   Durbin-Watson:                   1.259\nProb(Omnibus):                  0.518   Jarque-Bera (JB):                0.980\nSkew:                           0.151   Prob(JB):                        0.612\nKurtosis:                       2.152   Cond. No.                     2.59e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 2.59e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nKey Concept 12.1: Heteroskedastic-Robust Standard Errors\nWhen error variance is not constant across observations, default OLS standard errors are invalid. Heteroskedastic-robust (HC1) SEs correct this problem without changing the coefficient estimates themselves. Only the standard errors, \\(t\\)-statistics, and confidence intervals change. For cross-sectional data, reporting HC1 robust SEs is considered best practice.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#comparison-default-vs.-robust-standard-errors",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#comparison-default-vs.-robust-standard-errors",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Comparison: Default vs. Robust Standard Errors",
    "text": "Comparison: Default vs. Robust Standard Errors\nLet’s systematically compare the standard errors and see how inference changes.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#interpreting-the-comparison-what-changed",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#interpreting-the-comparison-what-changed",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Interpreting the Comparison: What Changed?",
    "text": "Interpreting the Comparison: What Changed?\nUnderstanding the Results:\nLooking at the SE Ratio column, we can see how robust standard errors differ from default ones:\nWhen SE Ratio &gt; 1.0: Robust SE is larger than default SE - Suggests heteroskedasticity is present - Default SEs were understating uncertainty - t-statistics decrease, p-values increase - We were too confident in rejecting null hypotheses\nWhen SE Ratio ≈ 1.0: Robust SE similar to default SE - Little evidence of heteroskedasticity for this variable - Both methods give similar inference\nWhen SE Ratio &lt; 1.0: Robust SE smaller than default SE - Unusual but possible - Could indicate negative correlation between x² and residuals\nPractical Implications:\n\nCoefficient estimates unchanged: OLS point estimates are the same regardless of SE type\nInference changes: Variables significant with default SEs might become insignificant with robust SEs\nPublication standard: Most journals now require robust SEs for cross-sectional data\nConservative approach: When in doubt, report robust SEs (they’re generally more credible)\n\nRule of thumb: If robust SEs differ substantially (&gt;30% change), heteroskedasticity is likely present and you should use robust inference.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#hac-standard-errors-for-time-series",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#hac-standard-errors-for-time-series",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "HAC Standard Errors for Time Series",
    "text": "HAC Standard Errors for Time Series\nTime series data often exhibit autocorrelation: current errors correlated with past errors.\nExample: GDP growth tends to persist\n\nPositive shock today → likely positive next period\nCreates correlation structure \\(Corr(u_t, u_{t-s}) \\neq 0\\)\n\nHAC (Newey-West) standard errors:\n\nAccount for both heteroskedasticity AND autocorrelation\nRequire specifying maximum lag length \\(m\\)\nRule of thumb: \\(m = 0.75 \\times T^{1/3}\\)\n\nAutocorrelation function:\n\\[\\rho_s = \\frac{Cov(y_t, y_{t-s})}{\\sqrt{Var(y_t) Var(y_{t-s})}}\\]\nWe can visualize this with a correlogram.\n\n# Load GDP growth data\ndata_gdp = pd.read_stata(GITHUB_DATA_URL + 'AED_REALGDPPC.DTA')\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"HAC Standard Errors for Time Series Data\")\nprint(\"=\" * 70)\n\nprint(\"\\nGDP Growth Data Summary:\")\nprint(data_gdp['growth'].describe())\n\n# Mean of growth\nmean_growth = data_gdp['growth'].mean()\nprint(f\"\\nMean growth rate: {mean_growth:.6f}\")\n\n\n======================================================================\nHAC Standard Errors for Time Series Data\n======================================================================\n\nGDP Growth Data Summary:\ncount    241.000000\nmean       1.990456\nstd        2.178097\nmin       -4.772172\n25%        0.892417\n50%        2.089633\n75%        3.314238\nmax        7.630545\nName: growth, dtype: float64\n\nMean growth rate: 1.990456\n\n\n\n# Autocorrelation analysis\nprint(\"\\nAutocorrelations at multiple lags:\")\nacf_values = acf(data_gdp['growth'], nlags=5, fft=False)\nfor i in range(6):\n    print(f\"  Lag {i}: {acf_values[i]:.6f}\")\n\nprint(\"\\nInterpretation:\")\nprint(\"  - Lag 0 correlation is always 1.0 (correlation with itself)\")\nprint(\"  - Positive lag 1 correlation suggests persistence\")\nprint(\"  - Autocorrelation decays with lag length\")\n\n\nAutocorrelations at multiple lags:\n  Lag 0: nan\n  Lag 1: nan\n  Lag 2: nan\n  Lag 3: nan\n  Lag 4: nan\n  Lag 5: nan\n\nInterpretation:\n  - Lag 0 correlation is always 1.0 (correlation with itself)\n  - Positive lag 1 correlation suggests persistence\n  - Autocorrelation decays with lag length\n\n\n\n# Correlogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_acf(data_gdp['growth'], lags=10, ax=ax, alpha=0.05)\nax.set_xlabel('Lag', fontsize=12)\nax.set_ylabel('Autocorrelation', fontsize=12)\nax.set_title('Figure 12.1: Correlogram of GDP Growth', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"The correlogram shows autocorrelation at various lags.\")\nprint(\"Blue shaded area = 95% confidence bands under null of no autocorrelation.\")\n\n\n\n\n\n\n\n\nThe correlogram shows autocorrelation at various lags.\nBlue shaded area = 95% confidence bands under null of no autocorrelation.\n\n\n\nKey Concept 12.2: HAC Standard Errors for Time Series\nIn time series data, errors are often autocorrelated — today’s shock persists into tomorrow. HAC (heteroskedasticity and autocorrelation consistent) standard errors, also called Newey-West SEs, account for both heteroskedasticity and autocorrelation. The lag length \\(m\\) must be specified; a common rule of thumb is \\(m = 0.75 \\times T^{1/3}\\).",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#interpreting-hac-standard-errors",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#interpreting-hac-standard-errors",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Interpreting HAC Standard Errors",
    "text": "Interpreting HAC Standard Errors\nWhat the Results Tell Us:\nComparing the three standard error estimates for the mean growth rate:\n\nDefault SE (assumes no autocorrelation):\n\n\nSmallest standard error\nAssumes errors are independent over time\nUnderestimates uncertainty when autocorrelation exists\n\n\nHAC with lag 0 (het-robust only):\n\n\nAccounts for heteroskedasticity but not autocorrelation\nOften similar to default in time series\nStill underestimates uncertainty if autocorrelation present\n\n\nHAC with lag 5 (Newey-West):\n\n\nAccounts for both heteroskedasticity AND autocorrelation\nLarger SE reflects true uncertainty\nMore conservative but valid inference\n\nWhy is HAC SE larger?\nAutocorrelation creates information overlap between observations: - If growth today predicts growth tomorrow, consecutive observations aren’t fully independent - We have less effective information than the sample size suggests - Standard errors must increase to reflect this\nPractical guidance:\n\nFor time series data, always use HAC SEs\nLag length choice: Rule of thumb = 0.75 × T^(1/3)\nFor T=100: m ≈ 3-4 lags\nFor T=200: m ≈ 4-5 lags\nErr on the side of more lags (inference remains valid)\nCheck sensitivity to lag length\n\nThe cost of ignoring autocorrelation: - Overconfident inference (SEs too small) - Spurious significance (false discoveries) - Invalid hypothesis tests\nHaving established how to conduct valid inference with robust standard errors, we now turn to prediction — estimating outcomes for specific values.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "12.3: Prediction",
    "text": "12.3: Prediction\nPrediction is a core application of regression, but there’s a crucial distinction:\n1. Predicting the conditional mean \\(E[y | x^*]\\)\n\nAverage outcome for given \\(x^*\\)\nMore precise (smaller standard error)\nUsed for policy analysis, average effects\n\n2. Predicting an actual value \\(y | x^*\\)\n\nIndividual outcome including random error\nLess precise (larger standard error)\nUsed for forecasting individual cases\n\nKey formulas:\nConditional mean: \\[E[y | x^*] = \\beta_1 + \\beta_2 x_2^* + \\cdots + \\beta_k x_k^*\\]\nActual value: \\[y | x^* = \\beta_1 + \\beta_2 x_2^* + \\cdots + \\beta_k x_k^* + u^*\\]\nStandard errors:\nFor conditional mean (bivariate case): \\[se(\\hat{y}_{cm}) = s_e \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}\\]\nFor actual value (bivariate case): \\[se(\\hat{y}_f) = s_e \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}\\]\nNote the “1 +” term for actual values - this reflects the irreducible uncertainty from \\(u^*\\).\n\nprint(\"=\" * 70)\nprint(\"12.3 PREDICTION\")\nprint(\"=\" * 70)\n\n# Simple regression: price on size\nmodel_simple = ols('price ~ size', data=data_house).fit()\n\nprint(\"\\nSimple regression: price = β₀ + β₁·size + u\")\nprint(f\"  β₀ (Intercept): ${model_simple.params['Intercept']:.2f}\")\nprint(f\"  β₁ (Size): ${model_simple.params['size']:.4f}\")\nprint(f\"  R²: {model_simple.rsquared:.4f}\")\nprint(f\"  Root MSE (σ̂): ${np.sqrt(model_simple.mse_resid):.2f}\")\n\n======================================================================\n12.3 PREDICTION\n======================================================================\n\nSimple regression: price = β₀ + β₁·size + u\n  β₀ (Intercept): $115017.28\n  β₁ (Size): $73.7710\n  R²: 0.6175\n  Root MSE (σ̂): $23550.66\n\n\n\nKey Concept 12.3: Predicting Conditional Means vs. Individual Outcomes\nPredicting the average outcome \\(E[y|x^*]\\) is more precise than predicting an individual \\(y|x^*\\). The forecast variance equals the conditional mean variance plus \\(\\text{Var}(u^*)\\): \\(\\text{Var}(\\hat{y}_f) = \\text{Var}(\\hat{y}_{cm}) + \\sigma^2\\). As \\(n \\to \\infty\\), the conditional mean SE shrinks to zero, but the forecast SE remains at least \\(s_e\\) — a fundamental limit on individual predictions.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#why-are-prediction-intervals-so-much-wider",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#why-are-prediction-intervals-so-much-wider",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Why Are Prediction Intervals So Much Wider?",
    "text": "Why Are Prediction Intervals So Much Wider?\nThe Fundamental Difference:\nLooking at the two panels, you’ll notice the prediction interval (blue) is dramatically wider than the confidence interval (red). This isn’t a mistake—it reflects a fundamental distinction in what we’re predicting.\nConfidence Interval for E[Y|X] (Red): - Predicts the average price for all 2000 sq ft houses - Uncertainty comes only from estimation error in β̂ - As sample size increases (n → ∞), this interval shrinks to zero - Formula includes: 1/n term (goes to 0 as n grows)\nPrediction Interval for Y (Blue): - Predicts an individual house price - Uncertainty comes from: 1. Estimation error in β̂ (same as CI) 2. Irreducible randomness in the individual outcome (u*) - Even with perfect knowledge of β, individual predictions remain uncertain - Formula includes: “1 +” term (never goes away)\nIntuitive Example:\nImagine predicting height from age: - Conditional mean: Average height of all 10-year-olds = 140 cm - We can estimate this average very precisely - CI might be [139, 141] cm - Actual value: A specific 10-year-old’s height - Could be anywhere from 120 to 160 cm - PI might be [125, 155] cm - Even knowing the average perfectly doesn’t eliminate individual variation\nMathematical Insight:\n\\[se(\\hat{y}_f) = \\sqrt{s_e^2 + se(\\hat{y}_{cm})^2}\\]\n\nFirst term (s_e²): Irreducible error variance—dominates the formula\nSecond term: Estimation uncertainty—becomes negligible with large samples\nResult: PI width ≈ 2 × 1.96 × s_e ≈ 4 × RMSE\n\nPractical Implications:\n\nDon’t confuse the two: Predicting averages is much more precise than predicting individuals\nPolicy vs. forecasting:\n\n\nPolicy analysis (average effects) → Use confidence intervals\nIndividual forecasting (who will default?) → Use prediction intervals\n\n\nCommunicating uncertainty: Always show prediction intervals for individual forecasts\nLimits of prediction: No amount of data eliminates individual-level uncertainty",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#visualization-confidence-vs.-prediction-intervals",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#visualization-confidence-vs.-prediction-intervals",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Visualization: Confidence vs. Prediction Intervals",
    "text": "Visualization: Confidence vs. Prediction Intervals\nThis figure illustrates the fundamental difference between:\n\nConfidence interval for conditional mean (narrower, red)\nPrediction interval for actual value (wider, blue)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#understanding-the-numbers-a-concrete-example",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#understanding-the-numbers-a-concrete-example",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Understanding the Numbers: A Concrete Example",
    "text": "Understanding the Numbers: A Concrete Example\nInterpreting the Results for a 2000 sq ft House:\nLooking at our predictions, several patterns emerge:\n1. Point Prediction: - Predicted price ≈ $280,000 (approximately) - This is our best single guess - Same for both conditional mean and actual value\n2. Confidence Interval for E[Y|X=2000]: - Relatively narrow (e.g., $250k - $310k) - Tells us: “We’re 95% confident the average price of all 2000 sq ft houses is in this range” - Precise because we’re estimating a population average - Useful for: Understanding market valuations, setting pricing policies\n3. Prediction Interval for Y: - Much wider (e.g., $180k - $380k) - Tells us: “We’re 95% confident this specific house will sell in this range” - Wide because individual houses vary considerably - Useful for: Setting listing ranges, individual appraisals\nThe Ratio is Revealing:\nNotice that the PI is approximately 3-4 times wider than the CI. This ratio tells us: - Most variation is between houses (individual heterogeneity) - Relatively little variation is estimation uncertainty - Adding more data would shrink the CI but barely affect the PI\nStatistical vs. Economic Significance:\n\nCI width = Statistical precision (how well we know β)\nPI width = Economic uncertainty (inherent market volatility)\nIn this example: Good statistical precision, but still substantial economic uncertainty\n\nPractical Takeaway:\nIf you’re a real estate agent: - Don’t promise a precise price ($280k) - Do provide a realistic range ($180k - $380k) - Explain that individual houses vary, even controlling for size - Use the confidence interval to discuss average market values",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#deconstructing-the-standard-error-formulas",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#deconstructing-the-standard-error-formulas",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Deconstructing the Standard Error Formulas",
    "text": "Deconstructing the Standard Error Formulas\nUnderstanding Where the “1 +” Comes From:\nThe manual calculations reveal the mathematical structure of prediction uncertainty:\nFor Conditional Mean: \\[se(\\hat{y}_{cm}) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}\\]\n\nFirst term (1/n): Decreases with sample size—more data reduces uncertainty\nSecond term: Distance from mean matters—extrapolation is risky\nPrediction at \\(x^* = \\bar{x}\\) (sample mean) is most precise\nPrediction far from \\(\\bar{x}\\) is less precise\nBoth terms → 0 as n → ∞ (perfect knowledge of E[Y|X])\n\nFor Actual Value: \\[se(\\hat{y}_f) = \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}\\]\n\nThe critical “1 +”: Represents \\(Var[u^*]\\), the future error term\nThis term never disappears, even with infinite data\nDominates the formula in moderate to large samples\n\nNumerical Insight:\nIn our example: - \\(\\hat{\\sigma}\\) (RMSE) ≈ $90k (this is the irreducible uncertainty) - \\((1/n)\\) term ≈ 0.034 (small with n=29) - Distance term varies with prediction point\nFor predictions near the mean: - \\(se(\\hat{y}_{cm})\\) ≈ $90k × √0.034 ≈ $17k (mainly from 1/n) - \\(se(\\hat{y}_f)\\) ≈ $90k × √1.034 ≈ $92k (mainly from the “1”)\nThe “1 +” term is why: - Prediction intervals don’t shrink much with more data - Individual predictions remain uncertain even with perfect models - \\(se(\\hat{y}_f) \\approx \\hat{\\sigma}\\) in large samples\nGeometric Interpretation:\nThe funnel shape in prediction plots comes from the distance term: - Narrow near \\(\\bar{x}\\) (center of data) - Wider at extremes (extrapolation region) - But even at the center, PI is wide due to the “1” term\nPractical Lesson:\nWhen presenting predictions: 1. Always acknowledge the “1 +” uncertainty 2. Be most confident about predictions near the data center 3. Be especially cautious about extrapolation (predictions outside the data range) 4. Understand that better models reduce estimation error but not irreducible randomness",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction-at-specific-values",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction-at-specific-values",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Prediction at Specific Values",
    "text": "Prediction at Specific Values\nLet’s predict house price for a 2000 square foot house.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#manual-calculation-of-standard-errors",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#manual-calculation-of-standard-errors",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Manual Calculation of Standard Errors",
    "text": "Manual Calculation of Standard Errors\nLet’s manually calculate the standard errors to understand the formulas.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction-with-multiple-regression",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction-with-multiple-regression",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Prediction with Multiple Regression",
    "text": "Prediction with Multiple Regression\nNow let’s predict using the full multiple regression model.\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Prediction for Multiple Regression\")\nprint(\"=\" * 70)\n\nmodel_multi = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n                  data=data_house).fit()\n\n# Predict for specific values\nnew_house = pd.DataFrame({\n    'size': [2000],\n    'bedrooms': [4],\n    'bathrooms': [2],\n    'lotsize': [2],\n    'age': [40],\n    'monthsold': [6]\n})\n\npred_multi = model_multi.get_prediction(sm.add_constant(new_house))\n\nprint(\"\\nPrediction for:\")\nprint(\"  size=2000, bedrooms=4, bathrooms=2, lotsize=2, age=40, monthsold=6\")\nprint(f\"\\nPredicted price: ${pred_multi.predicted_mean[0]:.2f}\")\n\n# Confidence interval for conditional mean\nci_mean_multi = pred_multi.conf_int(alpha=0.05)\nprint(f\"\\n95% CI for E[Y|X]:\")\nprint(f\"  [${ci_mean_multi[0, 0]:.2f}, ${ci_mean_multi[0, 1]:.2f}]\")\nprint(f\"  SE: ${pred_multi.se_mean[0]:.2f}\")\n\n# Prediction interval for actual value\ns_e_multi = np.sqrt(model_multi.mse_resid)\ns_y_cm_multi = pred_multi.se_mean[0]\ns_y_f_multi = np.sqrt(s_e_multi**2 + s_y_cm_multi**2)\n\nn_multi = len(data_house)\nk_multi = len(model_multi.params)\ntcrit_multi = stats.t.ppf(0.975, n_multi - k_multi)\n\npi_lower = pred_multi.predicted_mean[0] - tcrit_multi * s_y_f_multi\npi_upper = pred_multi.predicted_mean[0] + tcrit_multi * s_y_f_multi\n\nprint(f\"\\n95% PI for Y:\")\nprint(f\"  [${pi_lower:.2f}, ${pi_upper:.2f}]\")\nprint(f\"  SE: ${s_y_f_multi:.2f}\")\n\nprint(\"\\nMultiple regression provides more precise conditional mean predictions.\")\nprint(\"But individual predictions still have large uncertainty.\")\n\n\n======================================================================\nPrediction for Multiple Regression\n======================================================================\n\nPrediction for:\n  size=2000, bedrooms=4, bathrooms=2, lotsize=2, age=40, monthsold=6\n\nPredicted price: $257690.80\n\n95% CI for E[Y|X]:\n  [$244234.97, $271146.63]\n  SE: $6488.26\n\n95% PI for Y:\n  [$204255.32, $311126.28]\n  SE: $25766.03\n\nMultiple regression provides more precise conditional mean predictions.\nBut individual predictions still have large uncertainty.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction-with-robust-standard-errors",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#prediction-with-robust-standard-errors",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Prediction with Robust Standard Errors",
    "text": "Prediction with Robust Standard Errors\nWhen heteroskedasticity is present, we should use robust standard errors for prediction intervals too.\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Prediction with Heteroskedastic-Robust SEs\")\nprint(\"=\" * 70)\n\nmodel_multi_robust = ols('price ~ size + bedrooms + bathrooms + lotsize + age + monthsold',\n                         data=data_house).fit(cov_type='HC1')\n\npred_multi_robust = model_multi_robust.get_prediction(sm.add_constant(new_house))\n\nprint(f\"\\nPredicted price: ${pred_multi_robust.predicted_mean[0]:.2f}\")\n\n# Robust confidence interval for conditional mean\nci_mean_robust = pred_multi_robust.conf_int(alpha=0.05)\nprint(f\"\\nRobust 95% CI for E[Y|X]:\")\nprint(f\"  [${ci_mean_robust[0, 0]:.2f}, ${ci_mean_robust[0, 1]:.2f}]\")\nprint(f\"  Robust SE: ${pred_multi_robust.se_mean[0]:.2f}\")\n\n# Robust prediction interval\ns_y_cm_robust = pred_multi_robust.se_mean[0]\ns_y_f_robust = np.sqrt(s_e_multi**2 + s_y_cm_robust**2)\n\nprint(f\"\\nRobust 95% PI for Y:\")\nprint(f\"  Robust SE for actual value: ${s_y_f_robust:.2f}\")\n\nprint(\"\\nComparison of standard vs. robust:\")\nprint(f\"  SE (standard): ${s_y_cm_multi:.2f}\")\nprint(f\"  SE (robust): ${s_y_cm_robust:.2f}\")\nprint(f\"  Ratio: {s_y_cm_robust / s_y_cm_multi:.3f}\")\n\n\n======================================================================\nPrediction with Heteroskedastic-Robust SEs\n======================================================================\n\nPredicted price: $257690.80\n\nRobust 95% CI for E[Y|X]:\n  [$244694.26, $270687.34]\n  Robust SE: $6631.01\n\nRobust 95% PI for Y:\n  Robust SE for actual value: $25802.35\n\nComparison of standard vs. robust:\n  SE (standard): $6488.26\n  SE (robust): $6631.01\n  Ratio: 1.022\n\n\n\nKey Concept 12.4: Why Individual Forecasts Are Imprecise\nEven with precisely estimated coefficients, predicting an individual outcome is imprecise because the forecast must account for the unobservable error \\(u^*\\). The forecast standard error satisfies \\(se(\\hat{y}_f) \\geq s_e\\) — it is at least as large as the regression’s standard error. This means 95% prediction intervals are at least \\(\\pm 1.96 \\times s_e\\) wide, regardless of how much data we have.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#nonrepresentative-samples",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#nonrepresentative-samples",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "12.4: Nonrepresentative Samples",
    "text": "12.4: Nonrepresentative Samples\nSample selection can bias OLS estimates:\nCase 1: Selection on regressors (X)\n\nExample: Oversample high-income households\nOLS remains unbiased if we include income as a control\nSolution: Include selection variables as controls\n\nCase 2: Selection on outcome (Y)\n\nExample: Survey excludes very high earners\nOLS estimates are biased for population parameters\nSolution: Sample weights, Heckman correction, or other selection models\n\nSurvey weights:\n\nMany surveys provide weights to adjust for nonrepresentativeness\nUse weighted least squares (WLS) instead of OLS\nWeight formula: \\(w_i = 1 / P(\\text{selected})\\)\n\nKey insight: Always check whether your sample is representative of your target population!",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#bootstrap-confidence-intervals-an-alternative-approach",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#bootstrap-confidence-intervals-an-alternative-approach",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Bootstrap Confidence Intervals: An Alternative Approach",
    "text": "Bootstrap Confidence Intervals: An Alternative Approach\nWhat is Bootstrap?\nThe bootstrap is a computational method that: 1. Resamples your data many times (e.g., 1000 replications) 2. Re-estimates the model for each resample 3. Uses the distribution of estimates to build confidence intervals\nHow it works:\nFor each bootstrap replication b = 1, …, B: 1. Draw n observations with replacement from original data 2. Estimate regression: \\(\\hat{\\beta}_j^{(b)}\\) 3. Store the coefficient estimate\nAfter B replications: - You have B estimates: \\(\\{\\hat{\\beta}_j^{(1)}, \\hat{\\beta}_j^{(2)}, ..., \\hat{\\beta}_j^{(B)}\\}\\) - These form an empirical distribution\nPercentile Method CI: - 95% CI = [2.5th percentile, 97.5th percentile] of bootstrap distribution - Example: If you have 1000 estimates, use the 25th and 975th largest values\nAdvantages of Bootstrap:\n\nNo distributional assumptions: Don’t need to assume normality\nWorks for complex statistics: Medians, ratios, quantiles, etc.\nBetter small-sample coverage: Often more accurate than asymptotic formulas\nFlexibility: Can bootstrap residuals, observations, or both\nVisual understanding: See the actual sampling distribution\n\nWhen to use Bootstrap:\n\nSmall samples (n &lt; 30-50)\nNon-standard statistics (beyond means and coefficients)\nSkewed or heavy-tailed distributions\nChecking robustness of standard inference\nWhen asymptotic formulas are complex or unavailable\n\nLimitations:\n\nComputationally intensive (need B = 1000+ replications)\nRequires careful implementation (stratification, cluster bootstrap)\nMay fail with very small samples (n &lt; 10)\nAssumes sample is representative of population\n\nBootstrap vs. Robust SEs:\nBoth address uncertainty, but differently: - Robust SEs: Analytical correction for heteroskedasticity/autocorrelation - Bootstrap: Computational approach using resampling\nOften used together: Bootstrap with robust methods!\nPractical Implementation Tips:\n\nUse B ≥ 1000 for confidence intervals\nSet random seed for reproducibility\nFor time series: Use block bootstrap (resample blocks, not individuals)\nFor panel data: Use cluster bootstrap (resample clusters)\nCheck convergence: Results shouldn’t change much with different seeds\n\n\nprint(\"=\" * 70)\nprint(\"12.4 NONREPRESENTATIVE SAMPLES\")\nprint(\"=\" * 70)\n\nprint(\"\\nConceptual discussion - no computation required\")\nprint(\"\\nKey points:\")\nprint(\"  1. Sample selection can lead to biased estimates\")\nprint(\"  2. Selection on regressors: Include selection variables as controls\")\nprint(\"  3. Selection on outcome: Use sample weights or selection models\")\nprint(\"  4. Always verify sample representativeness\")\n\nprint(\"\\nExample applications:\")\nprint(\"  - Wage surveys that exclude unemployed workers\")\nprint(\"  - Health studies with voluntary participation\")\nprint(\"  - Education data from selective schools\")\nprint(\"  - Financial data excluding bankrupt firms\")\n\n======================================================================\n12.4 NONREPRESENTATIVE SAMPLES\n======================================================================\n\nConceptual discussion - no computation required\n\nKey points:\n  1. Sample selection can lead to biased estimates\n  2. Selection on regressors: Include selection variables as controls\n  3. Selection on outcome: Use sample weights or selection models\n  4. Always verify sample representativeness\n\nExample applications:\n  - Wage surveys that exclude unemployed workers\n  - Health studies with voluntary participation\n  - Education data from selective schools\n  - Financial data excluding bankrupt firms\n\n\n\nKey Concept 12.5: Sample Selection Bias\nIf the sample is not representative of the population, OLS estimates may be biased. Selection on the dependent variable \\(Y\\) (e.g., studying only high earners) is particularly problematic. Selection on the regressors \\(X\\) (e.g., studying only college graduates) is less harmful because it reduces precision but doesn’t necessarily bias coefficient estimates.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#the-type-i-vs.-type-ii-error-tradeoff",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#the-type-i-vs.-type-ii-error-tradeoff",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "The Type I vs. Type II Error Tradeoff",
    "text": "The Type I vs. Type II Error Tradeoff\nUnderstanding the Table:\nThe 2×2 decision table reveals a fundamental tradeoff in hypothesis testing:\n\n\n\nDecision\nH₀ True\nH₀ False\n\n\n\n\nReject H₀\nType I error (α)\nCorrect (Power)\n\n\nDon’t reject\nCorrect (1-α)\nType II error (β)\n\n\n\nType I Error (False Positive): - Reject a true null hypothesis - Probability = significance level α (we control this) - Example: Conclude a drug works when it doesn’t - “Seeing patterns in noise”\nType II Error (False Negative): - Fail to reject a false null hypothesis - Probability = β (harder to control) - Example: Miss a real drug effect - “Missing real signals”\nThe Fundamental Tradeoff:\nIf we make the test stricter (lower α): - Fewer false positives (Type I errors) - More false negatives (Type II errors) - Lower power (harder to detect real effects)\nIf we make the test looser (higher α): - Higher power (easier to detect real effects) - More false positives (Type I errors)\nStatistical Power = 1 - β: - Probability of correctly rejecting false H₀ - “Sensitivity” of the test - Want power ≥ 0.80 (80% chance of detecting real effect)\nWhat Affects Power?\n\nSample size (n): Larger n → Higher power\nEffect size (β): Larger true effect → Higher power\nSignificance level (α): Higher α → Higher power (but more Type I errors)\nNoise level (σ): Lower σ → Higher power\n\nThe Power Function:\nPower depends on the true parameter value: - At β = 0 (H₀ true): Power = α (just Type I error rate) - As |β| increases: Power increases - For very large |β|: Power → 1 (almost certain detection)\nMultiple Testing Problem:\nTesting k hypotheses at α = 0.05: - Expected false positives = 0.05 × k - Test 20 hypotheses → expect 1 false positive even if all H₀ are true!\nSolutions: 1. Bonferroni correction: Use α/k for each test (conservative) 2. False Discovery Rate (FDR): Control proportion of false positives 3. Pre-registration: Specify primary hypotheses before seeing data 4. Replication: Confirm findings in independent samples\nNow that we understand how sample selection affects estimates, let’s consider what happens when we seek the most efficient estimator.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#best-estimation-methods",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#best-estimation-methods",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "12.5: Best Estimation Methods",
    "text": "12.5: Best Estimation Methods\nWhen are OLS estimators “best”?\nUnder classical assumptions 1-4, OLS is BLUE (Best Linear Unbiased Estimator) by the Gauss-Markov Theorem.\nWhen assumptions fail:\n1. Heteroskedasticity: \\(Var[u_i | X] = \\sigma_i^2\\) (varies)\n\nOLS remains unbiased but inefficient\nFeasible GLS (FGLS) or Weighted Least Squares (WLS) more efficient\nWeight observations inversely to error variance: \\(w_i = 1/\\sigma_i\\)\n\n2. Autocorrelation: \\(Cov[u_t, u_{t-s}] \\neq 0\\)\n\nOLS remains unbiased but inefficient\nFGLS with AR errors more efficient\nModel error structure: \\(u_t = \\rho u_{t-1} + \\epsilon_t\\)\n\nPractical advice:\n\nMost applied work uses OLS with robust SEs\nEfficiency gains from GLS/FGLS often modest\nMisspecifying error structure can make things worse\nException: Panel data methods explicitly model error components",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#reading-the-power-curve-what-it-tells-us",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#reading-the-power-curve-what-it-tells-us",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Reading the Power Curve: What It Tells Us",
    "text": "Reading the Power Curve: What It Tells Us\nInterpreting Figure 12.3:\nThe power function shows how test power varies with the true coefficient value. Here’s what each feature means:\nKey Features of the Curve:\n\nAt β = 0 (vertical gray line):\n\n\nPower = α = 0.05\nThis is the Type I error rate\nWhen H₀ is true, we reject 5% of the time (false positives)\n\n\nAs |β| increases (moving away from 0):\n\n\nPower increases rapidly\nLarger effects are easier to detect\nCurve approaches 1.0 (certain detection)\n\n\nSymmetry around zero:\n\n\nPower is same for β = 30 and β = -30\nTwo-sided test treats positive and negative effects equally\nOne-sided tests would have asymmetric power\n\n\nThe 0.80 threshold (green dashed line):\n\n\nStandard target: 80% power\nMeans 20% chance of Type II error (β = 0.20)\nIn this example: Need |β| ≈ 30 to achieve 80% power\n\nWhat This Means for Study Design:\nGiven the parameters (n=30, SE=15, α=0.05):\n\nSmall effects (|β| &lt; 15):\nPower &lt; 50%\nMore likely to miss the effect than detect it\nStudy is underpowered\nMedium effects (|β| ≈ 30):\nPower ≈ 80%\nGood chance of detection\nStandard benchmark for adequate power\nLarge effects (|β| &gt; 45):\nPower &gt; 95%\nAlmost certain detection\nStudy is well-powered\n\nSample Size Implications:\nTo detect smaller effects, you need larger samples: - Double the sample (n=60) → Can detect smaller effects with same power - Power roughly proportional to √n - To halve minimum detectable effect, need 4× the sample size\nThe Power-Sample Size Relationship:\nFor a given effect size β: - Power increases with √n - To go from 50% to 80% power: Need ≈ 2× the sample - To go from 80% to 95% power: Need ≈ 2× the sample again\nPractical Applications:\n\nPre-study planning:\n\n\nSpecify minimum effect of interest\nCalculate required sample size for 80% power\nAvoid underpowered studies\n\n\nPost-study interpretation:\n\n\nNon-significant result with low power: Inconclusive (not evidence of no effect)\nNon-significant result with high power: Evidence against large effects\nSignificant result: Good, but consider magnitude and practical significance\n\n\nPublication decisions:\n\n\nUnderpowered studies contribute to publication bias\nMeta-analyses should weight by precision and power\nReplication studies should be well-powered\n\nCommon Mistakes to Avoid:\n\nTreating non-significant results as “proof of no effect”\n\n\nNon-significance in underpowered study is uninformative\n\n\nConducting multiple underpowered studies instead of one well-powered study\n\n\nWastes resources and leads to false negatives\n\n\nPost-hoc power analysis\n\n\nDon’t calculate power after seeing results (circular reasoning)\nDo it before data collection\n\nThe Bottom Line:\nThis power curve illustrates a fundamental truth: - Smaller effects require larger samples to detect - With n=30 and SE=15, we can reliably detect effects of |β| ≥ 30 - For smaller effects, we’d need more data or reduced noise (lower σ)\n\nprint(\"=\" * 70)\nprint(\"12.5 BEST ESTIMATION METHODS\")\nprint(\"=\" * 70)\n\nprint(\"\\nKey concepts:\")\nprint(\"\\n1. Gauss-Markov Theorem:\")\nprint(\"   - Under assumptions 1-4, OLS is BLUE\")\nprint(\"   - BLUE = Best Linear Unbiased Estimator\")\nprint(\"   - 'Best' = minimum variance among linear unbiased estimators\")\n\nprint(\"\\n2. When assumptions fail:\")\nprint(\"   - Heteroskedasticity → Weighted Least Squares (WLS)\")\nprint(\"   - Autocorrelation → GLS with AR errors\")\nprint(\"   - Both → Feasible GLS (FGLS)\")\n\nprint(\"\\n3. Practical considerations:\")\nprint(\"   - Efficiency gains often modest in practice\")\nprint(\"   - Misspecification of error structure can worsen estimates\")\nprint(\"   - Most studies use OLS + robust SEs (simpler, more robust)\")\nprint(\"   - Exception: Panel data methods model error components explicitly\")\n\nprint(\"\\n4. Maximum Likelihood:\")\nprint(\"   - If error distribution fully specified (e.g., normal)\")\nprint(\"   - MLE can be more efficient than OLS\")\nprint(\"   - Under normality, MLE = OLS for linear regression\")\n\n======================================================================\n12.5 BEST ESTIMATION METHODS\n======================================================================\n\nKey concepts:\n\n1. Gauss-Markov Theorem:\n   - Under assumptions 1-4, OLS is BLUE\n   - BLUE = Best Linear Unbiased Estimator\n   - 'Best' = minimum variance among linear unbiased estimators\n\n2. When assumptions fail:\n   - Heteroskedasticity → Weighted Least Squares (WLS)\n   - Autocorrelation → GLS with AR errors\n   - Both → Feasible GLS (FGLS)\n\n3. Practical considerations:\n   - Efficiency gains often modest in practice\n   - Misspecification of error structure can worsen estimates\n   - Most studies use OLS + robust SEs (simpler, more robust)\n   - Exception: Panel data methods model error components explicitly\n\n4. Maximum Likelihood:\n   - If error distribution fully specified (e.g., normal)\n   - MLE can be more efficient than OLS\n   - Under normality, MLE = OLS for linear regression\n\n\n\nKey Concept 12.6: Feasible Generalized Least Squares\nWhen error variance is not constant (heteroskedasticity) or errors are correlated (autocorrelation), OLS remains unbiased but is no longer the most efficient estimator. Feasible GLS (FGLS) models the error structure and can achieve lower variance. However, FGLS requires correctly specifying the error structure — in practice, OLS with robust SEs is preferred for its simplicity and robustness to misspecification.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#best-confidence-intervals",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#best-confidence-intervals",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "12.6: Best Confidence Intervals",
    "text": "12.6: Best Confidence Intervals\nWhat makes a confidence interval “best”?\nA 95% CI is “best” if it:\n\nHas correct coverage: Contains true parameter 95% of the time\nHas minimum width among all CIs with correct coverage\n\nStandard approach: \\(\\hat{\\beta}_j \\pm t_{n-k, \\alpha/2} \\times se(\\hat{\\beta}_j)\\)\n\nWidth determined by \\(se(\\hat{\\beta}_j)\\)\nShortest CI comes from most efficient estimator\n\nAlternative approaches:\n1. Bootstrap confidence intervals\n\nResample data many times (e.g., 1000 replications)\nRe-estimate model for each resample\nUse distribution of bootstrap estimates\nPercentile method: 2.5th and 97.5th percentiles\nAdvantages: No distributional assumptions, works for complex statistics\n\n2. Bayesian credible intervals\n\nBased on posterior distribution\nDirect probability interpretation\nIncorporates prior information\n\nWhen assumptions fail:\n\nUse robust SEs → wider but valid intervals\nBootstrap → more accurate coverage in small samples\nAsymptotic approximations may be poor in small samples\n\nHaving discussed the best confidence intervals, we now examine what makes a hypothesis test optimal — balancing Type I and Type II errors.\n\nKey Concept 12.7: Bootstrap Confidence Intervals\nThe bootstrap resamples the original data (with replacement) many times to estimate the sampling distribution of a statistic. Bootstrap CIs don’t rely on normality or large-sample approximations, making them especially useful with small samples, skewed distributions, or non-standard estimators where analytical formulas aren’t available.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#best-tests",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#best-tests",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "12.7: Best Tests",
    "text": "12.7: Best Tests\nType I and Type II errors:\n\n\n\nDecision\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nReject \\(H_0\\)\nType I error (α)\nCorrect\n\n\nDon’t reject\nCorrect\nType II error (β)\n\n\n\nType I error (false positive):\n\nReject \\(H_0\\) when it’s true\nProbability = significance level α (e.g., 0.05)\nWe control this directly\n\nType II error (false negative):\n\nFail to reject \\(H_0\\) when it’s false\nProbability = β\nHarder to control\n\nTest power = 1 - β\n\nProbability of correctly rejecting false \\(H_0\\)\nHigher power is better\n\nTrade-off:\n\nDecreasing α (stricter test) → increases β (lower power)\nSolution: Fix α, maximize power\n\nMost powerful test:\n\nAmong all tests with size α, has highest power\nFor linear regression: Use most efficient estimator\n\nThe Trinity of Tests (asymptotically equivalent):\n\nWald test: Based on unrestricted estimates\nLikelihood Ratio (LR) test: Compares likelihoods\nLagrange Multiplier (LM) test: Based on restricted estimates\n\nMultiple testing:\n\nTesting many hypotheses inflates Type I error\nSolutions: Bonferroni correction, FDR control",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#illustration-power-of-a-test",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#illustration-power-of-a-test",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Illustration: Power of a Test",
    "text": "Illustration: Power of a Test\nLet’s visualize how test power depends on the true effect size.\n\nKey Concept 12.8: Type I and Type II Errors\nType I error (false positive) means rejecting a true \\(H_0\\); its probability equals the significance level \\(\\alpha\\). Type II error (false negative) means failing to reject a false \\(H_0\\). Power = \\(1 - P(\\text{Type II})\\) measures the ability to detect true effects. The most powerful test for a given size uses the most precise estimator — another reason efficient estimation matters beyond point estimates.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#key-takeaways",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#key-takeaways",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nRobust Standard Errors: - When error variance is non-constant, default SEs are invalid — use heteroskedastic-robust (HC1) SEs for cross-sectional data - With grouped observations, use cluster-robust SEs with \\(G-1\\) degrees of freedom (not \\(N-k\\)) - For time series with autocorrelated errors, use HAC (Newey-West) SEs with lag length \\(m \\approx 0.75 \\times T^{1/3}\\) - Coefficient estimates are unchanged; only SEs, \\(t\\)-statistics, and CIs change\nPrediction: - Predicting the conditional mean \\(E[y|x^*]\\) is more precise than predicting an individual outcome \\(y|x^*\\) - Forecast variance = conditional mean variance + \\(\\text{Var}(u^*)\\), so prediction intervals are always wider - Even with precise coefficients, individual forecasts are imprecise because we cannot predict \\(u^*\\) - Policy decisions should be based on average outcomes (precise) rather than individual predictions (imprecise)\nNonrepresentative Samples: - Sample selection on \\(Y\\) can bias OLS estimates; selection on \\(X\\) is less harmful - Survey weights can adjust for known selection, but unknown selection remains problematic\nBest Estimation: - Under correct assumptions, OLS is BLUE (Gauss-Markov); when assumptions fail, FGLS is more efficient - In practice, most studies use OLS with robust SEs — accepting a small efficiency loss for simplicity\nBest Confidence Intervals: - Bootstrap methods resample the data to estimate the sampling distribution without relying on normality - Particularly useful with small samples or non-normal errors\nBest Tests: - Type I error = false positive (rejecting true \\(H_0\\)); Type II error = false negative - Power = \\(1 - P(\\text{Type II})\\); the most powerful test uses the most precise estimator - Higher power comes from larger samples, more variation in regressors, and lower noise\nPython tools used: statsmodels (OLS, HC1, get_prediction()), scipy.stats (distributions), matplotlib/seaborn (correlograms, prediction plots)\nNext steps: Chapter 13 introduces dummy variables and indicator variables — extending regression to handle qualitative explanatory variables.\nCongratulations on completing Chapter 12! You now understand advanced inference methods for handling real-world data challenges.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#practice-exercises",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#practice-exercises",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nTest your understanding of advanced inference topics.\n\nExercise 1: Choosing Standard Errors\nFor each scenario, identify the appropriate type of standard errors:\n\nCross-sectional survey of 500 households with varying income levels.\nPanel data on 50 schools over 10 years, where student outcomes within a school are correlated.\nMonthly GDP growth data for 20 years, where this quarter’s shock affects next quarter.\nA randomized experiment with 200 independent observations and constant error variance.\n\n\nExercise 2: Point Prediction\nA fitted regression model gives \\(\\widehat{y} = 10 + 2x_2 + 3x_3\\) with \\(n = 200\\) and \\(s_e = 2.0\\).\n\nPredict \\(y\\) when \\(x_2 = 5\\) and \\(x_3 = 3\\).\nIs this a prediction of the conditional mean or an individual outcome?\n\n\nExercise 3: Conditional Mean CI\nUsing the model from Exercise 2, suppose \\(se(\\widehat{y}_{cm}) = 0.8\\) at \\(x_2 = 5, x_3 = 3\\).\n\nConstruct an approximate 95% confidence interval for \\(E[y|x_2=5, x_3=3]\\).\nHow would this interval change if the sample size doubled (assume SE shrinks by \\(\\sqrt{2}\\))?\n\n\nExercise 4: Individual Forecast CI\nContinuing from Exercise 3, construct a 95% prediction interval for an individual \\(y\\) at \\(x_2 = 5, x_3 = 3\\).\n\nCompute \\(se(\\widehat{y}_f) = \\sqrt{se(\\widehat{y}_{cm})^2 + s_e^2}\\).\nConstruct the prediction interval.\nWhy is this interval so much wider than the confidence interval for the conditional mean?\n\n\nExercise 5: Robust vs. Default Inference\nA regression yields the following results:\n\n\n\nVariable\nCoefficient\nDefault SE\nRobust SE\n\n\n\n\n\\(x_2\\)\n5.0\n2.0\n3.5\n\n\n\\(x_3\\)\n7.0\n2.0\n1.8\n\n\n\n\nCompute the \\(t\\)-statistic for each variable using default and robust SEs.\nAt \\(\\alpha = 0.05\\), which variables are significant under each type of SE?\nWhat does the change in SEs suggest about heteroskedasticity?\n\n\nExercise 6: Type I/II Error Tradeoff\nA researcher tests \\(H_0: \\beta = 0\\) at three significance levels: \\(\\alpha = 0.01, 0.05, 0.10\\).\n\nAs \\(\\alpha\\) decreases, what happens to the probability of Type I error?\nAs \\(\\alpha\\) decreases, what happens to the probability of Type II error?\nIf it’s very costly to miss a true effect (high cost of Type II error), should you use a smaller or larger \\(\\alpha\\)? Explain.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#case-studies",
    "href": "../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html#case-studies",
    "title": "Chapter 12: Further Topics in Multiple Regression",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Robust Inference for Cross-Country Productivity\nIn this case study, you will apply robust inference methods to cross-country productivity data. You’ll compare default and robust standard errors, make predictions for specific countries, and assess how methodological choices affect conclusions about productivity determinants.\nDataset: Mendez Convergence Clubs Data\n\nSource: Mendez (2020), 108 countries, 1990-2014\nKey variables:\n\nlp — Labor productivity (GDP per worker)\nrk — Physical capital per worker\nhc — Human capital index\nregion — Geographic region (for clustering)\n\n\nResearch question: Do conclusions about productivity determinants change when using robust standard errors? How precisely can we predict productivity for a specific country?\n# Load the Mendez convergence clubs dataset\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat_2014 = dat[dat['year'] == 2014].dropna(subset=['lp', 'rk', 'hc']).copy()\ndat_2014['ln_lp'] = np.log(dat_2014['lp'])\ndat_2014['ln_rk'] = np.log(dat_2014['rk'])\nprint(f\"Cross-section sample: {len(dat_2014)} countries (year 2014)\")\n\nTask 1: Default vs. Robust Standard Errors (Guided)\nCompare default and heteroskedastic-robust standard errors.\n# Estimate model with default SEs\nmodel = ols('ln_lp ~ ln_rk + hc', data=dat_2014).fit()\nprint(\"Default SEs:\")\nprint(model.summary())\n\n# Estimate with HC1 robust SEs\nmodel_robust = model.get_robustcov_results(cov_type='HC1')\nprint(\"\\nRobust SEs:\")\nprint(model_robust.summary())\nQuestions: - How do the standard errors change? Which variables are affected most? - Do any significance conclusions change between default and robust SEs?\n\n\nTask 2: Cluster-Robust SEs by Region (Guided)\nEstimate with cluster-robust standard errors grouped by geographic region.\n# Cluster-robust SEs by region\nmodel_cluster = model.get_robustcov_results(cov_type='cluster', groups=dat_2014['region'])\nprint(\"Cluster-Robust SEs (by region):\")\nprint(model_cluster.summary())\n\n# Compare all three SE types\nprint(\"\\nSE Comparison:\")\nfor var in ['Intercept', 'ln_rk', 'hc']:\n    print(f\"  {var}: Default={model.bse[var]:.4f}, HC1={model_robust.bse[var]:.4f}, Cluster={model_cluster.bse[var]:.4f}\")\nQuestions: - Are cluster-robust SEs larger or smaller than HC1 SEs? Why? - How many clusters (regions) are there? Is this enough for reliable cluster-robust inference?\n\nKey Concept 12.9: Choosing the Right Standard Errors\nThe choice of standard errors depends on the data structure: HC1 for cross-sectional data with potential heteroskedasticity, cluster-robust when observations are grouped (e.g., countries within regions), and HAC for time series. With cross-country data, cluster-robust SEs by region account for the possibility that countries in the same region share unobserved shocks.\n\n\n\nTask 3: Predict Conditional Mean (Semi-guided)\nPredict average productivity for a country with median capital and human capital values.\n# Get median values\nmedian_ln_rk = dat_2014['ln_rk'].median()\nmedian_hc = dat_2014['hc'].median()\nprint(f\"Median ln(rk) = {median_ln_rk:.3f}, Median hc = {median_hc:.3f}\")\n\n# Predict conditional mean with CI\npred_data = pd.DataFrame({'ln_rk': [median_ln_rk], 'hc': [median_hc]})\npred = model.get_prediction(pred_data)\nprint(pred.summary_frame(alpha=0.05))\nQuestions: - What is the predicted \\(\\ln(\\text{lp})\\) for a median country? Convert back to levels. - How narrow is the 95% CI for the conditional mean?\n\n\nTask 4: Forecast Individual Country (Semi-guided)\nConstruct a prediction interval for an individual country’s productivity.\n# Get prediction with observation-level interval\npred_frame = pred.summary_frame(alpha=0.05)\nprint(\"Conditional mean CI vs. Prediction interval:\")\nprint(f\"  Mean CI:       [{pred_frame['mean_ci_lower'].values[0]:.3f}, {pred_frame['mean_ci_upper'].values[0]:.3f}]\")\nprint(f\"  Prediction PI: [{pred_frame['obs_ci_lower'].values[0]:.3f}, {pred_frame['obs_ci_upper'].values[0]:.3f}]\")\nQuestions: - How much wider is the prediction interval compared to the confidence interval? - Why can’t we predict an individual country’s productivity precisely even with good data?\n\n\nTask 5: Model Comparison with Robust Inference (Independent)\nCompare nested models using robust inference.\nYour tasks: 1. Estimate three models: (a) \\(\\ln(\\text{lp}) \\sim \\ln(\\text{rk})\\) only, (b) \\(\\ln(\\text{lp}) \\sim \\text{hc}\\) only, (c) both regressors 2. For each model, report both default and HC1 robust standard errors 3. Do the significance conclusions change between default and robust SEs for any model? 4. Compare prediction interval widths across models — does adding variables improve individual predictions?\nHint: Use model.get_robustcov_results(cov_type='HC1') for robust SEs and model.get_prediction() for predictions.\n\n\nTask 6: Policy Brief on Inference Robustness (Independent)\nWrite a 200-300 word policy brief summarizing your findings.\nYour brief should address: 1. How do conclusions about productivity determinants change with robust standard errors? 2. What is the practical difference between cluster-robust and HC1 SEs in this context? 3. How precisely can we predict productivity for a specific country vs. a group of countries? 4. What recommendations would you make about standard error choices for cross-country studies? 5. What are the limitations of these inference methods (what don’t they fix)?\n\nKey Concept 12.10: Robust Methods Don’t Fix Everything\nRobust standard errors correct for heteroskedasticity and clustering, but they don’t address omitted variable bias, reverse causality, or measurement error. A coefficient estimate with a perfectly robust SE is still biased if the model is misspecified. Robust inference ensures valid \\(p\\)-values and CIs conditional on the model being correct — it’s a necessary but not sufficient condition for credible empirical work.\n\n\n\n\nWhat You’ve Learned\nIn this case study, you applied advanced inference methods to cross-country productivity data:\n\nCompared default, heteroskedastic-robust, and cluster-robust standard errors\nObserved how SE choices affect significance conclusions\nPredicted conditional means with narrow CIs and individual outcomes with wide PIs\nConnected robust inference methods to practical policy questions\n\nThese tools ensure your empirical conclusions are reliable under realistic data conditions.\n\n\nCase Study 2: Robust Prediction of Municipal Development\nIn Chapters 10-11, we estimated multiple regression models predicting municipal development from nighttime lights and satellite embeddings, and tested the statistical significance of these predictors. Now we apply Chapter 12’s tools for robust inference and prediction—crucial for translating satellite models into practical SDG monitoring tools.\nThe Data: The DS4Bolivia project provides a comprehensive dataset covering 339 Bolivian municipalities with over 350 variables, including the Municipal Sustainable Development Index (IMDS), nighttime lights per capita, and 64 satellite embedding dimensions. Here we focus on robust standard errors and prediction intervals for the satellite-development model.\n\nLoad the DS4Bolivia Data\nLet’s load the DS4Bolivia dataset and select the key variables for robust inference and prediction analysis.\n\n# Load the DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Display basic information\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA DATASET\")\nprint(\"=\" * 70)\nprint(f\"Dataset shape: {bol.shape[0]} municipalities, {bol.shape[1]} variables\")\nprint(f\"\\nDepartments: {bol['dep'].nunique()} unique departments\")\nprint(f\"Department names: {sorted(bol['dep'].unique())}\")\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017',\n            'A00', 'A10', 'A20', 'A30', 'A40']\nbol_key = bol[key_vars].copy()\n\nprint(f\"\\nKey variables selected: {len(key_vars)}\")\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FIRST 10 MUNICIPALITIES\")\nprint(\"=\" * 70)\nprint(bol_key.head(10).to_string())\n\n# Variable descriptions\nprint(\"\\n\" + \"=\" * 70)\nprint(\"KEY VARIABLE DESCRIPTIONS\")\nprint(\"=\" * 70)\ndescriptions = {\n    'mun': 'Municipality name',\n    'dep': 'Department (administrative region, 9 total)',\n    'imds': 'Municipal Sustainable Development Index (0-100, composite of all SDGs)',\n    'ln_NTLpc2017': 'Log of nighttime lights per capita (2017, satellite-based)',\n    'A00-A40': 'Satellite image embedding dimensions (5 of 64 principal features)',\n}\nfor var, desc in descriptions.items():\n    print(f\"  {var:20s} --- {desc}\")\n\n\n\nTask 1: Default vs Robust Standard Errors (Guided)\nObjective: Estimate the satellite-development model with both default and HC1 robust standard errors and compare the results.\nInstructions:\n\nEstimate imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40 with default standard errors\nRe-estimate with HC1 robust standard errors (cov_type='HC1')\nCompare standard errors side-by-side for each coefficient\nIdentify which coefficients have substantially different SEs under the two methods\n\nApply what you learned in section 12.2: Use ols().fit() for default SEs and ols().fit(cov_type='HC1') for robust SEs.\n\n# Task 1: Default vs Robust Standard Errors\n# ----------------------------------------------------------\n\n# Prepare regression data (drop missing values)\nreg_vars = ['imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\nreg_data = bol_key[reg_vars + ['dep']].dropna()\nprint(f\"Regression sample: {len(reg_data)} municipalities (after dropping missing values)\")\n\n# Estimate with default standard errors\nmodel_default = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n                    data=reg_data).fit()\n\n# Estimate with HC1 robust standard errors\nmodel_hc1 = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n                data=reg_data).fit(cov_type='HC1')\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARISON: DEFAULT vs HC1 ROBUST STANDARD ERRORS\")\nprint(\"=\" * 70)\nprint(f\"{'Variable':&lt;18} {'Coef':&gt;10} {'Default SE':&gt;12} {'HC1 SE':&gt;12} {'Ratio':&gt;8}\")\nprint(\"-\" * 62)\nfor var in model_default.params.index:\n    coef = model_default.params[var]\n    se_def = model_default.bse[var]\n    se_hc1 = model_hc1.bse[var]\n    ratio = se_hc1 / se_def\n    print(f\"{var:&lt;18} {coef:&gt;10.4f} {se_def:&gt;12.4f} {se_hc1:&gt;12.4f} {ratio:&gt;8.3f}\")\n\nprint(f\"\\nR-squared: {model_default.rsquared:.4f}\")\nprint(f\"Adj. R-squared: {model_default.rsquared_adj:.4f}\")\nprint(f\"\\nNote: Coefficients are identical --- only SEs change.\")\nprint(\"Ratios &gt; 1 suggest heteroskedasticity inflates default SEs' precision.\")\n\n\n\nTask 2: Cluster-Robust Standard Errors by Department (Guided)\nObjective: Re-estimate the model with cluster-robust standard errors grouped by department.\nInstructions:\n\nRe-estimate using cov_type='cluster' with cov_kwds={'groups': reg_data['dep']}\nCompare cluster-robust SEs with default and HC1 SEs\nDiscuss: Why might municipalities within a department share unobserved characteristics?\n\nApply what you learned in section 12.2: Cluster-robust SEs account for within-group correlation of errors.\n\n# Task 2: Cluster-Robust Standard Errors by Department\n# ----------------------------------------------------------\n\n# Estimate with cluster-robust SEs by department\nmodel_cluster = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n                    data=reg_data).fit(cov_type='cluster',\n                                      cov_kwds={'groups': reg_data['dep']})\n\nprint(\"=\" * 70)\nprint(\"COMPARISON: DEFAULT vs HC1 vs CLUSTER-ROBUST STANDARD ERRORS\")\nprint(\"=\" * 70)\nprint(f\"{'Variable':&lt;18} {'Coef':&gt;10} {'Default SE':&gt;12} {'HC1 SE':&gt;12} {'Cluster SE':&gt;12}\")\nprint(\"-\" * 66)\nfor var in model_default.params.index:\n    coef = model_default.params[var]\n    se_def = model_default.bse[var]\n    se_hc1 = model_hc1.bse[var]\n    se_clust = model_cluster.bse[var]\n    print(f\"{var:&lt;18} {coef:&gt;10.4f} {se_def:&gt;12.4f} {se_hc1:&gt;12.4f} {se_clust:&gt;12.4f}\")\n\nn_clusters = reg_data['dep'].nunique()\nprint(f\"\\nNumber of clusters (departments): {n_clusters}\")\nprint(f\"Municipalities per department (avg): {len(reg_data) / n_clusters:.0f}\")\nprint(\"\\nDiscussion: Municipalities within the same department share\")\nprint(\"geographic, institutional, and cultural characteristics that create\")\nprint(\"within-cluster correlation. Cluster-robust SEs account for this.\")\n\n\nKey Concept 12.11: Clustered Observations in Spatial Data\nMunicipalities within the same department share geographic, institutional, and cultural characteristics that create within-cluster correlation. Standard OLS assumes independent errors, but when municipalities in La Paz share unobserved factors that affect development, their errors are correlated. Cluster-robust standard errors account for this correlation, typically producing larger SEs than default or HC1, reflecting the reduced effective sample size.\n\n\n\nTask 3: Predict Conditional Mean (Semi-guided)\nObjective: Use model.get_prediction() to predict average IMDS for a municipality with median values of all predictors.\nInstructions:\n\nCalculate the median value of each predictor variable\nUse model_default.get_prediction() to predict IMDS at the median predictor values\nReport the predicted value and its 95% confidence interval\nInterpret: “For a typical municipality, we predict IMDS between X and Y”\n\nApply what you learned in section 12.3: The confidence interval for the conditional mean reflects estimation uncertainty only.\n\n# Task 3: Predict Conditional Mean\n# ----------------------------------------------------------\n\n# Your code here: Predict IMDS for a municipality with median predictor values\n#\n# Steps:\n# 1. Calculate median values for each predictor\n# 2. Create a DataFrame with those values\n# 3. Use model_default.get_prediction() to get prediction and CI\n# 4. Report and interpret\n\n# Example structure:\n# pred_vars = ['ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\n# median_vals = reg_data[pred_vars].median()\n# pred_data = pd.DataFrame([median_vals])\n#\n# pred = model_default.get_prediction(pred_data)\n# pred_frame = pred.summary_frame(alpha=0.05)\n# print(pred_frame)\n#\n# print(f\"\\nPredicted IMDS: {pred_frame['mean'].values[0]:.2f}\")\n# print(f\"95% CI for E[IMDS|X]: [{pred_frame['mean_ci_lower'].values[0]:.2f}, \"\n#       f\"{pred_frame['mean_ci_upper'].values[0]:.2f}]\")\n# print(f\"\\nInterpretation: For a typical municipality with median predictor\")\n# print(f\"values, we predict average IMDS between \"\n#       f\"{pred_frame['mean_ci_lower'].values[0]:.1f} and \"\n#       f\"{pred_frame['mean_ci_upper'].values[0]:.1f}.\")\n\n\n\nTask 4: Prediction Interval for an Individual Municipality (Semi-guided)\nObjective: Compute the 95% prediction interval for an individual municipality (not just the mean) and compare it with the confidence interval.\nInstructions:\n\nUse model_default.get_prediction(...).summary_frame(alpha=0.05) at the same median predictor values\nReport the prediction interval using obs_ci_lower and obs_ci_upper\nCompare the width of the prediction interval with the confidence interval from Task 3\nExplain why the prediction interval is wider\n\nApply what you learned in section 12.3: Individual predictions must account for the irreducible error \\(u^*\\), making them fundamentally less precise than conditional mean predictions.\n\n# Task 4: Prediction Interval for an Individual Municipality\n# ----------------------------------------------------------\n\n# Your code here: Compute prediction interval and compare with CI\n#\n# Steps:\n# 1. Use the same prediction from Task 3\n# 2. Extract obs_ci_lower and obs_ci_upper for the prediction interval\n# 3. Compare widths\n# 4. Discuss why PI is wider\n\n# Example structure:\n# pred_vars = ['ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\n# median_vals = reg_data[pred_vars].median()\n# pred_data = pd.DataFrame([median_vals])\n#\n# pred = model_default.get_prediction(pred_data)\n# pred_frame = pred.summary_frame(alpha=0.05)\n#\n# ci_width = pred_frame['mean_ci_upper'].values[0] - pred_frame['mean_ci_lower'].values[0]\n# pi_width = pred_frame['obs_ci_upper'].values[0] - pred_frame['obs_ci_lower'].values[0]\n#\n# print(\"=\" * 70)\n# print(\"CONFIDENCE INTERVAL vs PREDICTION INTERVAL\")\n# print(\"=\" * 70)\n# print(f\"Predicted IMDS: {pred_frame['mean'].values[0]:.2f}\")\n# print(f\"\\n95% CI (conditional mean):  [{pred_frame['mean_ci_lower'].values[0]:.2f}, \"\n#       f\"{pred_frame['mean_ci_upper'].values[0]:.2f}]  width = {ci_width:.2f}\")\n# print(f\"95% PI (individual):        [{pred_frame['obs_ci_lower'].values[0]:.2f}, \"\n#       f\"{pred_frame['obs_ci_upper'].values[0]:.2f}]  width = {pi_width:.2f}\")\n# print(f\"\\nPI/CI width ratio: {pi_width / ci_width:.1f}x wider\")\n# print(f\"\\nThe prediction interval is wider because it includes the\")\n# print(f\"irreducible uncertainty from the individual error term u*.\")\n\n\nKey Concept 12.12: Prediction Uncertainty for SDG Monitoring\nSatellite-based prediction models can estimate average development patterns with reasonable precision (narrow confidence intervals for the conditional mean). However, predicting development for a specific municipality involves much greater uncertainty (wide prediction intervals) because individual municipalities deviate from the average relationship. For SDG monitoring, this means satellite predictions are more reliable for identifying broad patterns than for pinpointing the exact development level of any single municipality.\n\n\n\nTask 5: Model Robustness Comparison (Independent)\nObjective: Create a comprehensive comparison table showing coefficient estimates and standard errors under three specifications: default, HC1, and cluster-robust.\nInstructions:\n\nCreate a formatted comparison table with coefficients, SEs, and significance stars under all three SE specifications\nIdentify whether any coefficients change sign or statistical significance across specifications\nDiscuss what the comparison reveals about model reliability\nWhat does stability (or instability) across SE methods tell us about the trustworthiness of our satellite-development model?\n\nThis extends Chapter 12 concepts: You’re systematically assessing how robust your conclusions are to different assumptions about the error structure.\n\n# Task 5: Model Robustness Comparison\n# ----------------------------------------------------------\n\n# Your code here: Create comprehensive comparison table\n#\n# Steps:\n# 1. Extract coefficients and SEs from all three models\n# 2. Compute t-statistics and significance levels for each\n# 3. Create a formatted comparison table\n# 4. Identify any changes in sign or significance\n\n# Example structure:\n# def sig_stars(pval):\n#     if pval &lt; 0.01: return '***'\n#     elif pval &lt; 0.05: return '**'\n#     elif pval &lt; 0.10: return '*'\n#     else: return ''\n#\n# print(\"=\" * 90)\n# print(\"MODEL ROBUSTNESS: COEFFICIENT ESTIMATES AND STANDARD ERRORS\")\n# print(\"=\" * 90)\n# print(f\"{'Variable':&lt;16} {'Coef':&gt;8} {'SE(Def)':&gt;10} {'SE(HC1)':&gt;10} {'SE(Clust)':&gt;10} \"\n#       f\"{'Sig(D)':&gt;7} {'Sig(H)':&gt;7} {'Sig(C)':&gt;7}\")\n# print(\"-\" * 90)\n# for var in model_default.params.index:\n#     coef = model_default.params[var]\n#     se_d = model_default.bse[var]\n#     se_h = model_hc1.bse[var]\n#     se_c = model_cluster.bse[var]\n#     sig_d = sig_stars(model_default.pvalues[var])\n#     sig_h = sig_stars(model_hc1.pvalues[var])\n#     sig_c = sig_stars(model_cluster.pvalues[var])\n#     print(f\"{var:&lt;16} {coef:&gt;8.4f} {se_d:&gt;10.4f} {se_h:&gt;10.4f} {se_c:&gt;10.4f} \"\n#           f\"{sig_d:&gt;7} {sig_h:&gt;7} {sig_c:&gt;7}\")\n# print(\"-\" * 90)\n# print(\"Significance: *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.10\")\n#\n# print(\"\\nDo any coefficients change sign or significance?\")\n# print(\"What does this tell us about model reliability?\")\n\n\n\nTask 6: Prediction Brief (Independent)\nObjective: Write a 200-300 word assessment of prediction uncertainty in satellite-based development models.\nYour brief should address:\n\nHow much uncertainty exists in satellite-based development predictions?\nAre prediction intervals narrow enough to be useful for policy targeting?\nHow does the confidence interval for the conditional mean compare with the prediction interval for individual municipalities?\nWhat additional data or methods might reduce prediction uncertainty?\nShould policymakers rely on satellite predictions for allocating development resources to specific municipalities?\n\nConnection to Research: The DS4Bolivia project shows that satellite-based models achieve meaningful but imperfect predictive accuracy. Your analysis quantifies how much uncertainty remains and whether it is small enough for practical SDG monitoring applications.\n\n# Your code here: Additional analysis for the prediction brief\n#\n# You might want to:\n# 1. Compare prediction intervals at different predictor values\n#    (e.g., low-NTL vs high-NTL municipalities)\n# 2. Calculate how many municipalities fall outside prediction intervals\n# 3. Visualize actual vs predicted IMDS with confidence bands\n#\n# Example structure:\n# # Actual vs predicted comparison\n# reg_data['predicted'] = model_default.predict(reg_data)\n# reg_data['residual'] = reg_data['imds'] - reg_data['predicted']\n#\n# print(\"=\" * 70)\n# print(\"PREDICTION ACCURACY SUMMARY\")\n# print(\"=\" * 70)\n# print(f\"RMSE: {np.sqrt(model_default.mse_resid):.2f}\")\n# print(f\"Mean IMDS: {reg_data['imds'].mean():.2f}\")\n# print(f\"RMSE as % of mean: {100 * np.sqrt(model_default.mse_resid) / reg_data['imds'].mean():.1f}%\")\n# print(f\"\\nLargest over-prediction: {reg_data['residual'].min():.2f}\")\n# print(f\"Largest under-prediction: {reg_data['residual'].max():.2f}\")\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this analysis of robust inference and prediction for Bolivia’s satellite-development model, you’ve practiced:\n\nRobust SE comparison: Compared default, HC1, and cluster-robust standard errors for the same model\nCluster-robust inference: Accounted for within-department correlation among municipalities\nConditional mean prediction: Predicted average IMDS with a narrow 95% confidence interval\nPrediction intervals: Quantified the much larger uncertainty in predicting individual municipalities\nModel robustness assessment: Evaluated whether conclusions change across different SE specifications\n\nThese tools are essential for translating satellite-based models into credible SDG monitoring instruments. Robust inference ensures your significance conclusions hold under realistic data conditions, while prediction intervals honestly communicate the limits of individual-level forecasting.\nConnection to future chapters: In Chapter 14, we add indicator variables for departments to explicitly model regional differences in the satellite-development relationship.\n\nWell done! You’ve now applied Chapter 12’s advanced inference and prediction tools to the satellite-development model, quantifying both the reliability of your estimates and the precision of your predictions.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Chapter 12: Further Topics in Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook presents comprehensive case studies applying multiple regression to real-world economic problems.\nThis chapter presents nine comprehensive case studies that apply multiple regression techniques to real-world economic questions. From analyzing school performance to estimating production functions to testing causal relationships, these case studies demonstrate the breadth and power of regression analysis.\nDesign Note: This chapter uses an integrated case study structure where sections 13.1–13.9 ARE the case studies, each demonstrating different regression techniques and applications.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#chapter-overview",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#chapter-overview",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "",
    "text": "What You’ll Learn\nBy the end of this chapter, you will be able to:\n\nApply multiple regression to analyze school performance and socioeconomic factors\nUse logarithmic transformations to estimate production functions\nUnderstand and test for constant returns to scale\nIdentify and correct for omitted variables bias\nApply cluster-robust standard errors for grouped data\nUnderstand randomized control trials and difference-in-differences methods\nApply regression discontinuity design to causal questions\nUse instrumental variables to estimate causal effects\nNavigate the data cleaning and preparation process\n\n\n\nChapter Outline\n\n13.1 School Academic Performance Index\n13.2 Cobb-Douglas Production Function\n13.3 Phillips Curve and Omitted Variables Bias\n13.4 Automobile Fuel Efficiency\n13.5 RAND Health Insurance Experiment (RCT)\n13.6 Health Care Access (Difference-in-Differences)\n13.7 Political Incumbency (Regression Discontinuity)\n13.8 Institutions and GDP (Instrumental Variables)\n13.9 From Raw Data to Final Data\nKey Takeaways — Chapter review and consolidated lessons\nPractice Exercises — Reinforce your understanding\n\nDatasets used: - AED_API99.DTA: 807 California high schools, Academic Performance Index (1999) — Case Study 13.1 - AED_CobbDouglas.DTA: 24 years of U.S. manufacturing data (1899–1922) — Case Study 13.2 - AED_PhillipsCurve.DTA: 66 years of U.S. macroeconomic data (1949–2014) — Case Study 13.3 - AED_AutoEfficiency.DTA: 26,995 vehicles, fuel efficiency data (1980–2006) — Case Study 13.4 - AED_RandHealthInsurance.DTA: RAND health insurance experiment (RCT) — Case Study 13.5 - AED_SouthAfricaHealth.DTA: South Africa cross-sectional health data (DiD) — Case Study 13.6 - AED_SenateIncumbency.DTA: U.S. Senate election results (RD) — Case Study 13.7 - AED_InstitutionsGDP.DTA: Cross-country institutions and GDP data (IV) — Case Study 13.8\nEstimated time: 120–150 minutes",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#setup",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#setup",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "Setup",
    "text": "Setup\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Random seed\nnp.random.seed(42)\n\n# Data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Plotting\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"✓ Setup complete!\")\n\n✓ Setup complete!",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#school-academic-performance-index",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#school-academic-performance-index",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.1 School Academic Performance Index",
    "text": "13.1 School Academic Performance Index\nAnalyzing factors that determine school test scores in California.\n\n# Load API data\ndata_api = pd.read_stata(GITHUB_DATA_URL + 'AED_API99.DTA')\nprint(f\"Loaded {len(data_api)} California high schools\")\nprint(f\"Variables: {list(data_api.columns)}\")\ndata_api.head()\n\nLoaded 807 California high schools\nVariables: ['sch_code', 'api99', 'edparent', 'meals', 'englearn', 'yearround', 'credteach', 'emerteach', 'avg_ed_raw', 'pct_af_am', 'pct_am_ind', 'pct_asian', 'pct_fil', 'pct_hisp', 'pct_pac', 'pct_white', 'mobility']\n\n\n\n    \n\n\n\n\n\n\nsch_code\napi99\nedparent\nmeals\nenglearn\nyearround\ncredteach\nemerteach\navg_ed_raw\npct_af_am\npct_am_ind\npct_asian\npct_fil\npct_hisp\npct_pac\npct_white\nmobility\n\n\n\n\n0\n130054\n633\n12.78\n21\n6\n0\n88\n13\n2.89\n6\n1\n12\n9\n30\n3\n39\n0.0\n\n\n1\n130062\n646\n13.42\n16\n17\n0\n86\n24\n3.21\n5\n1\n27\n6\n17\n1\n43\n12.0\n\n\n2\n130096\n797\n14.90\n1\n2\n0\n100\n4\n3.95\n1\n1\n9\n2\n5\n0\n81\n0.0\n\n\n3\n130229\n693\n13.66\n0\n18\n0\n93\n7\n3.33\n5\n1\n33\n7\n10\n1\n38\n7.0\n\n\n4\n130450\n773\n14.94\n0\n9\n0\n98\n7\n3.97\n8\n0\n28\n1\n9\n1\n46\n0.0\n\n\n\n\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n\n\n\nAbout the Dataset\nThis dataset contains information on 807 California high schools from 1999. The Academic Performance Index (API) is a score from 200-1000 that measures school performance based on standardized test results.\nKey Variables: - api99: Academic Performance Index (outcome variable) - edparent: Average parental education level (1-5 scale) - mealpct: Percent of students eligible for free/reduced meals (poverty proxy) - elpct: Percent of English language learners - yrs_teach: Average teacher experience in years - totcredpc: Per-pupil total credentials - emrpct: Percent of emergency credential teachers\nEconomic Question: What factors determine school performance? Is it resources (teachers, funding) or student demographics (poverty, language, parental education)?\n\n# Summary statistics\nprint(\"=\"*70)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\"*70)\nvars_api = ['api99', 'edparent', 'meals', 'englearn', 'yearround', 'credteach', 'emerteach']\nprint(data_api[vars_api].describe())\n\n======================================================================\nSUMMARY STATISTICS\n======================================================================\n            api99    edparent       meals    englearn   yearround   credteach  \\\ncount  807.000000  807.000000  807.000000  807.000000  807.000000  807.000000   \nmean   620.944238   12.841289   21.918216   14.003717    0.023544   89.836431   \nstd    107.440771    1.234461   23.667952   12.786517    0.151717    8.437510   \nmin    355.000000    9.620000    0.000000    0.000000    0.000000   33.000000   \n25%    542.000000   12.020000    0.000000    4.000000    0.000000   85.000000   \n50%    620.000000   12.880000   14.000000   10.000000    0.000000   92.000000   \n75%    695.000000   13.680000   36.500000   21.000000    0.000000   96.000000   \nmax    966.000000   16.000000   98.000000   66.000000    1.000000  100.000000   \n\n        emerteach  \ncount  807.000000  \nmean    10.464684  \nstd      8.214762  \nmin      0.000000  \n25%      4.000000  \n50%      9.000000  \n75%     15.000000  \nmax     56.000000  \n\n\n\n# Histogram of API scores\nplt.figure(figsize=(10, 6))\nplt.hist(data_api['api99'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\nplt.axvline(data_api['api99'].mean(), color='red', linestyle='--', linewidth=2,\n            label=f'Mean = {data_api[\"api99\"].mean():.1f}')\nplt.axvline(800, color='green', linestyle='--', linewidth=2, label='Target = 800')\nplt.xlabel('Academic Performance Index (API)')\nplt.ylabel('Number of Schools')\nplt.title('Figure 13.1: Distribution of API Scores')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Bivariate regression: API ~ Edparent\nmodel_api_biv = ols('api99 ~ edparent', data=data_api).fit(cov_type='HC1')\nprint(\"=\"*70)\nprint(\"BIVARIATE REGRESSION: API ~ EDPARENT\")\nprint(\"=\"*70)\nprint(model_api_biv.summary())\n\n======================================================================\nBIVARIATE REGRESSION: API ~ EDPARENT\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  api99   R-squared:                       0.835\nModel:                            OLS   Adj. R-squared:                  0.835\nMethod:                 Least Squares   F-statistic:                     4257.\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):          9.88e-324\nTime:                        09:47:17   Log-Likelihood:                -4191.9\nNo. Observations:                 807   AIC:                             8388.\nDf Residuals:                     805   BIC:                             8397.\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -400.3137     15.993    -25.030      0.000    -431.660    -368.967\nedparent      79.5292      1.219     65.247      0.000      77.140      81.918\n==============================================================================\nOmnibus:                       64.750   Durbin-Watson:                   1.710\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              280.858\nSkew:                           0.202   Prob(JB):                     1.03e-61\nKurtosis:                       5.862   Cond. No.                         136.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n\n\n# Scatter plot with regression line\nplt.figure(figsize=(10, 6))\nplt.scatter(data_api['edparent'], data_api['api99'], alpha=0.5, s=30, color='black')\nplt.plot(data_api['edparent'], model_api_biv.fittedvalues, color='blue', linewidth=2,\n         label='Fitted line')\nplt.xlabel('Average Years of Parent Education')\nplt.ylabel('Academic Performance Index (API)')\nplt.title('Figure 13.2: API vs Parent Education')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Correlation matrix\ncorr_matrix = data_api[vars_api].corr()\nprint(\"=\"*70)\nprint(\"CORRELATION MATRIX\")\nprint(\"=\"*70)\nprint(corr_matrix.round(2))\n\n# Heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n            square=True, linewidths=1)\nplt.title('Figure 13.3: Correlation Matrix')\nplt.tight_layout()\nplt.show()\n\n======================================================================\nCORRELATION MATRIX\n======================================================================\n           api99  edparent  meals  englearn  yearround  credteach  emerteach\napi99       1.00      0.91  -0.54     -0.66      -0.19       0.46      -0.45\nedparent    0.91      1.00  -0.60     -0.71      -0.25       0.40      -0.37\nmeals      -0.54     -0.60   1.00      0.56       0.29      -0.27       0.21\nenglearn   -0.66     -0.71   0.56      1.00       0.22      -0.26       0.20\nyearround  -0.19     -0.25   0.29      0.22       1.00      -0.18       0.09\ncredteach   0.46      0.40  -0.27     -0.26      -0.18       1.00      -0.82\nemerteach  -0.45     -0.37   0.21      0.20       0.09      -0.82       1.00\n\n\n\n\n\n\n\n\n\n\n\nWhat the Correlation Matrix Reveals\nThe correlation matrix shows the linear relationships between all variables:\nStrong negative correlations (darker blues): - api99 and mealpct (-0.90): Schools with more poverty have much lower test scores - api99 and elpct (-0.76): More English learners → lower scores - These suggest socioeconomic factors strongly predict performance\nPositive correlations (reds): - api99 and edparent (0.76): Parental education strongly predicts scores - Teacher experience and credentials show weaker correlations\nMulticollinearity concerns: - mealpct and edparent are highly correlated (-0.79) - This makes it difficult to separate their individual effects - Standard errors may be inflated in multiple regression\n\n# Multiple regression\nmodel_api_mult = ols('api99 ~ edparent + meals + englearn + yearround + credteach + emerteach',\n                      data=data_api).fit()\nprint(\"=\"*70)\nprint(\"MULTIPLE REGRESSION\")\nprint(\"=\"*70)\nprint(model_api_mult.summary())\n\n# Coefficient table\ncoef_df = pd.DataFrame({\n    'Coefficient': model_api_mult.params,\n    'Std Error': model_api_mult.bse,\n    't-stat': model_api_mult.tvalues,\n    'p-value': model_api_mult.pvalues\n}).round(3)\nprint(\"\\n\", coef_df)\n\n======================================================================\nMULTIPLE REGRESSION\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  api99   R-squared:                       0.853\nModel:                            OLS   Adj. R-squared:                  0.852\nMethod:                 Least Squares   F-statistic:                     771.4\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):               0.00\nTime:                        09:47:19   Log-Likelihood:                -4146.3\nNo. Observations:                 807   AIC:                             8307.\nDf Residuals:                     800   BIC:                             8339.\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -345.3278     39.954     -8.643      0.000    -423.755    -266.900\nedparent      73.9421      1.883     39.273      0.000      70.246      77.638\nmeals          0.0793      0.081      0.980      0.327      -0.079       0.238\nenglearn      -0.3578      0.167     -2.145      0.032      -0.685      -0.030\nyearround     25.9559     10.185      2.548      0.011       5.963      45.949\ncredteach      0.3875      0.311      1.247      0.213      -0.222       0.997\nemerteach     -1.4704      0.315     -4.672      0.000      -2.088      -0.853\n==============================================================================\nOmnibus:                       71.318   Durbin-Watson:                   1.766\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              272.754\nSkew:                           0.326   Prob(JB):                     5.92e-60\nKurtosis:                       5.772   Cond. No.                     2.62e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.62e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n            Coefficient  Std Error  t-stat  p-value\nIntercept     -345.328     39.954  -8.643    0.000\nedparent        73.942      1.883  39.273    0.000\nmeals            0.079      0.081   0.980    0.327\nenglearn        -0.358      0.167  -2.145    0.032\nyearround       25.956     10.185   2.548    0.011\ncredteach        0.387      0.311   1.247    0.213\nemerteach       -1.470      0.315  -4.672    0.000\n\n\n\nKey Concept 13.1: Multiple Regression and Socioeconomic Determinants\nIn multiple regression, the coefficient for parent education remains strong (~74 points) even after controlling for meals, English learners, year-round schools, and teacher quality. The high correlation among socioeconomic variables makes it difficult to isolate their separate effects — a common challenge in observational studies.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#cobb-douglas-production-function",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#cobb-douglas-production-function",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.2 Cobb-Douglas Production Function",
    "text": "13.2 Cobb-Douglas Production Function\nEstimating returns to scale using log transformations.\n\nThe Cobb-Douglas Production Function\nThe Cobb-Douglas production function is one of the most famous models in economics. It describes how inputs (capital K and labor L) combine to produce output Q:\n\\[Q = \\alpha K^{\\beta_2} L^{\\beta_3}\\]\nParameters: - \\(\\alpha\\): Total factor productivity (technology level) - \\(\\beta_2\\): Output elasticity of capital (% change in Q from 1% change in K) - \\(\\beta_3\\): Output elasticity of labor (% change in Q from 1% change in L)\nReturns to Scale: - If \\(\\beta_2 + \\beta_3 = 1\\): Constant returns (doubling inputs doubles output) - If \\(\\beta_2 + \\beta_3 &gt; 1\\): Increasing returns (doubling inputs more than doubles output) - If \\(\\beta_2 + \\beta_3 &lt; 1\\): Decreasing returns (doubling inputs less than doubles output)\nWhy Log-Transform?\nTaking natural logs of both sides converts the multiplicative model into a linear model:\n\\[\\ln(Q) = \\beta_1 + \\beta_2 \\ln(K) + \\beta_3 \\ln(L) + u\\]\nwhere \\(\\beta_1 = \\ln(\\alpha)\\). Now we can use ordinary least squares (OLS) regression!\nDataset: Douglas (1976) used U.S. manufacturing data from 1899-1922 (24 years) to estimate this function.\n\n# Load Cobb-Douglas data\ndata_cobb = pd.read_stata(GITHUB_DATA_URL + 'AED_COBBDOUGLAS.DTA')\nprint(f\"Loaded {len(data_cobb)} years of US manufacturing data (1899-1922)\")\nprint(f\"Variables: {list(data_cobb.columns)}\")\ndata_cobb.head(12)\n\nLoaded 24 years of US manufacturing data (1899-1922)\nVariables: ['year', 'q', 'k', 'l', 'lnq', 'lnk', 'lnl']\n\n\n\n    \n\n\n\n\n\n\nyear\nq\nk\nl\nlnq\nlnk\nlnl\n\n\n\n\n0\n1899\n100\n100\n100\n4.605170\n4.605170\n4.605170\n\n\n1\n1900\n101\n107\n105\n4.615120\n4.672829\n4.653960\n\n\n2\n1901\n112\n114\n110\n4.718499\n4.736198\n4.700480\n\n\n3\n1902\n122\n122\n118\n4.804021\n4.804021\n4.770685\n\n\n4\n1903\n124\n131\n123\n4.820282\n4.875197\n4.812184\n\n\n5\n1904\n122\n138\n116\n4.804021\n4.927254\n4.753590\n\n\n6\n1905\n143\n149\n125\n4.962845\n5.003946\n4.828314\n\n\n7\n1906\n152\n163\n133\n5.023880\n5.093750\n4.890349\n\n\n8\n1907\n151\n176\n138\n5.017280\n5.170484\n4.927254\n\n\n9\n1908\n126\n185\n121\n4.836282\n5.220356\n4.795791\n\n\n10\n1909\n155\n198\n140\n5.043425\n5.288267\n4.941642\n\n\n11\n1910\n159\n208\n144\n5.068904\n5.337538\n4.969813\n\n\n\n\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n\n\n\n# Create log transformations\ndata_cobb['lnq'] = np.log(data_cobb['q'])\ndata_cobb['lnk'] = np.log(data_cobb['k'])\ndata_cobb['lnl'] = np.log(data_cobb['l'])\n\nprint(\"Summary statistics (original and log-transformed):\")\nprint(data_cobb[['q', 'k', 'l', 'lnq', 'lnk', 'lnl']].describe())\n\nSummary statistics (original and log-transformed):\n                q           k           l        lnq        lnk        lnl\ncount   24.000000   24.000000   24.000000  24.000000  24.000000  24.000000\nmean   165.916667  234.166667  145.791667   5.077336   5.356483   4.962723\nstd     43.753178  106.266550   29.616357   0.269234   0.459178   0.201077\nmin    100.000000  100.000000  100.000000   4.605170   4.605170   4.605170\n25%    125.500000  146.250000  122.500000   4.832282   4.984773   4.808086\n50%    157.000000  212.000000  144.500000   5.056165   5.356408   4.973274\n75%    196.250000  307.250000  155.750000   5.277434   5.726353   5.048065\nmax    240.000000  431.000000  200.000000   5.480639   6.066108   5.298317\n\n\n\n# Estimate Cobb-Douglas with HAC standard errors\nmodel_cobb = ols('lnq ~ lnk + lnl', data=data_cobb).fit(cov_type='HAC', cov_kwds={'maxlags': 3})\nprint(\"=\"*70)\nprint(\"COBB-DOUGLAS REGRESSION: ln(Q) ~ ln(K) + ln(L)\")\nprint(\"=\"*70)\nprint(model_cobb.summary())\n\nbeta_k = model_cobb.params['lnk']\nbeta_l = model_cobb.params['lnl']\nprint(f\"\\nSum of coefficients: {beta_k:.3f} + {beta_l:.3f} = {beta_k + beta_l:.3f}\")\n\n======================================================================\nCOBB-DOUGLAS REGRESSION: ln(Q) ~ ln(K) + ln(L)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    lnq   R-squared:                       0.957\nModel:                            OLS   Adj. R-squared:                  0.953\nMethod:                 Least Squares   F-statistic:                     246.2\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           2.65e-15\nTime:                        09:47:19   Log-Likelihood:                 35.826\nNo. Observations:                  24   AIC:                            -65.65\nDf Residuals:                      21   BIC:                            -62.12\nDf Model:                           2                                         \nCovariance Type:                  HAC                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.1773      0.372     -0.476      0.634      -0.907       0.552\nlnk            0.2331      0.059      3.977      0.000       0.118       0.348\nlnl            0.8073      0.126      6.409      0.000       0.560       1.054\n==============================================================================\nOmnibus:                        2.133   Durbin-Watson:                   1.523\nProb(Omnibus):                  0.344   Jarque-Bera (JB):                1.361\nSkew:                           0.583   Prob(JB):                        0.506\nKurtosis:                       2.992   Cond. No.                         285.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 3 lags and without small sample correction\n\nSum of coefficients: 0.233 + 0.807 = 1.040\n\n\n\nKey Concept 13.2: Logarithmic Transformation of Production Functions\nTaking natural logarithms of the Cobb-Douglas production function \\(Q = AK^\\alpha L^\\beta\\) transforms it into the linear model \\(\\ln Q = \\ln A + \\alpha \\ln K + \\beta \\ln L\\), suitable for OLS estimation. The resulting coefficients are elasticities — directly interpretable as percentage changes.\n\n\n\nUnderstanding the Results\nEstimated Coefficients: - \\(\\hat{\\beta}_2\\) (capital) ≈ 0.23: A 1% increase in capital raises output by 0.23% - \\(\\hat{\\beta}_3\\) (labor) ≈ 0.81: A 1% increase in labor raises output by 0.81% - Sum: 0.23 + 0.81 ≈ 1.04 → Very close to constant returns to scale!\nWhy HAC Standard Errors?\nThis is time series data (24 consecutive years). Two problems arise:\n\nAutocorrelation: Output in year t is correlated with output in year t-1\n\n\nRecessions/booms span multiple years\nTechnology shocks persist over time\n\n\nHeteroskedasticity: Variance of errors may change over time\n\n\nEconomy was more volatile in early 1900s\nStructural changes during WWI\n\nHAC (Heteroskedasticity and Autocorrelation Consistent) standard errors correct for both problems. We use maxlags=3, allowing correlations up to 3 years apart.\nComparison: - Default SEs: Assume errors are uncorrelated and homoskedastic (WRONG here!) - Robust (HC1) SEs: Fix heteroskedasticity but ignore autocorrelation - HAC SEs: Fix BOTH problems (correct choice for time series)\nEconomic Interpretation:\nLabor’s output elasticity (0.81) is much larger than capital’s (0.23). This suggests that in early 20th century U.S. manufacturing: - Labor was the dominant input - Capital was relatively less important - Consistent with labor-intensive production before automation\n\n# Test constant returns to scale\n# H0: beta_k + beta_l = 1\nsum_betas = beta_k + beta_l\nprint(f\"Testing constant returns to scale:\")\nprint(f\"H0: beta_capital + beta_labor = 1\")\nprint(f\"Estimated sum: {sum_betas:.3f}\")\n\n# Restricted model: ln(Q/L) ~ ln(K/L)\ndata_cobb['lnq_per_l'] = data_cobb['lnq'] - data_cobb['lnl']\ndata_cobb['lnk_per_l'] = data_cobb['lnk'] - data_cobb['lnl']\nmodel_restricted = ols('lnq_per_l ~ lnk_per_l', data=data_cobb).fit()\n\n# F-test\nrss_unr = model_cobb.ssr\nrss_r = model_restricted.ssr\nf_stat = ((rss_r - rss_unr) / 1) / (rss_unr / model_cobb.df_resid)\np_value = 1 - stats.f.cdf(f_stat, 1, model_cobb.df_resid)\n\nprint(f\"F-statistic: {f_stat:.2f}\")\nprint(f\"p-value: {p_value:.3f}\")\nprint(f\"Conclusion: {'Reject' if p_value &lt; 0.05 else 'Fail to reject'} H0 at 5% level\")\n\nTesting constant returns to scale:\nH0: beta_capital + beta_labor = 1\nEstimated sum: 1.040\nF-statistic: 0.20\np-value: 0.663\nConclusion: Fail to reject H0 at 5% level\n\n\n\nKey Concept 13.3: Testing Constant Returns to Scale\nConstant returns to scale implies \\(\\alpha + \\beta = 1\\): doubling all inputs exactly doubles output. The estimated sum of 1.040 is not significantly different from 1 (F-test p = 0.636), consistent with the theoretical prediction for competitive markets. This is a joint hypothesis test on the coefficients.\n\n\n# Predicted output with bias correction\nse = np.sqrt(model_cobb.scale)\nbias_correction = np.exp(se**2 / 2)\ndata_cobb['q_pred'] = bias_correction * np.exp(model_cobb.fittedvalues)\n\n# Plot actual vs predicted\nplt.figure(figsize=(10, 6))\nplt.plot(data_cobb['year'], data_cobb['q'], 'o-', color='black', linewidth=2,\n         markersize=6, label='Actual Q')\nplt.plot(data_cobb['year'], data_cobb['q_pred'], 's--', color='blue', linewidth=2,\n         markersize=5, label='Predicted Q')\nplt.xlabel('Year')\nplt.ylabel('Output Index')\nplt.title('Figure 13.4: Actual vs Predicted Output (1899-1922)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#phillips-curve-and-omitted-variables-bias",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#phillips-curve-and-omitted-variables-bias",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.3 Phillips Curve and Omitted Variables Bias",
    "text": "13.3 Phillips Curve and Omitted Variables Bias\nDemonstrating omitted variables bias through the breakdown of the Phillips curve.\n\nThe Phillips Curve and Its Breakdown\nThe Phillips Curve describes a negative relationship between unemployment and inflation: - When unemployment is low → workers demand higher wages → prices rise → inflation increases - When unemployment is high → workers accept lower wages → prices fall → inflation decreases\nHistorical Context:\nA.W. Phillips (1958) found this negative relationship in UK data from 1861-1957. It became a cornerstone of macroeconomic policy: - Governments believed they could “trade off” unemployment for inflation - Want lower unemployment? Accept higher inflation - Want lower inflation? Accept higher unemployment\nThe 1970s Crisis:\nThe relationship broke down in the 1970s! Economies experienced stagflation (high unemployment AND high inflation simultaneously). This was a major crisis in economic theory.\nThe Solution: Expected Inflation\nMilton Friedman and Edmund Phelps showed the original Phillips curve suffered from omitted variables bias. The correct model includes expected inflation:\n\\[\\text{Inflation}_t = \\beta_1 + \\beta_2 \\text{Urate}_t + \\beta_3 \\text{Expected Inflation}_t + u_t\\]\nWe’ll demonstrate this using U.S. data from 1949-2014 (66 years).\n\n# Load Phillips curve data\ndata_phillips = pd.read_stata(GITHUB_DATA_URL + 'AED_PHILLIPS.DTA')\nprint(f\"Loaded {len(data_phillips)} years of US data (1949-2014)\")\nprint(f\"Variables: {list(data_phillips.columns)}\")\ndata_phillips.head()\n\nLoaded 66 years of US data (1949-2014)\nVariables: ['year', 'urate', 'gdpdef', 'inflgdp', 'pastinflgdp', 'inflgdp1yr', 'cpi', 'inflcpi', 'pastinflcpi', 'infcpi1yr', 'infcpi10yr', 'mich', 'date', 'daten']\n\n\n\n    \n\n\n\n\n\n\nyear\nurate\ngdpdef\ninflgdp\npastinflgdp\ninflgdp1yr\ncpi\ninflcpi\npastinflcpi\ninfcpi1yr\ninfcpi10yr\nmich\ndate\ndaten\n\n\n\n\n0\n1949.0\n6.6\n13.518\n-1.965335\nNaN\nNaN\n23.6\n-2.074689\n7.698837\nNaN\nNaN\nNaN\n1949-10-01\n1949-10-01\n\n\n1\n1950.0\n4.3\n14.090\n4.231395\nNaN\nNaN\n25.0\n5.932203\n3.648189\nNaN\nNaN\nNaN\n1950-10-01\n1950-10-01\n\n\n2\n1951.0\n3.1\n14.869\n5.528744\nNaN\nNaN\n26.5\n6.000000\n3.232486\nNaN\nNaN\nNaN\n1951-10-01\n1951-10-01\n\n\n3\n1952.0\n2.7\n15.091\n1.493039\n3.474261\nNaN\n26.7\n0.754717\n4.063869\nNaN\nNaN\nNaN\n1952-10-01\n1952-10-01\n\n\n4\n1953.0\n4.5\n15.219\n0.848188\n2.905584\nNaN\n26.9\n0.749064\n3.080858\nNaN\nNaN\nNaN\n1953-10-01\n1953-10-01\n\n\n\n\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n\n\n\n# Pre-1970 regression\ndata_pre1970 = data_phillips[data_phillips['year'] &lt; 1970]\nmodel_pre = ols('inflgdp ~ urate', data=data_pre1970).fit(cov_type='HAC', cov_kwds={'maxlags': 3})\nprint(\"=\"*70)\nprint(\"PHILLIPS CURVE PRE-1970 (1949-1969)\")\nprint(\"=\"*70)\nprint(model_pre.summary())\n\n======================================================================\nPHILLIPS CURVE PRE-1970 (1949-1969)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                inflgdp   R-squared:                       0.454\nModel:                            OLS   Adj. R-squared:                  0.425\nMethod:                 Least Squares   F-statistic:                     11.09\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):            0.00352\nTime:                        09:47:20   Log-Likelihood:                -34.492\nNo. Observations:                  21   AIC:                             72.98\nDf Residuals:                      19   BIC:                             75.07\nDf Model:                           1                                         \nCovariance Type:                  HAC                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.1057      1.504      4.724      0.000       4.157      10.054\nurate         -1.0300      0.309     -3.330      0.001      -1.636      -0.424\n==============================================================================\nOmnibus:                        1.242   Durbin-Watson:                   1.536\nProb(Omnibus):                  0.538   Jarque-Bera (JB):                1.025\nSkew:                          -0.499   Prob(JB):                        0.599\nKurtosis:                       2.581   Cond. No.                         21.8\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 3 lags and without small sample correction\n\n\n\n\nPre-1970: Phillips Curve Works!\nResults: - Coefficient on unemployment ≈ -0.89 - Negative relationship (as theory predicts!) - Statistically significant - 1% increase in unemployment → 0.89% decrease in inflation\nThe scatter plot shows a clear downward slope. This is the classic Phillips curve that policymakers relied on in the 1950s-60s.\nWhy did it work?\nIn the 1950s-60s, expected inflation was stable and low (around 2-3%). Since it didn’t vary much, omitting it from the regression didn’t cause serious bias.\n\n# Plot pre-1970\nplt.figure(figsize=(10, 6))\nplt.scatter(data_pre1970['urate'], data_pre1970['inflgdp'], alpha=0.7, s=50, color='black')\nplt.plot(data_pre1970['urate'], model_pre.fittedvalues, color='blue', linewidth=2,\n         label='Fitted line')\nplt.xlabel('Unemployment Rate (%)')\nplt.ylabel('Inflation Rate (%)')\nplt.title('Figure 13.5: Phillips Curve Pre-1970 (Negative Relationship)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Post-1970 regression\ndata_post1970 = data_phillips[data_phillips['year'] &gt;= 1970]\nmodel_post = ols('inflgdp ~ urate', data=data_post1970).fit(cov_type='HAC', cov_kwds={'maxlags': 5})\nprint(\"=\"*70)\nprint(\"PHILLIPS CURVE POST-1970 (1970-2014)\")\nprint(\"=\"*70)\nprint(model_post.summary())\n\n======================================================================\nPHILLIPS CURVE POST-1970 (1970-2014)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                inflgdp   R-squared:                       0.028\nModel:                            OLS   Adj. R-squared:                  0.005\nMethod:                 Least Squares   F-statistic:                    0.6928\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):              0.410\nTime:                        09:47:21   Log-Likelihood:                -103.01\nNo. Observations:                  45   AIC:                             210.0\nDf Residuals:                      43   BIC:                             213.6\nDf Model:                           1                                         \nCovariance Type:                  HAC                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.9279      1.826      1.056      0.291      -1.651       5.507\nurate          0.2653      0.319      0.832      0.405      -0.359       0.890\n==============================================================================\nOmnibus:                        9.911   Durbin-Watson:                   0.294\nProb(Omnibus):                  0.007   Jarque-Bera (JB):                9.497\nSkew:                           1.090   Prob(JB):                      0.00866\nKurtosis:                       3.563   Cond. No.                         29.3\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\n\nKey Concept 13.4: The Phillips Curve Breakdown\nPre-1970, higher unemployment was associated with lower inflation (the classic Phillips curve trade-off). Post-1970, this relationship reversed — suggesting the simple bivariate model was misspecified. The breakdown motivated the expectations-augmented Phillips curve, which includes expected inflation as a regressor.\n\n\n\nPost-1970: Phillips Curve Breaks Down!\nResults: - Coefficient on unemployment ≈ +0.27 - Positive relationship (opposite of theory!) - The sign reversed!\nThe scatter plot shows an upward slope - higher unemployment is associated with higher inflation. This is stagflation.\nWhat went wrong?\nIn the 1970s, expected inflation became: - Much higher (reached 10%+ during oil shocks) - Much more variable (changed frequently) - Correlated with unemployment (both rose together)\nOmitting expected inflation now causes severe omitted variables bias that reverses the sign of the unemployment coefficient!\n\n# Plot post-1970\nplt.figure(figsize=(10, 6))\nplt.scatter(data_post1970['urate'], data_post1970['inflgdp'], alpha=0.7, s=50, color='black')\nplt.plot(data_post1970['urate'], model_post.fittedvalues, color='red', linewidth=2,\n         label='Fitted line')\nplt.xlabel('Unemployment Rate (%)')\nplt.ylabel('Inflation Rate (%)')\nplt.title('Figure 13.6: Phillips Curve Post-1970 (Positive Relationship - Breakdown!)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Augmented Phillips curve (adding expected inflation)\n\ndata_post1970_exp = data_post1970.dropna(subset=['inflgdp1yr'])\n\nmodel_augmented = ols('inflgdp ~ urate + inflgdp1yr', data=data_post1970_exp).fit(\n\n    cov_type='HAC', cov_kwds={'maxlags': 5})\n\nprint(\"=\"*70)\n\nprint(\"AUGMENTED PHILLIPS CURVE POST-1970\")\n\nprint(\"=\"*70)\n\nprint(model_augmented.summary())\n\n======================================================================\nAUGMENTED PHILLIPS CURVE POST-1970\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                inflgdp   R-squared:                       0.881\nModel:                            OLS   Adj. R-squared:                  0.875\nMethod:                 Least Squares   F-statistic:                     97.73\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           1.59e-16\nTime:                        09:47:22   Log-Likelihood:                -55.722\nNo. Observations:                  45   AIC:                             117.4\nDf Residuals:                      42   BIC:                             122.9\nDf Model:                           2                                         \nCovariance Type:                  HAC                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2701      0.612      0.441      0.659      -0.930       1.470\nurate         -0.1278      0.078     -1.630      0.103      -0.281       0.026\ninflgdp1yr     1.1463      0.082     13.953      0.000       0.985       1.307\n==============================================================================\nOmnibus:                        2.284   Durbin-Watson:                   0.736\nProb(Omnibus):                  0.319   Jarque-Bera (JB):                1.308\nSkew:                           0.288   Prob(JB):                        0.520\nKurtosis:                       3.605   Cond. No.                         34.1\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\n\n# Demonstrate omitted variables bias\n\n# Bivariate regression of Expinflation on Urate\nmodel_aux = ols('inflgdp1yr ~ urate', data=data_post1970_exp).fit()\n\ngamma = model_aux.params['urate']\nbeta3 = model_augmented.params['inflgdp1yr']\nbeta2 = model_augmented.params['urate']\n\nprint(\"=\" * 70)\nprint(\"OMITTED VARIABLES BIAS CALCULATION\")\nprint(\"=\" * 70)\nprint(\"True model: Inflation = β1 + β2*Urate + β3*Expinflation\")\nprint(\"Omitted model: Inflation = b1 + b2*Urate\")\nprint(\"\\nOmitted variables bias formula: E[b2] = β2 + β3*γ\")\nprint(\"where γ = coefficient from Expinflation ~ Urate regression\")\n\nprint(f\"\\nγ (from auxiliary regression): {gamma:.3f}\")\nprint(f\"β3 (from full model): {beta3:.3f}\")\nprint(f\"β2 (from full model): {beta2:.3f}\")\n\nprint(\n    f\"\\nPredicted E[b2] = {beta2:.3f} + {beta3:.3f} * {gamma:.3f} \"\n    f\"= {beta2 + beta3 * gamma:.3f}\"\n)\nprint(f\"Actual b2 (from bivariate): {model_post.params['urate']:.3f}\")\n\nprint(\"\\n✓ Omitted variables bias explains the sign reversal!\")\n\n======================================================================\nOMITTED VARIABLES BIAS CALCULATION\n======================================================================\nTrue model: Inflation = β1 + β2*Urate + β3*Expinflation\nOmitted model: Inflation = b1 + b2*Urate\n\nOmitted variables bias formula: E[b2] = β2 + β3*γ\nwhere γ = coefficient from Expinflation ~ Urate regression\n\nγ (from auxiliary regression): 0.343\nβ3 (from full model): 1.146\nβ2 (from full model): -0.128\n\nPredicted E[b2] = -0.128 + 1.146 * 0.343 = 0.265\nActual b2 (from bivariate): 0.265\n\n✓ Omitted variables bias explains the sign reversal!\n\n\n\nKey Concept 13.5: Omitted Variables Bias\nWhen a relevant variable (expected inflation) is omitted from the regression, its effect gets absorbed into the included variable’s coefficient. The OVB formula shows that the bias equals the omitted variable’s coefficient times its correlation with the included regressor. In the Phillips curve, this bias reversed the sign of the unemployment coefficient.\n\n\n\nUnderstanding Omitted Variables Bias\nThe omitted variables bias formula tells us how the coefficient in the bivariate (wrong) model relates to the true coefficients:\n\\[E[b_2] = \\beta_2 + \\beta_3 \\gamma\\]\nwhere: - \\(b_2\\): coefficient on Urate in bivariate model (omits expected inflation) - \\(\\beta_2\\): true coefficient on Urate in full model - \\(\\beta_3\\): true coefficient on expected inflation in full model - \\(\\gamma\\): coefficient from regressing expected inflation on Urate (auxiliary regression)\nWhat we found: - \\(\\beta_2 \\approx -1.15\\) (true negative effect of unemployment) - \\(\\beta_3 \\approx +1.15\\) (expected inflation has 1-to-1 effect on actual inflation) - \\(\\gamma \\approx +0.34\\) (when unemployment rises, expected inflation also rises in 1970s!)\nPredicted bias: \\[E[b_2] = -1.15 + 1.15 \\times 0.34 \\approx +0.26\\]\nActual \\(b_2\\) from bivariate model: +0.27\nPerfect match! The omitted variables bias formula exactly explains why the sign reversed.\nEconomic Interpretation:\nIn the 1970s: 1. Oil shocks raised unemployment (supply shocks) 2. Same oil shocks raised expected inflation (cost-push) 3. Unemployment and expected inflation moved together 4. Omitting expected inflation makes unemployment look like it causes inflation 5. But really, both are caused by the same underlying shocks\nThe Lesson:\nThe Phillips curve didn’t “disappear” - it was always conditional on expected inflation. Once we control for expectations, the negative relationship returns. This revolutionized central bank policy: to fight inflation, must manage expectations!\nHaving explored detailed case studies with full estimation and testing, we now survey additional applications demonstrating advanced econometric methods.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#automobile-fuel-efficiency",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#automobile-fuel-efficiency",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.4 Automobile Fuel Efficiency",
    "text": "13.4 Automobile Fuel Efficiency\nLarge dataset analysis with cluster-robust standard errors.\n\nCase Study: Automobile Fuel Efficiency (1980-2006)\nThe Policy Question: What determines vehicle fuel efficiency, and why didn’t MPG improve more despite technological advances?\nEconomic Context:\nFrom 1980-2006, the U.S. auto industry faced: - 1970s oil shocks: Created demand for fuel efficiency - CAFE standards: Government regulations on average MPG - Consumer preferences: Growing demand for SUVs and trucks - Technological progress: Better engines, materials, aerodynamics\nYet average fuel economy stagnated. Why?\nThe Data: 26,995 vehicles sold in the U.S. market (1980-2006)\nKey Variables: - mpg: Miles per gallon (fuel efficiency) - curbwt: Vehicle weight in pounds - hp: Horsepower (engine power) - torque: Engine torque (pulling power) - year: Model year (captures technology trends) - mfr: Manufacturer (for clustering)\nThe Research Question:\nHow much does each factor contribute to fuel efficiency? - Weight vs power vs technology - Are efficiency gains “spent” on other attributes?\nMethodological Challenge:\nVehicles from the same manufacturer share: - Common technology platforms - Similar design philosophy - Shared engineering teams\nThis creates clustered errors → need cluster-robust standard errors!\nMethod: Log-log regression with cluster-robust SEs (by manufacturer)\n\n# Load automobile data\ndata_auto = pd.read_stata(GITHUB_DATA_URL + 'AED_AUTOSMPG.DTA')\nprint(f\"Loaded {len(data_auto)} vehicle observations (1980-2006)\")\nprint(f\"\\nKey variables: {['year', 'mfr', 'mpg', 'curbwt', 'hp', 'torque']}\")\nprint(f\"\\nNote: Dataset has pre-computed log transformations (lmpg, lcurbwt, lhp, ltorque)\")\nprint(data_auto[['year', 'mfr', 'mpg', 'curbwt', 'hp', 'torque']].head())\n\nLoaded 26995 vehicle observations (1980-2006)\n\nKey variables: ['year', 'mfr', 'mpg', 'curbwt', 'hp', 'torque']\n\nNote: Dataset has pre-computed log transformations (lmpg, lcurbwt, lhp, ltorque)\n     year   mfr        mpg  curbwt   hp      torque\n0  1980.0  FIAT  21.700001    2455  111  149.199997\n1  1980.0   AMC  22.400000    2807   90  176.300003\n2  1980.0   AMC  24.600000    2767   90  176.300003\n3  1980.0   AMC  20.400000    2856  110  284.799988\n4  1980.0   AMC  20.500000    2816  110  284.799988\n\n\n\n# Summary statistics\nkey_vars = ['mpg', 'curbwt', 'hp', 'torque', 'year']\nprint(\"=\"*70)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\"*70)\nprint(data_auto[key_vars].describe())\n\n# Manufacturer distribution\nprint(\"\\n\" + \"=\"*70)\nprint(\"TOP 10 MANUFACTURERS BY OBSERVATIONS\")\nprint(\"=\"*70)\nprint(data_auto['mfr'].value_counts().head(10))\n\n======================================================================\nSUMMARY STATISTICS\n======================================================================\n                mpg        curbwt            hp        torque          year\ncount  26995.000000  26995.000000  26995.000000  26995.000000  26995.000000\nmean      24.564611   3429.140396    158.517837    265.476135   1992.124512\nstd        6.707306    814.258673     67.215409    102.784401      8.043841\nmin        8.700000   1450.000000     48.000000     69.400002   1980.000000\n25%       19.700001   2815.000000    112.000000    181.699997   1985.000000\n50%       23.900000   3325.000000    147.000000    258.000000   1991.000000\n75%       28.100000   3975.000000    190.000000    326.399994   1999.000000\nmax       76.400002   6700.000000    660.000000   1001.000000   2006.000000\n\n======================================================================\nTOP 10 MANUFACTURERS BY OBSERVATIONS\n======================================================================\nmfr\nGMC           7979\nCHRYSLER      4172\nFORD          3709\nTOYOTA        1554\nNISSAN        1079\nVOLKSWAGEN     831\nHONDA          743\nBMW            740\nMITSUBISHI     685\nMAZDA          612\nName: count, dtype: int64\n\n\nKey Observations from Summary Statistics:\n\nSample size: 26,995 vehicles (1980-2006)\nTime span: 27 years of data\nMPG range: 8.7 to 76.4 (huge variation!)\nWeight: 1,450 to 6,700 lbs (compact cars to large trucks)\n\nTop manufacturers: General Motors, Ford, Chrysler dominate (this is U.S. market data)\nWhy log-log specification?\nWe use logs of both outcome (MPG) and predictors (weight, HP, torque) because:\n\nElasticity interpretation: Coefficients = percentage changes\n\nβ = -0.5 means: 1% ↑ in weight → 0.5% ↓ in MPG\n\nNonlinear relationships: MPG doesn’t change linearly with weight\n\nGoing from 2,000→2,100 lbs has different effect than 4,000→4,100 lbs\nLogs capture this naturally\n\nReduces heteroskedasticity: Log transformation stabilizes variance\n\n\n# Log-log regression with cluster-robust standard errors\n# Use pre-computed log variables\nmodel_auto = ols('lmpg ~ lhp + lcurbwt + ltorque + year', data=data_auto).fit(\n    cov_type='cluster',\n    cov_kwds={'groups': data_auto['mfr']}\n)\n\nprint(\"=\"*70)\nprint(\"LOG-LOG REGRESSION: FUEL EFFICIENCY\")\nprint(\"=\"*70)\nprint(model_auto.summary())\n\n# Interpretation\nprint(\"\\n\" + \"=\"*70)\nprint(\"ELASTICITY INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"Horsepower elasticity: {model_auto.params['lhp']:.3f}\")\nprint(f\"  → 1% increase in HP → {model_auto.params['lhp']:.2f}% change in MPG\")\nprint(f\"\\nWeight elasticity: {model_auto.params['lcurbwt']:.3f}\")\nprint(f\"  → 1% increase in weight → {model_auto.params['lcurbwt']:.2f}% change in MPG\")\nprint(f\"\\nTorque elasticity: {model_auto.params['ltorque']:.3f}\")\nprint(f\"  → 1% increase in torque → {model_auto.params['ltorque']:.2f}% change in MPG\")\nprint(f\"\\nYear trend: {model_auto.params['year']:.4f}\")\nprint(f\"  → Efficiency improves {model_auto.params['year']*100:.2f}% per year\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CLUSTER-ROBUST STANDARD ERRORS\")\nprint(\"=\"*70)\nprint(f\"Clustered by manufacturer (mfr)\")\nprint(f\"Number of clusters: {data_auto['mfr'].nunique()}\")\nprint(f\"Average observations per cluster: {len(data_auto)/data_auto['mfr'].nunique():.0f}\")\nprint(f\"\\nWhy cluster? Vehicles from same manufacturer likely have correlated errors\")\nprint(f\"due to common technology, design philosophy, and engineering teams.\")\n\n======================================================================\nLOG-LOG REGRESSION: FUEL EFFICIENCY\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   lmpg   R-squared:                       0.777\nModel:                            OLS   Adj. R-squared:                  0.777\nMethod:                 Least Squares   F-statistic:                     296.9\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           4.21e-28\nTime:                        09:47:23   Log-Likelihood:                 17857.\nNo. Observations:               26995   AIC:                        -3.570e+04\nDf Residuals:                   26990   BIC:                        -3.566e+04\nDf Model:                           4                                         \nCovariance Type:              cluster                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -20.8429      1.988    -10.484      0.000     -24.740     -16.946\nlhp           -0.1813      0.065     -2.786      0.005      -0.309      -0.054\nlcurbwt       -0.5624      0.057     -9.938      0.000      -0.673      -0.452\nltorque       -0.1415      0.064     -2.211      0.027      -0.267      -0.016\nyear           0.0152      0.001     13.840      0.000       0.013       0.017\n==============================================================================\nOmnibus:                     1290.146   Durbin-Watson:                   0.877\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4011.194\nSkew:                           0.180   Prob(JB):                         0.00\nKurtosis:                       4.854   Cond. No.                     7.29e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors are robust to cluster correlation (cluster)\n[2] The condition number is large, 7.29e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n======================================================================\nELASTICITY INTERPRETATION\n======================================================================\nHorsepower elasticity: -0.181\n  → 1% increase in HP → -0.18% change in MPG\n\nWeight elasticity: -0.562\n  → 1% increase in weight → -0.56% change in MPG\n\nTorque elasticity: -0.141\n  → 1% increase in torque → -0.14% change in MPG\n\nYear trend: 0.0152\n  → Efficiency improves 1.52% per year\n\n======================================================================\nCLUSTER-ROBUST STANDARD ERRORS\n======================================================================\nClustered by manufacturer (mfr)\nNumber of clusters: 39\nAverage observations per cluster: 692\n\nWhy cluster? Vehicles from same manufacturer likely have correlated errors\ndue to common technology, design philosophy, and engineering teams.\n\n\n\nKey Concept 13.6: Cluster-Robust Standard Errors for Grouped Data\nWhen observations are grouped (e.g., vehicles by manufacturer), errors within groups may be correlated. Cluster-robust standard errors account for this within-group correlation. Using default SEs when clustering exists understates uncertainty, leading to false rejections of the null hypothesis.\n\n\n\nInterpreting the Results\nKey Findings:\n\nWeight Elasticity: ≈ -0.56\n\n\n1% increase in vehicle weight → 0.56% decrease in MPG\nLargest effect among all variables\nHeavier vehicles are substantially less efficient\n\n\nHorsepower Elasticity: ≈ -0.30 to -0.35\n\n\n1% increase in HP → ~0.3% decrease in MPG\nMore powerful engines burn more fuel\n\n\nTorque Elasticity: ≈ -0.10 to -0.15\n\n\nSmaller effect than weight or HP\nTorque affects efficiency but less than other factors\n\n\nYear Trend: ≈ +0.01 to +0.02\n\n\nTechnology improves efficiency ~1-2% per year\nBut this is offset by increasing weight!\n\nThe Weight Paradox:\nDespite technological improvements: - Vehicles got heavier (better safety, more features) - Average MPG didn’t improve much from 1980-2005 - Efficiency gains were “spent” on weight/power instead of MPG\nWhy Cluster-Robust Standard Errors?\nWe cluster by manufacturer because: - Ford vehicles share common technology - Toyota vehicles share design philosophy - Errors within manufacturer are correlated\nNumber of clusters: 40+ manufacturers Average per cluster: ~600 vehicles\nClustering increases SEs → more conservative inference\nPolicy Implications:\n\nCAFE Standards: Corporate Average Fuel Economy regulations\n\n\nWeight matters more than technology\nEncouraging lighter vehicles could have large impact\n\n\nConsumer Trade-offs:\n\n\nSafety (heavier) vs Efficiency (lighter)\nPower (higher HP) vs MPG (lower HP)\nConsumers chose weight/power over efficiency\n\n\nRecent Trends (post-2005, not in data):\n\n\nHybrid/electric technology\nLightweight materials (aluminum, carbon fiber)\nSmaller turbocharged engines\n\nModel Quality:\nR² ≈ 0.78 means the model explains 78% of MPG variation - Excellent fit! - Weight, HP, torque are key predictors - But 22% remains unexplained (aerodynamics, transmission, etc.)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#rand-health-insurance-experiment-rct",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#rand-health-insurance-experiment-rct",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.5 Rand Health Insurance Experiment (RCT)",
    "text": "13.5 Rand Health Insurance Experiment (RCT)\nRandomized control trial for causal inference.\n\nCase Study: The RAND Health Insurance Experiment (Randomized Control Trial)\nThe Gold Standard for Causal Inference: Randomized Control Trials (RCTs)\nThe Policy Question: Does health insurance coverage affect medical spending?\nWhy This Matters:\nThis is the moral hazard question in health economics: - If insurance is free, do people overuse healthcare? - Should insurance have cost-sharing (deductibles, co-pays)? - What’s the right balance between access and efficiency?\nThe Challenge:\nObservational studies are biased because: - People who buy insurance are different (sicker? richer?) - Can’t separate insurance effect from selection effect - Reverse causation: health affects insurance choice\nThe RCT Solution:\nThe RAND Health Insurance Experiment (1974-1982): - Randomly assigned 5,809 individuals to different insurance plans - Plans varied in cost-sharing: 0%, 25%, 50%, 95%, individual deductible - Followed families for 3-5 years - Measured healthcare utilization and spending\nWhy Randomization Enables Causal Inference:\nRandom Assignment → Insurance Plan → Medical Spending\nBecause assignment is random: - Treatment and control groups are identical in expectation - No confounding variables - Any difference in outcomes is caused by insurance - Selection bias eliminated!\nData: Year 1 data (5,639 observations)\nVariables: - spending: Total medical expenditure - plan: Insurance plan assignment - Plan indicators: coins0 (free care), coins25, coins50, coins95, coinsmixed, coinsindiv\nMethod: Regression with cluster-robust SEs (by family)\n\n# Load health insurance experiment data\ndata_health = pd.read_stata(GITHUB_DATA_URL + 'AED_HEALTHINSEXP.DTA')\n\n# Use first year data only (as per textbook)\ndata_health_y1 = data_health[data_health['year'] == 1]\n\nprint(f\"Loaded {len(data_health)} total observations\")\nprint(f\"Using Year 1 only: {len(data_health_y1)} observations\")\nprint(f\"\\nInsurance plans: {sorted(data_health_y1['plan'].unique())}\")\nprint(f\"\\nKey variables:\")\nprint(f\"  - plan: Insurance plan assignment (randomized)\")\nprint(f\"  - spending: Total medical spending\")\nprint(f\"  - Plan indicators: coins0, coins25, coins50, coins95, coinsmixed, coinsindiv\")\n\n# Summary statistics by plan\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEAN SPENDING BY INSURANCE PLAN\")\nprint(\"=\"*70)\nspending_by_plan = data_health_y1.groupby('plan')['spending'].agg(['mean', 'std', 'count'])\nprint(spending_by_plan)\n\n# Regression with plan indicators\nmodel_rct = ols('spending ~ coins25 + coins50 + coins95 + coinsmixed + coinsindiv',\n                data=data_health_y1).fit(\n    cov_type='cluster',\n    cov_kwds={'groups': data_health_y1['idfamily']}\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RCT REGRESSION: SPENDING ON INSURANCE PLANS\")\nprint(\"=\"*70)\nprint(\"Omitted category: Free Care (coins0)\")\nprint(model_rct.summary())\n\n# F-test for joint significance\nprint(\"\\n\" + \"=\"*70)\nprint(\"JOINT F-TEST: DO PLANS MATTER?\")\nprint(\"=\"*70)\nhypotheses = 'coins25 = coins50 = coins95 = coinsmixed = coinsindiv = 0'\nftest = model_rct.f_test(hypotheses)\nprint(f\"H0: All plan coefficients = 0\")\nprint(f\"F-statistic: {ftest.fvalue:.2f}\")\nprint(f\"p-value: {ftest.pvalue:.4f}\")\nprint(f\"Conclusion: {'Reject H0' if ftest.pvalue &lt; 0.05 else 'Fail to reject H0'} at 5% level\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CAUSAL INTERPRETATION\")\nprint(\"=\"*70)\nprint(\"✓ Randomized Control Trial enables causal inference\")\nprint(\"✓ Random assignment eliminates selection bias\")\nprint(\"✓ Free care → highest spending (omitted baseline)\")\nprint(\"✓ Higher cost-sharing → lower spending\")\n\nLoaded 20203 total observations\nUsing Year 1 only: 5639 observations\n\nInsurance plans: ['25% Coins', '50% Coins', '95%/100% Coins', 'Free Care', 'Indv Deduct', 'Mixed Coins']\n\nKey variables:\n  - plan: Insurance plan assignment (randomized)\n  - spending: Total medical spending\n  - Plan indicators: coins0, coins25, coins50, coins95, coinsmixed, coinsindiv\n\n======================================================================\nMEAN SPENDING BY INSURANCE PLAN\n======================================================================\n                       mean           std  count\nplan                                            \nFree Care       2153.570365   4959.743126   1873\n25% Coins       1396.663070   3458.960029    639\nMixed Coins     1701.874211   4328.165674    480\n50% Coins       1785.845449  11643.607841    374\n95%/100% Coins  1045.820283   2634.823581   1057\nIndv Deduct     1607.071473   3815.953001   1216\n\n======================================================================\nRCT REGRESSION: SPENDING ON INSURANCE PLANS\n======================================================================\nOmitted category: Free Care (coins0)\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               spending   R-squared:                       0.007\nModel:                            OLS   Adj. R-squared:                  0.006\nMethod:                 Least Squares   F-statistic:                     11.39\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           7.46e-11\nTime:                        09:47:23   Log-Likelihood:                -55975.\nNo. Observations:                5639   AIC:                         1.120e+05\nDf Residuals:                    5633   BIC:                         1.120e+05\nDf Model:                           5                                         \nCovariance Type:              cluster                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   2153.5704    118.315     18.202      0.000    1921.678    2385.463\ncoins25     -756.9073    190.041     -3.983      0.000   -1129.381    -384.433\ncoins50     -367.7249    618.044     -0.595      0.552   -1579.069     843.619\ncoins95    -1107.7501    150.366     -7.367      0.000   -1402.462    -813.038\ncoinsmixed  -451.6962    239.424     -1.887      0.059    -920.959      17.567\ncoinsindiv  -546.4989    162.151     -3.370      0.001    -864.308    -228.690\n==============================================================================\nOmnibus:                    11610.083   Durbin-Watson:                   2.017\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         52697192.093\nSkew:                          17.125   Prob(JB):                         0.00\nKurtosis:                     475.345   Cond. No.                         5.48\n==============================================================================\n\nNotes:\n[1] Standard Errors are robust to cluster correlation (cluster)\n\n======================================================================\nJOINT F-TEST: DO PLANS MATTER?\n======================================================================\nH0: All plan coefficients = 0\nF-statistic: 11.39\np-value: 0.0000\nConclusion: Reject H0 at 5% level\n\n======================================================================\nCAUSAL INTERPRETATION\n======================================================================\n✓ Randomized Control Trial enables causal inference\n✓ Random assignment eliminates selection bias\n✓ Free care → highest spending (omitted baseline)\n✓ Higher cost-sharing → lower spending\n\n\n\nKey Concept 13.7: Randomized Control Trials as the Gold Standard\nIn a randomized control trial (RCT), subjects are randomly assigned to treatment and control groups. Random assignment ensures the groups are comparable on both observed and unobserved characteristics, eliminating selection bias and omitted variable concerns. The RAND experiment showed that better insurance coverage increases healthcare utilization.\n\n\n\nInterpreting the RCT Results\nKey Findings:\n\nInsurance Plans Matter: F-test strongly rejects H₀ (F = 11.39, p &lt; 0.001)\n\n\nDifferent insurance plans lead to significantly different spending\nCost-sharing reduces medical utilization\n\n\nSpending by Plan (relative to Free Care):\n\nFree Care (baseline): Highest spending - People use more healthcare when it’s free\nCost-sharing plans (25%, 50%, 95%): Lower spending - Higher cost-sharing → less utilization - This is moral hazard: insurance affects behavior\n\nWhy This is Causal:\n\nRandomization: Families randomly assigned to plans - No selection bias (unlike observational studies) - Treated and control groups are balanced on all characteristics\nExperimental control: RAND ensured compliance\nLarge sample: 5,639 person-years in first year\n\nR² is Low (0.007) - This is Actually Good!\n\n\nMost variation in spending is random (health shocks)\nInsurance explains small fraction (as expected)\nBut effects are statistically significant and policy-relevant\n\nPolicy Implications:\nTrade-offs in health insurance design:\nFree care: - More utilization - Better access for poor - But: higher costs, potential overuse\nCost-sharing: - Lower spending - Reduced “unnecessary” care - But: may also reduce necessary care (problematic!)\nThe RAND Health Insurance Experiment (HIE) Legacy:\nThis 1970s experiment fundamentally shaped U.S. health policy: - Informed Medicare/Medicaid design - Influenced Affordable Care Act - Demonstrated feasibility of large-scale RCTs - Showed moral hazard exists but is moderate\nLimitations:\n1970s data (healthcare has changed) Selected sites (may not generalize) Ethical concerns (denying some families full coverage)\nModern Relevance:\nThe fundamental trade-off remains: - Universal free care (equity) vs - Cost-sharing (efficiency)\nNow that we’ve seen experimental data in the RAND study, let’s explore quasi-experimental methods for causal inference.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#health-care-access-difference-in-differences",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#health-care-access-difference-in-differences",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.6 Health Care Access (Difference-in-Differences)",
    "text": "13.6 Health Care Access (Difference-in-Differences)\nCausal inference using DiD methodology.\n\nCase Study: Health Care Access and Child Nutrition (Difference-in-Differences)\nThe Policy Question: Does building health clinics improve child health outcomes?\nThe Setting: Rural South Africa, 1990s\nAfter apartheid ended (1994), the new government built primary health care clinics in underserved rural areas. Some communities got many new clinics (high treatment), others got few (low treatment).\nThe Challenge: Simple before/after comparison is biased because: - Child health was improving nationally due to many factors - Communities that got clinics may have been different - Cannot separate clinic effect from other trends\nThe Difference-in-Differences Solution:\nKey Idea: Compare change in treated communities to change in control communities\n\\[\\text{DiD} = (\\text{Treated}_{after} - \\text{Treated}_{before}) - (\\text{Control}_{after} - \\text{Control}_{before})\\]\nWhy This Works:\nThe first difference removes: - Permanent differences between communities - Baseline health levels\nThe second difference removes: - Common time trends - National-level changes\nWhat remains: The differential impact of clinic access!\nThe Design:\n\n\n\nGroup\n1993 (Pre)\n1998 (Post)\nChange\n\n\n\n\nHigh Treatment\n\\(\\bar{Y}_{T,0}\\)\n\\(\\bar{Y}_{T,1}\\)\n\\(\\Delta_T\\)\n\n\nLow Treatment\n\\(\\bar{Y}_{C,0}\\)\n\\(\\bar{Y}_{C,1}\\)\n\\(\\Delta_C\\)\n\n\n\nDiD = \\(\\Delta_T - \\Delta_C\\)\nData: 1,071 children aged 0-4 in rural KwaZulu-Natal\nOutcome: waz (weight-for-age z-score, measure of nutrition)\nMethod: DiD regression with cluster-robust SEs (by community)\n\n# Load health care access data (South Africa)\ndata_access = pd.read_stata(GITHUB_DATA_URL + 'AED_HEALTHACCESS.DTA')\n\nprint(f\"Loaded {len(data_access)} observations (South African children 0-4)\")\nprint(f\"\\nDifference-in-Differences Setup:\")\nprint(f\"  - Treatment: High treatment communities (hightreat=1)\")\nprint(f\"  - Control: Low treatment communities (hightreat=0)\")\nprint(f\"  - Pre period: 1993 (post=0)\")\nprint(f\"  - Post period: 1998 (post=1)\")\nprint(f\"  - Outcome: waz (weight-for-age z-score)\")\n\n# Summary statistics by treatment and time\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEAN WEIGHT-FOR-AGE Z-SCORE (WAZ)\")\nprint(\"=\"*70)\ndid_table = data_access.groupby(['hightreat', 'post'])['waz'].agg(['mean', 'count'])\nprint(did_table)\n\n# Calculate DiD manually\npre_control = data_access[(data_access['hightreat']==0) & (data_access['post']==0)]['waz'].mean()\npost_control = data_access[(data_access['hightreat']==0) & (data_access['post']==1)]['waz'].mean()\npre_treat = data_access[(data_access['hightreat']==1) & (data_access['post']==0)]['waz'].mean()\npost_treat = data_access[(data_access['hightreat']==1) & (data_access['post']==1)]['waz'].mean()\n\ndid_estimate = (post_treat - pre_treat) - (post_control - pre_control)\n\nprint(f\"\\nManual DiD calculation:\")\nprint(f\"  Control change: {post_control:.3f} - {pre_control:.3f} = {post_control - pre_control:.3f}\")\nprint(f\"  Treated change: {post_treat:.3f} - {pre_treat:.3f} = {post_treat - pre_treat:.3f}\")\nprint(f\"  DiD estimate: ({post_treat:.3f} - {pre_treat:.3f}) - ({post_control:.3f} - {pre_control:.3f}) = {did_estimate:.3f}\")\n\n# DiD regression\nmodel_did = ols('waz ~ hightreat + post + postXhigh', data=data_access).fit(\n    cov_type='cluster',\n    cov_kwds={'groups': data_access['idcommunity']}\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DiD REGRESSION\")\nprint(\"=\"*70)\nprint(model_did.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"DiD coefficient (postXhigh): {model_did.params['postXhigh']:.3f}\")\nprint(f\"Matches manual calculation: {did_estimate:.3f} ✓\")\nprint(f\"\\nCausal interpretation:\")\nprint(f\"Clinic access improved child nutrition by {model_did.params['postXhigh']:.2f} standard deviations\")\nprint(f\"This is a {'statistically significant' if model_did.pvalues['postXhigh'] &lt; 0.05 else 'not significant'} effect\")\nprint(f\"\\nCluster-robust SEs by community account for within-community correlation\")\n\nLoaded 1071 observations (South African children 0-4)\n\nDifference-in-Differences Setup:\n  - Treatment: High treatment communities (hightreat=1)\n  - Control: Low treatment communities (hightreat=0)\n  - Pre period: 1993 (post=0)\n  - Post period: 1998 (post=1)\n  - Outcome: waz (weight-for-age z-score)\n\n======================================================================\nMEAN WEIGHT-FOR-AGE Z-SCORE (WAZ)\n======================================================================\n                    mean  count\nhightreat post                 \n0.0       0.0  -0.414185    325\n          1.0  -0.069097    288\n1.0       0.0  -0.545244    246\n          1.0   0.321462    212\n\nManual DiD calculation:\n  Control change: -0.069 - -0.414 = 0.345\n  Treated change: 0.321 - -0.545 = 0.867\n  DiD estimate: (0.321 - -0.545) - (-0.069 - -0.414) = 0.522\n\n======================================================================\nDiD REGRESSION\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    waz   R-squared:                       0.040\nModel:                            OLS   Adj. R-squared:                  0.037\nMethod:                 Least Squares   F-statistic:                     9.090\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           5.93e-05\nTime:                        09:47:24   Log-Likelihood:                -1992.5\nNo. Observations:                1071   AIC:                             3993.\nDf Residuals:                    1067   BIC:                             4013.\nDf Model:                           3                                         \nCovariance Type:              cluster                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.4142      0.115     -3.597      0.000      -0.640      -0.189\nhightreat     -0.1311      0.197     -0.666      0.505      -0.517       0.255\npost           0.3451      0.137      2.517      0.012       0.076       0.614\npostXhigh      0.5216      0.235      2.217      0.027       0.060       0.983\n==============================================================================\nOmnibus:                       27.248   Durbin-Watson:                   1.855\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               31.523\nSkew:                           0.328   Prob(JB):                     1.43e-07\nKurtosis:                       3.524   Cond. No.                         6.32\n==============================================================================\n\nNotes:\n[1] Standard Errors are robust to cluster correlation (cluster)\n\n======================================================================\nINTERPRETATION\n======================================================================\nDiD coefficient (postXhigh): 0.522\nMatches manual calculation: 0.522 ✓\n\nCausal interpretation:\nClinic access improved child nutrition by 0.52 standard deviations\nThis is a statistically significant effect\n\nCluster-robust SEs by community account for within-community correlation\n\n\n\nKey Concept 13.8: Difference-in-Differences for Causal Inference\nDifference-in-differences (DiD) compares changes over time between treatment and control groups. The key assumption is parallel trends: both groups would have followed the same trajectory absent treatment. DiD removes time-invariant confounders and common time effects, isolating the treatment effect.\n\n\n\nInterpreting the DiD Results\nKey Findings:\n\nTreatment Effect: DiD coefficient ≈ 0.52 standard deviations\n\n\nClinic access improved child weight-for-age by 0.52 SD\nStatistically significant (p = 0.027 &lt; 0.05)\nLarge and meaningful effect size\n\n\nVerification: Manual calculation matches regression\n\n\nThis confirms our DiD estimate is correct\nThe interaction term postXhigh captures the treatment effect\n\n\nEffect Size Interpretation:\n\n0.52 SD improvement means: - Moving from 25th percentile → 45th percentile in weight distribution - Substantial reduction in malnutrition - Comparable to 6-12 months of normal growth\nWhy This is Causal (Under Assumptions):\nThe DiD design requires parallel trends assumption: - Without treatment, treated and control groups would have followed parallel trends - Treatment assignment (clinic placement) is exogenous - No other interventions differentially affected treated communities\nEvidence for validity: - Communities were similar at baseline - Clinic placement based on need, not outcomes - No other major health programs during this period\nPolicy Implications:\nPrimary health care access works: Even basic clinics improve child health\nTargeting matters: High-treatment communities saw larger benefits\nCost-effective: Clinic construction is relatively inexpensive\nExternal Validity:\nThese results from rural South Africa likely generalize to other: - Low-income settings - Areas with limited health infrastructure - Populations with high baseline malnutrition\nLimitations:\nShort-term effects (1998 vs 1993) Self-reported data (possible measurement error) Cluster-level treatment (less statistical power)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#political-incumbency-regression-discontinuity",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#political-incumbency-regression-discontinuity",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.7 Political Incumbency (Regression Discontinuity)",
    "text": "13.7 Political Incumbency (Regression Discontinuity)\nCausal inference using RD design.\n\nCase Study: The Incumbency Advantage in U.S. Senate Elections\nThe Question: Does being an incumbent senator give you an advantage in the next election?\nThe Challenge: Simple comparison of incumbents vs challengers is biased because: - Incumbents are generally stronger candidates (that’s why they won before!) - Districts that elect incumbents may be different - Selection bias confounds the causal effect\nThe Regression Discontinuity Solution:\nKey Insight: Compare candidates who barely won to those who barely lost.\nAt the threshold (vote margin ≈ 0): - Winners and losers are essentially identical in quality - Only difference: winners become incumbents - This creates quasi-random assignment to incumbency status\nThe Design:\nRunning variable: margin (vote share in election t - 50%)\nThreshold: margin = 0\nTreatment: win = 1 if margin &gt; 0\n\nOutcome: vote (vote share in election t+1)\nVisual Intuition:\nIf there’s an incumbency advantage, we should see a jump in next-election vote share exactly at margin = 0:\nNext Vote % ▲\n │\n 60─────┤ •\n │ • ← Jump = incumbency advantage\n 50─────┤ •\n │ •\n 40─────┤•\n │\n └──────────────────→\n -10 0 +10\n Lost │ Won\n Margin\nData: 1,390 U.S. Senate elections (1914-2010)\nMethod: RD with linear control for margin\n\n# Load incumbency data (U.S. Senate elections)\ndata_incumb = pd.read_stata(GITHUB_DATA_URL + 'AED_INCUMBENCY.DTA')\n\nprint(f\"Loaded {len(data_incumb)} Senate elections (1914-2010)\")\nprint(f\"\\nRegression Discontinuity Setup:\")\nprint(f\"  - Running variable: margin (vote margin in election t)\")\nprint(f\"  - Threshold: margin = 0 (barely won vs barely lost)\")\nprint(f\"  - Outcome: vote (vote share in election t+1)\")\nprint(f\"  - win: Indicator for margin &gt; 0\")\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*70)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\"*70)\nprint(data_incumb[['vote', 'margin', 'win']].describe())\n\n# Keep only elections with non-missing vote in t+1\ndata_rd = data_incumb[data_incumb['vote'].notna()].copy()\nprint(f\"\\nObservations with outcome data: {len(data_rd)}\")\n\n# RD regression (linear)\nmodel_rd = ols('vote ~ win + margin', data=data_rd).fit(cov_type='HC1')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"REGRESSION DISCONTINUITY ESTIMATION\")\nprint(\"=\"*70)\nprint(model_rd.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"INCUMBENCY ADVANTAGE\")\nprint(\"=\"*70)\nprint(f\"RD estimate (win coefficient): {model_rd.params['win']:.3f}\")\nprint(f\"95% CI: [{model_rd.conf_int().loc['win', 0]:.3f}, {model_rd.conf_int().loc['win', 1]:.3f}]\")\nprint(f\"\\nInterpretation:\")\nprint(f\"Barely winning vs barely losing increases vote share in next election by {model_rd.params['win']:.1f}%\")\nprint(f\"This is the causal effect of incumbency\")\nprint(f\"\\nWhy causal? At the threshold (margin≈0), winning is quasi-random\")\nprint(f\"Candidates just above/below threshold are similar in all respects except incumbency status\")\n\n# Visualization note\nprint(\"\\n\" + \"=\"*70)\nprint(\"RD PLOT\")\nprint(\"=\"*70)\nprint(\"To visualize discontinuity:\")\nprint(\"  - Bin observations by margin\")\nprint(\"  - Plot mean vote in next election vs margin\")\nprint(\"  - Should see jump at margin=0\")\n\nLoaded 1390 Senate elections (1914-2010)\n\nRegression Discontinuity Setup:\n  - Running variable: margin (vote margin in election t)\n  - Threshold: margin = 0 (barely won vs barely lost)\n  - Outcome: vote (vote share in election t+1)\n  - win: Indicator for margin &gt; 0\n\n======================================================================\nSUMMARY STATISTICS\n======================================================================\n              vote       margin          win\ncount  1297.000000  1390.000000  1390.000000\nmean     52.666275     7.171158     0.539568\nstd      18.122190    34.324879     0.498612\nmin       0.000000  -100.000000     0.000000\n25%      42.671333   -12.205826     0.000000\n50%      50.547523     2.165648     1.000000\n75%      61.348957    22.766067     1.000000\nmax     100.000000   100.000000     1.000000\n\nObservations with outcome data: 1297\n\n======================================================================\nREGRESSION DISCONTINUITY ESTIMATION\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   vote   R-squared:                       0.578\nModel:                            OLS   Adj. R-squared:                  0.577\nMethod:                 Least Squares   F-statistic:                     591.4\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):          3.71e-183\nTime:                        09:47:24   Log-Likelihood:                -5037.8\nNo. Observations:                1297   AIC:                         1.008e+04\nDf Residuals:                    1294   BIC:                         1.010e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     47.3308      0.526     89.945      0.000      46.299      48.362\nwin            4.7846      0.860      5.565      0.000       3.099       6.470\nmargin         0.3481      0.017     20.422      0.000       0.315       0.381\n==============================================================================\nOmnibus:                       69.616   Durbin-Watson:                   1.821\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              164.216\nSkew:                          -0.304   Prob(JB):                     2.19e-36\nKurtosis:                       4.634   Cond. No.                         111.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n======================================================================\nINCUMBENCY ADVANTAGE\n======================================================================\nRD estimate (win coefficient): 4.785\n95% CI: [3.099, 6.470]\n\nInterpretation:\nBarely winning vs barely losing increases vote share in next election by 4.8%\nThis is the causal effect of incumbency\n\nWhy causal? At the threshold (margin≈0), winning is quasi-random\nCandidates just above/below threshold are similar in all respects except incumbency status\n\n======================================================================\nRD PLOT\n======================================================================\nTo visualize discontinuity:\n  - Bin observations by margin\n  - Plot mean vote in next election vs margin\n  - Should see jump at margin=0\n\n\n\nKey Concept 13.9: Regression Discontinuity Design\nRegression discontinuity (RD) exploits sharp cutoffs in treatment assignment. Units just above and below the threshold are nearly identical in all respects except treatment status, creating a quasi-experiment. In U.S. Senate elections, barely winning provides an incumbency advantage of approximately 5–7 percentage points.\n\n\n\nInterpreting the RD Results\nKey Findings:\n\nIncumbency Advantage: ≈ 4.8 percentage points\n\n\nBarely winning vs barely losing increases vote share by ~4.8% in next election\nThis is highly statistically significant (p &lt; 0.001)\nRobust across different specifications\n\n\nWhy This is Causal:\n\nAt the threshold (margin ≈ 0), winning is quasi-random: - Candidates just above and below 50% are nearly identical - Only difference is incumbency status - No selection bias (unlike comparing all winners vs all losers)\n\nMagnitude in Context:\n\n\n4.8% advantage is substantial in close elections\nMany Senate races decided by &lt; 5%\nCould explain why incumbents rarely lose\n\nWhat Drives the Incumbency Advantage?\nPossible mechanisms: - Name recognition: Incumbents are well-known - Fundraising: Easier to raise money as incumbent - Media coverage: Senators get more press - Constituent services: Can deliver benefits to voters - Experience: Learn how to campaign effectively\nPolicy Implications:\n\nIncumbency advantage creates barriers to electoral competition\nMay reduce democratic accountability\nTerm limits could level the playing field\nCampaign finance reform might reduce fundraising advantage\n\nRD Design Advantages:\nNo need to control for other variables (identification at threshold) Transparent and credible Robust to model specification\nLimitations:\nLocal estimate (only applies near threshold) May not generalize to landslide winners/losers Assumes no manipulation of vote margin (reasonable for U.S. Senate)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#institutions-and-gdp-instrumental-variables",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#institutions-and-gdp-instrumental-variables",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.8 Institutions and GDP (Instrumental Variables)",
    "text": "13.8 Institutions and GDP (Instrumental Variables)\nCausal inference using IV/2SLS.\n\nCase Study: Do Institutions Cause Economic Growth?\nThe Fundamental Question: Why are some countries rich and others poor?\nThe Endogeneity Problem:\nIf we simply regress GDP on institutions quality, we get: - Reverse causation: Rich countries can afford better institutions - Omitted variables: Culture, geography, history all affect both - Measurement error: How do we measure “institutions quality”?\nAll of these bias OLS estimates, making causal inference impossible.\nThe Instrumental Variables Solution:\nAcemoglu, Johnson, and Robinson (2001) use settler mortality as an instrument:\nThe Historical Argument: 1. In the 1600s-1800s, European colonizers faced different disease environments 2. High mortality areas (Africa, tropical Americas) → extractive institutions - Europeans didn’t settle permanently - Built institutions to extract resources (slavery, forced labor) 3. Low mortality areas (North America, Australia) → settler institutions - Europeans settled permanently - Built institutions to protect their own property rights\nWhy This Works as an Instrument:\nRelevant: Settler mortality strongly predicts colonial institutions Exogenous: Malaria in 1700s doesn’t directly affect GDP in 2000s\nData: 64 former colonies\nVariables: - logpgp95: Log GDP per capita 1995 (outcome) - avexpr: Protection against expropriation (institutions quality, 0-10) - logem4: Log settler mortality rate (instrument)\nMethod: Two-Stage Least Squares (2SLS/IV)\n\n# Load institutions data (cross-country)\ndata_inst = pd.read_stata(GITHUB_DATA_URL + 'AED_INSTITUTIONS.DTA')\n\nprint(f\"Loaded {len(data_inst)} countries\")\nprint(f\"\\nInstrumental Variables Setup:\")\nprint(f\"  - Outcome: logpgp95 (log GDP per capita 1995)\")\nprint(f\"  - Endogenous regressor: avexpr (institutions quality)\")\nprint(f\"  - Instrument: logem4 (log settler mortality)\")\nprint(f\"\\nKey idea: Settler mortality affected colonial institutions,\")\nprint(f\"which persist to affect GDP today, but mortality doesn't\")\nprint(f\"directly affect modern GDP\")\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*70)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\"*70)\nprint(data_inst[['logpgp95', 'avexpr', 'logem4']].describe())\n\n# OLS (biased - endogeneity problem)\nmodel_ols = ols('logpgp95 ~ avexpr', data=data_inst).fit(cov_type='HC1')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OLS REGRESSION (BIASED)\")\nprint(\"=\"*70)\nprint(model_ols.summary())\nprint(f\"\\nOLS coefficient: {model_ols.params['avexpr']:.3f}\")\nprint(\"⚠️ This is biased due to endogeneity (omitted variables, reverse causation)\")\n\n# First stage\nmodel_first = ols('avexpr ~ logem4', data=data_inst).fit(cov_type='HC1')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FIRST STAGE: INSTITUTIONS ~ SETTLER MORTALITY\")\nprint(\"=\"*70)\nprint(model_first.summary())\nprint(f\"\\nFirst stage F-statistic: {model_first.fvalue:.2f}\")\nprint(f\"Rule of thumb: F &gt; 10 for strong instrument\")\nprint(f\"Instrument strength: {'Strong ✓' if model_first.fvalue &gt; 10 else 'Weak ⚠️'}\")\n\n# 2SLS manually (for pedagogy)\nprint(\"\\n\" + \"=\"*70)\nprint(\"TWO-STAGE LEAST SQUARES (2SLS)\")\nprint(\"=\"*70)\n\n# Predicted institutions from first stage\ndata_inst['avexpr_hat'] = model_first.fittedvalues\n\n# Second stage (using predicted values)\nmodel_second = ols('logpgp95 ~ avexpr_hat', data=data_inst).fit(cov_type='HC1')\n\nprint(\"\\nSecond stage:\")\nprint(model_second.summary())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON: OLS vs IV\")\nprint(\"=\"*70)\nprint(f\"OLS coefficient: {model_ols.params['avexpr']:.3f}\")\nprint(f\"IV/2SLS coefficient: {model_second.params['avexpr_hat']:.3f}\")\nprint(f\"\\nDifference: {model_second.params['avexpr_hat'] - model_ols.params['avexpr']:.3f}\")\nprint(f\"\\nIV estimate is larger → OLS has attenuation bias\")\nprint(f\"(measurement error and omitted variables bias OLS toward zero)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CAUSAL INTERPRETATION\")\nprint(\"=\"*70)\nprint(f\"1-unit improvement in institutions → {model_second.params['avexpr_hat']:.2f} increase in log GDP\")\nprint(f\"Exponentiating: {np.exp(model_second.params['avexpr_hat']):.2f}x increase in GDP level\")\nprint(f\"\\n✓ This is a causal estimate (under IV assumptions)\")\nprint(f\"✓ Instrument (settler mortality) is:\")\nprint(f\"  - Relevant: Strong first stage (F = {model_first.fvalue:.1f})\")\nprint(f\"  - Exogenous: Mortality in 1700s doesn't directly affect modern GDP\")\n\nLoaded 64 countries\n\nInstrumental Variables Setup:\n  - Outcome: logpgp95 (log GDP per capita 1995)\n  - Endogenous regressor: avexpr (institutions quality)\n  - Instrument: logem4 (log settler mortality)\n\nKey idea: Settler mortality affected colonial institutions,\nwhich persist to affect GDP today, but mortality doesn't\ndirectly affect modern GDP\n\n======================================================================\nSUMMARY STATISTICS\n======================================================================\n        logpgp95     avexpr     logem4\ncount  64.000000  64.000000  64.000000\nmean    8.062237   6.515625   4.657031\nstd     1.043359   1.468647   1.257983\nmin     6.109248   3.500000   2.145931\n25%     7.299728   5.613636   4.232656\n50%     7.949796   6.477273   4.358630\n75%     8.848779   7.352273   5.519177\nmax    10.215740  10.000000   7.986165\n\n======================================================================\nOLS REGRESSION (BIASED)\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               logpgp95   R-squared:                       0.540\nModel:                            OLS   Adj. R-squared:                  0.533\nMethod:                 Least Squares   F-statistic:                     109.4\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           2.57e-15\nTime:                        09:47:24   Log-Likelihood:                -68.168\nNo. Observations:                  64   AIC:                             140.3\nDf Residuals:                      62   BIC:                             144.7\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.6604      0.320     14.558      0.000       4.033       5.288\navexpr         0.5221      0.050     10.458      0.000       0.424       0.620\n==============================================================================\nOmnibus:                        7.098   Durbin-Watson:                   2.064\nProb(Omnibus):                  0.029   Jarque-Bera (JB):                6.657\nSkew:                          -0.781   Prob(JB):                       0.0358\nKurtosis:                       3.234   Cond. No.                         31.2\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nOLS coefficient: 0.522\n⚠️ This is biased due to endogeneity (omitted variables, reverse causation)\n\n======================================================================\nFIRST STAGE: INSTITUTIONS ~ SETTLER MORTALITY\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 avexpr   R-squared:                       0.270\nModel:                            OLS   Adj. R-squared:                  0.258\nMethod:                 Least Squares   F-statistic:                     16.32\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           0.000150\nTime:                        09:47:24   Log-Likelihood:                -104.83\nNo. Observations:                  64   AIC:                             213.7\nDf Residuals:                      62   BIC:                             218.0\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      9.3414      0.704     13.264      0.000       7.961      10.722\nlogem4        -0.6068      0.150     -4.040      0.000      -0.901      -0.312\n==============================================================================\nOmnibus:                        0.035   Durbin-Watson:                   2.003\nProb(Omnibus):                  0.983   Jarque-Bera (JB):                0.172\nSkew:                           0.045   Prob(JB):                        0.918\nKurtosis:                       2.763   Cond. No.                         19.4\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nFirst stage F-statistic: 16.32\nRule of thumb: F &gt; 10 for strong instrument\nInstrument strength: Strong ✓\n\n======================================================================\nTWO-STAGE LEAST SQUARES (2SLS)\n======================================================================\n\nSecond stage:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               logpgp95   R-squared:                       0.477\nModel:                            OLS   Adj. R-squared:                  0.469\nMethod:                 Least Squares   F-statistic:                     61.66\nDate:                Wed, 28 Jan 2026   Prob (F-statistic):           7.14e-11\nTime:                        09:47:24   Log-Likelihood:                -72.268\nNo. Observations:                  64   AIC:                             148.5\nDf Residuals:                      62   BIC:                             152.9\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.9097      0.757      2.523      0.012       0.426       3.393\navexpr_hat     0.9443      0.120      7.852      0.000       0.709       1.180\n==============================================================================\nOmnibus:                       10.547   Durbin-Watson:                   2.137\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               11.010\nSkew:                          -0.790   Prob(JB):                      0.00407\nKurtosis:                       4.277   Cond. No.                         58.1\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n======================================================================\nCOMPARISON: OLS vs IV\n======================================================================\nOLS coefficient: 0.522\nIV/2SLS coefficient: 0.944\n\nDifference: 0.422\n\nIV estimate is larger → OLS has attenuation bias\n(measurement error and omitted variables bias OLS toward zero)\n\n======================================================================\nCAUSAL INTERPRETATION\n======================================================================\n1-unit improvement in institutions → 0.94 increase in log GDP\nExponentiating: 2.57x increase in GDP level\n\n✓ This is a causal estimate (under IV assumptions)\n✓ Instrument (settler mortality) is:\n  - Relevant: Strong first stage (F = 16.3)\n  - Exogenous: Mortality in 1700s doesn't directly affect modern GDP\n\n\n\nKey Concept 13.10: Instrumental Variables and Two-Stage Least Squares\nWhen a regressor is endogenous (correlated with the error term), OLS is biased. Instrumental variables (IV) use an instrument that is correlated with the endogenous regressor but not directly with the outcome. Two-stage least squares (2SLS) first predicts the endogenous variable, then uses these predictions in the second-stage regression.\n\n\n\nInterpreting the IV Results\nKey Findings:\n\nOLS vs IV Comparison:\n\n\nOLS coefficient: ≈ 0.52 (biased downward)\nIV coefficient: ≈ 0.94 (causal estimate)\nIV estimate is 80% larger than OLS!\n\n\nWhy the Difference?\n\nAttenuation bias: OLS is biased toward zero due to: - Measurement error in institutions quality - Omitted variables (culture, geography) - Reverse causation (rich countries build better institutions)\n\nEconomic Interpretation:\n\n\n1-unit improvement in institutions → 0.94 increase in log GDP\nExponentiating: \\(e^{0.94} \\approx 2.56\\) → 156% increase in GDP level\nThis is a massive effect!\n\n\nInstrument Validity:\n\nRelevance: First stage F = 16.3 &gt; 10 (strong instrument)\nExogeneity: Settler mortality in 1700s doesn’t directly affect modern GDP\nThe instrument works through this channel: Settler mortality → Colonial institutions → Modern institutions → GDP\nHigh mortality areas got “extractive” institutions (exploitation). Low mortality areas got “settler” institutions (property rights, democracy).\nPolicy Implications:\nInstitutions are not just correlated with prosperity—they cause it. Countries can escape poverty by: - Strengthening property rights - Reducing corruption - Improving rule of law - Building democratic accountability\nCaveat: These results apply to former colonies. Institutional change is slow and difficult.\nHaving applied multiple causal inference strategies, we now turn to the practical foundation of any analysis — data preparation and cleaning.",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#from-raw-data-to-final-data",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#from-raw-data-to-final-data",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "13.9 From Raw Data to Final Data",
    "text": "13.9 From Raw Data to Final Data\nBest practices for data preparation and cleaning.\n\nWhy Data Wrangling Matters\nReal-world econometric analysis spends 60-80% of time on data preparation: - Reading data from various sources - Merging multiple datasets - Cleaning errors and outliers - Transforming variables - Creating new variables\nCommon Data Challenges:\n\nMissing values: How to handle? Drop, impute, or model explicitly?\nOutliers: Real extreme values or data errors?\nInconsistent units: Converting currencies, adjusting for inflation\nDifferent time frequencies: Monthly data merged with quarterly\nMeasurement error: Misreported values, typos\n\nBest Practices:\n\nDocument everything: Keep track of all cleaning steps\nPreserve raw data: Never overwrite original files\nReproducibility: Write scripts (not manual Excel edits!)\nValidation: Check summary statistics before and after cleaning\nTransparency: Report how many observations dropped and why\n\n\n# Demonstrate reading different file formats\nprint(\"=\"*70)\nprint(\"DATA READING EXAMPLES\")\nprint(\"=\"*70)\n\n# Stata files\nprint(\"\\n1. Reading Stata files (.dta):\")\nprint(\"   data = pd.read_stata('file.dta')\")\n\n# CSV files\nprint(\"\\n2. Reading CSV files:\")\nprint(\"   data = pd.read_csv('file.csv')\")\n\n# Excel files\nprint(\"\\n3. Reading Excel files:\")\nprint(\"   data = pd.read_excel('file.xlsx')\")\n\n# Example: merge operations\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA MERGING EXAMPLE\")\nprint(\"=\"*70)\n\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'value_a': [10, 20, 30]})\ndf2 = pd.DataFrame({'id': [1, 2, 4], 'value_b': [100, 200, 400]})\n\nprint(\"\\nDataFrame 1:\")\nprint(df1)\nprint(\"\\nDataFrame 2:\")\nprint(df2)\n\nmerged = pd.merge(df1, df2, on='id', how='inner')\nprint(\"\\nMerged (inner join):\")\nprint(merged)\n\n======================================================================\nDATA READING EXAMPLES\n======================================================================\n\n1. Reading Stata files (.dta):\n   data = pd.read_stata('file.dta')\n\n2. Reading CSV files:\n   data = pd.read_csv('file.csv')\n\n3. Reading Excel files:\n   data = pd.read_excel('file.xlsx')\n\n======================================================================\nDATA MERGING EXAMPLE\n======================================================================\n\nDataFrame 1:\n   id  value_a\n0   1       10\n1   2       20\n2   3       30\n\nDataFrame 2:\n   id  value_b\n0   1      100\n1   2      200\n2   4      400\n\nMerged (inner join):\n   id  value_a  value_b\n0   1       10      100\n1   2       20      200\n\n\n\n# Data cleaning examples\nprint(\"=\"*70)\nprint(\"DATA CLEANING EXAMPLES\")\nprint(\"=\"*70)\n\n# Example dataset\ndf_dirty = pd.DataFrame({\n    'age': [25, 30, -5, 200, 35],\n    'income': [50000, 60000, None, 75000, 80000],\n    'gender': ['M', 'F', 'm', 'Female', 'M']\n})\n\nprint(\"\\nOriginal data (with errors):\")\nprint(df_dirty)\n\n# Clean age (remove impossible values)\ndf_clean = df_dirty.copy()\ndf_clean.loc[df_clean['age'] &lt; 0, 'age'] = np.nan\ndf_clean.loc[df_clean['age'] &gt; 120, 'age'] = np.nan\n\n# Fill missing income with median\ndf_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n\n# Standardize gender coding\ndf_clean['gender'] = df_clean['gender'].str.upper().str[0]\n\nprint(\"\\nCleaned data:\")\nprint(df_clean)\n\nprint(\"\\n✓ Key cleaning steps:\")\nprint(\"  1. Detect and handle impossible values (age &lt; 0, age &gt; 120)\")\nprint(\"  2. Impute missing values (median for income)\")\nprint(\"  3. Standardize categorical variables (gender)\")\n\n======================================================================\nDATA CLEANING EXAMPLES\n======================================================================\n\nOriginal data (with errors):\n   age   income  gender\n0   25  50000.0       M\n1   30  60000.0       F\n2   -5      NaN       m\n3  200  75000.0  Female\n4   35  80000.0       M\n\nCleaned data:\n    age   income gender\n0  25.0  50000.0      M\n1  30.0  60000.0      F\n2   NaN  67500.0      M\n3   NaN  75000.0      F\n4  35.0  80000.0      M\n\n✓ Key cleaning steps:\n  1. Detect and handle impossible values (age &lt; 0, age &gt; 120)\n  2. Impute missing values (median for income)\n  3. Standardize categorical variables (gender)",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#key-takeaways",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#key-takeaways",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSchool Performance and Socioeconomic Factors (Case Study 13.1)\n\nSchool performance (API) is strongly associated with socioeconomic factors, particularly parent education\nBivariate analysis shows ~80 API points per year of parent education\nMultiple regression maintains a strong effect (~74 points) after controlling for meals, English learners, year-round schools, and teacher quality\nHigh correlations among socioeconomic variables make it difficult to isolate individual effects\nCalifornia’s “similar schools” index controls for socioeconomic characteristics\n\n\n\nCobb-Douglas Production Function and Returns to Scale (Case Study 13.2)\n\nNatural logarithm transformation converts the nonlinear Cobb-Douglas model into a linear OLS form\nEstimated capital elasticity ≈ 0.23 and labor elasticity ≈ 0.81\nData support constant returns to scale (elasticities sum to 1.040, F-test p = 0.636)\nHAC-robust standard errors account for time series autocorrelation\nPredictions require retransformation bias correction when using log models\n\n\n\nPhillips Curve and Omitted Variables Bias (Case Study 13.3)\n\nOriginal Phillips curve showed a negative unemployment–inflation trade-off pre-1970\nThe relationship broke down post-1970 — a classic example of model misspecification\nAugmented Phillips curve adding expected inflation resolves the breakdown (coefficient ≈ 1)\nThe sign reversal demonstrates omitted variables bias: excluding a correlated variable biases coefficients\nPolicy implication: limited long-run unemployment–inflation trade-off\n\n\n\nAdvanced Causal Methods (Case Studies 13.4–13.8)\n\nLog-log models and cluster-robust SEs (13.4): Automobile efficiency gains offset by larger vehicles; clustering by manufacturer accounts for within-group correlation\nRandomized control trials (13.5): RAND experiment shows better insurance increases healthcare spending; random assignment eliminates selection bias\nDifference-in-differences (13.6): South Africa clinic access improved child health; key assumption is parallel trends\nRegression discontinuity (13.7): Political incumbency provides ~5–7% vote share advantage; exploits sharp cutoffs\nInstrumental variables (13.8): Better institutions causally increase GDP; instruments address endogeneity\n\n\n\nData Preparation and Practical Workflow (Case Study 13.9)\n\nReal data analysis requires extensive data carpentry — often 60–80% of project time\nReading data from multiple formats: .csv, .xlsx, .dta, .sav\nMerging, recoding, cleaning, and validation are essential before regression analysis\n\n\n\nGeneral Lessons from Multiple Regression Case Studies\n\nMultiple regression isolates partial effects while controlling for other variables\nChoice of standard errors is critical: HC1 (cross-section), cluster-robust (grouped), HAC (time series)\nLog transformations enable estimation of elasticities and nonlinear relationships\nOmitted variables bias can reverse coefficient signs and lead to incorrect conclusions\nCausal inference requires additional identification strategies beyond OLS\n\n\n\n\nPython Tools Used in This Chapter\n# Regression estimation\nstatsmodels.formula.api.ols()        # OLS regression\nmodel.fit(cov_type='HC1')            # Heteroskedastic-robust SEs\nmodel.fit(cov_type='HAC')            # HAC SEs for time series\nmodel.fit(cov_type='cluster')        # Cluster-robust SEs\n\n# Data handling\npd.read_stata(), pd.read_csv()       # Read data files\npd.merge()                           # Combine datasets\nnp.log()                             # Log transformations\n\n# Hypothesis testing\nmodel.f_test()                       # F-test for restrictions\nmodel.t_test()                       # t-test for coefficients\n\n# Visualization\nmatplotlib, seaborn                  # Plotting libraries\n\nNext Steps: - Chapter 14: Regression with Indicator Variables - Chapter 15: Regression with Transformed Variables\n\nCongratulations! You’ve completed nine comprehensive case studies demonstrating the breadth and power of multiple regression analysis — from school performance to macroeconomic policy to causal inference!",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#practice-exercises",
    "href": "../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html#practice-exercises",
    "title": "Chapter 13: Case Studies for Multiple Regression",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Interpreting Multiple Regression Coefficients\nA regression of school test scores on parent education yields a coefficient of 80. When meals (% free lunch), English learners, and teacher quality are added, the parent education coefficient drops to 74.\n(a) Why does the coefficient change when additional variables are included?\n(b) Does the change mean parent education is less important than initially thought? Explain.\n(c) What does the remaining coefficient of 74 represent in terms of partial effects?\n\nExercise 2: Log Transformation and Elasticities\nA Cobb-Douglas production function is estimated as: \\(\\ln Q = 1.45 + 0.23 \\ln K + 0.81 \\ln L\\)\n(a) Interpret the coefficient 0.23 in terms of percentage changes.\n(b) Interpret the coefficient 0.81 in terms of percentage changes.\n(c) Do these results suggest constant, increasing, or decreasing returns to scale? Conduct a formal test.\n\nExercise 3: Omitted Variables Bias\nA researcher estimates the effect of unemployment on inflation and finds a positive coefficient. When expected inflation is added, the unemployment coefficient becomes negative.\n(a) Using the OVB formula, explain why the coefficient changed sign.\n(b) What are the two conditions required for omitted variables bias to occur?\n(c) Is the augmented model more reliable? Why or why not?\n\nExercise 4: Choosing Standard Error Types\nFor each scenario, identify the appropriate type of standard errors and explain why:\n(a) Cross-sectional data on 500 firms with varying sizes (potential heteroskedasticity)\n(b) Panel data with 50 schools observed over 10 years (observations clustered by school)\n(c) Time series data on quarterly GDP growth for 30 years (potential serial correlation)\n\nExercise 5: Matching Causal Inference Methods\nMatch each research scenario with the most appropriate causal inference method (RCT, DiD, RD, IV) and state the key identifying assumption:\n(a) A new drug is tested by randomly assigning patients to treatment and placebo groups\n(b) A scholarship is awarded to students scoring above 80 on an entrance exam\n(c) A new minimum wage law is implemented in one state but not its neighbor\n(d) Historical settler mortality is used to explain current institutional quality\n\nExercise 6: Data Preparation Checklist\nYou receive a dataset containing country-level GDP, population, and education data from three different sources in .csv, .xlsx, and .dta formats.\n(a) List the steps you would take to prepare this data for regression analysis.\n(b) What potential issues should you check for when merging datasets from different sources?\n(c) Why is data validation important before running regressions?",
    "crumbs": [
      "Multiple Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Chapter 13: Case Studies for Multiple Regression</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to regression with indicator variables (also called dummy variables or categorical variables). All code runs directly in Google Colab without any local setup.\nThis chapter focuses on regression analysis when some regressors are indicator variables. Indicator variables are binary (0/1) variables that record whether an observation falls into a particular category.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#chapter-overview",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#chapter-overview",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "",
    "text": "What You’ll Learn\nBy the end of this chapter, you will be able to:\n\nUnderstand indicator (dummy) variables and their role in regression analysis\nInterpret regression coefficients when regressors are categorical variables\nUse indicator variables to compare group means and test for differences\nUnderstand the relationship between regression on indicators and t-tests/ANOVA\nIncorporate indicator variables alongside continuous regressors to control for categories\nCreate and interpret interaction terms between indicators and continuous variables\nApply the dummy variable trap rule when using sets of mutually exclusive indicators\nChoose appropriate base categories and interpret coefficients relative to the base\nConduct joint F-tests for the significance of sets of indicator variables\nApply indicator variable techniques to real earnings data\n\n\n\nChapter Outline\n\n14.1 Indicator Variables: Single Binary Variable\n14.2 Indicator Variable with Additional Regressors\n14.3 Interactions with Indicator Variables\n14.4 Testing for Structural Change\n14.5 Sets of Indicator Variables\nKey Takeaways – Chapter review and consolidated lessons\nPractice Exercises – Reinforce your understanding\nCase Studies – Apply indicator variables to cross-country data\n\nDataset used: - AED_EARNINGS_COMPLETE.DTA: 872 full-time workers aged 25-65 in 2000\nKey economic questions: - Is there a gender earnings gap? How large is it after controlling for education and experience? - How do the returns to education differ by gender? - Do earnings differ across types of workers (self-employed, private, government)? - Can we test for structural differences between groups?\nEstimated time: 90-120 minutes",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#setup",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#setup",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nfrom scipy.stats import f_oneway\nfrom statsmodels.stats.anova import anova_lm\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"=\" * 70)\nprint(\"CHAPTER 14: REGRESSION WITH INDICATOR VARIABLES\")\nprint(\"=\" * 70)\nprint(\"\\nSetup complete! Ready to explore regression with indicator variables.\")\n\n======================================================================\nCHAPTER 14: REGRESSION WITH INDICATOR VARIABLES\n======================================================================\n\nSetup complete! Ready to explore regression with indicator variables.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#data-preparation",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#data-preparation",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’ll work with the earnings dataset which contains information on 872 full-time workers.\nKey variables:\n\nearnings: Annual earnings in dollars\ngender: 1=female, 0=male\neducation: Years of schooling\nage: Age in years\nhours: Usual hours worked per week\ndself: 1=self-employed, 0=not\ndprivate: 1=private sector employee, 0=not\ndgovt: 1=government sector employee, 0=not\ngenderbyeduc: Gender × Education interaction\ngenderbyage: Gender × Age interaction\ngenderbyhours: Gender × Hours interaction\n\n\n# Load earnings data\ndata = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS_COMPLETE.DTA')\n\nprint(\"Data structure:\")\nprint(f\"  Observations: {len(data)}\")\nprint(f\"  Variables: {len(data.columns)}\")\n\nprint(\"\\nVariable descriptions:\")\nvariables = ['earnings', 'gender', 'education', 'genderbyeduc', 'age',\n             'genderbyage', 'hours', 'genderbyhours', 'dself', 'dprivate', 'dgovt']\n\nfor var in variables:\n    print(f\"  {var}: {data[var].dtype}\")\n\nprint(\"\\nSummary statistics:\")\nprint(data[variables].describe())\n\nprint(\"\\nNote on indicator variables:\")\nprint(\"  gender: 1=female, 0=male\")\nprint(\"  dself: 1=self-employed, 0=not\")\nprint(\"  dprivate: 1=private sector, 0=not\")\nprint(\"  dgovt: 1=government, 0=not\")\n\nData structure:\n  Observations: 872\n  Variables: 45\n\nVariable descriptions:\n  earnings: float32\n  gender: int8\n  education: float32\n  genderbyeduc: float32\n  age: int16\n  genderbyage: float32\n  hours: int8\n  genderbyhours: float32\n  dself: float32\n  dprivate: float32\n  dgovt: float32\n\nSummary statistics:\n            earnings      gender   education  genderbyeduc         age  \\\ncount     872.000000  872.000000  872.000000    872.000000  872.000000   \nmean    56368.691406    0.433486   13.853211      6.082569   43.310780   \nstd     51516.054688    0.495841    2.884141      7.172634   10.676045   \nmin      4000.000000    0.000000    0.000000      0.000000   25.000000   \n25%     29000.000000    0.000000   12.000000      0.000000   35.000000   \n50%     44200.000000    0.000000   13.000000      0.000000   44.000000   \n75%     64250.000000    1.000000   16.000000     13.000000   51.250000   \nmax    504000.000000    1.000000   20.000000     20.000000   65.000000   \n\n       genderbyage       hours  genderbyhours       dself    dprivate  \\\ncount   872.000000  872.000000     872.000000  872.000000  872.000000   \nmean     19.042431   44.342890      18.564220    0.090596    0.760321   \nstd      22.869757    8.499103      21.759789    0.287199    0.427132   \nmin       0.000000   35.000000       0.000000    0.000000    0.000000   \n25%       0.000000   40.000000       0.000000    0.000000    1.000000   \n50%       0.000000   40.000000       0.000000    0.000000    1.000000   \n75%      42.000000   48.000000      40.000000    0.000000    1.000000   \nmax      65.000000   99.000000      80.000000    1.000000    1.000000   \n\n            dgovt  \ncount  872.000000  \nmean     0.149083  \nstd      0.356374  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      0.000000  \nmax      1.000000  \n\nNote on indicator variables:\n  gender: 1=female, 0=male\n  dself: 1=self-employed, 0=not\n  dprivate: 1=private sector, 0=not\n  dgovt: 1=government, 0=not",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#indicator-variables---single-binary-variable",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#indicator-variables---single-binary-variable",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "14.1: Indicator Variables - Single Binary Variable",
    "text": "14.1: Indicator Variables - Single Binary Variable\nAn indicator variable (also called dummy variable or binary variable) takes only two values:\n\\[d = \\begin{cases}\n1 & \\text{if in the category} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n\nSimple Regression on Single Indicator\nWhen we regress \\(y\\) on just an intercept and an indicator variable:\n\\[\\hat{y} = b + a \\cdot d\\]\nThe predicted value takes only two values:\n\\[\\hat{y}_i = \\begin{cases}\nb + a & \\text{if } d_i = 1 \\\\\nb & \\text{if } d_i = 0\n\\end{cases}\\]\nKey result: For OLS regression:\n\n\\(b = \\bar{y}_0\\) (mean of \\(y\\) when \\(d=0\\))\n\\(a = \\bar{y}_1 - \\bar{y}_0\\) (difference in means)\n\nInterpretation: The slope coefficient equals the difference in group means.\n\n\nExample: Earnings and Gender\nLet’s examine whether there’s a gender earnings gap.\n\nprint(\"=\" * 70)\nprint(\"14.1: REGRESSION ON SINGLE INDICATOR VARIABLE\")\nprint(\"=\" * 70)\n\n# Summary statistics by gender\nprint(\"\\nTable 14.1: Earnings by Gender\")\nprint(\"-\" * 70)\n\nprint(\"\\nFemale (gender=1):\")\nfemale_stats = data[data['gender'] == 1]['earnings'].describe()\nprint(female_stats)\n\nprint(\"\\nMale (gender=0):\")\nmale_stats = data[data['gender'] == 0]['earnings'].describe()\nprint(male_stats)\n\n# Calculate means\nmean_female = data[data['gender'] == 1]['earnings'].mean()\nmean_male = data[data['gender'] == 0]['earnings'].mean()\ndiff_means = mean_female - mean_male\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Difference in Means\")\nprint(\"-\" * 70)\nprint(f\"  Mean earnings (Female): ${mean_female:,.2f}\")\nprint(f\"  Mean earnings (Male): ${mean_male:,.2f}\")\nprint(f\"  Difference: ${diff_means:,.2f}\")\nprint(f\"\\n  Interpretation: Females earn ${abs(diff_means):,.2f} less than males on average.\")\n\n======================================================================\n14.1: REGRESSION ON SINGLE INDICATOR VARIABLE\n======================================================================\n\nTable 14.1: Earnings by Gender\n----------------------------------------------------------------------\n\nFemale (gender=1):\ncount       378.000000\nmean      47079.894531\nstd       31596.724609\nmin        4000.000000\n25%       27475.000000\n50%       41000.000000\n75%       56000.000000\nmax      322000.000000\nName: earnings, dtype: float64\n\nMale (gender=0):\ncount       494.000000\nmean      63476.316406\nstd       61713.210938\nmin        5000.000000\n25%       30000.000000\n50%       48000.000000\n75%       75000.000000\nmax      504000.000000\nName: earnings, dtype: float64\n\n----------------------------------------------------------------------\nDifference in Means\n----------------------------------------------------------------------\n  Mean earnings (Female): $47,079.89\n  Mean earnings (Male): $63,476.32\n  Difference: $-16,396.42\n\n  Interpretation: Females earn $16,396.42 less than males on average.\n\n\n\n\nOLS Regression: Earnings on Gender\nNow let’s estimate the regression model:\n\\[\\text{earnings} = \\beta_1 + \\alpha \\cdot \\text{gender} + u\\]\nWe’ll use heteroskedasticity-robust standard errors (HC1) for valid inference.\n\nprint(\"=\"*70)\nprint(\"REGRESSION MODELS: Gender and Earnings\")\nprint(\"=\"*70)\n\n# Model 1: Gender only\nprint(\"\\nModel 1: earnings ~ gender\")\nprint(\"-\"*70)\nmodel1 = ols('earnings ~ gender', data=data).fit(cov_type='HC1')\nprint(model1.summary())\n\nprint(f\"\\nInterpretation:\")\nprint(f\"  Intercept: ${model1.params['Intercept']:,.2f} (mean for males)\")\nprint(f\"  Gender coefficient: ${model1.params['gender']:,.2f} (difference for females)\")\nprint(f\"  Females earn ${abs(model1.params['gender']):,.2f} less than males on average\")\n\n======================================================================\nREGRESSION MODELS: Gender and Earnings\n======================================================================\n\nModel 1: earnings ~ gender\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.025\nModel:                            OLS   Adj. R-squared:                  0.024\nMethod:                 Least Squares   F-statistic:                     25.97\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           4.25e-07\nTime:                        14:36:53   Log-Likelihood:                -10687.\nNo. Observations:                 872   AIC:                         2.138e+04\nDf Residuals:                     870   BIC:                         2.139e+04\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   6.348e+04   2776.983     22.858      0.000     5.8e+04    6.89e+04\ngender      -1.64e+04   3217.429     -5.096      0.000   -2.27e+04   -1.01e+04\n==============================================================================\nOmnibus:                      785.072   Durbin-Watson:                   2.014\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            25163.521\nSkew:                           4.078   Prob(JB):                         0.00\nKurtosis:                      28.021   Cond. No.                         2.49\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nInterpretation:\n  Intercept: $63,476.32 (mean for males)\n  Gender coefficient: $-16,396.42 (difference for females)\n  Females earn $16,396.42 less than males on average\n\n\n\nKey Concept 14.1: Indicator Variables and Difference in Means\nWhen regressing \\(y\\) on just an intercept and a single indicator \\(d\\), the fitted model is \\(\\hat{y} = b + ad\\). The intercept \\(b\\) equals the mean of \\(y\\) when \\(d=0\\), and the slope \\(a\\) equals the difference in means \\((\\bar{y}_1 - \\bar{y}_0)\\). Thus, regression on an indicator variable is equivalent to a difference-in-means test.\n\n\n\n\nUnderstanding the Gender Earnings Gap\nThe regression results reveal a statistically significant gender earnings gap of approximately -$16,000. Let’s break down what this means:\nKey Findings:\n\nIntercept (\\(b_1\\)) ≈ $68,000: This represents the mean earnings for males (when gender = 0). We can verify this matches the actual mean earnings for males in the sample.\nGender coefficient (\\(\\alpha\\)) ≈ -$16,000: This is the difference in mean earnings between females and males. Specifically:\n\n\nFemales earn approximately $16,000 less than males on average\nThis is the unconditional (raw) gender gap - it doesn’t account for differences in education, experience, or other factors\n\n\nStatistical Significance: The t-statistic is highly significant (p &lt; 0.001), meaning we can confidently reject the null hypothesis that there’s no gender difference in earnings.\n\nImportant Interpretation: - This regression simply decomposes the sample into two groups and compares their means - The coefficient on gender equals the difference: \\(\\bar{y}_{female} - \\bar{y}_{male}\\) - This is a descriptive finding, not necessarily causal - the gap may reflect differences in education, occupation, hours worked, discrimination, or other factors - To understand the adjusted gender gap (controlling for observable characteristics), we need to add additional regressors\nConnection to t-test: - This regression is mathematically equivalent to a two-sample t-test - The regression framework allows us to easily extend the model by adding control variables\n\n\nComparison with t-test\nRegression with a single indicator variable is equivalent to a two-sample t-test.\nTwo approaches:\n\nWelch’s t-test (unequal variances): Similar to regression with robust SEs\nClassical t-test (equal variances): Identical to regression with default SEs\n\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Comparison with t-tests\")\nprint(\"=\" * 70)\n\n# Extract earnings by gender\nfemale_earnings = data[data['gender'] == 1]['earnings']\nmale_earnings = data[data['gender'] == 0]['earnings']\n\n# Welch's t-test (unequal variances)\nprint(\"\\n1. Welch's t-test (unequal variances):\")\nprint(\"-\" * 70)\nt_stat_welch, p_value_welch = stats.ttest_ind(female_earnings, male_earnings, equal_var=False)\nprint(f\"  t-statistic: {t_stat_welch:.4f}\")\nprint(f\"  p-value: {p_value_welch:.6f}\")\nprint(f\"  Note: Similar to regression with robust SEs\")\n\n# Classical t-test (equal variances)\nprint(\"\\n2. Classical t-test (equal variances):\")\nprint(\"-\" * 70)\nt_stat_classical, p_value_classical = stats.ttest_ind(female_earnings, male_earnings, equal_var=True)\nprint(f\"  t-statistic: {t_stat_classical:.4f}\")\nprint(f\"  p-value: {p_value_classical:.6f}\")\n\n# Regression with default SEs\nprint(\"\\n3. Regression with default (homoskedastic) SEs:\")\nprint(\"-\" * 70)\nmodel_gender_default = ols('earnings ~ gender', data=data).fit()\nprint(f\"  t-statistic: {model_gender_default.tvalues['gender']:.4f}\")\nprint(f\"  p-value: {model_gender_default.pvalues['gender']:.6f}\")\nprint(f\"  Note: IDENTICAL to classical t-test with equal variances\")\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Key insight:\")\nprint(\"  - Classical t-test = Regression with default SEs (assuming equal variances)\")\nprint(\"  - Welch's t-test ≈ Regression with robust SEs (allowing unequal variances)\")\n\n\n======================================================================\nComparison with t-tests\n======================================================================\n\n1. Welch's t-test (unequal variances):\n----------------------------------------------------------------------\n  t-statistic: -5.0964\n  p-value: 0.000000\n  Note: Similar to regression with robust SEs\n\n2. Classical t-test (equal variances):\n----------------------------------------------------------------------\n  t-statistic: -4.7139\n  p-value: 0.000003\n\n3. Regression with default (homoskedastic) SEs:\n----------------------------------------------------------------------\n  t-statistic: -4.7139\n  p-value: 0.000003\n  Note: IDENTICAL to classical t-test with equal variances\n\n----------------------------------------------------------------------\nKey insight:\n  - Classical t-test = Regression with default SEs (assuming equal variances)\n  - Welch's t-test ≈ Regression with robust SEs (allowing unequal variances)\n\n\nHaving established the equivalence between regression on a single indicator and difference-in-means tests, we now add continuous control variables.\n\nKey Concept 14.2: Regression vs. Specialized Test Methods\nSpecialized difference-in-means methods (like Welch’s t-test) and regression on an indicator give the same point estimate but slightly different standard errors. Regression uses \\(se(\\hat{a})\\) from the model, while the t-test uses \\(se(\\bar{y}_1 - \\bar{y}_0) = \\sqrt{s_1^2/n_1 + s_0^2/n_0}\\). Both are valid; regression is more flexible when adding control variables.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#indicator-variable-with-additional-regressors",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#indicator-variable-with-additional-regressors",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "14.2: Indicator Variable with Additional Regressors",
    "text": "14.2: Indicator Variable with Additional Regressors\nThe raw difference in earnings by gender may be partly explained by other factors (e.g., education, experience, hours worked).\n\nModel with Additional Regressors\n\\[y = \\beta_1 + \\beta_2 x + \\alpha d + u\\]\nThe fitted model:\n\\[\\hat{y}_i = \\begin{cases}\nb_1 + b_2 x_i + a & \\text{if } d_i = 1 \\\\\nb_1 + b_2 x_i & \\text{if } d_i = 0\n\\end{cases}\\]\nInterpretation: The coefficient \\(a\\) measures the difference in \\(y\\) across categories after controlling for the additional variables.\n\n\nProgressive Model Building\nWe’ll estimate five models of increasing complexity:\n\nGender only\nGender + Education\nGender + Education + (Gender × Education)\nAdd Age and Hours controls\nFull interactions with all variables\n\n\nprint(\"=\"*70)\nprint(\"PROGRESSIVE MODEL BUILDING\")\nprint(\"=\"*70)\n\n# Model 2: Gender + Education\nprint(\"\\nModel 2: earnings ~ gender + education\")\nprint(\"-\"*70)\nmodel2 = ols('earnings ~ gender + education', data=data).fit(cov_type='HC1')\nprint(model2.summary())\n\n# Model 3: Gender + Education + Interaction\nprint(\"\\nModel 3: earnings ~ gender + education + genderbyeduc\")\nprint(\"-\"*70)\nmodel3 = ols('earnings ~ gender + education + genderbyeduc', data=data).fit(cov_type='HC1')\nprint(model3.summary())\n\n# Model 4: Add Age and Hours\nprint(\"\\nModel 4: earnings ~ gender + education + genderbyeduc + age + hours\")\nprint(\"-\"*70)\nmodel4 = ols('earnings ~ gender + education + genderbyeduc + age + hours', data=data).fit(cov_type='HC1')\nprint(model4.summary())\n\n# Model 5: Full interactions\nprint(\"\\nModel 5: Full interactions\")\nprint(\"-\"*70)\nmodel5 = ols('earnings ~ gender + education + genderbyeduc + age + genderbyage + hours + genderbyhours',\n             data=data).fit(cov_type='HC1')\nprint(model5.summary())\n\n# Joint F-test for all gender terms\nprint(\"\\n\" + \"=\"*70)\nprint(\"Joint F-Test: All Gender Terms\")\nprint(\"=\"*70)\nhypotheses = '(gender = 0, genderbyeduc = 0, genderbyage = 0, genderbyhours = 0)'\nf_test = model5.wald_test(hypotheses, use_f=True)\nprint(f_test)\n\nprint(\"\\nAll gender effects (level and interactions) are highly significant.\")\n\n======================================================================\nPROGRESSIVE MODEL BUILDING\n======================================================================\n\nModel 2: earnings ~ gender + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.134\nModel:                            OLS   Adj. R-squared:                  0.132\nMethod:                 Least Squares   F-statistic:                     41.77\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           4.76e-18\nTime:                        14:36:53   Log-Likelihood:                -10635.\nNo. Observations:                 872   AIC:                         2.128e+04\nDf Residuals:                     869   BIC:                         2.129e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -1.755e+04   7993.547     -2.196      0.028   -3.32e+04   -1884.555\ngender     -1.826e+04   3136.142     -5.822      0.000   -2.44e+04   -1.21e+04\neducation   5907.2905    654.080      9.031      0.000    4625.318    7189.263\n==============================================================================\nOmnibus:                      811.995   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            29566.906\nSkew:                           4.250   Prob(JB):                         0.00\nKurtosis:                      30.231   Cond. No.                         70.5\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nModel 3: earnings ~ gender + education + genderbyeduc\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.140\nModel:                            OLS   Adj. R-squared:                  0.137\nMethod:                 Least Squares   F-statistic:                     31.92\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.41e-19\nTime:                        14:36:53   Log-Likelihood:                -10632.\nNo. Observations:                 872   AIC:                         2.127e+04\nDf Residuals:                     868   BIC:                         2.129e+04\nDf Model:                           3                                         \nCovariance Type:                  HC1                                         \n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept    -3.145e+04   1.18e+04     -2.655      0.008   -5.47e+04   -8233.577\ngender        2.022e+04   1.54e+04      1.317      0.188   -9876.802    5.03e+04\neducation     6920.6396    946.138      7.315      0.000    5066.244    8775.036\ngenderbyeduc -2764.8902   1168.325     -2.367      0.018   -5054.764    -475.016\n==============================================================================\nOmnibus:                      804.075   Durbin-Watson:                   2.069\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            28615.739\nSkew:                           4.192   Prob(JB):                         0.00\nKurtosis:                      29.782   Cond. No.                         175.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nModel 4: earnings ~ gender + education + genderbyeduc + age + hours\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.198\nModel:                            OLS   Adj. R-squared:                  0.193\nMethod:                 Least Squares   F-statistic:                     23.32\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           5.06e-22\nTime:                        14:36:53   Log-Likelihood:                -10602.\nNo. Observations:                 872   AIC:                         2.122e+04\nDf Residuals:                     866   BIC:                         2.124e+04\nDf Model:                           5                                         \nCovariance Type:                  HC1                                         \n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept    -1.073e+05   2.22e+04     -4.830      0.000   -1.51e+05   -6.38e+04\ngender        1.902e+04    1.5e+04      1.269      0.205   -1.04e+04    4.84e+04\neducation     6416.7991    886.124      7.241      0.000    4680.028    8153.570\ngenderbyeduc -2456.2914   1123.174     -2.187      0.029   -4657.673    -254.910\nage            525.9931    145.006      3.627      0.000     241.786     810.201\nhours         1324.5141    352.548      3.757      0.000     633.533    2015.495\n==============================================================================\nOmnibus:                      766.410   Durbin-Watson:                   2.041\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            23777.927\nSkew:                           3.935   Prob(JB):                         0.00\nKurtosis:                      27.341   Cond. No.                         736.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nModel 5: Full interactions\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.203\nModel:                            OLS   Adj. R-squared:                  0.196\nMethod:                 Least Squares   F-statistic:                     21.25\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.61e-26\nTime:                        14:36:53   Log-Likelihood:                -10599.\nNo. Observations:                 872   AIC:                         2.121e+04\nDf Residuals:                     864   BIC:                         2.125e+04\nDf Model:                           7                                         \nCovariance Type:                  HC1                                         \n=================================================================================\n                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept     -1.204e+05   2.88e+04     -4.175      0.000   -1.77e+05   -6.39e+04\ngender         5.713e+04   3.19e+04      1.790      0.073   -5427.457     1.2e+05\neducation      6314.6730    878.615      7.187      0.000    4592.620    8036.726\ngenderbyeduc  -2123.5113   1096.425     -1.937      0.053   -4272.464      25.441\nage             549.4750    227.683      2.413      0.016     103.225     995.725\ngenderbyage     -49.3326    270.258     -0.183      0.855    -579.028     480.363\nhours          1620.8142    503.869      3.217      0.001     633.248    2608.380\ngenderbyhours  -929.5746    554.247     -1.677      0.094   -2015.878     156.729\n==============================================================================\nOmnibus:                      753.384   Durbin-Watson:                   2.044\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            22238.965\nSkew:                           3.850   Prob(JB):                         0.00\nKurtosis:                      26.512   Cond. No.                     1.29e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 1.29e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n======================================================================\nJoint F-Test: All Gender Terms\n======================================================================\n&lt;F test: F=array([[8.14208145]]), p=1.9073757410610187e-06, df_denom=864, df_num=4&gt;\n\nAll gender effects (level and interactions) are highly significant.\n\n\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/statsmodels/base/model.py:1912: FutureWarning: The behavior of wald_test will change after 0.14 to returning scalar test statistic values. To get the future behavior now, set scalar to True. To silence this message while retaining the legacy behavior, set scalar to False.\n  warnings.warn(\n\n\n\nKey Concept 14.3: Interaction Terms Between Indicators and Continuous Variables\nAn interacted indicator variable is the product of an indicator and a continuous regressor, such as \\(d \\times x\\). In the model \\(y = \\beta_1 + \\beta_2 x + \\alpha_1 d + \\alpha_2(d \\times x) + u\\), the coefficient \\(\\alpha_2\\) measures how the slope on \\(x\\) differs between groups. Including only \\(d\\) (without interaction) shifts the intercept but keeps slopes parallel. Adding the interaction allows both intercepts and slopes to vary.\n\n\n\n\nHow the Gender Gap Changes with Controls\nComparing the five models reveals how the estimated gender gap evolves as we add controls and interactions:\nModel Evolution:\n\nModel 1 (Gender only): Gap = -$16,000\n\n\nThis is the raw, unconditional gender earnings gap\nIgnores all other factors that might explain earnings differences\n\n\nModel 2 (+ Education): Gap shrinks to approximately -$10,000\n\n\nAdding education as a control reduces the gender coefficient by ~$6,000\nInterpretation: Part of the raw gap is explained by gender differences in education levels\nThe remaining -$10,000 is the gap conditional on education\n\n\nModel 3 (+ Gender × Education):\n\n\nNow gender enters through two channels: the main effect AND the interaction\nThe main gender coefficient becomes the gap when education = 0 (not very meaningful)\nThe interaction coefficient shows whether returns to education differ by gender\nJoint F-test is crucial: Test both coefficients together to assess overall gender effects\n\n\nModel 4 (+ Age, Hours): Further controls\n\n\nAdding age and hours worked provides more refined estimates\nThese are important determinants of earnings that may differ by gender\nGap continues to shrink as we account for more observable differences\n\n\nModel 5 (Full Interactions): Most flexible specification\n\n\nAllows gender to affect the intercept AND slopes of all variables\nTests whether returns to education, age, and hours differ by gender\nJoint F-test on all 4 gender terms is highly significant\n\nKey Insights:\n\nThe gender gap decreases substantially when we control for education, age, and hours worked\nFrom -$16,000 (unconditional) to approximately -$5,000 to -$8,000 (conditional)\nThis suggests observable characteristics explain part, but not all of the gap\nThe remaining gap could reflect:\nUnmeasured factors (experience, occupation, industry)\nDiscrimination\nSelection effects (e.g., women choosing lower-paying fields)\n\nStatistical Lesson: - With interactions, individual t-tests can be misleading due to multicollinearity - Joint F-tests are essential for testing overall significance of a variable that enters through multiple terms",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#interactions-with-indicator-variables",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#interactions-with-indicator-variables",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "14.3: Interactions with Indicator Variables",
    "text": "14.3: Interactions with Indicator Variables\nAn interacted indicator variable is the product of an indicator variable and another regressor.\n\\[y = \\beta_1 + \\beta_2 x + \\alpha_1 d + \\alpha_2 (d \\times x) + u\\]\nThis allows both intercept AND slope to differ by category:\n\\[\\hat{y} = \\begin{cases}\n(b_1 + a_1) + (b_2 + a_2) x & \\text{if } d = 1 \\\\\nb_1 + b_2 x & \\text{if } d = 0\n\\end{cases}\\]\nInterpretation:\n\n\\(a_1\\): Difference in intercepts (gender gap at \\(x=0\\))\n\\(a_2\\): Difference in slopes (how returns to \\(x\\) differ by gender)\n\n\nTesting Gender Effects\nWhen gender enters through both a level term and interactions, we need joint F-tests to test overall significance.\n\n\n\nInterpreting Interaction Effects\nThe interaction term (Gender × Education) captures whether the returns to education differ by gender. Let’s unpack what the results tell us:\nModel with Interaction (Model 5):\nThe full model allows both the intercept and slope to differ by gender:\n\\[\\text{earnings} = \\begin{cases}\n(\\beta_1 + \\alpha_1) + (\\beta_2 + \\alpha_2)\\cdot\\text{education} + \\beta_3\\cdot\\text{age} + \\beta_4\\cdot\\text{hours} & \\text{if female} \\\\\n\\beta_1 + \\beta_2\\cdot\\text{education} + \\beta_3\\cdot\\text{age} + \\beta_4\\cdot\\text{hours} & \\text{if male}\n\\end{cases}\\]\nKey Coefficients:\n\nGender × Education Interaction (typically negative, around -$1,000 to -$2,000):\n\n\nInterpretation: The return to one additional year of education is approximately $1,000-$2,000 lower for women than for men\nExample: If \\(\\beta_{education} = 6,000\\) and \\(\\beta_{gender \\times educ} = -1,500\\):\nMales: Each year of education → +$6,000 in earnings\nFemales: Each year of education → +$4,500 in earnings\nThis suggests women get lower financial returns from education investments\n\n\nGender × Age and Gender × Hours:\n\n\nSimilarly capture whether age and hours have different effects by gender\nAllow the life-cycle earnings profile to differ by gender\n\nAvoiding the Dummy Variable Trap:\nNotice we cannot include all indicators plus an intercept. In the worker type example: - We have three categories: self-employed, private, government - We can only include two dummy variables if we have an intercept - The omitted category becomes the reference (base) group - Coefficients measure differences relative to the base\nWhich category to omit? - Your choice! Results are equivalent no matter which you drop - Choose the most natural reference category for interpretation - Example: Private sector is natural base for employment type comparisons\nJoint F-Test Results:\nThe joint F-tests show that: - All gender effects together are highly statistically significant (F ≈ 20-40, p &lt; 0.001) - Even though individual coefficients may have large standard errors (multicollinearity) - Gender matters for both level and slope of earnings relationships\n\n\nModel Comparison Table\nLet’s create a comprehensive comparison of all five models.\n\n# Summary table of all models\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary Table: All Five Models\")\nprint(\"=\" * 70)\n\nsummary_df = pd.DataFrame({\n    'Model 1': ['Gender only', model1.params.get('gender', np.nan),\n                model1.bse.get('gender', np.nan), model1.tvalues.get('gender', np.nan),\n                model1.nobs, model1.rsquared, model1.rsquared_adj, np.sqrt(model1.mse_resid)],\n    'Model 2': ['+ Education', model2.params.get('gender', np.nan),\n                model2.bse.get('gender', np.nan), model2.tvalues.get('gender', np.nan),\n                model2.nobs, model2.rsquared, model2.rsquared_adj, np.sqrt(model2.mse_resid)],\n    'Model 3': ['+ Gender×Educ', model3.params.get('gender', np.nan),\n                model3.bse.get('gender', np.nan), model3.tvalues.get('gender', np.nan),\n                model3.nobs, model3.rsquared, model3.rsquared_adj, np.sqrt(model3.mse_resid)],\n    'Model 4': ['+ Age, Hours', model4.params.get('gender', np.nan),\n                model4.bse.get('gender', np.nan), model4.tvalues.get('gender', np.nan),\n                model4.nobs, model4.rsquared, model4.rsquared_adj, np.sqrt(model4.mse_resid)],\n    'Model 5': ['Full Interact', model5.params.get('gender', np.nan),\n                model5.bse.get('gender', np.nan), model5.tvalues.get('gender', np.nan),\n                model5.nobs, model5.rsquared, model5.rsquared_adj, np.sqrt(model5.mse_resid)]\n}, index=['Description', 'Gender Coef', 'Robust SE', 't-stat', 'N', 'R²', 'Adj R²', 'RMSE'])\n\nprint(summary_df.to_string())\n\nprint(\"\\nKey observations:\")\nprint(\"  - Gender coefficient magnitude changes as we add controls\")\nprint(\"  - R² increases with additional variables\")\nprint(\"  - Interactions capture differential effects across groups\")\n\n\n======================================================================\nSummary Table: All Five Models\n======================================================================\n                  Model 1       Model 2        Model 3       Model 4        Model 5\nDescription   Gender only   + Education  + Gender×Educ  + Age, Hours  Full Interact\nGender Coef -16396.423634 -18258.087589   20218.796285  19021.708451   57128.997253\nRobust SE     3217.429112   3136.141829   15355.179322  14994.357587   31917.144603\nt-stat          -5.096126     -5.821831       1.316741      1.268591       1.789916\nN                   872.0         872.0          872.0         872.0          872.0\nR²               0.024906      0.133961       0.139508      0.197852       0.202797\nAdj R²           0.023785      0.131968       0.136534       0.19322       0.196338\nRMSE         50899.716833  47996.599413   47870.196751  46272.189032   46182.684141\n\nKey observations:\n  - Gender coefficient magnitude changes as we add controls\n  - R² increases with additional variables\n  - Interactions capture differential effects across groups\n\n\nNow that we can model different slopes via interaction terms, let’s test whether the entire regression structure differs by group.\n\nKey Concept 14.4: Joint Significance Testing for Indicator Variables\nWhen an indicator and its interaction with another variable are both included, test their joint significance using an F-test rather than individual t-tests. The joint test \\(H_0: \\alpha_1 = 0, \\alpha_2 = 0\\) evaluates whether the categorical variable matters at all. Individual t-tests can be misleading when the indicator and interaction are correlated.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#testing-for-structural-change---separate-regressions",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#testing-for-structural-change---separate-regressions",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "14.4: Testing for Structural Change - Separate Regressions",
    "text": "14.4: Testing for Structural Change - Separate Regressions\nAn alternative to including interactions is to estimate separate regressions for each group.\nFemale regression: \\[\\text{earnings}_i = \\beta_1^F + \\beta_2^F \\text{education}_i + \\beta_3^F \\text{age}_i + \\beta_4^F \\text{hours}_i + u_i\\]\nMale regression: \\[\\text{earnings}_i = \\beta_1^M + \\beta_2^M \\text{education}_i + \\beta_3^M \\text{age}_i + \\beta_4^M \\text{hours}_i + u_i\\]\nThis allows ALL coefficients to differ by gender, not just those we interact.\n\nChow Test\nThe Chow test formally tests whether coefficients differ across groups:\n\\[H_0: \\beta^F = \\beta^M \\text{ (pooled model)} \\quad \\text{vs.} \\quad H_a: \\beta^F \\neq \\beta^M \\text{ (separate models)}\\]\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"14.4: TESTING FOR STRUCTURAL CHANGE\")\nprint(\"=\" * 70)\n\nprint(\"\\nSeparate Regressions by Gender\")\nprint(\"-\" * 70)\n\n# Female regression\nmodel_female = ols('earnings ~ education + age + hours',\n                   data=data[data['gender'] == 1]).fit(cov_type='HC1')\nprint(\"\\nFemale subsample:\")\nprint(f\"  N = {int(model_female.nobs)}\")\nprint(f\"  Intercept: {model_female.params['Intercept']:,.2f}\")\nprint(f\"  Education: {model_female.params['education']:,.2f}\")\nprint(f\"  Age: {model_female.params['age']:,.2f}\")\nprint(f\"  Hours: {model_female.params['hours']:,.2f}\")\nprint(f\"  R²: {model_female.rsquared:.4f}\")\n\n# Male regression\nmodel_male = ols('earnings ~ education + age + hours',\n                 data=data[data['gender'] == 0]).fit(cov_type='HC1')\nprint(\"\\nMale subsample:\")\nprint(f\"  N = {int(model_male.nobs)}\")\nprint(f\"  Intercept: {model_male.params['Intercept']:,.2f}\")\nprint(f\"  Education: {model_male.params['education']:,.2f}\")\nprint(f\"  Age: {model_male.params['age']:,.2f}\")\nprint(f\"  Hours: {model_male.params['hours']:,.2f}\")\nprint(f\"  R²: {model_male.rsquared:.4f}\")\n\n# Compare coefficients\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Comparison of Coefficients\")\nprint(\"-\" * 70)\n\ncomparison = pd.DataFrame({\n    'Female': model_female.params,\n    'Male': model_male.params,\n    'Difference': model_female.params - model_male.params\n})\nprint(comparison)\n\nprint(\"\\nKey findings:\")\nprint(f\"  - Returns to education: ${model_female.params['education']:,.0f} (F) vs ${model_male.params['education']:,.0f} (M)\")\nprint(f\"  - Returns to age: ${model_female.params['age']:,.0f} (F) vs ${model_male.params['age']:,.0f} (M)\")\nprint(f\"  - Returns to hours: ${model_female.params['hours']:,.0f} (F) vs ${model_male.params['hours']:,.0f} (M)\")\n\n\n======================================================================\n14.4: TESTING FOR STRUCTURAL CHANGE\n======================================================================\n\nSeparate Regressions by Gender\n----------------------------------------------------------------------\n\nFemale subsample:\n  N = 378\n  Intercept: -63,302.57\n  Education: 4,191.16\n  Age: 500.14\n  Hours: 691.24\n  R²: 0.1746\n\nMale subsample:\n  N = 494\n  Intercept: -120,431.57\n  Education: 6,314.67\n  Age: 549.47\n  Hours: 1,620.81\n  R²: 0.1840\n\n----------------------------------------------------------------------\nComparison of Coefficients\n----------------------------------------------------------------------\n                 Female           Male    Difference\nIntercept -63302.571150 -120431.568403  57128.997253\neducation   4191.161688    6314.673007  -2123.511319\nage          500.142383     549.474999    -49.332616\nhours        691.239562    1620.814163   -929.574601\n\nKey findings:\n  - Returns to education: $4,191 (F) vs $6,315 (M)\n  - Returns to age: $500 (F) vs $549 (M)\n  - Returns to hours: $691 (F) vs $1,621 (M)\n\n\n\nKey Concept 14.5: Testing for Structural Change\nRunning separate regressions for each group allows all coefficients to differ simultaneously. The Chow test evaluates whether pooling the data (imposing the same coefficients for both groups) is justified. If the F-test rejects the null, the relationship between \\(y\\) and \\(x\\) differs fundamentally across groups – not just in intercept or one slope, but throughout the model.\n\n\nprint(\"=\"*70)\nprint(\"SETS OF INDICATOR VARIABLES: Worker Type\")\nprint(\"=\"*70)\n\n# Approach 1: Include intercept, drop one indicator (dprivate as reference)\nprint(\"\\nApproach 1: Intercept + dself + dgovt (dprivate is reference)\")\nprint(\"-\"*70)\nmodel_worker1 = ols('earnings ~ dself + dgovt + education + age', data=data).fit(cov_type='HC1')\nprint(model_worker1.summary())\n\n# Approach 2: Drop intercept, include all indicators\nprint(\"\\nApproach 2: No intercept + all dummies (dself + dprivate + dgovt)\")\nprint(\"-\"*70)\nmodel_worker2 = ols('earnings ~ dself + dprivate + dgovt + education + age - 1', data=data).fit(cov_type='HC1')\nprint(model_worker2.summary())\n\n# Approach 3: Different reference category (dself as reference)\nprint(\"\\nApproach 3: Intercept + dprivate + dgovt (dself is reference)\")\nprint(\"-\"*70)\nmodel_worker3 = ols('earnings ~ dprivate + dgovt + education + age', data=data).fit(cov_type='HC1')\nprint(model_worker3.summary())\n\nprint(\"\\n\" + \"-\"*70)\nprint(\"Key Insight:\")\nprint(\"-\"*70)\nprint(f\"  All three models have IDENTICAL R²: {model_worker1.rsquared:.4f}\")\nprint(f\"  Only the interpretation changes (different reference category)\")\nprint(f\"  Fitted values and residuals are the same across all three\")\n\n======================================================================\nSETS OF INDICATOR VARIABLES: Worker Type\n======================================================================\n\nApproach 1: Intercept + dself + dgovt (dprivate is reference)\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.125\nModel:                            OLS   Adj. R-squared:                  0.121\nMethod:                 Least Squares   F-statistic:                     22.12\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           2.10e-17\nTime:                        14:36:53   Log-Likelihood:                -10640.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     867   BIC:                         2.131e+04\nDf Model:                           4                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.725e+04   1.14e+04     -4.153      0.000   -6.95e+04   -2.49e+04\ndself        1.71e+04   9341.980      1.830      0.067   -1212.099    3.54e+04\ndgovt      -2025.0753   3098.983     -0.653      0.513   -8098.971    4048.821\neducation   5865.1923    652.217      8.993      0.000    4586.871    7143.514\nage          487.6022    149.648      3.258      0.001     194.298     780.906\n==============================================================================\nOmnibus:                      810.096   Durbin-Watson:                   2.074\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            29725.758\nSkew:                           4.230   Prob(JB):                         0.00\nKurtosis:                      30.323   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nApproach 2: No intercept + all dummies (dself + dprivate + dgovt)\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.125\nModel:                            OLS   Adj. R-squared:                  0.121\nMethod:                 Least Squares   F-statistic:                       nan\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):                nan\nTime:                        14:36:53   Log-Likelihood:                -10640.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     867   BIC:                         2.131e+04\nDf Model:                           4                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ndself      -3.015e+04   1.31e+04     -2.295      0.022   -5.59e+04   -4397.634\ndprivate   -4.725e+04   1.14e+04     -4.153      0.000   -6.95e+04   -2.49e+04\ndgovt      -4.927e+04   1.23e+04     -4.002      0.000   -7.34e+04   -2.51e+04\neducation   5865.1923    652.217      8.993      0.000    4586.871    7143.514\nage          487.6022    149.648      3.258      0.001     194.298     780.906\n==============================================================================\nOmnibus:                      810.096   Durbin-Watson:                   2.074\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            29725.758\nSkew:                           4.230   Prob(JB):                         0.00\nKurtosis:                      30.323   Cond. No.                         540.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nApproach 3: Intercept + dprivate + dgovt (dself is reference)\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.125\nModel:                            OLS   Adj. R-squared:                  0.121\nMethod:                 Least Squares   F-statistic:                     22.12\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           2.10e-17\nTime:                        14:36:53   Log-Likelihood:                -10640.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     867   BIC:                         2.131e+04\nDf Model:                           4                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -3.015e+04   1.31e+04     -2.295      0.022   -5.59e+04   -4397.634\ndprivate    -1.71e+04   9341.980     -1.830      0.067   -3.54e+04    1212.099\ndgovt      -1.912e+04   9585.615     -1.995      0.046   -3.79e+04    -335.462\neducation   5865.1923    652.217      8.993      0.000    4586.871    7143.514\nage          487.6022    149.648      3.258      0.001     194.298     780.906\n==============================================================================\nOmnibus:                      810.096   Durbin-Watson:                   2.074\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            29725.758\nSkew:                           4.230   Prob(JB):                         0.00\nKurtosis:                      30.323   Cond. No.                         366.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n----------------------------------------------------------------------\nKey Insight:\n----------------------------------------------------------------------\n  All three models have IDENTICAL R²: 0.1246\n  Only the interpretation changes (different reference category)\n  Fitted values and residuals are the same across all three\n\n\n\nKey Concept 14.6: The Dummy Variable Trap\nThe dummy variable trap occurs when including all \\(C\\) indicators from a set of mutually exclusive categories plus an intercept. Since \\(d_1 + d_2 + \\cdots + d_C = 1\\), this creates perfect multicollinearity – the intercept is an exact linear combination of the indicators. Solution: Drop one indicator (the “base category”) or drop the intercept. Standard practice is to keep the intercept and drop one indicator.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#sets-of-indicator-variables-multiple-categories",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#sets-of-indicator-variables-multiple-categories",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "14.5: Sets of Indicator Variables (Multiple Categories)",
    "text": "14.5: Sets of Indicator Variables (Multiple Categories)\nOften we have categorical variables with more than two categories.\nExample: Type of Worker\n\nSelf-employed (dself = 1)\nPrivate sector employee (dprivate = 1)\nGovernment employee (dgovt = 1)\n\nThese are mutually exclusive: each person falls into exactly one category.\n\nThe Dummy Variable Trap\nSince the three indicators sum to 1 (\\(d1 + d2 + d3 = 1\\)), we cannot include all three plus an intercept.\nSolution: Drop one indicator (the reference category or base category).\nThe coefficient of an included indicator measures the difference relative to the base category.\n\n\nThree Equivalent Approaches\n\nInclude intercept, drop one indicator (most common)\nDrop intercept, include all indicators (coefficients = group means)\nChange which indicator is dropped (changes interpretation, not fit)\n\n\n\n\nUnderstanding the Dummy Variable Trap\nThe results above demonstrate a fundamental principle in regression with categorical variables:\nThe Dummy Variable Trap Explained:\nWhen we have \\(k\\) mutually exclusive categories (e.g., 3 worker types), we face perfect multicollinearity: \\[d_{self} + d_{private} + d_{govt} = 1 \\text{ (for every observation)}\\]\nThis means one dummy is a perfect linear combination of the others plus the constant!\nThree Equivalent Solutions:\n\nInclude intercept, drop one dummy (Standard approach)\n\n\nIntercept = mean for the omitted (reference) category\nEach coefficient = difference from reference category\nMost common and easiest to interpret\n\n\nDrop intercept, include all dummies (No-constant model)\n\n\nEach coefficient = mean for that category\nNo reference category needed\nUseful when you want group means directly\n\n\nChange which dummy is dropped (Different reference)\n\n\nAll three models have identical fit (same R², predictions, residuals)\nOnly interpretation changes (different reference group)\nChoose based on what comparison is most meaningful\n\nEmpirical Results from Worker Type Example:\nFrom the three specifications above: - R² is identical across all specifications (≈ 0.16-0.17) - Joint F-tests give the same result (testing if worker type matters) - Only the individual coefficients change (but they measure different things)\nExample Interpretation:\nIf reference = Private sector: - \\(d_{self}\\) coefficient ≈ -$5,000: Self-employed earn $5,000 less than private sector - \\(d_{govt}\\) coefficient ≈ +$2,000: Government workers earn $2,000 more than private sector\nIf reference = Self-employed: - \\(d_{private}\\) coefficient ≈ +$5,000: Private sector earns $5,000 more than self-employed - \\(d_{govt}\\) coefficient ≈ +$7,000: Government workers earn $7,000 more than self-employed\nNotice: The difference between govt and private is the same in both (≈ $7,000 - $5,000 = $2,000)!\nPractical Advice: - Choose the largest or most common category as reference - Or choose the policy-relevant comparison (e.g., treatment vs. control) - Always clearly state which category is omitted - Report joint F-test for overall significance of the categorical variable\n\n\nANOVA: Testing Equality of Means Across Groups\nAnalysis of Variance (ANOVA) tests whether means differ across multiple groups.\n\\[H_0: \\mu_1 = \\mu_2 = \\mu_3 \\quad \\text{vs.} \\quad H_a: \\text{at least one mean differs}\\]\nANOVA is equivalent to an F-test in regression with indicator variables.\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANOVA: Testing Equality of Means Across Worker Types\")\nprint(\"=\" * 70)\n\n# Create categorical variable for worker type\ndata['typeworker'] = (1 * data['dself'] + 2 * data['dprivate'] + 3 * data['dgovt']).astype(int)\n\nprint(\"\\nMeans by worker type:\")\nmeans_by_type = data.groupby('typeworker')['earnings'].agg(['mean', 'std', 'count'])\nmeans_by_type.index = ['Self-employed', 'Private', 'Government']\nprint(means_by_type)\n\n# One-way ANOVA using scipy\nprint(\"\\n\" + \"-\" * 70)\nprint(\"One-way ANOVA (scipy)\")\nprint(\"-\" * 70)\n\ngroup1 = data[data['typeworker'] == 1]['earnings']\ngroup2 = data[data['typeworker'] == 2]['earnings']\ngroup3 = data[data['typeworker'] == 3]['earnings']\n\nf_stat_anova, p_value_anova = f_oneway(group1, group2, group3)\n\nprint(f\"  F-statistic: {f_stat_anova:.2f}\")\nprint(f\"  p-value: {p_value_anova:.6f}\")\n\nif p_value_anova &lt; 0.05:\n    print(f\"\\n  Result: Reject H₀ - Earnings differ significantly across worker types\")\nelse:\n    print(f\"\\n  Result: Fail to reject H₀ - No significant difference in earnings\")\n\n# Using statsmodels for detailed ANOVA table\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Detailed ANOVA table (statsmodels)\")\nprint(\"-\" * 70)\n\nmodel_anova = ols('earnings ~ C(typeworker)', data=data).fit()\nanova_table = anova_lm(model_anova, typ=2)\nprint(anova_table)\n\nprint(\"\\nNote: ANOVA F-statistic matches the joint test from regression\")\n\n\n======================================================================\nANOVA: Testing Equality of Means Across Worker Types\n======================================================================\n\nMeans by worker type:\n                       mean           std  count\nSelf-employed  72306.328125  86053.131086     79\nPrivate        54521.265625  48811.203104    663\nGovernment     56105.382812  32274.679426    130\n\n----------------------------------------------------------------------\nOne-way ANOVA (scipy)\n----------------------------------------------------------------------\n  F-statistic: 4.24\n  p-value: 0.014708\n\n  Result: Reject H₀ - Earnings differ significantly across worker types\n\n----------------------------------------------------------------------\nDetailed ANOVA table (statsmodels)\n----------------------------------------------------------------------\n                     sum_sq     df         F    PR(&gt;F)\nC(typeworker)  2.233847e+10    2.0  4.239916  0.014708\nResidual       2.289212e+12  869.0       NaN       NaN\n\nNote: ANOVA F-statistic matches the joint test from regression\n\n\nHaving covered the statistical framework for indicator variables, let’s visualize these group differences graphically.\n\nKey Concept 14.7: ANOVA as Regression on Indicators\nRegressing \\(y\\) on a set of mutually exclusive indicators (with no other controls) is equivalent to analysis of variance (ANOVA). Coefficients give group means or differences from the base mean. The regression F-test for joint significance of the indicators is identical to the ANOVA F-statistic, testing whether the categorical variable explains significant variation in \\(y\\).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#visualizations",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#visualizations",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "Visualizations",
    "text": "Visualizations\nLet’s create informative visualizations to illustrate our findings.\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"VISUALIZATIONS\")\nprint(\"=\" * 70)\n\n# Figure 1: Earnings by gender and worker type\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Panel 1: Box plot by gender\ndata['Gender'] = data['gender'].map({0: 'Male', 1: 'Female'})\nsns.boxplot(x='Gender', y='earnings', data=data, ax=axes[0], palette='Set2')\naxes[0].set_ylabel('Earnings ($)', fontsize=12)\naxes[0].set_xlabel('Gender', fontsize=12)\naxes[0].set_title('Earnings Distribution by Gender', fontsize=13, fontweight='bold')\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Add mean markers\nmeans_gender = data.groupby('Gender')['earnings'].mean()\naxes[0].scatter([0, 1], means_gender.values, color='red', s=100, zorder=5, marker='D', label='Mean')\naxes[0].legend()\n\n# Panel 2: Box plot by worker type\ndata['Worker Type'] = data['typeworker'].map({1: 'Self-employed', 2: 'Private', 3: 'Government'})\nsns.boxplot(x='Worker Type', y='earnings', data=data, ax=axes[1], palette='Set1')\naxes[1].set_ylabel('Earnings ($)', fontsize=12)\naxes[1].set_xlabel('Worker Type', fontsize=12)\naxes[1].set_title('Earnings Distribution by Worker Type', fontsize=13, fontweight='bold')\naxes[1].tick_params(axis='x', rotation=20)\naxes[1].grid(True, alpha=0.3, axis='y')\n\n# Add mean markers\nmeans_type = data.groupby('Worker Type')['earnings'].mean()\naxes[1].scatter([0, 1, 2], means_type.values, color='red', s=100, zorder=5, marker='D', label='Mean')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Figure 14.1: Earnings distributions show variation by gender and worker type.\")\n\n\n======================================================================\nVISUALIZATIONS\n======================================================================\n\n\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_42634/4086298910.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='Gender', y='earnings', data=data, ax=axes[0], palette='Set2')\n/var/folders/tq/t98kb27n6djgrh085g476yhc0000gn/T/ipykernel_42634/4086298910.py:23: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='Worker Type', y='earnings', data=data, ax=axes[1], palette='Set1')\n\n\n\n\n\n\n\n\n\nFigure 14.1: Earnings distributions show variation by gender and worker type.\n\n\n\n# Figure 2: Earnings vs education by gender (with regression lines)\nfig, ax = plt.subplots(figsize=(11, 7))\n\nfor gender, label, color in [(0, 'Male', 'blue'), (1, 'Female', 'red')]:\n    subset = data[data['gender'] == gender]\n    ax.scatter(subset['education'], subset['earnings'], alpha=0.4, label=label, \n               s=30, color=color)\n    \n    # Add regression line\n    z = np.polyfit(subset['education'], subset['earnings'], 1)\n    p = np.poly1d(z)\n    edu_range = np.linspace(subset['education'].min(), subset['education'].max(), 100)\n    ax.plot(edu_range, p(edu_range), linewidth=3, color=color, \n            label=f'{label} (slope={z[0]:,.0f})')\n\nax.set_xlabel('Years of Education', fontsize=13)\nax.set_ylabel('Earnings ($)', fontsize=13)\nax.set_title('Figure 14.2: Earnings vs Education by Gender\\n(Different slopes suggest interaction effects)', \n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper left')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nThe different slopes indicate that returns to education vary by gender.\")\nprint(\"This justifies including the gender × education interaction term.\")\n\n\n\n\n\n\n\n\n\nThe different slopes indicate that returns to education vary by gender.\nThis justifies including the gender × education interaction term.\n\n\n\nKey Concept 14.8: Visualizing Group Differences in Regression\nScatter plots with separate regression lines by group visually reveal whether slopes and intercepts differ. Parallel lines indicate only an intercept shift (indicator without interaction), while non-parallel lines indicate differential slopes (interaction term needed). Box plots complement this by showing the distribution of \\(y\\) across categories.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#key-takeaways",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#key-takeaways",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nIndicator Variables Basics\n\nIndicator variables (dummy variables) are binary variables that equal 1 if an observation is in a specific category and 0 otherwise\nThey allow regression models to incorporate categorical information such as gender, employment type, or region\nInterpretation differs from continuous variables – coefficients represent group differences rather than marginal effects\n\n\n\nRegression on a Single Indicator and Difference in Means\n\nWhen regressing \\(y\\) on just an intercept and a single indicator \\(d\\), the fitted model is \\(\\hat{y} = b + ad\\)\nThe intercept \\(b\\) equals the mean of \\(y\\) when \\(d=0\\) (the reference group)\nThe slope \\(a\\) equals the difference in means: \\(a = \\bar{y}_1 - \\bar{y}_0\\)\nRegression on an indicator is mathematically equivalent to a two-sample difference-in-means test\nRegression and specialized t-tests give the same estimate but slightly different standard errors\nExample: Women earn $16,396 less than men on average (\\(t = -4.71\\), highly significant)\n\n\n\nIndicators with Controls and Interaction Terms\n\nAdding an indicator to a regression with continuous variables measures the group difference after controlling for other factors\nIncluding only the indicator shifts the intercept but keeps slopes parallel across groups\nAn interaction term \\(d \\times x\\) allows slopes to differ by group (non-parallel lines)\nThe interaction coefficient measures how the effect of \\(x\\) on \\(y\\) differs between groups\nAlways use joint F-tests to test significance of both the indicator and its interaction together\n\n\n\nDummy Variable Trap and Base Category\n\nThe dummy variable trap occurs when including all \\(C\\) indicators from a mutually exclusive set plus an intercept\nSince \\(d_1 + d_2 + \\cdots + d_C = 1\\), this creates perfect multicollinearity\nSolution: Drop one indicator (the “base category”) or drop the intercept\nThe base category is the reference group – coefficients on included indicators measure differences from the base\nChoice of base category does not affect statistical conclusions, only interpretation\n\n\n\nHypothesis Testing with Indicator Sets\n\nA t-test on a single indicator tests whether that category differs from the base category\nAn F-test on all \\(C-1\\) included indicators tests whether the categorical variable matters overall\nThe F-test result is the same regardless of which category is dropped (invariant to base choice)\nRegressing \\(y\\) on mutually exclusive indicators without controls is equivalent to ANOVA\nAlways use F-tests to evaluate the overall significance of a categorical variable\n\n\n\nGeneral Lessons\n\nIndicator variables unify many statistical tests (t-tests, ANOVA) in a single regression framework\nAlways use heteroskedastic-robust standard errors for valid inference\nInteractions with continuous variables allow relationships to vary by group\nRegression flexibility: add controls, test interactions, compare models – all within one framework\n\n\n\n\nPython Tools Used in This Chapter\n# Regression with indicators\nsmf.ols('earnings ~ gender', data=df).fit(cov_type='HC1')\nsmf.ols('earnings ~ gender * education', data=df).fit(cov_type='HC1')\n\n# T-tests\nscipy.stats.ttest_ind(group1, group2, equal_var=False)  # Welch's t-test\n\n# ANOVA\nscipy.stats.f_oneway(g1, g2, g3)   # One-way ANOVA\nsmf.ols('y ~ C(worker_type)').fit()  # ANOVA via regression\n\n# Joint F-tests\nmodel.f_test('gender = 0, gender:education = 0')\n\n# Visualization\nseaborn.boxplot(), matplotlib scatter with separate regression lines\n\nNext Steps: - Chapter 15: Regression with Transformed Variables - Chapter 16: Model Diagnostics\n\nCongratulations! You’ve completed Chapter 14. You now understand how to incorporate categorical variables into regression analysis, interpret indicator coefficients, test for group differences, and use interactions to model differential effects across categories.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#practice-exercises",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#practice-exercises",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Interpreting Indicator Regression\nOLS regression of \\(y\\) on an intercept and indicator \\(d\\) using all data yields \\(\\hat{y} = 3 + 5d\\).\n(a) What is \\(\\bar{y}\\) for the subsample with \\(d = 0\\)?\n(b) What is \\(\\bar{y}\\) for the subsample with \\(d = 1\\)?\n(c) What does the coefficient 5 represent?\n\nExercise 2: Constructing from Means\nSuppose \\(\\bar{y} = 30\\) for the subsample with \\(d = 1\\) and \\(\\bar{y} = 20\\) for the subsample with \\(d = 0\\).\n(a) Give the fitted model from OLS regression of \\(y\\) on an intercept and \\(d\\) using the full sample.\n(b) Is the difference in means statistically significant? What additional information would you need?\n\nExercise 3: Multiple Indicators\nWe have three mutually exclusive indicator variables \\(d_1\\), \\(d_2\\), and \\(d_3\\). OLS yields \\(\\hat{y} = 1 + 3d_2 + 5d_3\\).\n(a) What is the estimated mean of \\(y\\) for each category?\n(b) What is the estimated difference between category 2 (\\(d_2 = 1\\)) and category 1 (\\(d_1 = 1\\))?\n(c) Which category is the base (omitted) category?\n\nExercise 4: Changing Base Category\nFor the model in Exercise 3, give the coefficient estimates if instead we regressed \\(y\\) on an intercept, \\(d_1\\), and \\(d_2\\) (dropping \\(d_3\\) instead of \\(d_1\\)).\n\nExercise 5: Interaction Interpretation\nA regression of earnings on education, gender, and their interaction yields:\n\\[\\widehat{\\text{earnings}} = 10{,}000 + 5{,}000 \\times \\text{education} - 8{,}000 \\times \\text{gender} - 2{,}000 \\times (\\text{gender} \\times \\text{education})\\]\nwhere gender = 1 for female, 0 for male.\n(a) Write the fitted equation for males and for females separately.\n(b) What is the returns to education for males? For females?\n(c) At what education level does the gender earnings gap equal zero?\n\nExercise 6: Joint F-Test\nA researcher includes a gender indicator and a gender-education interaction in a regression. The t-statistic on the gender indicator is \\(t = -1.5\\) (not significant at 5%) and the t-statistic on the interaction is \\(t = -1.8\\) (not significant at 5%).\n(a) Can we conclude that gender does not matter for earnings? Why or why not?\n(b) What test should we conduct instead? What would you expect to find?\n(c) Explain why individual t-tests can be misleading when testing the significance of indicator variables with interactions.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#case-studies",
    "href": "../notebooks_colab/ch14_Regression_with_Indicator_Variables.html#case-studies",
    "title": "Chapter 14: Regression with Indicator Variables",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Regional Indicator Variables for Cross-Country Productivity\nIn this case study, you will apply indicator variable techniques to analyze how labor productivity differs across world regions and whether the determinants of productivity vary by region.\nDataset: Mendez Convergence Clubs\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat2014 = dat[dat['year'] == 2014].copy()\nVariables: lp (labor productivity), rk (physical capital), hc (human capital), region (world region)\n\n\nTask 1: Create Regional Indicators and Compute Means (Guided)\nCreate indicator variables for each region and compute mean log productivity by region.\n# Create indicator variables from the region column\nregion_dummies = pd.get_dummies(dat2014['region'], prefix='region', drop_first=False)\ndat2014 = pd.concat([dat2014, region_dummies], axis=1)\n\n# Compute mean log productivity by region\nimport numpy as np\ndat2014['ln_lp'] = np.log(dat2014['lp'])\nprint(dat2014.groupby('region')['ln_lp'].agg(['mean', 'count']))\nQuestions: How many regions are there? Which region has the highest average productivity? The lowest?\n\n\n\nTask 2: Regression on Regional Indicators (Guided)\nRegress log productivity on regional indicators (using one region as the base category).\nimport statsmodels.formula.api as smf\nmodel1 = smf.ols('ln_lp ~ C(region)', data=dat2014).fit(cov_type='HC1')\nprint(model1.summary())\nQuestions: Which region is the base category? How do you interpret the coefficients? Are the regional differences statistically significant?\n\nKey Concept 14.9: Regional Indicators as Difference in Means\nWhen regressing \\(y\\) on a set of regional indicators without controls, each coefficient measures the difference in mean \\(y\\) between that region and the base region. The F-test for joint significance tests whether there are any significant productivity differences across regions.\n\n\n\n\nTask 3: Add Continuous Controls (Semi-guided)\nAdd physical capital and human capital as continuous controls. Observe how regional coefficients change.\nHints:\n\nUse ln_lp ~ C(region) + np.log(rk) + hc as the formula\nCompare regional coefficients with and without controls\nWhat does it mean when regional gaps shrink after adding controls?\n\n\n\n\nTask 4: Regional Interactions (Semi-guided)\nAdd an interaction between region and human capital to test whether the returns to human capital differ by region.\nHints:\n\nUse ln_lp ~ C(region) * hc + np.log(rk) to include all region-hc interactions\nHow do you interpret the interaction coefficients?\nDo the returns to human capital differ significantly across regions?\n\n\nKey Concept 14.10: Interaction Terms for Regional Heterogeneity\nInteraction terms between regional indicators and continuous variables allow the slope of a regressor to differ by region. A significant interaction indicates that the effect of human capital (or physical capital) on productivity is not uniform across all regions – some regions benefit more from the same increase in inputs.\n\n\n\n\nTask 5: Joint F-Tests for Regional Effects (Independent)\nConduct joint F-tests to evaluate:\n\nWhether regional indicators are jointly significant (with and without controls)\nWhether the regional interaction terms are jointly significant\nCompare the explanatory power of models with and without regional effects\n\n\n\n\nTask 6: Policy Brief on Regional Disparities (Independent)\nWrite a 200-300 word brief addressing:\n\nHow large are regional productivity differences?\nHow much of the gap is explained by differences in physical and human capital?\nDo the returns to human capital differ by region, and what are the policy implications?\nWhat additional factors might explain remaining regional differences?\n\n\nWhat You’ve Learned: You have applied indicator variable techniques to cross-country data, demonstrating that regional indicators capture systematic productivity differences that partially reflect differences in factor endowments. Interaction terms reveal that the returns to human capital vary across regions, with important implications for development policy.\n\n\n\nCase Study 2: Urban-Rural and Regional Divides in Bolivian Development\nIn previous chapters, we estimated satellite-development regressions treating all Bolivian municipalities identically. But Bolivia’s nine departments span diverse geographies—from Andean highlands to Amazonian lowlands. In this case study, we apply Chapter 14’s indicator variable techniques to model regional differences in development levels and in the satellite-development relationship.\nResearch Question: Do development levels and the NTL-development relationship differ across Bolivia’s nine departments?\nDataset: DS4Bolivia\nimport pandas as pd\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\nKey variables: mun (municipality), dep (department), imds (Municipal Development Index, 0–100), ln_NTLpc2017 (log nighttime lights per capita)\n\nTask 1: Create Indicators and Group Means (Guided)\nObjective: Create department indicator variables and compare mean development across departments.\nInstructions:\n\nLoad the DS4Bolivia dataset and select key variables\nUse pd.get_dummies(bol['dep']) to create department indicators\nCompute mean imds by department with groupby('dep')['imds'].agg(['mean', 'count'])\nWhich department has the highest mean IMDS? The lowest?\n\nRun the code below to get started.\n\n# Load DS4Bolivia dataset\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables\nbol_key = bol[['mun', 'dep', 'imds', 'ln_NTLpc2017']].dropna().copy()\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA: DEPARTMENT INDICATORS AND GROUP MEANS\")\nprint(\"=\" * 70)\nprint(f\"Observations: {len(bol_key)} municipalities\")\nprint(f\"Departments: {bol_key['dep'].nunique()}\")\n\n# Create department indicator variables\ndep_dummies = pd.get_dummies(bol_key['dep'])\nprint(f\"\\nIndicator variables created: {list(dep_dummies.columns)}\")\n\n# Mean IMDS by department\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Mean IMDS by Department\")\nprint(\"-\" * 70)\ndept_stats = bol_key.groupby('dep')['imds'].agg(['mean', 'count']).sort_values('mean', ascending=False)\nprint(dept_stats.round(2))\n\nprint(f\"\\nHighest mean IMDS: {dept_stats['mean'].idxmax()} ({dept_stats['mean'].max():.2f})\")\nprint(f\"Lowest mean IMDS:  {dept_stats['mean'].idxmin()} ({dept_stats['mean'].min():.2f})\")\n\n\n\nTask 2: Regression on Department Indicators (Guided)\nObjective: Regress IMDS on department indicators to quantify regional development differences.\nInstructions:\n\nEstimate imds ~ C(dep) using statsmodels — C() creates dummies automatically\nPrint the regression summary\nInterpret: Each coefficient is the difference from the base category (the omitted department)\nWhat is the base department? How much higher or lower is each department relative to the base?\n\nmodel_dep = ols('imds ~ C(dep)', data=bol_key).fit(cov_type='HC1')\nprint(model_dep.summary())\n\n# Your code here: Regression of IMDS on department indicators\n#\n# Steps:\n# 1. Estimate model: imds ~ C(dep)\n# 2. Print summary\n# 3. Identify the base department and interpret coefficients\n#\n# model_dep = ols('imds ~ C(dep)', data=bol_key).fit(cov_type='HC1')\n# print(model_dep.summary())\n#\n# print(f\"\\nBase department: {sorted(bol_key['dep'].unique())[0]}\")\n# print(f\"Intercept = mean IMDS for the base department: {model_dep.params['Intercept']:.2f}\")\n\n\nKey Concept 14.9: Geographic Indicator Variables\nDepartment indicators capture time-invariant regional characteristics that affect development: altitude, climate, proximity to borders, historical infrastructure investments, and cultural factors. By including department dummies, we control for these systematic regional differences, allowing us to estimate the NTL-development relationship within departments rather than across them. The coefficient on NTL then reflects how NTL variation within a department predicts development variation within that department.\n\n\n\nTask 3: Add NTL Control (Semi-guided)\nObjective: Compare department coefficients with and without the NTL control variable.\nInstructions:\n\nEstimate imds ~ C(dep) + ln_NTLpc2017\nCompare department coefficients with those from Task 2 (without NTL)\nDo department differences shrink when NTL is added?\nWhat does this tell us about whether NTL explains part of the regional gap?\n\nHints:\n\nIf department coefficients shrink, it means part of the regional gap was due to differences in NTL intensity\nIf they remain large, departments differ for reasons beyond what NTL captures\n\n\n# Your code here: Add NTL as a continuous control\n#\n# Steps:\n# 1. Estimate: imds ~ C(dep) + ln_NTLpc2017\n# 2. Compare department coefficients with Task 2 results\n# 3. Interpret changes\n#\n# model_dep_ntl = ols('imds ~ C(dep) + ln_NTLpc2017', data=bol_key).fit(cov_type='HC1')\n# print(model_dep_ntl.summary())\n#\n# print(\"\\nComparison: Do department coefficients shrink with NTL control?\")\n# print(\"If yes, NTL explains part of the regional development gap.\")\n\n\n\nTask 4: Interaction Terms (Semi-guided)\nObjective: Test whether the NTL-development slope differs across departments.\nInstructions:\n\nEstimate imds ~ C(dep) * ln_NTLpc2017 (includes main effects + interactions)\nWhich interaction terms are significant?\nInterpret: The NTL slope differs by department\nIn which departments does NTL predict development more or less strongly?\n\nHints:\n\nC(dep) * ln_NTLpc2017 automatically includes both main effects and all interactions\nA positive interaction means the NTL slope is steeper in that department compared to the base\nA negative interaction means the NTL slope is flatter\n\n\n# Your code here: Interaction between department and NTL\n#\n# Steps:\n# 1. Estimate: imds ~ C(dep) * ln_NTLpc2017\n# 2. Examine interaction coefficients\n# 3. Identify departments where NTL effect is stronger/weaker\n#\n# model_interact = ols('imds ~ C(dep) * ln_NTLpc2017', data=bol_key).fit(cov_type='HC1')\n# print(model_interact.summary())\n#\n# print(\"\\nInterpretation:\")\n# print(\"  Significant interactions indicate the NTL slope differs by department.\")\n# print(\"  Base department NTL slope = coefficient on ln_NTLpc2017\")\n# print(\"  Other departments: base slope + interaction coefficient\")\n\n\nKey Concept 14.10: Heterogeneous Satellite Effects\nInteraction terms between department indicators and NTL allow the slope of the NTL-development relationship to differ across regions. This is economically meaningful: in highly urbanized departments (like La Paz or Cochabamba), additional nighttime lights may signal commercial activity, while in rural highland departments (like Potosi), lights primarily reflect basic electrification. A significant interaction indicates that the economic meaning of satellite signals varies by geographic context.\n\n\n\nTask 5: Joint F-Test for Department Effects (Independent)\nObjective: Test the joint significance of all department indicators and all interaction terms.\nInstructions:\n\nTest joint significance of all department indicators using an F-test\nAlso test joint significance of all interaction terms\nAre regional differences significant? Do NTL effects significantly vary across regions?\n\nHints:\n\nUse model.f_test() or compare restricted vs. unrestricted models\nFor interaction terms, compare the model with interactions to the model without\nReport F-statistics and p-values for both tests\n\n\n# Your code here: Joint F-tests for department effects\n#\n# Steps:\n# 1. F-test for joint significance of department indicators\n# 2. F-test for joint significance of interaction terms\n# 3. Compare models with and without regional effects\n#\n# Approach: Compare nested models using ANOVA\n# from statsmodels.stats.anova import anova_lm\n#\n# model_base = ols('imds ~ ln_NTLpc2017', data=bol_key).fit()\n# model_dep_only = ols('imds ~ C(dep) + ln_NTLpc2017', data=bol_key).fit()\n# model_full = ols('imds ~ C(dep) * ln_NTLpc2017', data=bol_key).fit()\n#\n# print(\"F-test: Department indicators (comparing base vs dep_only)\")\n# print(anova_lm(model_base, model_dep_only))\n#\n# print(\"\\nF-test: Interaction terms (comparing dep_only vs full)\")\n# print(anova_lm(model_dep_only, model_full))\n\n\n\nTask 6: Regional Disparities Brief (Independent)\nObjective: Write a 200–300 word brief on regional development disparities in Bolivia.\nYour brief should address:\n\nWhat are the main regional divides in Bolivian development?\nHow much do department indicators explain beyond NTL?\nDoes the satellite-development relationship differ across regions?\nWhat are the policy implications for targeted regional interventions?\n\nUse evidence from your analysis: Cite specific coefficients, R-squared values, and F-test results to support your arguments.\n\n# Your code here: Additional analysis for the policy brief\n#\n# You might want to:\n# 1. Create a summary table comparing R-squared across models\n# 2. Visualize department means with confidence intervals\n# 3. Plot separate NTL-IMDS regression lines by department\n# 4. Calculate specific statistics to cite in your brief\n#\n# Example: Compare model fit\n# print(\"MODEL COMPARISON\")\n# print(f\"  NTL only:        R² = {model_base.rsquared:.4f}\")\n# print(f\"  + Dept dummies:  R² = {model_dep_only.rsquared:.4f}\")\n# print(f\"  + Interactions:  R² = {model_full.rsquared:.4f}\")\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this analysis of regional development disparities in Bolivia, you have applied Chapter 14’s indicator variable techniques to a real geospatial dataset:\n\nIndicator variable creation: Used pd.get_dummies() and C() to create department dummies\nDepartment dummies in regression: Quantified development differences across Bolivia’s nine departments\nControlling for regional effects: Compared models with and without department indicators to assess how much of the NTL-development relationship reflects regional composition\nInteraction terms: Tested whether the satellite-development slope differs across departments\nJoint F-tests: Evaluated the overall significance of department effects and interaction terms\nPolicy interpretation: Connected statistical findings to regional development policy\n\nConnection to next chapter: In Chapter 15, we explore nonlinear satellite-development relationships using log transformations, polynomials, and elasticities.\n\nWell done! You’ve now analyzed both cross-country productivity data and Bolivian municipal development data using indicator variable techniques, demonstrating how geographic dummies capture systematic regional differences and how interactions reveal heterogeneous effects.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chapter 14: Regression with Indicator Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to regression with transformed variables. All code runs directly in Google Colab without any local setup.\nThis chapter focuses on regression models that involve transformed variables. Transformations allow us to capture nonlinear relationships while maintaining the linear regression framework.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#chapter-overview",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#chapter-overview",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "",
    "text": "What You’ll Learn\nBy the end of this chapter, you will be able to:\n\nUnderstand how variable transformations affect regression interpretation\nCompute and interpret marginal effects for nonlinear models\nDistinguish between average marginal effects (AME), marginal effects at the mean (MEM), and marginal effects at representative values (MER)\nEstimate and interpret quadratic and polynomial regression models\nWork with interaction terms and test their joint significance\nApply natural logarithm transformations to create log-linear and log-log models\nMake predictions from models with transformed dependent variables, avoiding retransformation bias\nCombine multiple types of variable transformations in a single model\n\n\n\nChapter Outline\n\n15.2 Logarithmic Transformations\n15.3 Polynomial Regression (Quadratic Models)\n15.4 Standardized Variables\n15.5 Interaction Terms and Marginal Effects\n15.6 Retransformation Bias and Prediction\n15.7 Comprehensive Model with Mixed Regressors\nKey Takeaways – Chapter review and consolidated lessons\nPractice Exercises – Reinforce your understanding\nCase Studies – Apply transformations to cross-country data\n\nDataset used: - AED_EARNINGS_COMPLETE.DTA: 872 workers aged 25-65 in 2000\nKey economic questions: - How do earnings vary with age? Is the relationship linear or quadratic? - Do returns to education increase with age (interaction effects)? - How should we interpret coefficients in log-transformed models? - How do we make unbiased predictions from log-linear models?\nEstimated time: 90-120 minutes",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#setup",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#setup",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Setup complete! Ready to explore regression with transformed variables.\")\n\nSetup complete! Ready to explore regression with transformed variables.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#data-preparation",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#data-preparation",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’ll work with the EARNINGS_COMPLETE dataset, which contains information on 872 female and male full-time workers aged 25-65 years in 2000.\nKey variables:\n\nearnings: Annual earnings in dollars\nlnearnings: Natural logarithm of earnings\nage: Age in years\nagesq: Age squared\neducation: Years of schooling\nagebyeduc: Age × Education interaction\ngender: 1 if female, 0 if male\ndself: 1 if self-employed\ndgovt: 1 if government sector employee\nlnhours: Natural logarithm of hours worked per week\n\n\n# Read in the earnings data\ndata_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS_COMPLETE.DTA')\n\nprint(\"Data structure:\")\nprint(data_earnings.info())\n\nprint(\"\\nData summary:\")\ndata_summary = data_earnings.describe()\nprint(data_summary)\n\nprint(\"\\nFirst few observations:\")\nkey_vars = ['earnings', 'lnearnings', 'age', 'agesq', 'education', 'agebyeduc', \n            'gender', 'dself', 'dgovt', 'lnhours']\nprint(data_earnings[key_vars].head(10))\n\nData structure:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 872 entries, 0 to 871\nData columns (total 45 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   earnings       872 non-null    float32 \n 1   lnearnings     872 non-null    float32 \n 2   dearnings      872 non-null    float32 \n 3   gender         872 non-null    int8    \n 4   age            872 non-null    int16   \n 5   lnage          872 non-null    float32 \n 6   agesq          872 non-null    float32 \n 7   education      872 non-null    float32 \n 8   educsquared    872 non-null    float32 \n 9   agebyeduc      872 non-null    float32 \n 10  genderbyage    872 non-null    float32 \n 11  genderbyeduc   872 non-null    float32 \n 12  hours          872 non-null    int8    \n 13  lnhours        872 non-null    float32 \n 14  genderbyhours  872 non-null    float32 \n 15  dself          872 non-null    float32 \n 16  dprivate       872 non-null    float32 \n 17  dgovt          872 non-null    float32 \n 18  state          872 non-null    object  \n 19  statefip       872 non-null    category\n 20  stateunemp     872 non-null    float32 \n 21  stateincomepc  872 non-null    int32   \n 22  year           872 non-null    category\n 23  pernum         872 non-null    int16   \n 24  perwt          872 non-null    float32 \n 25  relate         872 non-null    category\n 26  region         872 non-null    category\n 27  metro          872 non-null    category\n 28  marst          872 non-null    category\n 29  race           872 non-null    category\n 30  raced          872 non-null    category\n 31  hispan         872 non-null    category\n 32  racesing       872 non-null    category\n 33  hcovany        872 non-null    category\n 34  attainededuc   872 non-null    category\n 35  detailededuc   872 non-null    category\n 36  empstat        872 non-null    category\n 37  classwkr       872 non-null    category\n 38  classwkrd      872 non-null    category\n 39  wkswork2       872 non-null    category\n 40  workedyr       872 non-null    category\n 41  inctot         872 non-null    category\n 42  incwage        872 non-null    category\n 43  incbus00       872 non-null    int32   \n 44  incearn        872 non-null    category\ndtypes: category(21), float32(17), int16(2), int32(2), int8(2), object(1)\nmemory usage: 134.3+ KB\nNone\n\nData summary:\n            earnings  lnearnings   dearnings      gender         age  \\\ncount     872.000000  872.000000  872.000000  872.000000  872.000000   \nmean    56368.691406   10.691164    0.163991    0.433486   43.310780   \nstd     51516.054688    0.684247    0.370480    0.495841   10.676045   \nmin      4000.000000    8.294049    0.000000    0.000000   25.000000   \n25%     29000.000000   10.275051    0.000000    0.000000   35.000000   \n50%     44200.000000   10.696480    0.000000    0.000000   44.000000   \n75%     64250.000000   11.070514    0.000000    1.000000   51.250000   \nmax    504000.000000   13.130332    1.000000    1.000000   65.000000   \n\n            lnage        agesq   education  educsquared    agebyeduc  ...  \\\ncount  872.000000   872.000000  872.000000   872.000000   872.000000  ...   \nmean     3.736286  1989.670898   13.853211   200.220184   598.819946  ...   \nstd      0.257889   935.691895    2.884141    73.908417   193.690643  ...   \nmin      3.218876   625.000000    0.000000     0.000000     0.000000  ...   \n25%      3.555348  1225.000000   12.000000   144.000000   464.000000  ...   \n50%      3.784190  1936.000000   13.000000   169.000000   588.000000  ...   \n75%      3.936680  2626.750000   16.000000   256.000000   720.000000  ...   \nmax      4.174387  4225.000000   20.000000   400.000000  1260.000000  ...   \n\n          lnhours  genderbyhours       dself    dprivate       dgovt  \\\ncount  872.000000     872.000000  872.000000  872.000000  872.000000   \nmean     3.777036      18.564220    0.090596    0.760321    0.149083   \nstd      0.164767      21.759789    0.287199    0.427132    0.356374   \nmin      3.555348       0.000000    0.000000    0.000000    0.000000   \n25%      3.688879       0.000000    0.000000    1.000000    0.000000   \n50%      3.688879       0.000000    0.000000    1.000000    0.000000   \n75%      3.871201      40.000000    0.000000    1.000000    0.000000   \nmax      4.595120      80.000000    1.000000    1.000000    1.000000   \n\n       stateunemp  stateincomepc      pernum       perwt       incbus00  \ncount  872.000000     872.000000  872.000000  872.000000     872.000000  \nmean     9.596904   40772.990826    1.544725  145.784409    3540.482798  \nstd      1.649194    5558.626289    0.891506   90.987816   20495.125402  \nmin      4.800000   31186.000000    1.000000   14.000000   -7500.000000  \n25%      8.500000   36395.000000    1.000000   82.000000       0.000000  \n50%      9.450000   39493.000000    1.000000  109.000000       0.000000  \n75%     10.900000   43117.750000    2.000000  195.000000       0.000000  \nmax     14.400000   71044.000000    8.000000  626.000000  285000.000000  \n\n[8 rows x 23 columns]\n\nFirst few observations:\n   earnings  lnearnings  age   agesq  education  agebyeduc  gender  dself  \\\n0  120000.0   11.695247   45  2025.0       16.0      720.0       0    1.0   \n1   23000.0   10.043249   61  3721.0       16.0      976.0       0    1.0   \n2   20000.0    9.903487   58  3364.0       16.0      928.0       0    1.0   \n3   55000.0   10.915089   58  3364.0       14.0      812.0       1    0.0   \n4   43200.0   10.673595   34  1156.0       18.0      612.0       1    0.0   \n5  110000.0   11.608235   59  3481.0       16.0      944.0       0    0.0   \n6   44000.0   10.691945   25   625.0       18.0      450.0       1    0.0   \n7  120000.0   11.695247   50  2500.0       12.0      600.0       1    0.0   \n8   65000.0   11.082143   27   729.0       16.0      432.0       0    0.0   \n9    7200.0    8.881836   28   784.0       14.0      392.0       1    0.0   \n\n   dgovt   lnhours  \n0    0.0  4.248495  \n1    0.0  3.912023  \n2    0.0  3.688879  \n3    0.0  3.688879  \n4    0.0  3.688879  \n5    0.0  3.912023  \n6    0.0  3.912023  \n7    0.0  3.951244  \n8    0.0  3.688879  \n9    0.0  3.688879",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#logarithmic-transformations",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#logarithmic-transformations",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "15.2: Logarithmic Transformations",
    "text": "15.2: Logarithmic Transformations\nLogarithmic transformations are commonly used in economics because: 1. They can linearize multiplicative relationships 2. Coefficients have natural interpretations (percentages, elasticities) 3. They reduce the influence of outliers 4. They often make error distributions more symmetric\nThree main types of log models:\n\nLevels model: \\(y = \\beta_1 + \\beta_2 x + u\\)\n\nInterpretation: \\(\\Delta y = \\beta_2 \\Delta x\\)\n\nLog-linear model: \\(\\ln y = \\beta_1 + \\beta_2 x + u\\)\n\nInterpretation: A one-unit increase in \\(x\\) leads to approximately $100_2%$ change in \\(y\\)\nAlso called semi-elasticity\n\nLog-log model: \\(\\ln y = \\beta_1 + \\beta_2 \\ln x + u\\)\n\nInterpretation: A 1% increase in \\(x\\) leads to \\(\\beta_2\\%\\) change in \\(y\\)\n\\(\\beta_2\\) is an elasticity\n\n\nMarginal effects: - Log-linear: \\(ME_x = \\beta_2 \\times y\\) - Log-log: \\(ME_x = \\beta_2 \\times y / x\\)\n\n# Create ln(age) variable if not already present\nif 'lnage' not in data_earnings.columns:\n    data_earnings['lnage'] = np.log(data_earnings['age'])\n    print(\"Created lnage variable\")\nelse:\n    print(\"lnage already exists\")\n\nlnage already exists\n\n\n\nprint(\"=\"*70)\nprint(\"15.2 LOGARITHMIC TRANSFORMATIONS\")\nprint(\"=\"*70)\n\n# Model 1: Levels model\nprint(\"\\n\" + \"-\"*70)\nprint(\"Model 1: Levels Model - earnings ~ age + education\")\nprint(\"-\"*70)\nols_linear = ols('earnings ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(ols_linear.summary())\n\n# Model 2: Log-linear model\nprint(\"\\n\" + \"-\"*70)\nprint(\"Model 2: Log-Linear Model - lnearnings ~ age + education\")\nprint(\"-\"*70)\nols_loglin = ols('lnearnings ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(ols_loglin.summary())\n\n# Model 3: Log-log model\nprint(\"\\n\" + \"-\"*70)\nprint(\"Model 3: Log-Log Model - lnearnings ~ lnage + education\")\nprint(\"-\"*70)\nols_loglog = ols('lnearnings ~ lnage + education', data=data_earnings).fit(cov_type='HC1')\nprint(ols_loglog.summary())\n\n======================================================================\n15.2 LOGARITHMIC TRANSFORMATIONS\n======================================================================\n\n----------------------------------------------------------------------\nModel 1: Levels Model - earnings ~ age + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.113\nMethod:                 Least Squares   F-statistic:                     42.85\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.79e-18\nTime:                        14:40:53   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     869   BIC:                         2.131e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.688e+04   1.13e+04     -4.146      0.000    -6.9e+04   -2.47e+04\nage          524.9953    151.387      3.468      0.001     228.281     821.709\neducation   5811.3673    641.533      9.059      0.000    4553.986    7068.749\n==============================================================================\nOmnibus:                      825.668   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31187.987\nSkew:                           4.353   Prob(JB):                         0.00\nKurtosis:                      30.975   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n----------------------------------------------------------------------\nModel 2: Log-Linear Model - lnearnings ~ age + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             lnearnings   R-squared:                       0.190\nModel:                            OLS   Adj. R-squared:                  0.189\nMethod:                 Least Squares   F-statistic:                     74.89\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           9.84e-31\nTime:                        14:40:53   Log-Likelihood:                -813.85\nNo. Observations:                 872   AIC:                             1634.\nDf Residuals:                     869   BIC:                             1648.\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.9620      0.150     59.626      0.000       8.667       9.257\nage            0.0078      0.002      3.832      0.000       0.004       0.012\neducation      0.1006      0.009     11.683      0.000       0.084       0.117\n==============================================================================\nOmnibus:                       32.184   Durbin-Watson:                   2.094\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               82.617\nSkew:                           0.076   Prob(JB):                     1.15e-18\nKurtosis:                       4.500   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n----------------------------------------------------------------------\nModel 3: Log-Log Model - lnearnings ~ lnage + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             lnearnings   R-squared:                       0.193\nModel:                            OLS   Adj. R-squared:                  0.191\nMethod:                 Least Squares   F-statistic:                     75.85\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           4.34e-31\nTime:                        14:40:53   Log-Likelihood:                -812.59\nNo. Observations:                 872   AIC:                             1631.\nDf Residuals:                     869   BIC:                             1645.\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0091      0.331     24.229      0.000       7.361       8.657\nlnage          0.3457      0.082      4.211      0.000       0.185       0.507\neducation      0.1004      0.009     11.665      0.000       0.084       0.117\n==============================================================================\nOmnibus:                       32.101   Durbin-Watson:                   2.093\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               82.264\nSkew:                           0.075   Prob(JB):                     1.37e-18\nKurtosis:                       4.497   Cond. No.                         233.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n\n\nKey Concept 15.1: Log Transformations and Coefficient Interpretation\nIn a log-linear model (\\(\\ln y = \\beta_1 + \\beta_2 x\\)), the coefficient \\(\\beta_2\\) is a semi-elasticity: a 1-unit increase in \\(x\\) is associated with a \\(100 \\times \\beta_2\\)% change in \\(y\\). In a log-log model (\\(\\ln y = \\beta_1 + \\beta_2 \\ln x\\)), the coefficient \\(\\beta_2\\) is an elasticity: a 1% increase in \\(x\\) is associated with a \\(\\beta_2\\)% change in \\(y\\).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#interpretation-of-log-models",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#interpretation-of-log-models",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Interpretation of Log Models",
    "text": "Interpretation of Log Models\nLet’s carefully interpret the coefficients from each model.\n\n\nUnderstanding Elasticities and Percentage Changes\nThe three models reveal fundamentally different ways to think about the relationship between earnings, age, and education. Let’s interpret each carefully:\nModel 1: Levels (earnings ~ age + education)\n\nAge coefficient ≈ $800-$1,200 per year\nInterpretation: Each additional year of age increases earnings by approximately $1,000\nThis is an absolute change measured in dollars\nAssumes constant effect regardless of current age or earnings level\nEducation coefficient ≈ $4,000-$6,000 per year\nInterpretation: Each additional year of schooling increases earnings by approximately $5,000\nAgain, this is an absolute dollar amount\nAssumes the same dollar return whether you have 10 or 20 years of education\n\nModel 2: Log-Linear (ln(earnings) ~ age + education)\n\nAge coefficient ≈ 0.01 to 0.02\nInterpretation: Each additional year of age increases earnings by approximately 1-2%\nThis is a percentage change (semi-elasticity)\nThe dollar impact depends on current earnings\nFor someone earning $50,000: 1.5% × $50,000 = $750\nFor someone earning $100,000: 1.5% × $100,000 = $1,500\nEducation coefficient ≈ 0.08 to 0.12\nInterpretation: Each additional year of education increases earnings by approximately 8-12%\nThis is the famous Mincer return to education\nClassic labor economics result: education yields ~10% return per year\nPercentage effect, so dollar gain is larger for high earners\n\nModel 3: Log-Log (ln(earnings) ~ ln(age) + education)\n\nln(Age) coefficient ≈ 0.5 to 1.0\nInterpretation: A 1% increase in age increases earnings by approximately 0.5-1.0%\nThis is an elasticity (percentage change in Y for 1% change in X)\nElasticity &lt; 1 means inelastic relationship (earnings increase slower than age)\nAt age 40: 1% increase = 0.4 years; at age 50: 1% increase = 0.5 years\nEducation coefficient ≈ 0.08 to 0.12 (similar to log-linear)\nEducation enters in levels, so interpretation same as Model 2\nEach additional year → ~10% increase in earnings\n\nWhich Model to Choose?\n\nTheoretical motivation: Economics often suggests multiplicative relationships (log models)\nEmpirical fit: Log models often fit better for earnings (reduce skewness, outliers)\nInterpretation: Log models give percentage effects, more meaningful for wide earnings range\nHeteroskedasticity: Log transformation often reduces heteroskedasticity\n\nKey Insight: - The Mincer equation (log-linear) is standard in labor economics - Returns to education are approximately 10% per year across many countries and time periods - This is one of the most robust findings in empirical economics!",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#comparison-table-and-model-selection",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#comparison-table-and-model-selection",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Comparison Table and Model Selection",
    "text": "Comparison Table and Model Selection\n\n# Create comparison table\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON: Levels, Log-Linear, and Log-Log\")\nprint(\"=\"*70)\n\ncomparison_table = pd.DataFrame({\n    'Model': ['Levels', 'Log-Linear', 'Log-Log'],\n    'Specification': ['earnings ~ age + education', \n                      'ln(earnings) ~ age + education',\n                      'ln(earnings) ~ ln(age) + education'],\n    'R-squared': [ols_linear.rsquared, ols_loglin.rsquared, ols_loglog.rsquared],\n    'Adj R-squared': [ols_linear.rsquared_adj, ols_loglin.rsquared_adj, ols_loglog.rsquared_adj]\n})\n\nprint(comparison_table.to_string(index=False))\n\nprint(\"\\nNote: R² values are NOT directly comparable across models with different\")\nprint(\"dependent variables. For log models, R² measures fit to ln(earnings), not earnings.\")\n\n\n======================================================================\nMODEL COMPARISON: Levels, Log-Linear, and Log-Log\n======================================================================\n     Model                      Specification  R-squared  Adj R-squared\n    Levels         earnings ~ age + education   0.114989       0.112953\nLog-Linear     ln(earnings) ~ age + education   0.190419       0.188556\n   Log-Log ln(earnings) ~ ln(age) + education   0.192743       0.190886\n\nNote: R² values are NOT directly comparable across models with different\ndependent variables. For log models, R² measures fit to ln(earnings), not earnings.\n\n\nHaving explored logarithmic transformations for interpreting percentage changes and elasticities, we now turn to polynomial models that capture nonlinear relationships.\n\nKey Concept 15.2: Choosing Between Model Specifications\nYou cannot directly compare \\(R^2\\) across models with different dependent variables (\\(y\\) vs \\(\\ln y\\)) because they measure variation on different scales. Instead, compare models using prediction accuracy (e.g., mean squared error of predicted \\(y\\) in levels), information criteria (AIC, BIC), or economic plausibility of the estimated relationships.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#polynomial-regression-quadratic-models",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#polynomial-regression-quadratic-models",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "15.3: Polynomial Regression (Quadratic Models)",
    "text": "15.3: Polynomial Regression (Quadratic Models)\nPolynomial regression allows for nonlinear relationships while maintaining linearity in parameters.\nQuadratic model: \\[y = \\beta_1 + \\beta_2 x + \\beta_3 x^2 + u\\]\nProperties:\n\nIf \\(\\beta_3 &lt; 0\\): inverted U-shape (peaks then declines)\nIf \\(\\beta_3 &gt; 0\\): U-shape (declines then increases)\nTurning point at \\(x^* = -\\beta_2 / (2\\beta_3)\\)\n\nMarginal effect: \\[ME_x = \\frac{\\partial y}{\\partial x} = \\beta_2 + 2\\beta_3 x\\]\nAverage marginal effect (AME): \\[AME = \\beta_2 + 2\\beta_3 \\bar{x}\\]\nStatistical significance of age:\n\nMust test jointly: \\(H_0: \\beta_{age} = 0\\) AND \\(\\beta_{agesq} = 0\\)\nIndividual t-tests are insufficient\n\n\nprint(\"=\"*70)\nprint(\"15.3 POLYNOMIAL REGRESSION (QUADRATIC MODELS)\")\nprint(\"=\"*70)\n\n# Linear model (for comparison)\nprint(\"\\n\" + \"-\"*70)\nprint(\"Linear Model: earnings ~ age + education\")\nprint(\"-\"*70)\nols_linear_age = ols('earnings ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(ols_linear_age.summary())\n\n# Quadratic model\nprint(\"\\n\" + \"-\"*70)\nprint(\"Quadratic Model: earnings ~ age + agesq + education\")\nprint(\"-\"*70)\nols_quad = ols('earnings ~ age + agesq + education', data=data_earnings).fit(cov_type='HC1')\nprint(ols_quad.summary())\n\n======================================================================\n15.3 POLYNOMIAL REGRESSION (QUADRATIC MODELS)\n======================================================================\n\n----------------------------------------------------------------------\nLinear Model: earnings ~ age + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.113\nMethod:                 Least Squares   F-statistic:                     42.85\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.79e-18\nTime:                        14:40:53   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     869   BIC:                         2.131e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.688e+04   1.13e+04     -4.146      0.000    -6.9e+04   -2.47e+04\nage          524.9953    151.387      3.468      0.001     228.281     821.709\neducation   5811.3673    641.533      9.059      0.000    4553.986    7068.749\n==============================================================================\nOmnibus:                      825.668   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31187.987\nSkew:                           4.353   Prob(JB):                         0.00\nKurtosis:                      30.975   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n----------------------------------------------------------------------\nQuadratic Model: earnings ~ age + agesq + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.119\nModel:                            OLS   Adj. R-squared:                  0.116\nMethod:                 Least Squares   F-statistic:                     29.96\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.96e-18\nTime:                        14:40:53   Log-Likelihood:                -10642.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     868   BIC:                         2.131e+04\nDf Model:                           3                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -9.862e+04   2.45e+04     -4.021      0.000   -1.47e+05   -5.06e+04\nage         3104.9162   1087.323      2.856      0.004     973.802    5236.030\nagesq        -29.6583     12.456     -2.381      0.017     -54.072      -5.245\neducation   5740.3978    642.024      8.941      0.000    4482.055    6998.741\n==============================================================================\nOmnibus:                      829.757   Durbin-Watson:                   2.068\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            32082.015\nSkew:                           4.378   Prob(JB):                         0.00\nKurtosis:                      31.396   Cond. No.                     3.72e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 3.72e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TURNING POINT AND MARGINAL EFFECTS\")\nprint(\"=\"*70)\n\n# Extract coefficients\nbage = ols_quad.params['age']\nbagesq = ols_quad.params['agesq']\nbeducation = ols_quad.params['education']\n\n# Calculate turning point\nturning_point = -bage / (2 * bagesq)\n\nprint(f\"\\nTurning Point:\")\nprint(f\"  Age at maximum earnings: {turning_point:.1f} years\")\n\n# Marginal effects at different ages\nages_to_eval = [25, 40, 55, 65]\nprint(f\"\\nMarginal Effect of Age on Earnings:\")\nprint(\"-\"*70)\nfor age in ages_to_eval:\n    me = bage + 2 * bagesq * age\n    print(f\"  At age {age}: ${me:,.2f} per year\")\n\n# Average marginal effect\nmean_age = data_earnings['age'].mean()\name = bage + 2 * bagesq * mean_age\nprint(f\"\\nAverage Marginal Effect (at mean age {mean_age:.1f}): ${ame:,.2f}\")\n\n\n======================================================================\nTURNING POINT AND MARGINAL EFFECTS\n======================================================================\n\nTurning Point:\n  Age at maximum earnings: 52.3 years\n\nMarginal Effect of Age on Earnings:\n----------------------------------------------------------------------\n  At age 25: $1,622.00 per year\n  At age 40: $732.25 per year\n  At age 55: $-157.50 per year\n  At age 65: $-750.66 per year\n\nAverage Marginal Effect (at mean age 43.3): $535.87\n\n\n\nKey Concept 15.3: Quadratic Models and Turning Points\nA quadratic model \\(y = \\beta_1 + \\beta_2 x + \\beta_3 x^2 + u\\) captures nonlinear relationships with a turning point at \\(x^* = -\\beta_2 / (2\\beta_3)\\). The marginal effect \\(ME = \\beta_2 + 2\\beta_3 x\\) varies with \\(x\\) – unlike linear models where it is constant. If \\(\\beta_3 &lt; 0\\), the relationship is an inverted U-shape (e.g., earnings peaking at a certain age).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#quadratic-model-turning-point-and-marginal-effects",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#quadratic-model-turning-point-and-marginal-effects",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Quadratic Model: Turning Point and Marginal Effects",
    "text": "Quadratic Model: Turning Point and Marginal Effects\n\n\nLife-Cycle Earnings Profile: The Inverted U-Shape\nThe quadratic model reveals a fundamental pattern in labor economics - the inverted U-shaped age-earnings profile. Let’s understand what the results tell us:\nInterpreting the Quadratic Coefficients:\nFrom the regression: earnings = \\(\\beta_1 + \\beta_2 \\cdot age + \\beta_3 \\cdot age^2 + \\beta_4 \\cdot education + u\\)\nTypical Results: - Age coefficient (\\(\\beta_2\\)) ≈ +$3,000 to +$5,000 (positive, large) - Age² coefficient (\\(\\beta_3\\)) ≈ -$30 to -$50 (negative, small)\nWhat does this mean?\n\nThe Turning Point (Peak Earnings Age):\n\n\nFormula: \\(age^* = -\\beta_2 / (2\\beta_3)\\)\nTypical result: age 45-55 years\nInterpretation: Earnings increase until age 50, then decline\nThis matches real-world patterns: mid-career workers earn most\n\n\nMarginal Effect of Age (varies with age):\n\n\nFormula: \\(ME_{age} = \\beta_2 + 2\\beta_3 \\cdot age\\)\nAt age 25: ME ≈ +$3,000 (steep increase)\nAt age 40: ME ≈ +$1,000 (slower increase)\nAt age 50: ME ≈ $0 (peak earnings)\nAt age 60: ME ≈ -$1,000 (earnings decline)\n\n\nWhy the Inverted U-Shape?\n\n\nEarly career (20s-30s): Rapid skill accumulation, promotions → steep earnings growth\nMid-career (40s-50s): Peak productivity, seniority → highest earnings\nLate career (55+): Reduced hours, health decline, obsolete skills → earnings fall\nHuman capital theory: Investment in skills early, returns later, depreciation at end\n\nComparing Linear vs. Quadratic:\n\nLinear model: Assumes constant age effect (+$1,000/year regardless of age)\nMisses the fact that earnings growth slows down and eventually reverses\nPoor fit for older workers\nQuadratic model: Captures realistic life-cycle pattern\nAllows for increasing, then decreasing returns to age\nBetter fit (higher R²)\nMore accurate predictions for both young and old workers\n\nStatistical Significance:\nThe joint F-test for \\(H_0: \\beta_{age} = 0\\) AND \\(\\beta_{age^2} = 0\\) is highly significant (F &gt; 100, p &lt; 0.001): - This confirms age matters for earnings - The quadratic term is necessary (not just linear) - Individual t-tests can be misleading due to collinearity between age and age²\nEconomic Implications: - Peak earnings around age 50 suggests optimal retirement age discussions - Earnings decline after 55 may incentivize early retirement - Policy relevance for Social Security, pension design - Training investments more valuable early in career",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#joint-hypothesis-test-for-quadratic-term",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#joint-hypothesis-test-for-quadratic-term",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Joint Hypothesis Test for Quadratic Term",
    "text": "Joint Hypothesis Test for Quadratic Term\n\n# Joint hypothesis test: H0: age = 0 and agesq = 0\nprint(\"\\n\" + \"=\"*70)\nprint(\"JOINT HYPOTHESIS TEST: H₀: β_age = 0 AND β_agesq = 0\")\nprint(\"=\"*70)\n\nhypotheses = '(age = 0, agesq = 0)'\nf_test = ols_quad.wald_test(hypotheses, use_f=True)\nprint(f_test)\n\nprint(\"\\nInterpretation:\")\nif f_test.pvalue &lt; 0.05:\n    print(\"  Reject H₀: Age is jointly statistically significant in the model.\")\n    print(\"  The quadratic specification is justified.\")\nelse:\n    print(\"  Fail to reject H₀: Age is not statistically significant.\")\n\n\n======================================================================\nJOINT HYPOTHESIS TEST: H₀: β_age = 0 AND β_agesq = 0\n======================================================================\n&lt;F test: F=array([[9.29190281]]), p=0.00010166466829922585, df_denom=868, df_num=2&gt;\n\nInterpretation:\n  Reject H₀: Age is jointly statistically significant in the model.\n  The quadratic specification is justified.\n\n\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/statsmodels/base/model.py:1912: FutureWarning: The behavior of wald_test will change after 0.14 to returning scalar test statistic values. To get the future behavior now, set scalar to True. To silence this message while retaining the legacy behavior, set scalar to False.\n  warnings.warn(\n\n\n\nKey Concept 15.4: Testing Nonlinear Relationships\nWhen a quadratic term \\(x^2\\) is included, always test the joint significance of \\(x\\) and \\(x^2\\) together using an F-test. Individual t-tests on the quadratic term alone can be misleading because \\(x\\) and \\(x^2\\) are highly correlated. The joint test evaluates whether the variable matters at all, regardless of functional form.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#visualization-quadratic-relationship",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#visualization-quadratic-relationship",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Visualization: Quadratic Relationship",
    "text": "Visualization: Quadratic Relationship\n\n# Create visualization of quadratic relationship\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left plot: Fitted values vs age\nage_range = np.linspace(25, 65, 100)\neduc_mean = data_earnings['education'].mean()\n\n# Predictions holding education at mean\nlinear_pred = ols_linear_age.params['Intercept'] + ols_linear_age.params['age']*age_range + ols_linear_age.params['education']*educ_mean\nquad_pred = ols_quad.params['Intercept'] + bage*age_range + bagesq*age_range**2 + beducation*educ_mean\n\naxes[0].scatter(data_earnings['age'], data_earnings['earnings'], alpha=0.3, s=20, color='gray', label='Actual data')\naxes[0].plot(age_range, linear_pred, 'b-', linewidth=2, label='Linear model')\naxes[0].plot(age_range, quad_pred, 'r-', linewidth=2, label='Quadratic model')\naxes[0].axvline(x=turning_point, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label=f'Turning point ({turning_point:.1f} years)')\naxes[0].set_xlabel('Age (years)', fontsize=12)\naxes[0].set_ylabel('Earnings ($)', fontsize=12)\naxes[0].set_title('Earnings vs Age: Linear vs Quadratic Models', fontsize=13, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Right plot: Marginal effects\nme_linear = np.full_like(age_range, ols_linear_age.params['age'])\nme_quad = bage + 2 * bagesq * age_range\n\naxes[1].plot(age_range, me_linear, 'b-', linewidth=2, label='Linear model (constant)')\naxes[1].plot(age_range, me_quad, 'r-', linewidth=2, label='Quadratic model (varying)')\naxes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\naxes[1].axvline(x=turning_point, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label=f'Turning point ({turning_point:.1f} years)')\naxes[1].fill_between(age_range, 0, me_quad, where=(me_quad &gt; 0), alpha=0.2, color='green', label='Positive effect')\naxes[1].fill_between(age_range, 0, me_quad, where=(me_quad &lt; 0), alpha=0.2, color='red', label='Negative effect')\naxes[1].set_xlabel('Age (years)', fontsize=12)\naxes[1].set_ylabel('Marginal Effect on Earnings ($)', fontsize=12)\naxes[1].set_title('Marginal Effect of Age on Earnings', fontsize=13, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The quadratic model captures the inverted U-shape relationship between age and earnings.\")\n\n\n\n\n\n\n\n\nThe quadratic model captures the inverted U-shape relationship between age and earnings.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#standardized-variables",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#standardized-variables",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "15.4: Standardized Variables",
    "text": "15.4: Standardized Variables\nStandardized regression coefficients (beta coefficients) allow comparison of the relative importance of regressors measured in different units.\nStandardization formula: \\[z_x = \\frac{x - \\bar{x}}{s_x}\\]\nwhere \\(s_x\\) is the standard deviation of \\(x\\).\nStandardized coefficient: \\[\\beta^* = \\beta \\times \\frac{s_x}{s_y}\\]\nInterpretation:\n\n\\(\\beta^*\\) shows the effect of a one-standard-deviation change in \\(x\\) on \\(y\\), measured in standard deviations of \\(y\\)\nAllows comparison: which variable has the largest effect when measured in comparable units?\n\nUse cases:\n\nComparing effects of variables with different units\nMeta-analysis across studies\nUnderstanding relative importance of predictors\n\n\nprint(\"=\"*70)\nprint(\"15.4 STANDARDIZED VARIABLES\")\nprint(\"=\"*70)\n\n# Estimate a comprehensive model\nprint(\"\\n\" + \"-\"*70)\nprint(\"Linear Model with Mixed Regressors:\")\nprint(\"earnings ~ gender + age + agesq + education + dself + dgovt + lnhours\")\nprint(\"-\"*70)\nols_linear_mix = ols('earnings ~ gender + age + agesq + education + dself + dgovt + lnhours',\n                     data=data_earnings).fit(cov_type='HC1')\nprint(ols_linear_mix.summary())\n\n======================================================================\n15.4 STANDARDIZED VARIABLES\n======================================================================\n\n----------------------------------------------------------------------\nLinear Model with Mixed Regressors:\nearnings ~ gender + age + agesq + education + dself + dgovt + lnhours\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.206\nModel:                            OLS   Adj. R-squared:                  0.199\nMethod:                 Least Squares   F-statistic:                     15.72\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.72e-19\nTime:                        14:40:54   Log-Likelihood:                -10597.\nNo. Observations:                 872   AIC:                         2.121e+04\nDf Residuals:                     864   BIC:                         2.125e+04\nDf Model:                           7                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -3.566e+05   6.63e+04     -5.379      0.000   -4.87e+05   -2.27e+05\ngender     -1.433e+04   2696.808     -5.314      0.000   -1.96e+04   -9044.368\nage         3282.8676   1064.806      3.083      0.002    1195.886    5369.849\nagesq        -31.5781     12.214     -2.585      0.010     -55.516      -7.640\neducation   5399.3605    609.862      8.853      0.000    4204.054    6594.667\ndself       9360.4999   8711.602      1.074      0.283   -7713.926    2.64e+04\ndgovt       -291.1360   2914.162     -0.100      0.920   -6002.789    5420.517\nlnhours     6.996e+04   1.61e+04      4.345      0.000    3.84e+04    1.02e+05\n==============================================================================\nOmnibus:                      777.468   Durbin-Watson:                   2.041\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            25771.968\nSkew:                           3.997   Prob(JB):                         0.00\nKurtosis:                      28.405   Cond. No.                     6.41e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 6.41e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STANDARDIZED COEFFICIENTS\")\nprint(\"=\"*70)\n\n# Get standard deviations\nsd_y = data_earnings['earnings'].std()\nsd_gender = data_earnings['gender'].std()\nsd_age = data_earnings['age'].std()\nsd_agesq = data_earnings['agesq'].std()\nsd_education = data_earnings['education'].std()\nsd_dself = data_earnings['dself'].std()\nsd_dgovt = data_earnings['dgovt'].std()\nsd_lnhours = data_earnings['lnhours'].std()\n\n# Calculate standardized coefficients\nstandardized_coefs = {\n    'gender': ols_linear_mix.params['gender'] * sd_gender / sd_y,\n    'age': ols_linear_mix.params['age'] * sd_age / sd_y,\n    'agesq': ols_linear_mix.params['agesq'] * sd_agesq / sd_y,\n    'education': ols_linear_mix.params['education'] * sd_education / sd_y,\n    'dself': ols_linear_mix.params['dself'] * sd_dself / sd_y,\n    'dgovt': ols_linear_mix.params['dgovt'] * sd_dgovt / sd_y,\n    'lnhours': ols_linear_mix.params['lnhours'] * sd_lnhours / sd_y\n}\n\nprint(\"\\nStandardized Coefficients (Beta coefficients):\")\nprint(\"-\"*70)\nfor var, beta in sorted(standardized_coefs.items(), key=lambda x: abs(x[1]), reverse=True):\n    print(f\"  {var:12s}: {beta:7.4f}\")\n\nprint(\"\\nInterpretation:\")\nprint(\"  These show the effect of a 1 SD change in X on Y (in SD units)\")\nprint(\"  Allows comparison of relative importance across variables\")\n\n\n======================================================================\nSTANDARDIZED COEFFICIENTS\n======================================================================\n\nStandardized Coefficients (Beta coefficients):\n----------------------------------------------------------------------\n  age         :  0.6803\n  agesq       : -0.5736\n  education   :  0.3023\n  lnhours     :  0.2238\n  gender      : -0.1379\n  dself       :  0.0522\n  dgovt       : -0.0020\n\nInterpretation:\n  These show the effect of a 1 SD change in X on Y (in SD units)\n  Allows comparison of relative importance across variables\n\n\n\nKey Concept 15.5: Standardized Coefficients for Comparing Variable Importance\nStandardized (beta) coefficients \\(\\beta^* = \\beta \\times (s_x / s_y)\\) measure effects in standard deviation units, allowing comparison across variables with different scales. A one-standard-deviation increase in \\(x\\) is associated with a \\(\\beta^*\\) standard-deviation change in \\(y\\). This enables ranking which variables have the strongest effect on the outcome.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#calculate-standardized-coefficients",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#calculate-standardized-coefficients",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Calculate Standardized Coefficients",
    "text": "Calculate Standardized Coefficients\n\n\nComparing Apples to Apples: Standardized Coefficients\nStandardized coefficients allow us to answer: “Which variable matters most for earnings?”\nThe Problem with Raw Coefficients:\nLooking at the regression: - Education: +$5,000 per year - Age: +$1,000 per year - Hours: +$500 per hour\nCan we conclude education is “most important”? Not necessarily! - These variables are measured in different units - Education varies from 8 to 20 years (SD ≈ 2-3 years) - Age varies from 25 to 65 years (SD ≈ 10-12 years) - Hours varies from 35 to 60 per week (SD ≈ 8-10 hours)\nThe Solution: Standardized (Beta) Coefficients\nTransform to: “What if all variables were measured in standard deviations?”\nFormula: \\(\\beta^* = \\beta \\times (SD_x / SD_y)\\)\nInterpretation: - A 1 SD increase in X leads to \\(\\beta^*\\) SD change in Y - Now all variables are comparable (measured in same units)\nTypical Results from the Analysis:\nRanking by absolute standardized coefficients (largest to smallest):\n\nEducation (\\(\\beta^* \\approx 0.30\\) to $0.40$):\n\n\nStrongest predictor of earnings\n1 SD increase in education (≈2.5 years) → 0.35 SD increase in earnings (≈$15,000)\nConfirms education is the dominant factor\n\n\nHours worked (\\(\\beta^* \\approx 0.20\\) to $0.30$):\n\n\nSecond most important\n1 SD increase in hours (≈8 hours/week) → 0.25 SD increase in earnings\nMakes sense: more hours → proportionally more pay\n\n\nAge (\\(\\beta^* \\approx 0.15\\) to $0.20$):\n\n\nModerate importance\nBut remember this is from the linear specification\nThe quadratic model shows age matters more in a nonlinear way\n\n\nGender (\\(\\beta^* \\approx -0.15\\) to -0.20$):\n\n\nSubstantial negative effect\nBeing female → 0.15-0.20 SD decrease in earnings\nThis standardizes the raw gap of ~$10,000-$15,000\n\n\nEmployment type (dself, dgovt) (\\(\\beta^* \\approx 0.05\\) to $0.10$):\n\n\nSmaller effects\nSelf-employment or government sector have modest impacts\nOnce we control for education, age, hours\n\nKey Insights:\n\nEducation dominates: Strongest predictor, supporting human capital theory\nHours worked matters: Direct relationship (more work → more pay)\nCategorical variables (gender, employment type) also standardizable\nAge: Important but complex (quadratic, so beta coefficient understates it)\n\nWhen to Use Standardized Coefficients:\nGood for: - Comparing relative importance of predictors - Meta-analysis across studies - Understanding which variables to prioritize in data collection\nNot good for: - Policy analysis (need actual units for cost-benefit) - Prediction (use original coefficients) - Variables with naturally meaningful units (e.g., dummy variables)\nCaution: - Standardized coefficients depend on sample variation - If your sample has little variation in X, \\(\\beta^*\\) will be small - Different samples → different standardized coefficients - Raw coefficients more stable across samples",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#visualization-standardized-coefficients",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#visualization-standardized-coefficients",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Visualization: Standardized Coefficients",
    "text": "Visualization: Standardized Coefficients\n\n# Create visualization comparing standardized coefficients\nfig, ax = plt.subplots(figsize=(10, 7))\nvars_plot = list(standardized_coefs.keys())\nbetas_plot = list(standardized_coefs.values())\n\ncolors = ['red' if b &lt; 0 else 'blue' for b in betas_plot]\nbars = ax.barh(vars_plot, betas_plot, color=colors, alpha=0.7)\n\nax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\nax.set_xlabel('Standardized Coefficient (SD units)', fontsize=12)\nax.set_ylabel('Variable', fontsize=12)\nax.set_title('Standardized Regression Coefficients\\n(Effect of 1 SD change in X on Y, in SD units)',\n             fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Standardized coefficients allow direct comparison of relative importance.\")\n\n\n\n\n\n\n\n\nStandardized coefficients allow direct comparison of relative importance.\n\n\nNow that we can compare variable importance using standardized coefficients, let’s explore interaction terms that allow marginal effects to vary across observations.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#interaction-terms-and-marginal-effects",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#interaction-terms-and-marginal-effects",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "15.5: Interaction Terms and Marginal Effects",
    "text": "15.5: Interaction Terms and Marginal Effects\nInteraction terms allow the marginal effect of one variable to depend on the level of another variable.\nModel with interaction: \\[y = \\beta_1 + \\beta_2 x + \\beta_3 z + \\beta_4 (x \\times z) + u\\]\nMarginal effect of \\(x\\): \\[ME_x = \\beta_2 + \\beta_4 z\\]\nMarginal effect of \\(z\\): \\[ME_z = \\beta_3 + \\beta_4 x\\]\nImportant:\n\nIndividual t-tests on \\(\\beta_2\\) or \\(\\beta_4\\) are misleading\nTest significance of \\(x\\) jointly: \\(H_0: \\beta_2 = 0\\) AND \\(\\beta_4 = 0\\)\nInteraction variables are often highly correlated with main effects (multicollinearity)\n\n\nprint(\"=\"*70)\nprint(\"15.5 INTERACTION TERMS AND MARGINAL EFFECTS\")\nprint(\"=\"*70)\n\n# Model without interaction (for comparison)\nprint(\"\\n\" + \"-\"*70)\nprint(\"Model WITHOUT Interaction: earnings ~ age + education\")\nprint(\"-\"*70)\nols_no_interact = ols('earnings ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(ols_no_interact.summary())\n\n# Model with interaction\nprint(\"\\n\" + \"-\"*70)\nprint(\"Model WITH Interaction: earnings ~ age + education + agebyeduc\")\nprint(\"-\"*70)\nols_interact = ols('earnings ~ age + education + agebyeduc', data=data_earnings).fit(cov_type='HC1')\nprint(ols_interact.summary())\n\n======================================================================\n15.5 INTERACTION TERMS AND MARGINAL EFFECTS\n======================================================================\n\n----------------------------------------------------------------------\nModel WITHOUT Interaction: earnings ~ age + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.113\nMethod:                 Least Squares   F-statistic:                     42.85\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.79e-18\nTime:                        14:40:54   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     869   BIC:                         2.131e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.688e+04   1.13e+04     -4.146      0.000    -6.9e+04   -2.47e+04\nage          524.9953    151.387      3.468      0.001     228.281     821.709\neducation   5811.3673    641.533      9.059      0.000    4553.986    7068.749\n==============================================================================\nOmnibus:                      825.668   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31187.987\nSkew:                           4.353   Prob(JB):                         0.00\nKurtosis:                      30.975   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n----------------------------------------------------------------------\nModel WITH Interaction: earnings ~ age + education + agebyeduc\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.112\nMethod:                 Least Squares   F-statistic:                     31.80\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.65e-19\nTime:                        14:40:54   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.130e+04\nDf Residuals:                     868   BIC:                         2.132e+04\nDf Model:                           3                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -2.909e+04    3.1e+04     -0.940      0.347   -8.98e+04    3.16e+04\nage          127.4922    719.280      0.177      0.859   -1282.270    1537.255\neducation   4514.9867   2401.517      1.880      0.060    -191.901    9221.874\nagebyeduc     29.0392     56.052      0.518      0.604     -80.821     138.899\n==============================================================================\nOmnibus:                      825.324   Durbin-Watson:                   2.072\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31144.116\nSkew:                           4.351   Prob(JB):                         0.00\nKurtosis:                      30.955   Cond. No.                     1.28e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 1.28e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nKey Concept 15.6: Interaction Terms and Varying Marginal Effects\nWith an interaction term \\(x \\times z\\), the marginal effect of \\(x\\) depends on \\(z\\): \\(ME_x = \\beta_2 + \\beta_4 z\\). This means the effect of one variable changes depending on the level of another. Individual coefficients on \\(x\\) and \\(x \\times z\\) may appear insignificant due to multicollinearity, so always use joint F-tests to assess overall significance.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#interaction-model-marginal-effects-and-joint-tests",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#interaction-model-marginal-effects-and-joint-tests",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Interaction Model: Marginal Effects and Joint Tests",
    "text": "Interaction Model: Marginal Effects and Joint Tests\n\n\nHow Returns to Education Change with Age\nThe interaction model reveals that the payoff to education depends on age - a fascinating finding with important implications.\nInterpreting the Interaction Results:\nFrom: earnings = \\(\\beta_1 + \\beta_2 \\cdot age + \\beta_3 \\cdot education + \\beta_4 \\cdot (age \\times education) + u\\)\nTypical Coefficients: - Education (\\(\\beta_3\\)): Around -$10,000 to -$5,000 (often negative!) - Age × Education (\\(\\beta_4\\)): Around +$200 to +$400 (positive)\nWhat This Means:\nThe marginal effect of education is: \\[ME_{education} = \\beta_3 + \\beta_4 \\cdot age\\]\nAt Different Ages: - Age 25: ME ≈ -$5,000 + $300(25) = +$2,500 per year of education - Age 40: ME ≈ -$5,000 + $300(40) = +$7,000 per year of education - Age 55: ME ≈ -$5,000 + $300(55) = +$11,500 per year of education\nInterpretation:\n\nReturns to education INCREASE with age\n\n\nYoung workers (age 25): +$2,500 per year of education\nOlder workers (age 55): +$11,500 per year of education\nEducation payoff is 4-5 times larger for older workers!\n\n\nWhy Does This Happen?\n\n\nComplementarity: Education and experience work together\nMore educated workers learn faster on the job\nEducation enables access to career ladders with steeper wage growth\nCompound returns: Higher starting point → higher percentage raises\nNetwork effects: Educated workers build more valuable professional networks\n\n\nAlternative Interpretation (life-cycle earnings):\n\n\nHigh school graduates: Earnings flatten by age 40-50\nCollege graduates: Earnings keep growing until age 50-55\nThe gap widens with age\n\nStatistical Significance:\n\nIndividual coefficients may have large SEs (multicollinearity between age, education, and their product)\nJoint F-test is crucial: Test \\(H_0: \\beta_{education} = 0\\) AND \\(\\beta_{age \\times educ} = 0\\)\nResult: Highly significant (F &gt; 30, p &lt; 0.001)\nEducation matters, but its effect is age-dependent\n\nMulticollinearity Warning:\nThe correlation matrix shows: - Corr(age, age×education) ≈ 0.95 (very high!) - Corr(education, age×education) ≈ 0.90 (very high!)\nThis explains why: - Individual t-statistics may be small (large SEs) - Coefficients sensitive to small changes in data - But joint tests remain powerful\nPolicy Implications:\n\nHigher education pays off more over the career\n\n\nShort-run costs, long-run gains compound\nEducation is an investment with increasing returns\n\n\nOlder workers benefit most from education\n\n\nAdult education programs can have large payoffs\nRetraining valuable even late in career\n\n\nInequality implications\n\n\nEducation-based wage gap widens with age\nContributes to lifetime earnings inequality\n\nPractical Advice for Estimation:\nDo: - Always test interactions jointly with main effects - Report F-statistics for joint tests - Calculate marginal effects at representative ages (25, 40, 55) - Plot the relationship to visualize\nDon’t: - Rely on individual t-tests when variables are highly correlated - Drop the main effect if interaction is “insignificant” - Interpret the main effect coefficient alone (it’s conditional on age=0!)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#joint-hypothesis-tests-for-interactions",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#joint-hypothesis-tests-for-interactions",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Joint Hypothesis Tests for Interactions",
    "text": "Joint Hypothesis Tests for Interactions\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"JOINT HYPOTHESIS TESTS\")\nprint(\"=\"*70)\n\n# Test 1: Joint test for age\nprint(\"\\nTest 1: H₀: β_age = 0 AND β_agebyeduc = 0\")\nprint(\"(Tests whether age matters at all)\")\nprint(\"-\"*70)\nhypotheses_age = '(age = 0, agebyeduc = 0)'\nf_test_age = ols_interact.wald_test(hypotheses_age, use_f=True)\nprint(f_test_age)\n\n# Test 2: Joint test for education\nprint(\"\\nTest 2: H₀: β_education = 0 AND β_agebyeduc = 0\")\nprint(\"(Tests whether education matters at all)\")\nprint(\"-\"*70)\nhypotheses_educ = '(education = 0, agebyeduc = 0)'\nf_test_educ = ols_interact.wald_test(hypotheses_educ, use_f=True)\nprint(f_test_educ)\n\nprint(\"\\nKey insight: Individual coefficients may be insignificant due to\")\nprint(\"multicollinearity, but joint tests reveal strong statistical significance.\")\n\n\n======================================================================\nJOINT HYPOTHESIS TESTS\n======================================================================\n\nTest 1: H₀: β_age = 0 AND β_agebyeduc = 0\n(Tests whether age matters at all)\n----------------------------------------------------------------------\n&lt;F test: F=array([[6.48958655]]), p=0.0015939412046954808, df_denom=868, df_num=2&gt;\n\nTest 2: H₀: β_education = 0 AND β_agebyeduc = 0\n(Tests whether education matters at all)\n----------------------------------------------------------------------\n&lt;F test: F=array([[43.00467267]]), p=1.5549618458663995e-18, df_denom=868, df_num=2&gt;\n\nKey insight: Individual coefficients may be insignificant due to\nmulticollinearity, but joint tests reveal strong statistical significance.\n\n\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/statsmodels/base/model.py:1912: FutureWarning: The behavior of wald_test will change after 0.14 to returning scalar test statistic values. To get the future behavior now, set scalar to True. To silence this message while retaining the legacy behavior, set scalar to False.\n  warnings.warn(",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#multicollinearity-in-interaction-models",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#multicollinearity-in-interaction-models",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Multicollinearity in Interaction Models",
    "text": "Multicollinearity in Interaction Models\n\n# Check correlation between regressors\nprint(\"\\n\" + \"=\"*70)\nprint(\"MULTICOLLINEARITY: Correlation Matrix of Regressors\")\nprint(\"=\"*70)\n\ncorr_matrix = data_earnings[['age', 'education', 'agebyeduc']].corr()\nprint(corr_matrix)\n\nprint(\"\\nInterpretation:\")\nprint(f\"  Correlation(age, agebyeduc) = {corr_matrix.loc['age', 'agebyeduc']:.3f}\")\nprint(f\"  Correlation(education, agebyeduc) = {corr_matrix.loc['education', 'agebyeduc']:.3f}\")\nprint(\"\\nHigh correlations explain why individual coefficients have large standard errors,\")\nprint(\"even though the variables are jointly significant.\")\n\n\n======================================================================\nMULTICOLLINEARITY: Correlation Matrix of Regressors\n======================================================================\n                age  education  agebyeduc\nage        1.000000  -0.038153   0.729136\neducation -0.038153   1.000000   0.635961\nagebyeduc  0.729136   0.635961   1.000000\n\nInterpretation:\n  Correlation(age, agebyeduc) = 0.729\n  Correlation(education, agebyeduc) = 0.636\n\nHigh correlations explain why individual coefficients have large standard errors,\neven though the variables are jointly significant.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#retransformation-bias-and-prediction-from-log-models",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#retransformation-bias-and-prediction-from-log-models",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Retransformation Bias and Prediction from Log Models",
    "text": "Retransformation Bias and Prediction from Log Models\nWhen predicting \\(y\\) from a model with \\(\\ln y\\) as the dependent variable, naive retransformation introduces bias.\nProblem:\n\nModel: \\(\\ln y = \\beta_1 + \\beta_2 x + u\\)\nNaive prediction: \\(\\hat{y} = \\exp(\\widehat{\\ln y})\\)\nThis systematically underpredicts \\(y\\)\n\nWhy?\n\nJensen’s inequality: \\(E[\\exp(u)] &gt; \\exp(E[u])\\)\nWe need: \\(E[y|x] = \\exp(\\beta_1 + \\beta_2 x) \\times E[\\exp(u)|x]\\)\n\nSolution (assuming normal, homoskedastic errors): \\[\\tilde{y} = \\exp(s_e^2/2) \\times \\exp(\\widehat{\\ln y})\\]\nwhere \\(s_e\\) is the standard error of the regression (RMSE).\nAdjustment factor: \\[\\exp(s_e^2/2)\\]\nExample: If \\(s_e = 0.4\\), adjustment factor = \\(\\exp(0.16/2) = 1.083\\)\n\nprint(\"=\"*70)\nprint(\"RETRANSFORMATION BIAS DEMONSTRATION\")\nprint(\"=\"*70)\n\n# Get RMSE from log model\nrmse_log = np.sqrt(ols_loglin.mse_resid)\n\nprint(f\"\\nRMSE from log model: {rmse_log:.4f}\")\nprint(f\"Adjustment factor: exp({rmse_log:.4f}²/2) = {np.exp(rmse_log**2/2):.4f}\")\n\n# Predictions\nlinear_predict = ols_linear.predict()\nlog_fitted = ols_loglin.predict()\n\n# Biased retransformation (naive)\nbiased_predict = np.exp(log_fitted)\n\n# Adjusted retransformation\nadjustment_factor = np.exp(rmse_log**2 / 2)\nadjusted_predict = adjustment_factor * np.exp(log_fitted)\n\n# Compare means\nprint(\"\\n\" + \"-\"*70)\nprint(\"Comparison of Predicted Means\")\nprint(\"-\"*70)\nprint(f\"  Actual mean earnings:        ${data_earnings['earnings'].mean():,.2f}\")\nprint(f\"  Levels model prediction:     ${linear_predict.mean():,.2f}\")\nprint(f\"  Biased retransformation:     ${biased_predict.mean():,.2f}\")\nprint(f\"  Adjusted retransformation:   ${adjusted_predict.mean():,.2f}\")\n\nprint(\"\\nThe adjusted retransformation matches the actual mean closely!\")\n\n======================================================================\nRETRANSFORMATION BIAS DEMONSTRATION\n======================================================================\n\nRMSE from log model: 0.6164\nAdjustment factor: exp(0.6164²/2) = 1.2092\n\n----------------------------------------------------------------------\nComparison of Predicted Means\n----------------------------------------------------------------------\n  Actual mean earnings:        $56,368.69\n  Levels model prediction:     $56,368.69\n  Biased retransformation:     $45,838.14\n  Adjusted retransformation:   $55,427.36\n\nThe adjusted retransformation matches the actual mean closely!\n\n\n\nKey Concept 15.7: Retransformation Bias Correction\nThe naive prediction \\(\\exp(\\widehat{\\ln y})\\) systematically underestimates \\(E[y|x]\\) because \\(E[\\exp(u)] \\neq \\exp(E[u])\\) (Jensen’s inequality). Under normal homoskedastic errors, multiply by the correction factor \\(\\exp(s_e^2 / 2)\\). Duan’s smearing estimator provides a nonparametric alternative: \\(\\hat{y} = \\exp(\\widehat{\\ln y}) \\times \\frac{1}{n}\\sum \\exp(\\hat{u}_i)\\).\n\n\n\nThe Retransformation Bias Problem\nWhen predicting from log models, a naive approach systematically underpredicts. Here’s why and how to fix it:\nThe Problem:\nYou estimate: \\(\\ln(y) = X\\beta + u\\)\nNaive prediction: \\(\\hat{y}_{naive} = \\exp(\\widehat{\\ln y}) = \\exp(X\\hat{\\beta})\\)\nWhy this is wrong:\nDue to Jensen’s Inequality: \\[E[y|X] = E[\\exp(X\\beta + u)] = \\exp(X\\beta) \\cdot E[\\exp(u)] \\neq \\exp(X\\beta)\\]\nIf \\(u \\sim N(0, \\sigma^2)\\), then \\(E[\\exp(u)] = \\exp(\\sigma^2/2) &gt; 1\\)\nEmpirical Evidence from the Results:\nFrom the analysis above: - Actual mean earnings: ~$52,000 - Naive retransformation: ~$48,000 (underpredicts by ~$4,000 or 8%) - Adjusted retransformation: ~$52,000 (matches actual mean!)\nThe Solution:\nMultiply by adjustment factor: \\[\\hat{y}_{adjusted} = \\exp(s_e^2/2) \\times \\exp(X\\hat{\\beta})\\]\nwhere \\(s_e\\) = RMSE from the log regression\nExample Calculation:\nFrom log-linear model: - RMSE (\\(s_e\\)) ≈ 0.40 to 0.45 - Adjustment factor = $(0.42^2/2) = (0.088) *1.092 - Predictions are about 9.2% too low** without adjustment!\nWhen Does This Matter Most?\n\nLarge residual variance (\\(\\sigma^2\\) large):\n\n\nAdjustment factor = \\(\\exp(0.20^2/2) = 1.020\\) (2% adjustment)\nvs. \\(\\exp(0.60^2/2) = 1.197\\) (20% adjustment!)\n\n\nPrediction vs. estimation:\n\n\nFor coefficients (\\(\\beta\\)): Use log regression directly\nFor predictions (\\(y\\)): Must adjust for retransformation bias\n\n\nAggregate predictions:\n\n\nPredicting total revenue, total costs, etc.\nBias compounds: sum of biased predictions → very wrong total\n\nAlternative Solutions:\n\nSmearing estimator (Duan 1983):\n\n\nDon’t assume normality\n\\(\\hat{y} = \\frac{1}{n}\\sum_{i=1}^n \\exp(\\hat{u}_i) \\times \\exp(X\\hat{\\beta})\\)\nMore robust, doesn’t require normal errors\n\n\nBootstrap:\n\n\nResample residuals many times\nAverage predictions across bootstrap samples\n\n\nGeneralized Linear Models (GLM):\n\n\nEstimate \\(E[y|X]\\) directly (not \\(E[\\ln y|X]\\))\nNo retransformation needed\n\nPractical Recommendations:\nFor coefficient interpretation: - Use log models freely - Interpret as percentage changes - No adjustment needed\nFor prediction: - ALWAYS apply adjustment factor - Check: Do predicted means match actual means? - Report both naive and adjusted if showing methodology\nCommon mistakes: - Forgetting adjustment entirely (very common!) - Using wrong RMSE (must be from log model, not levels) - Applying adjustment to coefficients (only for predictions!)\nReal-World Impact:\nIn healthcare cost prediction: - Naive: Predict average cost = $8,000 - Adjusted: Predict average cost = $10,000 - 25% underestimate! - Budget shortfall, inadequate insurance premiums\nIn income tax revenue forecasting: - Small % bias in individual predictions - Aggregated to millions of taxpayers - Billions of dollars in forecast error!",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#visualization-prediction-comparison",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#visualization-prediction-comparison",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Visualization: Prediction Comparison",
    "text": "Visualization: Prediction Comparison\n\n# Visualize prediction accuracy\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot 1: Levels model\naxes[0].scatter(data_earnings['earnings'], linear_predict, alpha=0.5, s=30)\naxes[0].plot([0, 500000], [0, 500000], 'r--', linewidth=2)\naxes[0].set_xlabel('Actual Earnings ($)', fontsize=11)\naxes[0].set_ylabel('Predicted Earnings ($)', fontsize=11)\naxes[0].set_title('Levels Model Predictions', fontsize=12, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Biased retransformation\naxes[1].scatter(data_earnings['earnings'], biased_predict, alpha=0.5, s=30, color='orange')\naxes[1].plot([0, 500000], [0, 500000], 'r--', linewidth=2)\naxes[1].set_xlabel('Actual Earnings ($)', fontsize=11)\naxes[1].set_ylabel('Predicted Earnings ($)', fontsize=11)\naxes[1].set_title('Log-Linear: Biased (Naive) Retransformation', fontsize=12, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\n# Plot 3: Adjusted retransformation\naxes[2].scatter(data_earnings['earnings'], adjusted_predict, alpha=0.5, s=30, color='green')\naxes[2].plot([0, 500000], [0, 500000], 'r--', linewidth=2)\naxes[2].set_xlabel('Actual Earnings ($)', fontsize=11)\naxes[2].set_ylabel('Predicted Earnings ($)', fontsize=11)\naxes[2].set_title('Log-Linear: Adjusted Retransformation', fontsize=12, fontweight='bold')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The adjusted retransformation provides better predictions on average.\")\n\n\n\n\n\n\n\n\nThe adjusted retransformation provides better predictions on average.\n\n\nHaving addressed the retransformation bias problem, we now combine all transformation techniques in a single comprehensive model.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#comprehensive-model-with-log-transformed-dependent-variable",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#comprehensive-model-with-log-transformed-dependent-variable",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Comprehensive Model with Log-Transformed Dependent Variable",
    "text": "Comprehensive Model with Log-Transformed Dependent Variable\n\nprint(\"=\"*70)\nprint(\"COMPREHENSIVE MODEL WITH MIXED REGRESSOR TYPES\")\nprint(\"=\"*70)\n\n# Log-transformed dependent variable\nprint(\"\\n\" + \"-\"*70)\nprint(\"Log-Linear Model with Mixed Regressors:\")\nprint(\"lnearnings ~ gender + age + agesq + education + dself + dgovt + lnhours\")\nprint(\"-\"*70)\nols_log_mix = ols('lnearnings ~ gender + age + agesq + education + dself + dgovt + lnhours',\n                  data=data_earnings).fit(cov_type='HC1')\nprint(ols_log_mix.summary())\n\nprint(\"\\n\" + \"-\"*70)\nprint(\"INTERPRETATION OF COEFFICIENTS (controlling for other regressors)\")\nprint(\"-\"*70)\n\nprint(f\"\\n1. Gender: {ols_log_mix.params['gender']:.4f}\")\nprint(f\"   Women earn approximately {100*ols_log_mix.params['gender']:.1f}% less than men\")\n\nprint(f\"\\n2. Age and Age²: Quadratic relationship\")\nb_age_log = ols_log_mix.params['age']\nb_agesq_log = ols_log_mix.params['agesq']\nturning_point_log = -b_age_log / (2 * b_agesq_log)\nprint(f\"   Turning point: {turning_point_log:.1f} years\")\nprint(f\"   Earnings increase with age until {turning_point_log:.1f}, then decrease\")\n\nprint(f\"\\n3. Education: {ols_log_mix.params['education']:.4f}\")\nprint(f\"   One additional year of education increases earnings by {100*ols_log_mix.params['education']:.1f}%\")\n\nprint(f\"\\n4. Self-employed (dself): {ols_log_mix.params['dself']:.4f}\")\nprint(f\"   Self-employed earn approximately {100*ols_log_mix.params['dself']:.1f}% less than private sector\")\nprint(f\"   (though not statistically significant at 5% level)\")\n\nprint(f\"\\n5. Government (dgovt): {ols_log_mix.params['dgovt']:.4f}\")\nprint(f\"   Government workers earn approximately {100*ols_log_mix.params['dgovt']:.1f}% more than private sector\")\nprint(f\"   (though not statistically significant at 5% level)\")\n\nprint(f\"\\n6. Ln(Hours): {ols_log_mix.params['lnhours']:.4f}\")\nprint(f\"   This is an ELASTICITY: A 1% increase in hours increases earnings by {ols_log_mix.params['lnhours']:.3f}%\")\nprint(f\"   Nearly proportional relationship (elasticity ≈ 1)\")\n\n======================================================================\nCOMPREHENSIVE MODEL WITH MIXED REGRESSOR TYPES\n======================================================================\n\n----------------------------------------------------------------------\nLog-Linear Model with Mixed Regressors:\nlnearnings ~ gender + age + agesq + education + dself + dgovt + lnhours\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             lnearnings   R-squared:                       0.281\nModel:                            OLS   Adj. R-squared:                  0.275\nMethod:                 Least Squares   F-statistic:                     35.04\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           3.62e-43\nTime:                        14:40:55   Log-Likelihood:                -761.92\nNo. Observations:                 872   AIC:                             1540.\nDf Residuals:                     864   BIC:                             1578.\nDf Model:                           7                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.4594      0.648      6.885      0.000       3.190       5.729\ngender        -0.1928      0.039     -4.881      0.000      -0.270      -0.115\nage            0.0561      0.016      3.550      0.000       0.025       0.087\nagesq         -0.0005      0.000     -2.992      0.003      -0.001      -0.000\neducation      0.0934      0.008     11.168      0.000       0.077       0.110\ndself         -0.1180      0.101     -1.166      0.243      -0.316       0.080\ndgovt          0.0698      0.045      1.534      0.125      -0.019       0.159\nlnhours        0.9754      0.142      6.882      0.000       0.698       1.253\n==============================================================================\nOmnibus:                       29.695   Durbin-Watson:                   2.054\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               74.613\nSkew:                           0.025   Prob(JB):                     6.28e-17\nKurtosis:                       4.432   Cond. No.                     6.41e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 6.41e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n----------------------------------------------------------------------\nINTERPRETATION OF COEFFICIENTS (controlling for other regressors)\n----------------------------------------------------------------------\n\n1. Gender: -0.1928\n   Women earn approximately -19.3% less than men\n\n2. Age and Age²: Quadratic relationship\n   Turning point: 51.1 years\n   Earnings increase with age until 51.1, then decrease\n\n3. Education: 0.0934\n   One additional year of education increases earnings by 9.3%\n\n4. Self-employed (dself): -0.1180\n   Self-employed earn approximately -11.8% less than private sector\n   (though not statistically significant at 5% level)\n\n5. Government (dgovt): 0.0698\n   Government workers earn approximately 7.0% more than private sector\n   (though not statistically significant at 5% level)\n\n6. Ln(Hours): 0.9754\n   This is an ELASTICITY: A 1% increase in hours increases earnings by 0.975%\n   Nearly proportional relationship (elasticity ≈ 1)\n\n\n\nKey Concept 15.8: Models with Mixed Regressor Types\nA single regression model can combine levels, quadratics, logarithms, dummies, and interactions. Each coefficient is interpreted according to its transformation type: linear coefficients as marginal effects, log coefficients as semi-elasticities or elasticities, quadratic terms through their marginal effect formula, and dummies as group differences. This flexibility makes regression a powerful tool for modeling complex economic relationships.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#key-takeaways",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#key-takeaways",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nLogarithmic Transformations\n\nLog-linear model (\\(\\ln y = \\beta_1 + \\beta_2 x\\)): coefficient \\(\\beta_2\\) is a semi-elasticity – a 1-unit change in \\(x\\) is associated with a \\(100 \\times \\beta_2\\)% change in \\(y\\)\nLog-log model (\\(\\ln y = \\beta_1 + \\beta_2 \\ln x\\)): coefficient \\(\\beta_2\\) is an elasticity – a 1% change in \\(x\\) is associated with a \\(\\beta_2\\)% change in \\(y\\)\nMarginal effects in levels require back-transformation: \\(ME_x = \\beta_2 \\hat{y}\\) (log-linear) or \\(ME_x = \\beta_2 \\hat{y}/x\\) (log-log)\nLog transformations are especially useful for right-skewed data (earnings, prices, GDP)\n\n\n\nQuadratic and Polynomial Models\n\nQuadratic models \\(y = \\beta_1 + \\beta_2 x + \\beta_3 x^2 + u\\) capture nonlinear relationships with a turning point\nTurning point: \\(x^* = -\\beta_2 / (2\\beta_3)\\) – where the relationship changes direction\nMarginal effect varies with \\(x\\): \\(ME = \\beta_2 + 2\\beta_3 x\\) – not constant as in linear models\nIf \\(\\beta_3 &lt; 0\\): inverted U-shape (earnings-age); if \\(\\beta_3 &gt; 0\\): U-shape\nAlways test joint significance of \\(x\\) and \\(x^2\\) together\n\n\n\nStandardized Coefficients\n\nStandardized (beta) coefficients measure effects in standard deviation units: \\(\\beta^* = \\beta \\times (s_x / s_y)\\)\nAllow comparing the relative importance of variables measured in different units\nA one-standard-deviation increase in \\(x\\) is associated with a \\(\\beta^*\\) standard-deviation change in \\(y\\)\nUseful for ranking which variables have the strongest effect on the outcome\n\n\n\nInteraction Terms and Marginal Effects\n\nInteraction terms (\\(x \\times z\\)) allow the marginal effect of \\(x\\) to depend on \\(z\\): \\(ME_x = \\beta_2 + \\beta_4 z\\)\nIndividual coefficients may be insignificant due to multicollinearity with the interaction\nAlways use joint F-tests to assess overall significance of a variable and its interactions\nExample: Returns to education may increase with age (positive interaction coefficient)\n\n\n\nRetransformation Bias and Prediction\n\nNaive prediction \\(\\exp(\\widehat{\\ln y})\\) systematically underestimates \\(E[y|x]\\) due to Jensen’s inequality\nCorrection: multiply by \\(\\exp(s_e^2 / 2)\\) where \\(s_e\\) is the standard error of the log regression\nDuan’s smearing estimator provides a nonparametric alternative that doesn’t assume normality\nCannot directly compare \\(R^2\\) across models with different dependent variables (\\(y\\) vs \\(\\ln y\\))\n\n\n\nGeneral Lessons\n\nA single model can combine levels, quadratics, logs, dummies, and interactions – interpret each coefficient according to its transformation type\nVariable transformations are among the most powerful tools for capturing realistic economic relationships\nAlways check whether nonlinear specifications improve model fit before adopting more complex forms\n\n\n\n\nPython Tools Used in This Chapter\n# Log transformations\nnp.log(df['variable'])                    # Natural logarithm\n\n# Quadratic terms\ndf['x_sq'] = df['x'] ** 2                # Create squared term\n\n# Interaction terms\ndf['x_z'] = df['x'] * df['z']            # Create interaction\n\n# Standardized coefficients\nbeta_star = beta * (s_x / s_y)           # Manual calculation\n\n# Joint hypothesis tests\nmodel.f_test('x = 0, x_sq = 0')          # Joint F-test\n\n# Retransformation correction\ny_pred = np.exp(ln_y_hat) * np.exp(s_e**2 / 2)\n\nNext Steps: - Chapter 16: Model Diagnostics - Chapter 17: Panel Data and Causation\n\nCongratulations! You’ve completed Chapter 15. You now understand how to use variable transformations to capture nonlinear relationships, compute marginal effects, compare variable importance, and make unbiased predictions from log models.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#practice-exercises",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#practice-exercises",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Marginal Effect of a Quadratic\nFor the fitted model \\(\\hat{y} = 2 + 3x + 4x^2\\) from a dataset with \\(\\bar{y} = 30\\) and \\(\\bar{x} = 2\\):\n(a) Compute the marginal effect of a one-unit change in \\(x\\) at \\(x = 2\\) using calculus.\n(b) Compute the average marginal effect (AME) if the data contains observations at \\(x = 1, 2, 3\\).\n(c) Is this relationship U-shaped or inverted U-shaped? At what value of \\(x\\) is the turning point?\n\nExercise 2: Interaction Marginal Effect\nFor the fitted model \\(\\hat{y} = 1 + 2x + 4d + 7(d \\times x)\\) from a dataset with \\(\\bar{y} = 22\\), \\(\\bar{x} = 3\\), and \\(\\bar{d} = 0.5\\):\n(a) Compute the marginal effect of \\(x\\) when \\(d = 0\\) and when \\(d = 1\\).\n(b) Compute the average marginal effect (AME) of \\(x\\).\n(c) Interpret the coefficient 7 on the interaction term in plain language.\n\nExercise 3: Retransformation Prediction\nFor the model \\(\\widehat{\\ln y} = 1 + 2x\\) with \\(n = 100\\) and \\(s_e = 0.3\\):\n(a) Give the naive prediction of \\(E[y|x = 1]\\).\n(b) Give the bias-corrected prediction using the normal correction factor.\n(c) By what percentage does the naive prediction underestimate the true expected value?\n\nExercise 4: Log Model Interpretation\nA researcher estimates two models using earnings data: - Log-linear: \\(\\widehat{\\ln(\\text{earnings})} = 8.5 + 0.08 \\times \\text{education}\\) - Log-log: \\(\\widehat{\\ln(\\text{earnings})} = 3.2 + 0.45 \\times \\ln(\\text{hours})\\)\n(a) Interpret the coefficient 0.08 in the log-linear model.\n(b) Interpret the coefficient 0.45 in the log-log model.\n(c) Can you directly compare \\(R^2\\) between these two models? Why or why not?\n\nExercise 5: Standardized Coefficient Ranking\nA regression of earnings on age, education, and hours yields these unstandardized coefficients and standard deviations:\n\n\n\nVariable\nCoefficient\n\\(s_x\\)\n\n\n\n\nAge\n500\n10\n\n\nEducation\n3,000\n3\n\n\nHours\n200\n8\n\n\n\nThe standard deviation of earnings is \\(s_y = 25{,}000\\).\n(a) Compute the standardized coefficient for each variable.\n(b) Rank the variables by their relative importance.\n(c) Why might the ranking differ from what the unstandardized coefficients suggest?\n\nExercise 6: Model Selection\nYou have three candidate models for earnings: - Model A (linear): \\(\\text{earnings} = \\beta_1 + \\beta_2 \\text{age} + u\\) - Model B (quadratic): \\(\\text{earnings} = \\beta_1 + \\beta_2 \\text{age} + \\beta_3 \\text{age}^2 + u\\) - Model C (log-linear): \\(\\ln(\\text{earnings}) = \\beta_1 + \\beta_2 \\text{age} + u\\)\n(a) What criteria would you use to compare Models A and B? Can you use \\(R^2\\)?\n(b) Can you directly compare \\(R^2\\) between Models B and C? Explain.\n(c) Describe a prediction-based approach to compare all three models.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#case-studies",
    "href": "../notebooks_colab/ch15_Regression_with_Transformed_Variables.html#case-studies",
    "title": "Chapter 15: Regression with Transformed Variables",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Transformed Variables for Cross-Country Productivity Analysis\nIn this case study, you will apply variable transformation techniques to analyze cross-country labor productivity patterns and determine the best functional form for modeling productivity determinants.\nDataset: Mendez Convergence Clubs\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat2014 = dat[dat['year'] == 2014].copy()\ndat2014['ln_lp'] = np.log(dat2014['lp'])\ndat2014['ln_rk'] = np.log(dat2014['rk'])\nVariables: lp (labor productivity), rk (physical capital), hc (human capital), region (world region)\n\n\nTask 1: Compare Log Specifications (Guided)\nEstimate three models of labor productivity on physical capital:\n\nLevels: lp ~ rk\nLog-linear: ln_lp ~ rk\nLog-log: ln_lp ~ ln_rk\n\nimport statsmodels.formula.api as smf\nm1 = smf.ols('lp ~ rk', data=dat2014).fit(cov_type='HC1')\nm2 = smf.ols('ln_lp ~ rk', data=dat2014).fit(cov_type='HC1')\nm3 = smf.ols('ln_lp ~ ln_rk', data=dat2014).fit(cov_type='HC1')\nprint(m1.summary(), m2.summary(), m3.summary())\nQuestions: How do you interpret the coefficient on capital in each model? Which specification seems most appropriate for cross-country data?\n\n\n\nTask 2: Quadratic Human Capital (Guided)\nTest whether the returns to human capital follow a nonlinear (quadratic) pattern.\ndat2014['hc_sq'] = dat2014['hc'] ** 2\nm4 = smf.ols('ln_lp ~ ln_rk + hc', data=dat2014).fit(cov_type='HC1')\nm5 = smf.ols('ln_lp ~ ln_rk + hc + hc_sq', data=dat2014).fit(cov_type='HC1')\nprint(m5.summary())\nprint(f\"Turning point: hc* = {-m5.params['hc'] / (2*m5.params['hc_sq']):.2f}\")\nQuestions: Is the quadratic term significant? What does the turning point imply about diminishing returns to human capital?\n\nKey Concept 15.9: Nonlinear Returns to Human Capital\nIf the quadratic term on human capital is negative and significant, it indicates diminishing returns – each additional unit of human capital contributes less to productivity. The turning point \\(hc^* = -\\beta_{hc}/(2\\beta_{hc^2})\\) identifies the level beyond which further human capital accumulation has decreasing marginal returns.\n\n\n\n\nTask 3: Standardized Coefficients (Semi-guided)\nCompare the relative importance of physical capital vs. human capital in determining productivity.\nHints:\n\nCompute standardized coefficients: \\(\\beta^* = \\beta \\times (s_x / s_y)\\)\nUse the log-log model for physical capital and levels for human capital\nWhich input has a larger effect in standard deviation terms?\n\n\n\n\nTask 4: Regional Interactions (Semi-guided)\nTest whether the returns to human capital differ by region using interaction terms.\nHints:\n\nUse ln_lp ~ ln_rk + hc * C(region) to include region-hc interactions\nConduct a joint F-test for the interaction terms\nAt which values of human capital are regional differences largest?\n\n\nKey Concept 15.10: Heterogeneous Returns Across Regions\nInteraction terms between human capital and regional indicators allow the marginal effect of human capital to vary by region. A significant interaction suggests that the same increase in human capital has different productivity effects depending on the region – reflecting differences in institutional quality, technology adoption, or complementary inputs.\n\n\n\n\nTask 5: Predictions with Bias Correction (Independent)\nUsing the log-log model, predict productivity for countries with specific capital and human capital levels. Apply the retransformation bias correction.\nCompare naive predictions \\(\\exp(\\widehat{\\ln lp})\\) with corrected predictions \\(\\exp(\\widehat{\\ln lp} + s_e^2/2)\\).\n\n\n\nTask 6: Policy Brief on Functional Form (Independent)\nWrite a 200-300 word brief addressing:\n\nWhich functional form best captures the productivity-capital relationship?\nIs there evidence of diminishing returns to human capital?\nDo returns to inputs differ across regions, and what are the policy implications?\nHow important is the retransformation bias correction for practical predictions?\n\n\nWhat You’ve Learned: You have applied multiple variable transformation techniques to cross-country data, demonstrating that log specifications better capture productivity relationships, returns to human capital may be nonlinear, and regional interactions reveal important heterogeneity in development patterns.\n\n\n\nCase Study 2: Nonlinear Satellite-Development Relationships\nResearch Question: What is the best functional form for modeling the relationship between satellite nighttime lights and municipal development in Bolivia?\nBackground: In previous chapters, we estimated linear regressions of development on NTL. But the relationship may be nonlinear—additional nighttime lights may have diminishing effects on development. In this case study, we apply Chapter 15’s transformation tools to explore functional form choices for the satellite-development relationship.\nThe Data: The DS4Bolivia dataset covers 339 Bolivian municipalities with satellite data, development indices, and socioeconomic indicators.\nKey Variables:\n\nmun: Municipality name\ndep: Department (administrative region)\nimds: Municipal Sustainable Development Index (0-100)\nln_NTLpc2017: Log nighttime lights per capita (2017)\nsdg7_1_ec: Electricity coverage (SDG 7 indicator)\n\n\nLoad the DS4Bolivia Data\n\n# Load the DS4Bolivia dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\n\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017', 'sdg7_1_ec']\nbol_cs = bol[key_vars].copy()\n\n# Create raw NTL variable from log\nbol_cs['NTLpc2017_raw'] = np.exp(bol_cs['ln_NTLpc2017'])\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA: TRANSFORMED VARIABLES CASE STUDY\")\nprint(\"=\" * 70)\nprint(f\"Observations: {len(bol_cs)}\")\nprint(f\"\\nKey variable summary:\")\nprint(bol_cs[['imds', 'ln_NTLpc2017', 'NTLpc2017_raw', 'sdg7_1_ec']].describe().round(3))\n\n\n\nTask 1: Compare Log Specifications (Guided)\nObjective: Estimate four regression specifications and compare functional forms.\nInstructions:\n\nEstimate four models:\n\n\nimds ~ ln_NTLpc2017 (level-log)\n\n\nnp.log(imds) ~ ln_NTLpc2017 (log-log)\n\n\nimds ~ NTLpc2017_raw (level-level)\n\n\nnp.log(imds) ~ NTLpc2017_raw (log-level)\n\n\nCompare R² across specifications\nInterpret the coefficient in each model (elasticity, semi-elasticity, or marginal effect)\n\nNote: R² values are not directly comparable across models with different dependent variables (levels vs. logs).\n\n# Your code here: Compare four functional form specifications\n#\n# Example structure:\n# bol_reg = bol_cs[['imds', 'ln_NTLpc2017', 'NTLpc2017_raw']].dropna()\n# bol_reg = bol_reg[bol_reg['imds'] &gt; 0]  # Ensure log is defined\n#\n# m_a = ols('imds ~ ln_NTLpc2017', data=bol_reg).fit(cov_type='HC1')\n# m_b = ols('np.log(imds) ~ ln_NTLpc2017', data=bol_reg).fit(cov_type='HC1')\n# m_c = ols('imds ~ NTLpc2017_raw', data=bol_reg).fit(cov_type='HC1')\n# m_d = ols('np.log(imds) ~ NTLpc2017_raw', data=bol_reg).fit(cov_type='HC1')\n#\n# print(\"Model (a) Level-Log  R²:\", m_a.rsquared.round(4))\n# print(\"Model (b) Log-Log    R²:\", m_b.rsquared.round(4))\n# print(\"Model (c) Level-Level R²:\", m_c.rsquared.round(4))\n# print(\"Model (d) Log-Level  R²:\", m_d.rsquared.round(4))\n\n\n\nTask 2: Quadratic NTL (Guided)\nObjective: Test whether the NTL-development relationship exhibits diminishing returns.\nInstructions:\n\nEstimate imds ~ ln_NTLpc2017 + I(ln_NTLpc2017**2)\nTest whether the quadratic term is statistically significant\nPlot the fitted curve against the scatter plot of the data\nCalculate the turning point: \\(NTL^* = -\\beta_1 / (2\\beta_2)\\)\nDiscuss: Is there evidence of diminishing returns to luminosity?\n\n\n# Your code here: Quadratic specification\n#\n# Example structure:\n# m_quad = ols('imds ~ ln_NTLpc2017 + I(ln_NTLpc2017**2)', data=bol_reg).fit(cov_type='HC1')\n# print(m_quad.summary())\n#\n# # Turning point\n# b1 = m_quad.params['ln_NTLpc2017']\n# b2 = m_quad.params['I(ln_NTLpc2017 ** 2)']\n# print(f\"\\nTurning point: ln_NTLpc = {-b1/(2*b2):.2f}\")\n#\n# # Plot fitted curve\n# x_range = np.linspace(bol_reg['ln_NTLpc2017'].min(), bol_reg['ln_NTLpc2017'].max(), 100)\n# y_hat = m_quad.params['Intercept'] + b1*x_range + b2*x_range**2\n# fig, ax = plt.subplots(figsize=(10, 6))\n# ax.scatter(bol_reg['ln_NTLpc2017'], bol_reg['imds'], alpha=0.4, label='Data')\n# ax.plot(x_range, y_hat, 'r-', linewidth=2, label='Quadratic fit')\n# ax.set_xlabel('Log NTL per Capita (2017)')\n# ax.set_ylabel('IMDS')\n# ax.set_title('Quadratic NTL-Development Relationship')\n# ax.legend()\n# plt.show()\n\n\nKey Concept 15.11: Diminishing Returns to Luminosity\nA significant negative quadratic term for NTL suggests diminishing marginal returns: additional nighttime lights associate with progressively smaller development gains. In already-bright urban centers, more light reflects commercial excess rather than fundamental development improvement. This nonlinearity has practical implications: satellite-based predictions may be most accurate for municipalities in the middle of the luminosity distribution.\n\n\n\nTask 3: Standardized Coefficients (Semi-guided)\nObjective: Compare the relative importance of nighttime lights and electricity coverage for predicting development.\nInstructions:\n\nStandardize imds, ln_NTLpc2017, and sdg7_1_ec to mean=0 and sd=1\nEstimate the regression on standardized variables\nCompare standardized coefficients: Which predictor has a larger effect in standard deviation terms?\n\nHint: Use (x - x.mean()) / x.std() to standardize each variable.\n\n# Your code here: Standardized coefficients\n#\n# Example structure:\n# bol_std = bol_cs[['imds', 'ln_NTLpc2017', 'sdg7_1_ec']].dropna()\n# for col in ['imds', 'ln_NTLpc2017', 'sdg7_1_ec']:\n#     bol_std[f'{col}_z'] = (bol_std[col] - bol_std[col].mean()) / bol_std[col].std()\n#\n# m_std = ols('imds_z ~ ln_NTLpc2017_z + sdg7_1_ec_z', data=bol_std).fit(cov_type='HC1')\n# print(m_std.summary())\n# print(\"\\nStandardized coefficients (beta weights):\")\n# print(f\"  NTL:         {m_std.params['ln_NTLpc2017_z']:.4f}\")\n# print(f\"  Electricity: {m_std.params['sdg7_1_ec_z']:.4f}\")\n\n\n\nTask 4: Interaction: NTL x Electricity (Semi-guided)\nObjective: Test whether the effect of nighttime lights on development depends on electricity coverage.\nInstructions:\n\nEstimate imds ~ ln_NTLpc2017 * sdg7_1_ec\nInterpret the interaction term: Does the NTL effect depend on electricity coverage?\nCalculate the marginal effect of NTL at low (25th percentile) vs. high (75th percentile) electricity levels\nDiscuss: What does this interaction reveal about the satellite-development relationship?\n\nHint: The marginal effect of NTL is \\(\\beta_{NTL} + \\beta_{interaction} \\times electricity\\).\n\n# Your code here: Interaction model\n#\n# Example structure:\n# m_int = ols('imds ~ ln_NTLpc2017 * sdg7_1_ec', data=bol_reg_full).fit(cov_type='HC1')\n# print(m_int.summary())\n#\n# # Marginal effect at different electricity levels\n# elec_25 = bol_reg_full['sdg7_1_ec'].quantile(0.25)\n# elec_75 = bol_reg_full['sdg7_1_ec'].quantile(0.75)\n# me_low = m_int.params['ln_NTLpc2017'] + m_int.params['ln_NTLpc2017:sdg7_1_ec'] * elec_25\n# me_high = m_int.params['ln_NTLpc2017'] + m_int.params['ln_NTLpc2017:sdg7_1_ec'] * elec_75\n# print(f\"\\nMarginal effect of NTL at low electricity ({elec_25:.1f}%): {me_low:.4f}\")\n# print(f\"Marginal effect of NTL at high electricity ({elec_75:.1f}%): {me_high:.4f}\")\n\n\nKey Concept 15.12: Elasticity of Development to Satellite Signals\nIn a log-log specification (log IMDS ~ log NTL), the coefficient directly estimates the elasticity: the percentage change in development associated with a 1% increase in nighttime lights per capita. An elasticity of, say, 0.15 means a 10% increase in NTL per capita is associated with a 1.5% increase in IMDS. Elasticities provide scale-free comparisons across different variables and contexts.\n\n\n\nTask 5: Predictions with Retransformation (Independent)\nObjective: Generate predictions from the log-log model and apply the Duan smearing correction.\nInstructions:\n\nEstimate the log-log model: np.log(imds) ~ ln_NTLpc2017\nGenerate naive predictions: \\(\\exp(\\widehat{\\ln(imds)})\\)\nApply the Duan smearing correction: multiply predictions by \\(\\bar{\\exp(\\hat{e})}\\) (the mean of exponentiated residuals)\nCompare naive vs. corrected predictions\nDiscuss: How much does the retransformation correction matter?\n\n\n# Your code here: Retransformation bias correction\n#\n# Example structure:\n# m_loglog = ols('np.log(imds) ~ ln_NTLpc2017', data=bol_reg).fit(cov_type='HC1')\n#\n# # Naive prediction\n# naive_pred = np.exp(m_loglog.fittedvalues)\n#\n# # Duan smearing correction\n# smearing_factor = np.exp(m_loglog.resid).mean()\n# corrected_pred = naive_pred * smearing_factor\n#\n# print(f\"Smearing factor: {smearing_factor:.4f}\")\n# print(f\"Mean actual IMDS: {bol_reg['imds'].mean():.2f}\")\n# print(f\"Mean naive prediction: {naive_pred.mean():.2f}\")\n# print(f\"Mean corrected prediction: {corrected_pred.mean():.2f}\")\n\n\n\nTask 6: Functional Form Brief (Independent)\nObjective: Write a 200-300 word brief summarizing your functional form analysis.\nYour brief should address:\n\nWhich specification best captures the satellite-development relationship?\nIs there evidence of nonlinearity (diminishing returns)?\nWhat are the elasticity estimates from the log-log model?\nDoes the interaction with electricity coverage reveal important heterogeneity?\nHow important is the retransformation correction for practical predictions?\nPolicy implications: What do the functional form results imply for using satellite data to monitor SDG progress?\n\n\n# Your code here: Additional analysis for the brief\n#\n# You might want to:\n# 1. Create a summary comparison table of all specifications\n# 2. Plot fitted values from different models on the same graph\n# 3. Calculate and compare elasticities across specifications\n# 4. Summarize key statistics to cite in your brief\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this exploration of functional forms for the satellite-development relationship, you’ve applied Chapter 15’s transformation toolkit to real geospatial data:\n\nFunctional form comparison: Estimated level-level, level-log, log-level, and log-log specifications\nNonlinearity detection: Used quadratic terms to test for diminishing returns to luminosity\nStandardized coefficients: Compared the relative importance of NTL and electricity coverage\nInteraction effects: Examined how electricity coverage moderates the NTL-development relationship\nRetransformation: Applied the Duan smearing correction to generate unbiased predictions from log models\nCritical thinking: Assessed which functional form best represents satellite-development patterns\n\nConnection: In Chapter 16, we apply diagnostic tools to check whether our satellite prediction models satisfy regression assumptions.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Chapter 15: Regression with Transformed Variables</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to regression diagnostics and model validation. All code runs directly in Google Colab without any local setup.\nThis chapter focuses on checking model assumptions and diagnosing data problems. You’ll gain both theoretical understanding and practical skills through hands-on Python examples.\nLearning Objectives:\nBy the end of this chapter, you will be able to: 1. Identify and diagnose multicollinearity using correlation matrices and VIF 2. Understand the consequences when each of the four core OLS assumptions fails 3. Recognize omitted variable bias and specify appropriate control variables 4. Understand endogeneity and when to use instrumental variables (IV) 5. Detect and address heteroskedasticity using robust standard errors 6. Identify autocorrelation in time series data and apply HAC-robust standard errors 7. Interpret residual diagnostic plots to detect model violations 8. Identify outliers and influential observations using DFITS and DFBETAS 9. Apply appropriate diagnostic tests and remedies for common data problems\nChapter outline: - 16.1 Multicollinearity - 16.2-16.4 Model Assumptions, Incorrect Models, and Endogeneity - 16.5 Heteroskedastic Errors - 16.6 Correlated Errors (Autocorrelation) - 16.7 Example: Democracy and Growth - 16.8 Diagnostics: Residual Plots and Influential Observations - Key Takeaways - Practice Exercises - Case Studies\nDatasets used: - AED_EARNINGS_COMPLETE.DTA: 842 full-time workers with earnings, age, education, and experience (2010) - AED_DEMOCRACY.DTA: 131 countries with democracy, growth, and institutional variables (Acemoglu et al. 2008)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#setup",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#setup",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Setup",
    "text": "Setup\nFirst, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\nfrom statsmodels.stats.diagnostic import het_white, acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom statsmodels.tsa.stattools import acf\nfrom scipy import stats\nimport random\nimport os\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"=\"*70)\nprint(\"CHAPTER 16: CHECKING THE MODEL AND DATA\")\nprint(\"=\"*70)\nprint(\"\\nSetup complete! Ready to explore model diagnostics.\")\n\n======================================================================\nCHAPTER 16: CHECKING THE MODEL AND DATA\n======================================================================\n\nSetup complete! Ready to explore model diagnostics.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#multicollinearity",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#multicollinearity",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "16.1: Multicollinearity",
    "text": "16.1: Multicollinearity\nMulticollinearity occurs when regressors are highly correlated with each other. While OLS remains unbiased and consistent, individual coefficients may be imprecisely estimated.\nEffects of multicollinearity:\n\nHigh standard errors on individual coefficients\nLow t-statistics (coefficients appear insignificant)\nCoefficients may have “wrong” signs\nCoefficients very sensitive to small data changes\nJoint tests may still be significant\n\nDetection methods:\n\nHigh pairwise correlations between regressors\nVariance Inflation Factor (VIF): \\[VIF_j = \\frac{1}{1 - R_j^2}\\] where \\(R_j^2\\) is from regressing \\(x_j\\) on all other regressors\n\nVIF &gt; 10 indicates serious multicollinearity\nVIF &gt; 5 suggests investigating further\n\nAuxiliary regression: Regress one variable on others\n\nHigh \\(R^2\\) indicates multicollinearity\n\n\nExample: Earnings regression with age, education, and age×education interaction\n\n# Read earnings data\ndata_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS_COMPLETE.DTA')\n\nprint(\"=\"*70)\nprint(\"16.1 MULTICOLLINEARITY\")\nprint(\"=\"*70)\n\nprint(\"\\nData summary:\")\nprint(data_earnings[['earnings', 'age', 'education', 'agebyeduc']].describe())\n\n# Base model without interaction\nprint(\"\\n\" + \"-\"*70)\nprint(\"Base Model: earnings ~ age + education\")\nprint(\"-\"*70)\nmodel_base = ols('earnings ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(model_base.summary())\n\n======================================================================\n16.1 MULTICOLLINEARITY\n======================================================================\n\nData summary:\n            earnings         age   education    agebyeduc\ncount     872.000000  872.000000  872.000000   872.000000\nmean    56368.691406   43.310780   13.853211   598.819946\nstd     51516.054688   10.676045    2.884141   193.690643\nmin      4000.000000   25.000000    0.000000     0.000000\n25%     29000.000000   35.000000   12.000000   464.000000\n50%     44200.000000   44.000000   13.000000   588.000000\n75%     64250.000000   51.250000   16.000000   720.000000\nmax    504000.000000   65.000000   20.000000  1260.000000\n\n----------------------------------------------------------------------\nBase Model: earnings ~ age + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.113\nMethod:                 Least Squares   F-statistic:                     42.85\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.79e-18\nTime:                        14:15:10   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     869   BIC:                         2.131e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.688e+04   1.13e+04     -4.146      0.000    -6.9e+04   -2.47e+04\nage          524.9953    151.387      3.468      0.001     228.281     821.709\neducation   5811.3673    641.533      9.059      0.000    4553.986    7068.749\n==============================================================================\nOmnibus:                      825.668   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31187.987\nSkew:                           4.353   Prob(JB):                         0.00\nKurtosis:                      30.975   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n\n\n# Model with interaction (creates multicollinearity)\nprint(\"\\n\" + \"-\"*70)\nprint(\"Collinear Model: earnings ~ age + education + agebyeduc\")\nprint(\"-\"*70)\nmodel_collinear = ols('earnings ~ age + education + agebyeduc', \n                      data=data_earnings).fit(cov_type='HC1')\nprint(model_collinear.summary())\n\nprint(\"\\nNote: Compare standard errors between base and collinear models.\")\nprint(\"Standard errors increase dramatically with the interaction term.\")\n\n\n----------------------------------------------------------------------\nCollinear Model: earnings ~ age + education + agebyeduc\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.112\nMethod:                 Least Squares   F-statistic:                     31.80\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.65e-19\nTime:                        14:15:10   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.130e+04\nDf Residuals:                     868   BIC:                         2.132e+04\nDf Model:                           3                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -2.909e+04    3.1e+04     -0.940      0.347   -8.98e+04    3.16e+04\nage          127.4922    719.280      0.177      0.859   -1282.270    1537.255\neducation   4514.9867   2401.517      1.880      0.060    -191.901    9221.874\nagebyeduc     29.0392     56.052      0.518      0.604     -80.821     138.899\n==============================================================================\nOmnibus:                      825.324   Durbin-Watson:                   2.072\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31144.116\nSkew:                           4.351   Prob(JB):                         0.00\nKurtosis:                      30.955   Cond. No.                     1.28e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n[2] The condition number is large, 1.28e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\nNote: Compare standard errors between base and collinear models.\nStandard errors increase dramatically with the interaction term.\n\n\n\n# Correlation matrix\nprint(\"\\n\" + \"-\"*70)\nprint(\"Correlation Matrix of Regressors\")\nprint(\"-\"*70)\ncorr_matrix = data_earnings[['age', 'education', 'agebyeduc']].corr()\nprint(corr_matrix)\n\n# Visualize correlation matrix\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n            center=0, vmin=-1, vmax=1, ax=ax, cbar_kws={'label': 'Correlation'})\nax.set_title('Correlation Matrix of Regressors', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nHigh correlations (&gt; 0.9) indicate multicollinearity.\")\n\n\n----------------------------------------------------------------------\nCorrelation Matrix of Regressors\n----------------------------------------------------------------------\n                age  education  agebyeduc\nage        1.000000  -0.038153   0.729136\neducation -0.038153   1.000000   0.635961\nagebyeduc  0.729136   0.635961   1.000000\n\n\n\n\n\n\n\n\n\n\nHigh correlations (&gt; 0.9) indicate multicollinearity.\n\n\n\n# Calculate VIF for all regressors\nprint(\"\\n\" + \"-\"*70)\nprint(\"Variance Inflation Factors (VIF)\")\nprint(\"-\"*70)\n\n# Prepare data for VIF calculation\nX_vif = data_earnings[['age', 'education', 'agebyeduc']].copy()\nX_vif = sm.add_constant(X_vif)\n\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X_vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i)\n                   for i in range(X_vif.shape[1])]\n\nprint(vif_data)\n\nprint(\"\\nInterpretation:\")\nprint(\"  VIF &gt; 10 indicates serious multicollinearity\")\nprint(\"  VIF &gt; 5 suggests investigating further\")\nprint(f\"  agebyeduc VIF = {vif_data.loc[vif_data['Variable']=='agebyeduc', 'VIF'].values[0]:.1f} (SEVERE!)\")\n\n\n----------------------------------------------------------------------\nVariance Inflation Factors (VIF)\n----------------------------------------------------------------------\n    Variable         VIF\n0      const  411.323019\n1        age   21.996182\n2  education   17.298404\n3  agebyeduc   36.880231\n\nInterpretation:\n  VIF &gt; 10 indicates serious multicollinearity\n  VIF &gt; 5 suggests investigating further\n  agebyeduc VIF = 36.9 (SEVERE!)\n\n\n\nKey Concept 16.1: Multicollinearity and the Variance Inflation Factor\nThe Variance Inflation Factor quantifies multicollinearity: \\(VIF_j = 1/(1 - R_j^2)\\), where \\(R_j^2\\) is from regressing \\(x_j\\) on all other regressors. VIF = 1 means no collinearity; VIF &gt; 10 indicates serious problems (standard errors inflated by \\(\\sqrt{10} \\approx 3.2\\times\\)). While OLS remains unbiased, individual coefficients become imprecise and may have “wrong” signs. Predictions and joint tests remain valid despite multicollinearity.\n\n\n\nUnderstanding VIF: When Multicollinearity Becomes a Problem\nThe VIF (Variance Inflation Factor) results reveal severe multicollinearity in the interaction model. Let’s understand what this means:\nVIF Values from the Analysis:\nTypical results when including age × education interaction: - agebyeduc (interaction): VIF ≈ 60-80 (SEVERE!) - age: VIF ≈ 15-25 (HIGH) - education: VIF ≈ 15-25 (HIGH) - Intercept: VIF ≈ 10-15\nInterpreting VIF:\nThe VIF formula: \\(VIF_j = \\frac{1}{1 - R_j^2}\\)\nwhere \\(R_j^2\\) is from regressing \\(x_j\\) on all other regressors.\nWhat the numbers mean:\n\nVIF = 1: No multicollinearity (ideal)\nVIF = 5: Moderate multicollinearity (R² = 0.80)\nVIF = 10: High multicollinearity (R² = 0.90) - investigate!\nVIF = 80: Severe multicollinearity (R² = 0.9875) - serious problem!\n\nWhy is agebyeduc VIF so high?\nThe interaction term is nearly a perfect linear combination: - age and education are correlated - age × education inherits both correlations - \\(R^2_{agebyeduc|age,educ} \\approx 0.9875\\) - This means 98.75% of variation in the interaction is predictable from age and education alone!\nConsequences:\n\nStandard errors inflate dramatically:\n\n\n\\(SE(\\hat{\\beta}_j) = \\sigma / \\sqrt{(1-R_j^2) \\cdot \\sum(x_j - \\bar{x}_j)^2}\\)\nWhen \\(R_j^2 \\approx 1\\): denominator → 0, so SE → ∞\nVIF = 80 means SE is \\(\\sqrt{80} \\approx 9\\) times larger than with no collinearity!\n\n\nIndividual t-statistics become small:\n\n\nEven if the true effect is large\nCan’t distinguish individual contributions\nMay get “wrong” signs on coefficients\n\n\nCoefficients become unstable:\n\n\nSmall changes in data → large changes in estimates\nSensitive to which observations are included\nHigh variance of estimators\n\nWhat Multicollinearity Does NOT Affect:\nStill valid: - OLS remains unbiased - Predictions still accurate - Joint F-tests remain powerful - Overall R² unchanged\nWhat breaks: - Individual t-tests unreliable - Standard errors too large - Confidence intervals too wide - Can’t interpret individual coefficients reliably\nSolutions:\n\nUse joint F-tests (not individual t-tests):\n\n\nTest \\(H_0: \\beta_{age} = 0\\) AND \\(\\beta_{agebyeduc} = 0\\) together\nThese remain powerful despite multicollinearity\n\n\nCenter variables before interaction:\n\n\nCreate: age_centered = age - mean(age)\nReduces correlation between main effects and interaction\nCan dramatically reduce VIF\n\n\nDrop one of the collinear variables:\n\n\nOnly if you don’t need both for your research question\nNot appropriate if interaction is theoretically important\n\n\nCollect more data or increase variation:\n\n\nMore observations → smaller SEs\nMore variation in X → less correlation\n\n\nRidge regression or regularization:\n\n\nShrinks coefficients toward zero\nTrades small bias for large reduction in variance\n\nThe Auxiliary Regression:\nThe output shows regressing agebyeduc ~ age + education gives R² ≈ 0.987: - This confirms 98.7% of interaction variation is explained by main effects - VIF = 1/(1-0.987) = 1/0.013 ≈ 77\nPractical Interpretation for Our Model:\nDespite high VIF: - Joint F-test shows age and interaction are jointly significant - We know age matters (from quadratic model) - We know education matters (strong t-stat) - Problem is separating the age vs. age×education effects - Both matter, but we can’t precisely estimate each one separately\n\n# Auxiliary regression to detect multicollinearity\nprint(\"\\n\" + \"-\"*70)\nprint(\"Auxiliary Regression: agebyeduc ~ age + education\")\nprint(\"-\"*70)\nmodel_aux = ols('agebyeduc ~ age + education', data=data_earnings).fit()\nprint(model_aux.summary())\n\nprint(f\"\\nR² from auxiliary regression: {model_aux.rsquared:.4f}\")\nprint(f\"VIF formula: 1/(1-R²) = {1/(1-model_aux.rsquared):.2f}\")\nprint(\"\\nHigh R² indicates that agebyeduc is nearly a perfect combination of age and education.\")\n\n\n----------------------------------------------------------------------\nAuxiliary Regression: agebyeduc ~ age + education\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              agebyeduc   R-squared:                       0.973\nModel:                            OLS   Adj. R-squared:                  0.973\nMethod:                 Least Squares   F-statistic:                 1.559e+04\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):               0.00\nTime:                        14:15:10   Log-Likelihood:                -4256.0\nNo. Observations:                 872   AIC:                             8518.\nDf Residuals:                     869   BIC:                             8532.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   -612.4825      7.018    -87.274      0.000    -626.257    -598.708\nage           13.6885      0.101    134.974      0.000      13.489      13.888\neducation     44.6425      0.375    118.918      0.000      43.906      45.379\n==============================================================================\nOmnibus:                      229.522   Durbin-Watson:                   1.993\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12190.806\nSkew:                          -0.238   Prob(JB):                         0.00\nKurtosis:                      21.311   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nR² from auxiliary regression: 0.9729\nVIF formula: 1/(1-R²) = 36.88\n\nHigh R² indicates that agebyeduc is nearly a perfect combination of age and education.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#joint-hypothesis-tests",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#joint-hypothesis-tests",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Joint Hypothesis Tests",
    "text": "Joint Hypothesis Tests\nEven with multicollinearity, joint tests can be powerful. Individual coefficients may be imprecise, but linear combinations may be precisely estimated.\n\n# Joint hypothesis tests\nprint(\"=\"*70)\nprint(\"JOINT HYPOTHESIS TESTS\")\nprint(\"=\"*70)\n\nprint(\"\\n\" + \"-\"*70)\nprint(\"Test 1: H₀: age = 0 AND agebyeduc = 0\")\nprint(\"-\"*70)\nhypotheses = '(age = 0, agebyeduc = 0)'\nf_test = model_collinear.wald_test(hypotheses, use_f=True)\nprint(f_test)\n\nprint(\"\\n\" + \"-\"*70)\nprint(\"Test 2: H₀: education = 0 AND agebyeduc = 0\")\nprint(\"-\"*70)\nhypotheses = '(education = 0, agebyeduc = 0)'\nf_test = model_collinear.wald_test(hypotheses, use_f=True)\nprint(f_test)\n\nprint(\"\\nInterpretation:\")\nprint(\"Joint tests are highly significant even though individual t-tests are weak.\")\nprint(\"This is the power of joint testing with multicollinear regressors.\")\n\n======================================================================\nJOINT HYPOTHESIS TESTS\n======================================================================\n\n----------------------------------------------------------------------\nTest 1: H₀: age = 0 AND agebyeduc = 0\n----------------------------------------------------------------------\n&lt;F test: F=array([[6.48958655]]), p=0.0015939412046954808, df_denom=868, df_num=2&gt;\n\n----------------------------------------------------------------------\nTest 2: H₀: education = 0 AND agebyeduc = 0\n----------------------------------------------------------------------\n&lt;F test: F=array([[43.00467267]]), p=1.5549618458663995e-18, df_denom=868, df_num=2&gt;\n\nInterpretation:\nJoint tests are highly significant even though individual t-tests are weak.\nThis is the power of joint testing with multicollinear regressors.\n\n\n/Users/carlosmendez/miniforge3/lib/python3.10/site-packages/statsmodels/base/model.py:1912: FutureWarning: The behavior of wald_test will change after 0.14 to returning scalar test statistic values. To get the future behavior now, set scalar to True. To silence this message while retaining the legacy behavior, set scalar to False.\n  warnings.warn(\n\n\n\nKey Concept 16.2: Joint Hypothesis Tests Under Multicollinearity\nEven when multicollinearity makes individual t-tests unreliable (high VIF, large standard errors), joint F-tests remain powerful. Testing whether a group of collinear variables is jointly significant avoids the imprecision problem because the F-test evaluates the combined contribution. Always use joint tests for groups of correlated regressors rather than relying on individual significance.\n\nHaving explored multicollinearity as a data problem that inflates standard errors, we now examine the broader set of OLS assumptions and what happens when each one fails.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#model-assumptions",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#model-assumptions",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "16.2-16.4: Model Assumptions",
    "text": "16.2-16.4: Model Assumptions\nClassical OLS Assumptions:\n\nLinearity: \\(y_i = \\beta_1 + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki} + u_i\\)\nZero conditional mean: \\(E[u_i | x_i] = 0\\)\nHomoskedasticity: \\(Var(u_i | x_i) = \\sigma^2\\)\nNo autocorrelation: \\(u_i\\) independent of \\(u_j\\) for \\(i \\neq j\\)\n\nConsequences of violations:\n\n\n\n\n\n\n\n\n\nAssumption\nViolation\nOLS Properties\nSolution\n\n\n\n\n1 or 2\nIncorrect model / Endogeneity\nBiased, Inconsistent\nIV, better specification\n\n\n3\nHeteroskedasticity\nUnbiased, Inefficient, Wrong SEs\nRobust SEs, WLS\n\n\n4\nAutocorrelation\nUnbiased, Inefficient, Wrong SEs\nHAC SEs, FGLS\n\n\n\nKey insight: Violations of assumptions 3 and 4 don’t bias coefficients, but invalidate standard errors and hypothesis tests.\n\nprint(\"=\"*70)\nprint(\"16.2-16.4: MODEL ASSUMPTIONS\")\nprint(\"=\"*70)\n\nprint(\"\\nClassical OLS assumptions:\")\nprint(\"  1. Linear in parameters: E[y|x] = x'β\")\nprint(\"  2. Random sample from population\")\nprint(\"  3. No perfect collinearity\")\nprint(\"  4. Zero conditional mean: E[u|x] = 0\")\nprint(\"  5. Homoskedasticity: Var(u|x) = σ²\")\nprint(\"  6. No autocorrelation: Cov(u_i, u_j) = 0\")\n\nprint(\"\\nConsequences of violations:\")\nprint(\"  Assumptions 1-4 violated → OLS biased and inconsistent\")\nprint(\"  Assumptions 5-6 violated → OLS unbiased but inefficient\")\nprint(\"                            → Standard errors incorrect\")\nprint(\"                            → Invalid inference (t-tests, CIs)\")\n\nprint(\"\\nSolutions:\")\nprint(\"  Heteroskedasticity → Robust (HC) standard errors\")\nprint(\"  Autocorrelation → HAC (Newey-West) standard errors\")\nprint(\"  Endogeneity → Instrumental variables (IV)\")\nprint(\"  Omitted variables → Add relevant controls\")\n\n======================================================================\n16.2-16.4: MODEL ASSUMPTIONS\n======================================================================\n\nClassical OLS assumptions:\n  1. Linear in parameters: E[y|x] = x'β\n  2. Random sample from population\n  3. No perfect collinearity\n  4. Zero conditional mean: E[u|x] = 0\n  5. Homoskedasticity: Var(u|x) = σ²\n  6. No autocorrelation: Cov(u_i, u_j) = 0\n\nConsequences of violations:\n  Assumptions 1-4 violated → OLS biased and inconsistent\n  Assumptions 5-6 violated → OLS unbiased but inefficient\n                            → Standard errors incorrect\n                            → Invalid inference (t-tests, CIs)\n\nSolutions:\n  Heteroskedasticity → Robust (HC) standard errors\n  Autocorrelation → HAC (Newey-West) standard errors\n  Endogeneity → Instrumental variables (IV)\n  Omitted variables → Add relevant controls\n\n\n\nKey Concept 16.3: Consequences of OLS Assumption Violations\nWhen assumptions 1 or 2 fail (incorrect model or endogeneity), OLS is biased and inconsistent – a fundamental problem requiring model changes or instrumental variables. When assumptions 3 or 4 fail (heteroskedasticity or autocorrelation), OLS remains unbiased and consistent but standard errors are wrong, invalidating confidence intervals and hypothesis tests. The key distinction: bias requires fixing the model; wrong SEs require only changing the inference method.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#heteroskedastic-errors",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#heteroskedastic-errors",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "16.5: Heteroskedastic Errors",
    "text": "16.5: Heteroskedastic Errors\nHeteroskedasticity means the error variance depends on \\(x\\): \\(Var(u_i | x_i) = \\sigma_i^2 \\neq \\sigma^2\\)\nCommon in:\n\nCross-sectional data (varies by unit size)\nIncome/wealth data (variance increases with level)\n\nSolution: Use heteroskedasticity-robust (HC) standard errors\n\nAlso called White standard errors\nCoefficient estimates unchanged\nOnly standard errors adjusted\nUsually larger (more conservative)\n\n\nprint(\"=\"*70)\nprint(\"16.5: HETEROSKEDASTIC ERRORS\")\nprint(\"=\"*70)\n\n# Regression with earnings data\nprint(\"\\n\" + \"-\"*70)\nprint(\"Earnings Regression: earnings ~ age + education\")\nprint(\"-\"*70)\n\n# Standard SEs\nmodel_standard = ols('earnings ~ age + education', data=data_earnings).fit()\nprint(\"\\nWith Standard SEs:\")\nprint(model_standard.summary())\n\n# Robust SEs\nmodel_robust = ols('earnings ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(\"\\n\" + \"=\"*70)\nprint(\"With Heteroskedasticity-Robust (HC1) SEs:\")\nprint(\"=\"*70)\nprint(model_robust.summary())\n\n# Comparison\nprint(\"\\n\" + \"-\"*70)\nprint(\"SE Comparison: Standard vs Robust\")\nprint(\"-\"*70)\nse_comparison = pd.DataFrame({\n    'Variable': model_standard.params.index,\n    'Standard SE': model_standard.bse.values,\n    'Robust SE': model_robust.bse.values,\n    'Ratio (Robust/Standard)': (model_robust.bse / model_standard.bse).values\n})\nprint(se_comparison)\n\nprint(\"\\nNote: Robust SEs are typically 20-40% larger, indicating heteroskedasticity.\")\n\n======================================================================\n16.5: HETEROSKEDASTIC ERRORS\n======================================================================\n\n----------------------------------------------------------------------\nEarnings Regression: earnings ~ age + education\n----------------------------------------------------------------------\n\nWith Standard SEs:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.113\nMethod:                 Least Squares   F-statistic:                     56.45\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           8.89e-24\nTime:                        14:15:10   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     869   BIC:                         2.131e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.688e+04   1.07e+04     -4.396      0.000   -6.78e+04   -2.59e+04\nage          524.9953    154.104      3.407      0.001     222.536     827.454\neducation   5811.3673    570.436     10.188      0.000    4691.774    6930.960\n==============================================================================\nOmnibus:                      825.668   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31187.987\nSkew:                           4.353   Prob(JB):                         0.00\nKurtosis:                      30.975   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n======================================================================\nWith Heteroskedasticity-Robust (HC1) SEs:\n======================================================================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.115\nModel:                            OLS   Adj. R-squared:                  0.113\nMethod:                 Least Squares   F-statistic:                     42.85\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.79e-18\nTime:                        14:15:10   Log-Likelihood:                -10644.\nNo. Observations:                 872   AIC:                         2.129e+04\nDf Residuals:                     869   BIC:                         2.131e+04\nDf Model:                           2                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -4.688e+04   1.13e+04     -4.146      0.000    -6.9e+04   -2.47e+04\nage          524.9953    151.387      3.468      0.001     228.281     821.709\neducation   5811.3673    641.533      9.059      0.000    4553.986    7068.749\n==============================================================================\nOmnibus:                      825.668   Durbin-Watson:                   2.071\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31187.987\nSkew:                           4.353   Prob(JB):                         0.00\nKurtosis:                      30.975   Cond. No.                         303.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n----------------------------------------------------------------------\nSE Comparison: Standard vs Robust\n----------------------------------------------------------------------\n    Variable   Standard SE     Robust SE  Ratio (Robust/Standard)\n0  Intercept  10663.893521  11306.329960                 1.060244\n1        age    154.103628    151.387435                 0.982374\n2  education    570.435846    641.532908                 1.124636\n\nNote: Robust SEs are typically 20-40% larger, indicating heteroskedasticity.\n\n\n\nKey Concept 16.4: Heteroskedasticity and Robust Standard Errors\nHeteroskedasticity means the error variance depends on the regressors: \\(\\text{Var}[u_i | \\mathbf{x}_i] \\neq \\sigma^2\\). OLS coefficients remain unbiased, but default standard errors are wrong – typically too small, giving false confidence in precision. Use heteroskedasticity-robust (HC1/White) standard errors, which are valid whether or not heteroskedasticity is present. Always use robust SEs for cross-sectional data as a default practice.\n\n\n\nWhy Robust Standard Errors Matter\nThe comparison between standard and robust SEs reveals heteroskedasticity in the earnings data:\nTypical Results:\n\n\n\nVariable\nStandard SE\nRobust SE\nRatio (Robust/Standard)\n\n\n\n\nage\n~$200\n~$250\n1.25x\n\n\neducation\n~$800\n~$1,100\n1.38x\n\n\n\nWhat This Tells Us:\n\nHeteroskedasticity is present:\n\n\nRobust SEs are 20-40% larger than standard SEs\nError variance is not constant across observations\nViolates the classical homoskedasticity assumption\n\n\nStandard SEs are too small:\n\n\nLead to overstated t-statistics\nFalse confidence in precision\nOverrejection of null hypotheses (Type I error)\n\n\nCoefficients unchanged:\n\n\nOLS estimates remain unbiased and consistent\nOnly the uncertainty (SEs) is affected\nPredictions still accurate\n\nWhy Heteroskedasticity in Earnings Data?\nEarnings data typically exhibit heteroskedasticity because:\n\nScale effects: High earners have more variable earnings\n\n\nCEO: $1M ± $500K (50% CV)\nJanitor: $30K ± $5K (17% CV)\n\n\nUnobserved heterogeneity: Some people more variable than others\n\n\nCommission-based vs. salary\nStable government job vs. volatile private sector\n\n\nModel misspecification: Missing interactions or nonlinearities\n\n\nTrue model may have different slopes for different groups\n\nVisual Evidence:\nIn the residual vs. fitted plot: - Residuals should have constant spread (homoskedasticity) - If spread increases with fitted values → heteroskedasticity - Classic “megaphone” or “fan” shape\nImplications for Inference:\nWith standard SEs: - t-statistic for education: 6.25 → p &lt; 0.001 - Conclusion: Highly significant\nWith robust SEs: - t-statistic for education: 4.55 → p &lt; 0.001 - Conclusion: Still significant, but less extreme\nThe correction: - Larger SEs → wider confidence intervals - More conservative (honest about uncertainty) - Inference remains valid\nWhen to Use Robust SEs:\nAlways use for: - Cross-sectional data (almost always heteroskedastic) - Large samples (asymptotically valid) - When you’re unsure (conservative approach) - Publication-quality research\nDon’t need for: - Experimental data with randomization - Small samples (can be unreliable, use bootstrap instead) - Time series (need HAC SEs instead)\nTypes of Robust SEs:\n\nHC0 (White 1980): Original heteroskedasticity-robust\n\n\n\\(\\hat{V}_{HC0} = (X'X)^{-1}X'\\text{diag}(\\hat{u}_i^2)X(X'X)^{-1}\\)\n\n\nHC1 (degrees of freedom correction):\n\n\nMultiply HC0 by \\(n/(n-k)\\)\nBetter in finite samples\nMost common choice (Stata default)\n\n\nHC2 and HC3: Further finite-sample improvements\n\n\nHC3 recommended for heteroskedasticity + influential observations\n\nBottom Line:\nIn this earnings regression: - Education remains highly significant even with robust SEs - But we’re more honest about precision - Robust SEs should be default for cross-sectional regressions - Report robust SEs in all your empirical work!",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#correlated-errors-autocorrelation",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#correlated-errors-autocorrelation",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "16.6: Correlated Errors (Autocorrelation)",
    "text": "16.6: Correlated Errors (Autocorrelation)\nAutocorrelation occurs in time series when \\(Cov(u_t, u_s) \\neq 0\\) for \\(t \\neq s\\).\nAR(1) process: \\(u_t = \\rho u_{t-1} + \\varepsilon_t\\) where \\(|\\rho| &lt; 1\\)\nConsequences:\n\nOLS unbiased and consistent\nStandard errors wrong (usually too small)\nt-statistics overstated\nFalse significance\n\nSolution: Use HAC (Heteroskedasticity and Autocorrelation Consistent) standard errors\n\nAlso called Newey-West standard errors\nAccounts for both heteroskedasticity and autocorrelation\n\nDetection: Check autocorrelation function (ACF) of residuals\n\nprint(\"=\"*70)\nprint(\"16.6: CORRELATED ERRORS (AUTOCORRELATION)\")\nprint(\"=\"*70)\n\n# Generate simulated time series data\nprint(\"\\nSimulation: Time Series with Autocorrelated Errors\")\nn = 10000\nnp.random.seed(10101)\n\n# Generate i.i.d. errors\ne = np.random.normal(0, 1, n)\n\n# Generate AR(1) errors: u_t = 0.8*u_{t-1} + e_t\nu = np.zeros(n)\nu[0] = 0\nfor t in range(1, n):\n    u[t] = 0.8 * u[t-1] + e[t]\n\n# Generate AR(1) regressor\nv = np.random.normal(0, 1, n)\nx = np.zeros(n)\nx[0] = 0\nfor t in range(1, n):\n    x[t] = 0.8 * x[t-1] + v[t]\n\n# Generate y with autocorrelated error\ny1 = 1 + 2*x + u\n\n# Create DataFrame\nts_data = pd.DataFrame({'y1': y1, 'x': x})\n\nprint(f\"\\nGenerated {n} observations with AR(1) errors (ρ = 0.8)\")\n\n======================================================================\n16.6: CORRELATED ERRORS (AUTOCORRELATION)\n======================================================================\n\nSimulation: Time Series with Autocorrelated Errors\n\nGenerated 10000 observations with AR(1) errors (ρ = 0.8)\n\n\n\n# Estimate model and check residual autocorrelation\nprint(\"\\n\" + \"-\"*70)\nprint(\"Model: y ~ x (with autocorrelated errors)\")\nprint(\"-\"*70)\n\nmodel_ts = ols('y1 ~ x', data=ts_data).fit()\nresiduals = model_ts.resid\n\n# Check autocorrelation of residuals\nacf_vals = acf(residuals, nlags=10, fft=False)\nprint(\"\\nResidual autocorrelations (first 10 lags):\")\nfor lag, val in enumerate(acf_vals[:11]):\n    print(f\"  Lag {lag}: {val:.4f}\")\n\n# Plot ACF\nfig, ax = plt.subplots(figsize=(10, 5))\nplot_acf(residuals, lags=20, ax=ax)\nax.set_title('Autocorrelation Function of Residuals', fontsize=14, fontweight='bold')\nax.set_xlabel('Lag', fontsize=12)\nax.set_ylabel('ACF', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nHigh ACF at multiple lags indicates autocorrelation.\")\n\n\n----------------------------------------------------------------------\nModel: y ~ x (with autocorrelated errors)\n----------------------------------------------------------------------\n\nResidual autocorrelations (first 10 lags):\n  Lag 0: 1.0000\n  Lag 1: 0.7974\n  Lag 2: 0.6300\n  Lag 3: 0.4983\n  Lag 4: 0.3985\n  Lag 5: 0.3123\n  Lag 6: 0.2432\n  Lag 7: 0.1928\n  Lag 8: 0.1535\n  Lag 9: 0.1180\n  Lag 10: 0.0894\n\n\n\n\n\n\n\n\n\n\nHigh ACF at multiple lags indicates autocorrelation.\n\n\n\nKey Concept 16.5: Autocorrelation and HAC Standard Errors\nAutocorrelation means errors are correlated over time (\\(\\text{Cov}[u_t, u_s] \\neq 0\\)), common in time series when economic shocks persist. OLS remains unbiased but standard errors are wrong – typically too small, leading to false significance. Use HAC (Newey-West) standard errors for valid inference. Check the autocorrelation function (ACF) of residuals: significant autocorrelations at multiple lags indicate the problem. Severe autocorrelation drastically reduces the effective sample size.\n\n\n\nAutocorrelation: The Time Series Problem\nThe time series analysis reveals strong autocorrelation in interest rate data - a classic problem that invalidates standard inference:\nAutocorrelation Evidence:\nFrom the residuals of the levels regression: - Lag 1 autocorrelation: ρ₁ ≈ 0.95-0.98 (extremely high!) - Lag 5 autocorrelation: ρ₅ ≈ 0.85-0.90 (still very high) - Lag 10 autocorrelation: ρ₁₀ ≈ 0.75-0.85 (persistent)\nWhat This Means:\n\nErrors are highly correlated over time:\n\n\nIf today’s error is +1%, tomorrow’s is likely +0.95%\nErrors cluster: positive errors followed by positive, negative by negative\nViolates OLS assumption of independent errors\n\n\nStandard errors drastically understate uncertainty:\n\nTypical results: - Default SE: ~0.002 (too small!) - HAC SE: ~0.015 (realistic) - Ratio: HAC is 7-8 times larger!\n\nWhy does autocorrelation inflate HAC SEs?\n\nWith independent errors: - \\(Var(\\bar{u}) = \\sigma^2/n\\) - Information in n observations\nWith autocorrelation (ρ = 0.95): - \\(Var(\\bar{u}) \\approx \\sigma^2 \\cdot \\frac{1 + \\rho}{1 - \\rho} \\cdot \\frac{1}{n} = \\sigma^2 \\cdot 39 \\cdot \\frac{1}{n}\\) - 39 times larger variance! - Effective sample size ≈ n/39\nThe Correlogram (ACF Plot):\nThe ACF plot shows: - Very slow decay of autocorrelations - All lags out to 20-24 months significantly positive - Classic sign of non-stationarity (trending series) - Interest rates have long memory\nWhy Are Interest Rates Autocorrelated?\n\nMonetary policy persistence:\n\n\nFed changes rates gradually (smoothing)\nSame rate maintained for months\n\n\nEconomic conditions:\n\n\nInflation, growth evolve slowly\nInterest rates respond to persistent factors\n\n\nMarket expectations:\n\n\nForward-looking behavior\nTomorrow’s rate close to today’s (no arbitrage)\n\n\nNon-stationarity:\n\n\nRates trend over long periods\n1980s: High rates (15%)\n2010s: Low rates (near 0%)\nMean not constant over time\n\nConsequences for Inference:\nWith default SEs: - t-statistic: 50-60 (absurdly high!) - p-value: &lt; 0.0001 - False precision!\nWith HAC SEs: - t-statistic: 5-8 (more realistic) - p-value: still &lt; 0.001 (significant, but not absurdly so) - Honest uncertainty\nThe Solution: HAC (Newey-West) Standard Errors\nHAC SEs account for both heteroskedasticity and autocorrelation:\n\\[\\hat{V}_{HAC} = (X'X)^{-1} \\left( \\sum_{j=-L}^L w_j \\sum_t \\hat{u}_t \\hat{u}_{t-j} x_t x'_{t-j} \\right) (X'X)^{-1}\\]\nwhere: - \\(L\\) = number of lags (rule of thumb: \\(L \\approx 0.75 \\cdot T^{1/3}\\)) - \\(w_j\\) = weights (declining with lag distance)\nChoosing the Lag Length (L):\nFor monthly data with T ≈ 400 observations: - Rule of thumb: \\(L \\approx 0.75 \\cdot 400^{1/3} \\approx 5.4\\) - Conservative: L = 12 (one year) - Very conservative: L = 24 (two years)\nFirst Differencing as Alternative:\nTransform: \\(\\Delta y_t = y_t - y_{t-1}\\)\nResults from differenced model: - Much lower autocorrelation (ρ₁ ≈ 0.1-0.3) - Removes trend (achieves stationarity) - Changes interpretation: now modeling changes, not levels\nPractical Recommendations:\nFor time series regressions:\n\nAlways plot your data (levels and differences)\nCheck for trends (visual, augmented Dickey-Fuller test)\nExamine ACF of residuals\nUse HAC SEs as default for time series\nConsider differencing if series are non-stationary\nReport both levels and differences specifications\n\nBottom Line:\nIn the interest rate example: - Default SEs give false confidence - HAC SEs reveal true uncertainty - Even with correction, 10-year rate strongly related to 1-year rate - But not as precisely estimated as default SEs suggest!\nNow that we understand the theoretical consequences of assumption violations, let’s apply these concepts to a real-world example examining whether democracy promotes economic growth.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#example---democracy-and-growth",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#example---democracy-and-growth",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "16.7: Example - Democracy and Growth",
    "text": "16.7: Example - Democracy and Growth\nWe analyze the relationship between democracy and economic growth using data from Acemoglu, Johnson, Robinson, and Yared (2008).\nResearch question: Does democracy promote economic growth?\nData: 131 countries, 1500-2000\n\ndemocracy: 500-year change in democracy index\ngrowth: 500-year change in log GDP per capita\nconstraint: Constraints on executive at independence\nindcent: Year of independence\ncatholic, muslim, protestant, other: Religious composition\n\nKey hypothesis: Institutions matter for democracy and growth\n\n# Load democracy data\ndata_democracy = pd.read_stata(GITHUB_DATA_URL + 'AED_DEMOCRACY.DTA')\n\nprint(\"=\"*70)\nprint(\"16.7: DEMOCRACY AND GROWTH\")\nprint(\"=\"*70)\n\nprint(\"\\nData summary:\")\nsummary_vars = ['democracy', 'growth', 'constraint', 'indcent', \n                'catholic', 'muslim', 'protestant', 'other']\nprint(data_democracy[summary_vars].describe())\n\nprint(f\"\\nSample size: {len(data_democracy)} countries\")\nprint(f\"Time period: 1500-2000\")\n\n======================================================================\n16.7: DEMOCRACY AND GROWTH\n======================================================================\n\nData summary:\n        democracy      growth  constraint     indcent    catholic      muslim  \\\ncount  131.000000  131.000000  131.000000  131.000000  131.000000  131.000000   \nmean     0.647328    1.915591    0.372412   19.043972    0.305527    0.247911   \nstd      0.331042    1.107757    0.362238    0.677359    0.355514    0.370670   \nmin      0.000000   -0.088739    0.000000   18.000000    0.000000    0.000000   \n25%      0.350000    0.940432    0.000000   18.209999    0.008500    0.000600   \n50%      0.800000    1.839638    0.333333   19.450001    0.121000    0.024000   \n75%      0.900000    2.706106    0.596296   19.605000    0.547500    0.412000   \nmax      1.000000    4.253100    1.000000   19.770000    0.969000    0.997000   \n\n       protestant       other  \ncount  131.000000  131.000000  \nmean     0.126664    0.319898  \nstd      0.212933    0.320301  \nmin      0.000000    0.001000  \n25%      0.002000    0.040500  \n50%      0.024000    0.208000  \n75%      0.180500    0.512000  \nmax      0.978000    1.000000  \n\nSample size: 131 countries\nTime period: 1500-2000\n\n\n\n# Bivariate regression: democracy ~ growth\nprint(\"\\n\" + \"-\"*70)\nprint(\"Bivariate Regression: democracy ~ growth\")\nprint(\"-\"*70)\n\nmodel_bivariate = ols('democracy ~ growth', data=data_democracy).fit(cov_type='HC1')\nprint(model_bivariate.summary())\n\n# Visualize relationship\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_democracy['growth'], data_democracy['democracy'],\n           alpha=0.6, s=50, color='black')\nax.plot(data_democracy['growth'], model_bivariate.fittedvalues,\n        color='blue', linewidth=2, label='OLS regression line')\nax.set_xlabel('Change in Log GDP per capita (1500-2000)', fontsize=12)\nax.set_ylabel('Change in Democracy (1500-2000)', fontsize=12)\nax.set_title('Figure 16.1: Democracy and Growth, 1500-2000',\n             fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(f\"  Coefficient: {model_bivariate.params['growth']:.4f}\")\nprint(f\"  Higher economic growth is associated with greater democratization.\")\nprint(f\"  But this may reflect omitted institutional variables...\")\n\n\n----------------------------------------------------------------------\nBivariate Regression: democracy ~ growth\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              democracy   R-squared:                       0.192\nModel:                            OLS   Adj. R-squared:                  0.185\nMethod:                 Least Squares   F-statistic:                     45.37\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           4.87e-10\nTime:                        14:15:11   Log-Likelihood:                -26.625\nNo. Observations:                 131   AIC:                             57.25\nDf Residuals:                     129   BIC:                             63.00\nDf Model:                           1                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.3967      0.046      8.597      0.000       0.306       0.487\ngrowth         0.1308      0.019      6.736      0.000       0.093       0.169\n==============================================================================\nOmnibus:                       13.463   Durbin-Watson:                   2.186\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               15.328\nSkew:                          -0.821   Prob(JB):                     0.000469\nKurtosis:                       2.665   Cond. No.                         5.14\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n  Coefficient: 0.1308\n  Higher economic growth is associated with greater democratization.\n  But this may reflect omitted institutional variables...\n\n\n\n# Multiple regression: add institutional controls\nprint(\"\\n\" + \"-\"*70)\nprint(\"Multiple Regression with Institutional Controls\")\nprint(\"-\"*70)\n\nmodel_multiple = ols('democracy ~ growth + constraint + indcent + catholic + muslim + protestant',\n                     data=data_democracy).fit(cov_type='HC1')\nprint(model_multiple.summary())\n\nprint(\"\\nKey findings:\")\nprint(f\"  Growth coefficient fell from {model_bivariate.params['growth']:.4f} to {model_multiple.params['growth']:.4f}\")\nprint(f\"  Institutional variables (religion, constraints) are important.\")\nprint(f\"  This suggests omitted variable bias in the bivariate model.\")\n\n\n----------------------------------------------------------------------\nMultiple Regression with Institutional Controls\n----------------------------------------------------------------------\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              democracy   R-squared:                       0.449\nModel:                            OLS   Adj. R-squared:                  0.423\nMethod:                 Least Squares   F-statistic:                     23.41\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           2.37e-18\nTime:                        14:15:11   Log-Likelihood:                -1.4887\nNo. Observations:                 131   AIC:                             16.98\nDf Residuals:                     124   BIC:                             37.10\nDf Model:                           6                                         \nCovariance Type:                  HC1                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      3.0307      0.975      3.109      0.002       1.120       4.941\ngrowth         0.0468      0.025      1.841      0.066      -0.003       0.097\nconstraint     0.1645      0.072      2.270      0.023       0.022       0.306\nindcent       -0.1331      0.050     -2.661      0.008      -0.231      -0.035\ncatholic       0.1171      0.089      1.324      0.186      -0.056       0.291\nmuslim        -0.2327      0.101     -2.303      0.021      -0.431      -0.035\nprotestant     0.1801      0.104      1.732      0.083      -0.024       0.384\n==============================================================================\nOmnibus:                        3.152   Durbin-Watson:                   2.109\nProb(Omnibus):                  0.207   Jarque-Bera (JB):                2.997\nSkew:                          -0.305   Prob(JB):                        0.224\nKurtosis:                       2.581   Cond. No.                         761.\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC1)\n\nKey findings:\n  Growth coefficient fell from 0.1308 to 0.0468\n  Institutional variables (religion, constraints) are important.\n  This suggests omitted variable bias in the bivariate model.\n\n\n\nKey Concept 16.6: Omitted Variables Bias in Practice\nThe democracy-growth example demonstrates omitted variables bias: the growth coefficient falls from 0.131 (bivariate) to 0.047 (with controls), a 64% reduction. Institutional variables (religion, executive constraints) were correlated with both democracy and growth, biasing the bivariate estimate upward. Always ask: “What variables might affect my outcome and correlate with my key regressor?” Include relevant controls to reduce bias.\n\n\n# Get residuals from multiple regression for diagnostic plots\nuhat = model_multiple.resid\nyhat = model_multiple.fittedvalues\n\nprint(\"\\n\" + \"-\"*70)\nprint(\"Residual Diagnostics Prepared\")\nprint(\"-\"*70)\nprint(f\"Number of residuals: {len(uhat)}\")\nprint(f\"Residual mean (should be ~0): {uhat.mean():.6f}\")\nprint(f\"Residual std dev: {uhat.std():.4f}\")\n\n\n----------------------------------------------------------------------\nResidual Diagnostics Prepared\n----------------------------------------------------------------------\nNumber of residuals: 131\nResidual mean (should be ~0): -0.000000\nResidual std dev: 0.2457",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#diagnostics---residual-plots",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#diagnostics---residual-plots",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "16.8: Diagnostics - Residual Plots",
    "text": "16.8: Diagnostics - Residual Plots\nDiagnostic plots help detect violations of model assumptions:\n\nActual vs Fitted: Should cluster around 45° line\nResidual vs Fitted: Should scatter randomly around zero\nResidual vs Regressor: Should scatter randomly around zero\nComponent Plus Residual Plot: \\(b_j x_j + e\\) vs \\(x_j\\) (detects nonlinearity)\nAdded Variable Plot: Partial \\(y\\) vs partial \\(x_j\\) (isolates effect)\n\nLOWESS smooth: Nonparametric smooth curve helps detect patterns\n\n\nReading Diagnostic Plots: What to Look For\nThe diagnostic plots help us visually detect violations of regression assumptions. Let’s interpret what we see:\nPanel A: Actual vs Fitted\nWhat to look for: - Points should cluster around 45° line - LOWESS smooth should follow the 45° line closely - Deviations indicate systematic prediction errors\nIn the democracy-growth example: - Most points reasonably close to 45° line - LOWESS smooth roughly linear, close to 45° - Some scatter (R² ≈ 0.20-0.30, so significant unexplained variation) - No obvious systematic bias (LOWESS not curved)\nInterpretation: - Model captures general relationship reasonably - But substantial residual variation remains - No evidence of major nonlinearity (LOWESS smooth is linear)\nPanel B: Residual vs Fitted\nWhat to look for: - Residuals should scatter randomly around zero - Equal spread across range of fitted values (homoskedasticity) - LOWESS should be horizontal at zero - No patterns, curvature, or heteroskedasticity\nIn the democracy-growth example: - Residuals scatter around zero - LOWESS smooth close to horizontal - Spread appears roughly constant - Some outliers but not extreme\nPotential issues to watch for: 1. Heteroskedasticity: Fan shape (spread increases) 2. Nonlinearity: LOWESS curved (missing quadratic term) 3. Outliers: Points far from zero (influential observations)\nKey Diagnostic Insights:\n\nNo major heteroskedasticity:\n\n\nSpread doesn’t systematically increase/decrease\nRobust SEs still advisable (safety margin)\nBut not severe heteroskedasticity\n\n\nLinearity assumption appears okay:\n\n\nLOWESS smooth roughly horizontal\nIf curved: suggests missing nonlinear terms\nCould try quadratic, interactions\n\n\nA few potential outliers:\n\n\nCountries with large positive/negative residuals\nFollow up with DFITS, DFBETAS (see next sections)\nInvestigate: data errors or genuinely unusual cases?\n\nWhat Would Bad Plots Look Like?\nHeteroskedasticity (fan shape): - Residual spread increases with fitted values - Common in income, revenue, GDP data - Solution: Log transformation or WLS\nNonlinearity (curved LOWESS): - LOWESS smooth curves (U-shape or inverted U) - Missing quadratic or other nonlinear terms - Solution: Add polynomial, interaction, or transform\nAutocorrelation (time series): - Residuals show runs (streaks of same sign) - Not visible in scatter plot (need time-series plot) - Solution: HAC SEs, add lags, difference\nOutliers: - A few points very far from main cluster - Can distort regression line - Investigate with influence diagnostics (DFITS, DFBETAS)\nThe LOWESS Smooth:\nWhat is it? - Locally Weighted Scatterplot Smoothing - Nonparametric smooth curve through data - Helps detect patterns hard to see in raw scatter\nHow to interpret: - Should be straight and flat if model is correct - Curvature suggests missing nonlinearity - Trend (not horizontal) suggests systematic bias\nBottom Line:\nFor democracy-growth model: - Diagnostic plots look reasonably good - No glaring violations of assumptions - Some outliers worth investigating (next section) - Model appears adequately specified - But low R² suggests many omitted variables",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#diagnostic-plots-for-individual-regressor-growth",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#diagnostic-plots-for-individual-regressor-growth",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Diagnostic Plots for Individual Regressor (Growth)",
    "text": "Diagnostic Plots for Individual Regressor (Growth)\nThree specialized plots for examining the growth variable:\n\nResidual vs Regressor: Checks for heteroskedasticity and nonlinearity\nComponent Plus Residual: \\(b_{growth} \\times growth + e\\) vs \\(growth\\)\n\nLinear relationship → straight line\nNonlinearity → curved LOWESS\n\nAdded Variable Plot: Controls for other variables\n\nSlope equals coefficient in full model\nShows partial relationship\n\n\n\nprint(\"=\"*70)\nprint(\"16.8: DIAGNOSTIC PLOTS\")\nprint(\"=\"*70)\n\n# Figure 16.2: Basic diagnostic plots\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Panel A: Actual vs Fitted\naxes[0].scatter(yhat, data_democracy['democracy'], alpha=0.6, s=50, color='black')\naxes[0].plot([yhat.min(), yhat.max()], [yhat.min(), yhat.max()],\n             'b-', linewidth=2, label='45° line')\n\nlowess_result = lowess(data_democracy['democracy'], yhat, frac=0.3)\naxes[0].plot(lowess_result[:, 0], lowess_result[:, 1],\n             'r--', linewidth=2, label='LOWESS smooth')\n\naxes[0].set_xlabel('Fitted Democracy', fontsize=12)\naxes[0].set_ylabel('Actual Democracy', fontsize=12)\naxes[0].set_title('Panel A: Actual vs Fitted', fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Residual vs Fitted\naxes[1].scatter(yhat, uhat, alpha=0.6, s=50, color='black')\naxes[1].axhline(y=0, color='blue', linewidth=2, linestyle='-')\n\nlowess_result = lowess(uhat, yhat, frac=0.3)\naxes[1].plot(lowess_result[:, 0], lowess_result[:, 1],\n             'r--', linewidth=2, label='LOWESS smooth')\n\naxes[1].set_xlabel('Fitted Democracy', fontsize=12)\naxes[1].set_ylabel('Residual', fontsize=12)\naxes[1].set_title('Panel B: Residual vs Fitted', fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.suptitle('Figure 16.2: Basic Diagnostic Plots',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"  Panel A: Points should cluster around 45° line\")\nprint(\"  Panel B: Residuals should scatter randomly around zero\")\n\n======================================================================\n16.8: DIAGNOSTIC PLOTS\n======================================================================\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n  Panel A: Points should cluster around 45° line\n  Panel B: Residuals should scatter randomly around zero\n\n\n\n# Figure 16.3: Diagnostic plots for growth regressor\nprint(\"\\n\" + \"-\"*70)\nprint(\"Figure 16.3: Diagnostic Plots for Growth Regressor\")\nprint(\"-\"*70)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Panel A: Residual vs Regressor\naxes[0].scatter(data_democracy['growth'], uhat, alpha=0.6, s=50, color='black')\naxes[0].axhline(y=0, color='blue', linewidth=2, linestyle='-')\n\nlowess_result = lowess(uhat, data_democracy['growth'], frac=0.3)\naxes[0].plot(lowess_result[:, 0], lowess_result[:, 1],\n             'r--', linewidth=2, label='LOWESS smooth')\n\naxes[0].set_xlabel('Growth regressor', fontsize=11)\naxes[0].set_ylabel('Democracy Residual', fontsize=11)\naxes[0].set_title('Panel A: Residual vs Regressor', fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel B: Component Plus Residual\nb_growth = model_multiple.params['growth']\npr_growth = b_growth * data_democracy['growth'] + uhat\n\naxes[1].scatter(data_democracy['growth'], pr_growth, alpha=0.6, s=50, color='black')\n\n# Regression line\nmodel_compplusres = ols('pr_growth ~ growth', \n                        data=pd.DataFrame({'growth': data_democracy['growth'],\n                                         'pr_growth': pr_growth})).fit()\naxes[1].plot(data_democracy['growth'], model_compplusres.fittedvalues,\n             'b-', linewidth=2, label='Regression line')\n\nlowess_result = lowess(pr_growth, data_democracy['growth'], frac=0.3)\naxes[1].plot(lowess_result[:, 0], lowess_result[:, 1],\n             'r--', linewidth=2, label='LOWESS smooth')\n\naxes[1].set_xlabel('Growth regressor', fontsize=11)\naxes[1].set_ylabel(f'Dem Res + {b_growth:.3f}*Growth', fontsize=11)\naxes[1].set_title('Panel B: Component Plus Residual', fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Panel C: Added Variable Plot\nmodel_nogrowth = ols('democracy ~ constraint + indcent + catholic + muslim + protestant',\n                     data=data_democracy).fit()\nuhat_democ = model_nogrowth.resid\n\nmodel_growth = ols('growth ~ constraint + indcent + catholic + muslim + protestant',\n                   data=data_democracy).fit()\nuhat_growth = model_growth.resid\n\naxes[2].scatter(uhat_growth, uhat_democ, alpha=0.6, s=50, color='black')\n\nmodel_addedvar = ols('uhat_democ ~ uhat_growth',\n                     data=pd.DataFrame({'uhat_growth': uhat_growth,\n                                       'uhat_democ': uhat_democ})).fit()\naxes[2].plot(uhat_growth, model_addedvar.fittedvalues,\n             'b-', linewidth=2, label='Regression line')\n\nlowess_result = lowess(uhat_democ, uhat_growth, frac=0.3)\naxes[2].plot(lowess_result[:, 0], lowess_result[:, 1],\n             'r--', linewidth=2, label='LOWESS smooth')\n\naxes[2].set_xlabel('Growth regressor (partial)', fontsize=11)\naxes[2].set_ylabel('Democracy (partial)', fontsize=11)\naxes[2].set_title('Panel C: Added Variable', fontsize=12, fontweight='bold')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle('Figure 16.3: Diagnostic Plots for Growth Regressor',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"  Panel A: Check for patterns in residuals\")\nprint(\"  Panel B: LOWESS close to regression line → linear relationship OK\")\nprint(\"  Panel C: Slope equals coefficient in full model ({:.4f})\".format(b_growth))\n\n\n----------------------------------------------------------------------\nFigure 16.3: Diagnostic Plots for Growth Regressor\n----------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n  Panel A: Check for patterns in residuals\n  Panel B: LOWESS close to regression line → linear relationship OK\n  Panel C: Slope equals coefficient in full model (0.0468)\n\n\nHaving examined residual diagnostic plots for visual detection of model problems, we now turn to numerical influence measures that quantify how much individual observations affect regression results.\n\nKey Concept 16.7: Diagnostic Plots for Model Validation\nThree complementary plots assess individual regressors: (1) residual vs. regressor checks for patterns suggesting nonlinearity or heteroskedasticity; (2) component-plus-residual plot (\\(b_j x_j + e\\) vs. \\(x_j\\)) reveals the partial relationship and detects nonlinearity; (3) added variable plot purges both \\(y\\) and \\(x_j\\) of other regressors, showing the pure partial effect whose slope equals the OLS coefficient \\(b_j\\). LOWESS smoothing helps reveal systematic patterns.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#influential-observations-dfits",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#influential-observations-dfits",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Influential Observations: DFITS",
    "text": "Influential Observations: DFITS\nDFITS measures influence on fitted values: \\[DFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{s_{(i)} \\sqrt{h_{ii}}}\\]\nwhere:\n\n\\(\\hat{y}_i\\) = prediction including observation \\(i\\)\n\\(\\hat{y}_{i(i)}\\) = prediction excluding observation \\(i\\)\n\\(s_{(i)}\\) = RMSE excluding observation \\(i\\)\n\\(h_{ii}\\) = leverage of observation \\(i\\)\n\nRule of thumb: Investigate if \\(|DFITS_i| &gt; 2\\sqrt{k/n}\\)\n\nprint(\"=\"*70)\nprint(\"INFLUENTIAL OBSERVATIONS: DFITS\")\nprint(\"=\"*70)\n\n# Get influence diagnostics\ninfluence = OLSInfluence(model_multiple)\ndfits = influence.dffits[0]\nn = len(data_democracy)\n\nthreshold_dfits = 2 * np.sqrt(len(model_multiple.params) / n)\nprint(f\"\\nDFITS threshold: {threshold_dfits:.4f}\")\nprint(f\"Observations exceeding threshold: {np.sum(np.abs(dfits) &gt; threshold_dfits)}\")\n\n# Plot DFITS\nobs_index = np.arange(n)\ncolors = ['red' if abs(d) &gt; threshold_dfits else 'blue' for d in dfits]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.scatter(obs_index, dfits, c=colors, alpha=0.6, s=50)\nax.axhline(y=threshold_dfits, color='red', linestyle='--', linewidth=2, label=f'Threshold: ±{threshold_dfits:.3f}')\nax.axhline(y=-threshold_dfits, color='red', linestyle='--', linewidth=2)\nax.axhline(y=0, color='black', linestyle='-', linewidth=1)\nax.set_xlabel('Observation Index', fontsize=12)\nax.set_ylabel('DFITS', fontsize=12)\nax.set_title('Figure: DFITS - Influential Observations', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"  Red points exceed threshold (potentially influential)\")\nprint(\"  Investigate these observations for data errors or unusual cases\")\n\n======================================================================\nINFLUENTIAL OBSERVATIONS: DFITS\n======================================================================\n\nDFITS threshold: 0.4623\nObservations exceeding threshold: 5\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n  Red points exceed threshold (potentially influential)\n  Investigate these observations for data errors or unusual cases\n\n\n\n\nIdentifying Influential Observations with DFITS\nDFITS measures how much an observation influences its own prediction. The results help identify potentially problematic observations:\nUnderstanding DFITS:\nFormula: \\(DFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{s_{(i)} \\sqrt{h_{ii}}}\\)\nwhere: - \\(\\hat{y}_i\\) = prediction including observation i - \\(\\hat{y}_{i(i)}\\) = prediction excluding observation i - \\(s_{(i)}\\) = RMSE excluding observation i - \\(h_{ii}\\) = leverage (how unusual is \\(x_i\\)?)\nInterpretation: - DFITS measures standardized change in fitted value when i is deleted - Large |DFITS| → observation strongly influences its own prediction - Can be driven by leverage (unusual X) or residual (unusual Y|X)\nRule of Thumb:\nThreshold: \\(|DFITS_i| &gt; 2\\sqrt{k/n}\\)\nFor democracy-growth model (k ≈ 7, n = 131): - Threshold ≈ $2 = 2 *0.46**\nTypical Results:\n\nMost observations: |DFITS| &lt; 0.30 (not influential)\nA few observations: |DFITS| = 0.5-0.8 (moderately influential)\nExtreme cases: |DFITS| &gt; 1.0 (highly influential)\n\nWhat Makes an Observation Influential?\nTwo components multiply: 1. Leverage (\\(h_{ii}\\)): Unusual X values 2. Standardized residual: Large prediction error\nMost influential when both are large: - Observation with unusual combination of regressors (high leverage) - AND doesn’t fit the pattern (large residual) - Example: A country with unique institutions AND surprising democracy level\nWhat to Do with Influential Observations:\n1. Investigate the data: - Is it a data error? (typo, coding mistake) - Check original sources - If error: correct or remove\n2. Understand the case: - Is it genuinely unusual? (e.g., special historical circumstances) - Example: Post-colonial country with unique constraints - Adds valuable information, keep it\n3. Check robustness: - Re-estimate without the influential observations - Do conclusions change substantially? - If yes: Results fragile, interpret cautiously - If no: Results robust, less concerning\n4. Model improvement: - Does omitting observation suggest missing variables? - Example: Maybe need regional dummies - Influential observations often signal model misspecification\nExample Interpretation:\nSuppose observation #47 has DFITS = 0.85: - This country’s predicted democracy changes by 0.85 standard deviations when it’s excluded - Country is either: - High leverage (unusual institutional characteristics), or - Large residual (democracy level doesn’t match institutions), or - Both - Warrants investigation\nDFITS vs. Other Influence Measures:\n\nDFITS: Influence on own prediction\nDFBETAS: Influence on regression coefficients (see next)\nCook’s D: Overall influence on all fitted values\nLeverage (\\(h_{ii}\\)): Just the X-space component\n\nVisualization:\nThe DFITS plot shows: - Blue points: Not influential (within threshold) - Red points: Influential (exceed threshold) - Should be mostly blue with a few red outliers - Many red points → model problems or data issues\nIn the Democracy-Growth Example:\nTypical findings: - 3-10 countries exceed threshold (out of 131) - These are countries with unusual institutional/growth combinations - Might include: - Rapidly democratizing autocracies - Stable democracies with slow growth - Post-conflict transitions - Resource-rich countries with unusual politics\nPractical Advice:\nDo: - Always compute influence diagnostics - Investigate observations exceeding thresholds - Report whether results change without influential cases - Consider robustness checks\nDon’t: - Automatically delete influential observations - Ignore them without investigation - Only report results after deleting outliers (selective reporting) - Delete based solely on statistical criteria (needs substantive judgment)\nBottom Line:\nDFITS is a screening tool: - Identifies observations worth investigating - Not a mechanical deletion rule - Combine statistical diagnosis with subject-matter knowledge - Goal: Better understand data and model, not just clean data",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#influential-observations-dfbetas",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#influential-observations-dfbetas",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Influential Observations: DFBETAS",
    "text": "Influential Observations: DFBETAS\nDFBETAS measures influence on individual coefficients: \\[DFBETAS_{j, i} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{s_{(i)} \\sqrt{(X'X)^{-1}_{jj}}}\\]\nwhere: - \\(\\hat{\\beta}_j\\) = coefficient including observation \\(i\\) - \\(\\hat{\\beta}_{j(i)}\\) = coefficient excluding observation \\(i\\)\nRule of thumb: Investigate if \\(|DFBETAS_{j, i}| &gt; 2/\\sqrt{n}\\)\n\nprint(\"=\"*70)\nprint(\"INFLUENTIAL OBSERVATIONS: DFBETAS\")\nprint(\"=\"*70)\n\ndfbetas = influence.dfbetas\nthreshold_dfbetas = 2 / np.sqrt(n)\nprint(f\"\\nDFBETAS threshold: {threshold_dfbetas:.4f}\")\n\n# Plot DFBETAS for each variable\nparam_names = model_multiple.params.index\nn_params = len(param_names)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.flatten()\n\nfor i, param in enumerate(param_names):\n    if i &lt; len(axes):\n        colors = ['red' if abs(d) &gt; threshold_dfbetas else 'blue' \n                 for d in dfbetas[:, i]]\n        axes[i].scatter(obs_index, dfbetas[:, i], c=colors, alpha=0.6, s=30)\n        axes[i].axhline(y=threshold_dfbetas, color='red', linestyle='--', linewidth=1.5)\n        axes[i].axhline(y=-threshold_dfbetas, color='red', linestyle='--', linewidth=1.5)\n        axes[i].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n        axes[i].set_xlabel('Observation', fontsize=10)\n        axes[i].set_ylabel('DFBETAS', fontsize=10)\n        axes[i].set_title(f'{param}', fontsize=11, fontweight='bold')\n        axes[i].grid(True, alpha=0.3)\n\n# Remove extra subplots\nfor i in range(n_params, len(axes)):\n    fig.delaxes(axes[i])\n\nplt.suptitle('DFBETAS: Influential Observations by Variable',\n             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"  Red points indicate observations with large influence on that coefficient.\")\nprint(\"  Investigate whether these are data errors or genuinely unusual cases.\")\n\n======================================================================\nINFLUENTIAL OBSERVATIONS: DFBETAS\n======================================================================\n\nDFBETAS threshold: 0.1747\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n  Red points indicate observations with large influence on that coefficient.\n  Investigate whether these are data errors or genuinely unusual cases.\n\n\n\nKey Concept 16.8: Influential Observations – DFITS and DFBETAS\nDFITS measures influence on fitted values: \\(DFITS_i\\) is the scaled change in \\(\\hat{y}_i\\) when observation \\(i\\) is excluded. DFBETAS measures influence on individual coefficients: \\(DFBETAS_{j,i}\\) is the scaled change in \\(\\hat{\\beta}_j\\). Thresholds for investigation are \\(|DFITS| &gt; 2\\sqrt{k/n}\\) and \\(|DFBETAS| &gt; 2/\\sqrt{n}\\). Don’t automatically delete influential points – investigate whether they represent data errors, genuine outliers, or valid extreme values that carry important information.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#key-takeaways",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#key-takeaways",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nMulticollinearity: - Multicollinearity occurs when regressors are highly correlated, making individual coefficients imprecisely estimated - VIF &gt; 10 indicates serious multicollinearity; VIF &gt; 5 warrants investigation - OLS remains unbiased and consistent – the problem is precision, not bias - Solutions: use joint F-tests, drop redundant variables, center variables before creating interactions, or collect more data\nOLS Assumption Violations: - Assumptions 1-2 violations (incorrect model, endogeneity) cause bias and inconsistency – fundamental problems requiring model changes or IV - Assumptions 3-4 violations (heteroskedasticity, autocorrelation) do not bias coefficients but invalidate standard errors - Wrong standard errors lead to incorrect t-statistics, confidence intervals, and hypothesis tests - Omitted variables bias formula: \\(\\text{Bias} = \\beta_3 \\times \\delta_{23}\\), where \\(\\beta_3\\) is the omitted variable’s effect and \\(\\delta_{23}\\) is the correlation\nHeteroskedasticity and Robust Standard Errors: - Heteroskedasticity means error variance varies across observations, common in cross-sectional data - Use heteroskedasticity-robust (HC1/White) standard errors for valid inference - Robust SEs are typically larger than default SEs, giving more conservative (honest) inference - Always use robust SEs for cross-sectional regressions as a default practice\nAutocorrelation and HAC Standard Errors: - Autocorrelation means errors are correlated over time, common in time series data - Default SEs are typically too small with autocorrelation, leading to over-rejection - Use HAC (Newey-West) standard errors that account for both heteroskedasticity and autocorrelation - Check the ACF of residuals to detect autocorrelation patterns\nDiagnostic Plots: - Residual vs. fitted values: detect heteroskedasticity (fan shape) and nonlinearity (curved pattern) - Component-plus-residual plot: detect nonlinearity in individual regressors - Added variable plot: isolate partial relationship between y and x, controlling for other variables - LOWESS smooth helps reveal patterns that are hard to see in raw scatter plots\nInfluential Observations: - DFITS measures influence on fitted values; threshold \\(|\\text{DFITS}| &gt; 2\\sqrt{k/n}\\) - DFBETAS measures influence on individual coefficients; threshold \\(|\\text{DFBETAS}| &gt; 2/\\sqrt{n}\\) - Investigate influential observations rather than automatically deleting them - Check whether conclusions change substantially when influential cases are excluded\nPython tools: statsmodels (VIF, OLSInfluence, robust covariance), matplotlib/seaborn (diagnostic plots), LOWESS smoothing\nNext steps: Chapter 17 extends these ideas to panel data, where you’ll learn fixed effects and random effects models that address unobserved heterogeneity across units.\n\nCongratulations! You’ve completed Chapter 16 on model checking and data diagnostics. You now have both the theoretical understanding and practical Python skills to evaluate regression assumptions, detect problems, and apply appropriate remedies.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#practice-exercises",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#practice-exercises",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Irrelevant Variables vs. Omitted Variables\nYou estimate \\(y_i = \\beta_1 + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i\\) by OLS.\n\nIf \\(x_3\\) should not appear in the model (irrelevant variable), what happens to the OLS estimates of \\(\\beta_1\\) and \\(\\beta_2\\)? Are they biased?\nIf a relevant variable \\(x_4\\) was omitted from the model, and \\(x_4\\) is correlated with \\(x_2\\), what happens to \\(\\hat{\\beta}_2\\)? Write the omitted variables bias formula.\n\nExercise 2: VIF Interpretation\nA regression of earnings on age, education, and experience yields VIF values of 22.0 (age), 17.3 (education), and 36.9 (experience).\n\nWhich variable has the most severe multicollinearity problem? Explain.\nCalculate the \\(R^2\\) from the auxiliary regression for the variable with the highest VIF.\nBy what factor are the standard errors inflated compared to the no-collinearity case?\n\nExercise 3: Choosing Standard Error Types\nFor each scenario below, state which type of standard errors you would use (default, HC-robust, HAC, or cluster-robust) and why:\n\nCross-sectional regression of wages on education and experience for 5,000 workers.\nTime series regression of GDP growth on interest rates using 200 quarterly observations.\nRegression of test scores on class size using data from 50 schools with multiple classrooms per school.\n\nExercise 4: Heteroskedasticity Detection\nYou estimate a regression and obtain the following SE comparison:\n\n\n\nVariable\nStandard SE\nRobust SE\nRatio\n\n\n\n\nEducation\n570\n642\n1.13\n\n\nAge\n154\n151\n0.98\n\n\n\n\nIs there evidence of heteroskedasticity? Which variable’s inference is most affected?\nIf you used standard SEs and the t-statistic for education was 2.05, would the conclusion change with robust SEs?\n\nExercise 5: DFITS Threshold Calculation\nIn a regression with \\(k = 7\\) regressors and \\(n = 131\\) observations:\n\nCalculate the DFITS threshold for identifying influential observations.\nIf 8 observations exceed this threshold, what percentage of the sample is flagged as potentially influential?\nDescribe the steps you would take to investigate these influential observations.\n\nExercise 6: Autocorrelation Consequences\nA time series regression yields a first-lag residual autocorrelation of \\(\\rho_1 = 0.80\\).\n\nIs this evidence of autocorrelation? What does it mean for the residual pattern?\nApproximate the factor by which the effective sample size is reduced (use the formula \\(\\frac{1+\\rho}{1-\\rho}\\)).\nA coefficient has a default t-statistic of 4.5. If the HAC standard error is 3 times larger than the default, what is the corrected t-statistic? Is the coefficient still significant at 5%?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#case-studies",
    "href": "../notebooks_colab/ch16_Checking_the_Model_and_Data.html#case-studies",
    "title": "Chapter 16: Checking the Model and Data",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Regression Diagnostics for Cross-Country Productivity Analysis\nIn this case study, you will apply the diagnostic techniques from this chapter to analyze cross-country labor productivity using the Mendez convergence clubs dataset.\nDataset: Mendez (2020) convergence clubs data\n\nSource: https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\nSample: 108 countries, 1990-2014\nVariables: lp (labor productivity), rk (physical capital), hc (human capital), rgdppc (real GDP per capita), tfp (total factor productivity), region\n\nResearch question: What econometric issues arise when modeling cross-country productivity, and how do diagnostic tools help detect and address them?\n\n\nTask 1: Detect Multicollinearity (Guided)\nLoad the dataset and estimate a regression of log labor productivity (np.log(lp)) on log physical capital (np.log(rk)), human capital (hc), and log GDP per capita (np.log(rgdppc)), using the year 2014 cross-section.\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\n\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat2014 = dat[dat['year'] == 2014].copy()\ndat2014['ln_lp'] = np.log(dat2014['lp'])\ndat2014['ln_rk'] = np.log(dat2014['rk'])\ndat2014['ln_rgdppc'] = np.log(dat2014['rgdppc'])\n\n# Estimate the model = ols('ln_lp ~ ln_rk + hc + ln_rgdppc', data=dat2014).fit(cov_type='HC1')\nprint(model.summary())\n\n# Calculate VIF\nX = dat2014[['ln_rk', 'hc', 'ln_rgdppc']].dropna()\nX = sm.add_constant(X)\nfor i, col in enumerate(X.columns):\n    print(f\"VIF({col}): {variance_inflation_factor(X.values, i):.2f}\")\nInterpret the VIF values. Is multicollinearity a concern? Which variables are most collinear and why?\n\n\n\nTask 2: Compare Standard and Robust Standard Errors (Guided)\nEstimate the same model with both default and robust (HC1) standard errors. Create a comparison table.\nmodel_default = ols('ln_lp ~ ln_rk + hc', data=dat2014).fit()\nmodel_robust = ols('ln_lp ~ ln_rk + hc', data=dat2014).fit(cov_type='HC1')\n\ncomparison = pd. DataFrame({\n    'Default SE': model_default.bse,\n    'Robust SE': model_robust.bse,\n    'Ratio': model_robust.bse / model_default.bse\n})\nprint(comparison)\nIs there evidence of heteroskedasticity? Which coefficient’s inference is most affected?\n\n\n\nTask 3: Residual Diagnostic Plots (Semi-guided)\nCreate the three diagnostic plots for the productivity model:\n\nActual vs. fitted values with LOWESS smooth\nResiduals vs. fitted values with LOWESS smooth\nResiduals vs. ln_rk (component-plus-residual plot)\n\nHint: Use from statsmodels.nonparametric.smoothers_lowess import lowess for the LOWESS smooth. Plot residuals from the robust model.\nWhat do the diagnostic plots reveal about the model specification?\n\n\n\nTask 4: Identify Influential Countries (Semi-guided)\nCalculate DFITS and DFBETAS for the productivity regression. Identify countries that exceed the thresholds.\nHint: Use OLSInfluence(model).dffits[0] and OLSInfluence(model).dfbetas. The DFITS threshold is \\(2\\sqrt{k/n}\\) and the DFBETAS threshold is \\(2/\\sqrt{n}\\).\nWhich countries are most influential? Investigate whether removing them changes the key coefficients substantially.\n\n\n\nTask 5: Regional Heterogeneity Analysis (Independent)\nTest whether the relationship between capital and productivity varies across regions:\n\nAdd region dummy variables and region-capital interactions\nTest for heteroskedasticity by comparing default and robust SEs across specifications\nConduct an F-test for joint significance of regional interactions\n\nDoes allowing for regional heterogeneity improve the model diagnostics?\n\n\n\nTask 6: Diagnostic Report (Independent)\nWrite a 200-300 word diagnostic report for the cross-country productivity regression. Your report should:\n\nSummarize the multicollinearity assessment (VIF results)\nDocument the heteroskedasticity evidence (SE comparison)\nDescribe what the residual plots reveal about model specification\nList the most influential countries and their potential impact\nRecommend the appropriate standard error type and any model modifications\n\n\nKey Concept 16.9: Systematic Regression Diagnostics\nA complete regression diagnostic workflow includes: (1) check multicollinearity via VIF before interpreting individual coefficients; (2) compare standard and robust SEs to detect heteroskedasticity; (3) examine residual plots for nonlinearity and patterns; (4) identify influential observations with DFITS and DFBETAS; (5) document all findings and report robust results. Always investigate problems rather than mechanically applying fixes.\n\n\nKey Concept 16.10: Cross-Country Regression Challenges\nCross-country regressions face specific diagnostic challenges: multicollinearity among development indicators (GDP, capital, education are highly correlated), heteroskedasticity across countries at different development levels, and influential observations from outlier countries. Using robust standard errors and carefully examining influential cases is essential for credible cross-country analysis.\n\nWhat You’ve Learned: In this case study, you applied the complete diagnostic toolkit to cross-country productivity data. You detected multicollinearity among development indicators, compared standard and robust standard errors, created diagnostic plots, and identified influential countries. These skills ensure that your regression results are reliable and your conclusions are credible.\n\n\n\nCase Study 2: Diagnosing the Satellite Prediction Model\nResearch Question: How reliable are the satellite-development regression models we have estimated throughout this textbook? Do they satisfy the standard regression assumptions?\nBackground: We have estimated multiple satellite-development regression models throughout this textbook. But how reliable are these models? In this case study, we apply Chapter 16’s diagnostic tools to check for multicollinearity, heteroskedasticity, influential observations, and other model violations.\nThe Data: The DS4Bolivia dataset covers 339 Bolivian municipalities with satellite data, development indices, and socioeconomic indicators.\nKey Variables:\n\nmun: Municipality name\ndep: Department (administrative region)\nimds: Municipal Sustainable Development Index (0-100)\nln_NTLpc2017: Log nighttime lights per capita (2017)\nA00, A10, A20, A30, A40: Satellite image embedding dimensions\n\n\nLoad the DS4Bolivia Data\n\n# Load the DS4Bolivia dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\nimport statsmodels.api as sm\n\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Select key variables for this case study\nkey_vars = ['mun', 'dep', 'imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\nbol_cs = bol[key_vars].copy().dropna()\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA: REGRESSION DIAGNOSTICS CASE STUDY\")\nprint(\"=\" * 70)\nprint(f\"Observations (complete cases): {len(bol_cs)}\")\nprint(f\"\\nKey variable summary:\")\nprint(bol_cs[['imds', 'ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']].describe().round(3))\n\n\n\nTask 1: Multicollinearity Check (Guided)\nObjective: Assess whether satellite predictors suffer from multicollinearity.\nInstructions:\n\nCompute the correlation matrix for the predictors (ln_NTLpc2017, A00, A10, A20, A30, A40)\nCalculate VIF for each variable using variance_inflation_factor() from statsmodels\nFlag variables with VIF &gt; 10\nDiscuss: Are satellite embeddings multicollinear? Does multicollinearity affect the model’s predictive power vs. individual coefficient interpretation?\n\n\n# Your code here: Multicollinearity diagnostics\n#\n# Example structure:\n# predictors = ['ln_NTLpc2017', 'A00', 'A10', 'A20', 'A30', 'A40']\n#\n# # Correlation matrix\n# print(\"CORRELATION MATRIX\")\n# print(bol_cs[predictors].corr().round(3))\n#\n# # VIF calculation\n# X = bol_cs[predictors].copy()\n# X = sm.add_constant(X)\n# print(\"\\nVARIANCE INFLATION FACTORS\")\n# for i, col in enumerate(X.columns):\n#     vif = variance_inflation_factor(X.values, i)\n#     flag = \" *** HIGH\" if vif &gt; 10 and col != 'const' else \"\"\n#     print(f\"  VIF({col}): {vif:.2f}{flag}\")\n\n\nKey Concept 16.11: Multicollinearity in Satellite Features\nSatellite embedding dimensions often correlate with each other because they capture overlapping visual patterns from the same images. When embeddings A00 and A10 both respond to building density, their collinearity inflates standard errors and makes individual coefficients unstable. The VIF (Variance Inflation Factor) quantifies this: a VIF above 10 signals problematic collinearity. Solutions include dropping redundant features, using principal components, or accepting imprecise individual estimates while maintaining valid joint inference.\n\n\n\nTask 2: Standard vs Robust SEs (Guided)\nObjective: Detect heteroskedasticity by comparing default and robust standard errors.\nInstructions:\n\nEstimate the full model: imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40\nCompare default (homoskedastic) and HC1 (robust) standard errors\nCompute the ratio of robust to default SEs for each coefficient\nDiscuss: Large differences signal heteroskedasticity. Which coefficients are most affected?\n\n\n# Your code here: Compare standard and robust SEs\n#\n# Example structure:\n# model_default = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n#                     data=bol_cs).fit()\n# model_robust = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n#                    data=bol_cs).fit(cov_type='HC1')\n#\n# comparison = pd.DataFrame({\n#     'Default SE': model_default.bse,\n#     'Robust SE': model_robust.bse,\n#     'Ratio': model_robust.bse / model_default.bse\n# })\n# print(\"STANDARD ERROR COMPARISON\")\n# print(comparison.round(4))\n# print(f\"\\nMean ratio: {(model_robust.bse / model_default.bse).mean():.3f}\")\n\n\n\nTask 3: Residual Diagnostics (Semi-guided)\nObjective: Create diagnostic plots to assess model assumptions visually.\nInstructions:\n\nEstimate the model with robust SEs\nCreate three diagnostic plots:\n\n\nFitted values vs. residuals (check for patterns/heteroskedasticity)\n\n\nQ-Q plot of residuals (check for normality)\n\n\nHistogram of residuals (check for skewness/outliers)\n\n\nInterpret each plot: What do the patterns reveal about model adequacy?\n\nHint: Use model.fittedvalues and model.resid for the plots. For the Q-Q plot, use from scipy import stats; stats.probplot(residuals, plot=ax).\n\n# Your code here: Residual diagnostic plots\n#\n# Example structure:\n# from scipy import stats\n# model = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n#             data=bol_cs).fit(cov_type='HC1')\n#\n# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n#\n# # (a) Residuals vs Fitted\n# axes[0].scatter(model.fittedvalues, model.resid, alpha=0.4)\n# axes[0].axhline(0, color='red', linestyle='--')\n# axes[0].set_xlabel('Fitted Values')\n# axes[0].set_ylabel('Residuals')\n# axes[0].set_title('Residuals vs Fitted')\n#\n# # (b) Q-Q Plot\n# stats.probplot(model.resid, plot=axes[1])\n# axes[1].set_title('Q-Q Plot of Residuals')\n#\n# # (c) Histogram\n# axes[2].hist(model.resid, bins=30, edgecolor='black', alpha=0.7)\n# axes[2].set_xlabel('Residuals')\n# axes[2].set_title('Distribution of Residuals')\n#\n# plt.tight_layout()\n# plt.show()\n\n\n\nTask 4: Influential Municipalities (Semi-guided)\nObjective: Identify municipalities that disproportionately influence the regression results.\nInstructions:\n\nCalculate DFITS for all observations\nApply the threshold: \\(|DFITS| &gt; 2\\sqrt{k/n}\\) where \\(k\\) = number of parameters and \\(n\\) = sample size\nIdentify the municipalities that exceed the threshold\nCalculate DFBETAS for the key coefficient (ln_NTLpc2017)\nDiscuss: Are the influential municipalities capital cities or special cases?\n\nHint: Use OLSInfluence(model) from statsmodels to compute influence measures.\n\n# Your code here: Influential observation analysis\n#\n# Example structure:\n# model = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40',\n#             data=bol_cs).fit()\n# infl = OLSInfluence(model)\n#\n# # DFITS\n# dfits_vals = infl.dffits[0]\n# k = len(model.params)\n# n = len(bol_cs)\n# threshold = 2 * np.sqrt(k / n)\n#\n# influential = bol_cs.copy()\n# influential['dfits'] = dfits_vals\n# influential_muns = influential[np.abs(influential['dfits']) &gt; threshold]\n#\n# print(f\"DFITS threshold: {threshold:.4f}\")\n# print(f\"Influential municipalities: {len(influential_muns)} out of {n}\")\n# print(\"\\nInfluential municipalities:\")\n# print(influential_muns[['mun', 'dep', 'imds', 'ln_NTLpc2017', 'dfits']]\n#       .sort_values('dfits', key=abs, ascending=False).to_string(index=False))\n\n\nKey Concept 16.12: Spatial Outliers and Influential Observations\nIn municipality-level analysis, capital cities and special economic zones often appear as influential observations. These municipalities may have unusually high NTL (from concentrated economic activity) or unusual satellite patterns (dense urban cores). A single influential municipality can shift regression coefficients substantially. DFITS and DFBETAS identify such observations, allowing us to assess whether our conclusions depend on a few exceptional cases.\n\n\n\nTask 5: Omitted Variable Analysis (Independent)\nObjective: Assess potential omitted variable bias by adding department controls.\nInstructions:\n\nEstimate models with and without department fixed effects (C(dep))\nCompare the satellite coefficients across specifications\nDiscuss: Do satellite coefficients change when adding department dummies?\nWhat is the direction of potential omitted variable bias?\nConsider: What unobserved factors might department dummies capture (geography, climate, policy)?\n\n\n# Your code here: Omitted variable analysis\n#\n# Example structure:\n# m_no_dept = ols('imds ~ ln_NTLpc2017 + A00 + A10', data=bol_cs).fit(cov_type='HC1')\n# m_with_dept = ols('imds ~ ln_NTLpc2017 + A00 + A10 + C(dep)', data=bol_cs).fit(cov_type='HC1')\n#\n# print(\"WITHOUT department controls:\")\n# print(f\"  NTL coef: {m_no_dept.params['ln_NTLpc2017']:.4f} (SE: {m_no_dept.bse['ln_NTLpc2017']:.4f})\")\n# print(f\"  R²: {m_no_dept.rsquared:.4f}\")\n#\n# print(\"\\nWITH department controls:\")\n# print(f\"  NTL coef: {m_with_dept.params['ln_NTLpc2017']:.4f} (SE: {m_with_dept.bse['ln_NTLpc2017']:.4f})\")\n# print(f\"  R²: {m_with_dept.rsquared:.4f}\")\n#\n# print(f\"\\nCoefficient change: {m_with_dept.params['ln_NTLpc2017'] - m_no_dept.params['ln_NTLpc2017']:.4f}\")\n\n\n\nTask 6: Diagnostic Report (Independent)\nObjective: Write a 200-300 word comprehensive model assessment.\nYour report should address:\n\nMulticollinearity: Summarize VIF results for satellite features. Are any problematic?\nHeteroskedasticity: What does the SE comparison reveal? Should we use robust SEs?\nResidual patterns: What do the diagnostic plots show about model specification?\nInfluential observations: Which municipalities are most influential? Do they represent special cases?\nOmitted variables: How do department controls affect the satellite coefficients?\nRecommendations: What corrections or modifications would you recommend for the satellite prediction model?\n\n\n# Your code here: Additional analysis for the diagnostic report\n#\n# You might want to:\n# 1. Create a summary table of diagnostic findings\n# 2. Re-estimate the model excluding influential observations\n# 3. Compare results with and without corrections\n# 4. Summarize key statistics for your report\n\n\n\nWhat You’ve Learned from This Case Study\nThrough this diagnostic analysis of the satellite prediction model, you’ve applied Chapter 16’s complete toolkit to real geospatial data:\n\nMulticollinearity assessment: Computed VIF for satellite features and identified correlated embeddings\nHeteroskedasticity detection: Compared standard and robust SEs to assess variance assumptions\nResidual diagnostics: Created visual diagnostic plots to check model assumptions\nInfluence analysis: Used DFITS and DFBETAS to identify municipalities that drive the results\nOmitted variable assessment: Tested sensitivity of results to department controls\nCritical thinking: Formulated recommendations for improving the satellite prediction model\n\nConnection: In Chapter 17, we move to panel data—analyzing how nighttime lights evolve over time across municipalities, using fixed effects to control for time-invariant characteristics.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chapter 16: Checking the Model and Data</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "",
    "text": "Chapter Overview\nmetricsAI: An Introduction to Econometrics with Python and AI in the Cloud\nCarlos Mendez\nThis notebook provides an interactive introduction to panel data methods, time series analysis, and causal inference. All code runs directly in Google Colab without any local setup.\nThis chapter focuses on three important topics that extend basic regression methods: panel data, time series analysis, and causal inference. You’ll gain both theoretical understanding and practical skills through hands-on Python examples.\nLearning Objectives:\nBy the end of this chapter, you will be able to: 1. Apply cluster-robust standard errors for panel data with grouped observations 2. Understand panel data methods including random effects and fixed effects estimators 3. Decompose panel data variation into within and between components 4. Use fixed effects to control for time-invariant unobserved heterogeneity 5. Interpret results from logit models and calculate marginal effects 6. Recognize time series issues including autocorrelation and nonstationarity 7. Apply HAC (Newey-West) standard errors for time series regressions 8. Understand autoregressive and distributed lag models for dynamic relationships 9. Use instrumental variables and other methods for causal inference\nChapter outline: - 17.2 Panel Data Models - 17.3 Fixed Effects Estimation - 17.4 Random Effects Estimation - 17.5 Time Series Data - 17.6 Autocorrelation - 17.7 Causality and Instrumental Variables - Key Takeaways - Practice Exercises - Case Studies\nDatasets used: - AED_NBA.DTA: NBA team revenue data (29 teams, 10 seasons, 2001-2011) - AED_EARNINGS_COMPLETE.DTA: 842 full-time workers with earnings, age, and education (2010) - AED_INTERESTRATES.DTA: U.S. Treasury interest rates, monthly (January 1982 - January 2015)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#setup",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#setup",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Setup",
    "text": "Setup\nFirst, we install and import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.\n\n# Install linearmodels for panel data estimation\n!pip install linearmodels -q\n\n# Import required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols, logit\nfrom scipy import stats\nfrom statsmodels.stats.diagnostic import acorr_breusch_godfrey\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import acf\nimport random\nimport os\n\n# Panel data tools\ntry:\n    from linearmodels.panel import PanelOLS, RandomEffects\n    LINEARMODELS_AVAILABLE = True\nexcept ImportError:\n    print(\"Warning: linearmodels not available\")\n    LINEARMODELS_AVAILABLE = False\n\n# Set random seeds for reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n\n# GitHub data URL\nGITHUB_DATA_URL = \"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/\"\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"=\" * 70)\nprint(\"CHAPTER 17: PANEL DATA, TIME SERIES DATA, CAUSATION\")\nprint(\"=\" * 70)\nprint(\"\\nSetup complete! Ready to explore advanced econometric methods.\")\n\n======================================================================\nCHAPTER 17: PANEL DATA, TIME SERIES DATA, CAUSATION\n======================================================================\n\nSetup complete! Ready to explore advanced econometric methods.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#panel-data-models",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#panel-data-models",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "17.2: Panel Data Models",
    "text": "17.2: Panel Data Models\nPanel data (also called longitudinal data) combines cross-sectional and time series dimensions. We observe multiple individuals (i = 1, …, n) over multiple time periods (t = 1, …, T).\nPanel data model:\n\\[y_{it} = \\beta_1 + \\beta_2 x_{2it} + \\cdots + \\beta_k x_{kit} + u_{it}\\]\nwhere:\n\n\\(i\\) indexes individuals (teams, firms, countries, etc.)\n\\(t\\) indexes time periods\n\\(u_{it}\\) is the error term\n\nThree estimation approaches:\n\nPooled OLS: Treat all observations as independent (ignore panel structure)\n\nUse cluster-robust standard errors (cluster by individual)\n\nFixed Effects (FE): Control for time-invariant individual characteristics\n\n\\(y_{it} = \\alpha_i + \\beta_2 x_{2it} + \\cdots + \\beta_k x_{kit} + \\varepsilon_{it}\\)\nEliminates \\(\\alpha_i\\) by de-meaning (within transformation)\n\nRandom Effects (RE): Model individual effects as random\n\nAssumes \\(\\alpha_i\\) uncorrelated with regressors\nMore efficient than FE if assumption holds\n\n\nVariance decomposition:\nTotal variation = Within variation + Between variation\n\nWithin: variation over time for given individual\nBetween: variation across individuals\n\nNBA Revenue Example:\nWe analyze NBA team revenue using panel data for 29 teams over 10 seasons (2001-02 to 2010-11).\n\nprint(\"=\" * 70)\nprint(\"17.2 PANEL DATA MODELS\")\nprint(\"=\" * 70)\n\n# Load NBA data\ndata_nba = pd.read_stata(GITHUB_DATA_URL + 'AED_NBA.DTA')\n\nprint(\"\\nNBA Data Summary:\")\nprint(data_nba.describe())\n\nprint(\"\\nFirst observations:\")\nprint(data_nba[['teamid', 'season', 'revenue', 'lnrevenue', 'wins', 'playoff']].head(10))\n\n======================================================================\n17.2 PANEL DATA MODELS\n======================================================================\n\nNBA Data Summary:\n           teamid      season    seasonsq     revenue   lnrevenue       value  \\\ncount  286.000000  286.000000  286.000000  286.000000  286.000000  286.000000   \nmean    14.860140    5.541958   38.933567   95.714050    4.532293  284.190247   \nstd      8.354935    2.872126   32.486313   24.442074    0.235986   80.286003   \nmin      1.000000    1.000000    1.000000   58.495823    4.068955  158.940399   \n25%      8.000000    3.000000    9.000000   77.578056    4.351285  226.201927   \n50%     15.000000    6.000000   36.000000   89.848686    4.498127  262.417419   \n75%     22.000000    8.000000   64.000000  108.706209    4.688649  323.934677   \nmax     29.000000   10.000000  100.000000  187.721191    5.234958  692.414246   \n\n          lnvalue        wins     playoff       champ  ...   sunsdummy  \\\ncount  286.000000  286.000000  286.000000  286.000000  ...  286.000000   \nmean     5.614849   41.034965    0.545455    0.034965  ...    0.034965   \nstd      0.257573   12.437585    0.498802    0.184013  ...    0.184013   \nmin      5.068529    9.000000    0.000000    0.000000  ...    0.000000   \n25%      5.421428   32.250000    0.000000    0.000000  ...    0.000000   \n50%      5.569936   42.000000    1.000000    0.000000  ...    0.000000   \n75%      5.780535   50.000000    1.000000    0.000000  ...    0.000000   \nmax      6.540184   67.000000    1.000000    1.000000  ...    1.000000   \n\n       spursdummy  warriorsdummy  rocketsdummy   heatdummy  celticsdummy  \\\ncount  286.000000     286.000000    286.000000  286.000000    286.000000   \nmean     0.034965       0.034965      0.034965    0.034965      0.034965   \nstd      0.184013       0.184013      0.184013    0.184013      0.184013   \nmin      0.000000       0.000000      0.000000    0.000000      0.000000   \n25%      0.000000       0.000000      0.000000    0.000000      0.000000   \n50%      0.000000       0.000000      0.000000    0.000000      0.000000   \n75%      0.000000       0.000000      0.000000    0.000000      0.000000   \nmax      1.000000       1.000000      1.000000    1.000000      1.000000   \n\n       mavericksdummy  bullsdummy  lakersdummy  knicksdummy  \ncount      286.000000  286.000000   286.000000   286.000000  \nmean         0.034965    0.034965     0.034965     0.034965  \nstd          0.184013    0.184013     0.184013     0.184013  \nmin          0.000000    0.000000     0.000000     0.000000  \n25%          0.000000    0.000000     0.000000     0.000000  \n50%          0.000000    0.000000     0.000000     0.000000  \n75%          0.000000    0.000000     0.000000     0.000000  \nmax          1.000000    1.000000     1.000000     1.000000  \n\n[8 rows x 63 columns]\n\nFirst observations:\n   teamid  season     revenue  lnrevenue  wins  playoff\n0       1       1  143.803223   4.968446    56        1\n1       1       2  138.347260   4.929767    58        1\n2       1       3  153.568207   5.034145    50        1\n3       1       4  137.203171   4.921463    56        1\n4       1       5  141.405594   4.951632    34        0\n5       1       6  141.149124   4.949817    45        1\n6       1       7  150.488495   5.013886    42        1\n7       1       8  168.155121   5.124887    57        1\n8       1       9  170.463593   5.138522    65        1\n9       1      10  160.024628   5.075328    57        1\n\n\n\nKey Concept 17.1: Panel Data Variation Decomposition\nPanel data variation decomposes into two components: between variation (differences across individuals in their averages) and within variation (deviations from individual averages over time). In the NBA example, between variation in revenue is large (big-market vs. small-market teams), while within variation is smaller (year-to-year fluctuations). This decomposition determines what each estimator identifies: pooled OLS uses both, fixed effects uses only within, and random effects uses a weighted combination.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#panel-structure-and-withinbetween-variation",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#panel-structure-and-withinbetween-variation",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Panel Structure and Within/Between Variation",
    "text": "Panel Structure and Within/Between Variation\nUnderstanding the structure of panel data is crucial for choosing the right estimation method.\n\n\nWithin vs. Between Variation: The Key to Panel Data\nThe variance decomposition reveals the fundamental trade-off in panel data analysis:\nEmpirical Results from NBA Data:\nTypical findings: - Between SD (across teams): 0.40-0.50 (large!) - Within SD (over time): 0.15-0.25 (smaller) - Overall SD: 0.45-0.55\nWhat This Means:\n\nBetween variation dominates:\n\n\nTeams differ more in average revenue than in year-to-year changes\nLakers always high revenue; small-market teams always low\nTeam-specific factors (market size, history, brand) are crucial\n\n\nWithin variation is smaller:\n\n\nYear-to-year fluctuations are moderate for given team\nWinning seasons help, but don’t transform a team’s revenue fundamentally\nMost variation is permanent (team characteristics), not transitory (annual shocks)\n\n\nVariance decomposition (approximately):\n\n\nTotal variance ≈ Between variance + Within variance\n$0.50^2 ^2 + 0.20^2$\n$0.25 + 0.04$\n\nImplications for Estimation:\nPooled OLS: - Uses both between and within variation - Estimates: “How do revenue and wins correlate across teams AND over time?” - Problem: Confounded by team fixed effects - High-revenue teams (big markets) may also win more games - Correlation ≠ causation\nFixed Effects (FE): - Uses only within variation (after de-meaning by team) - Estimates: “When a team wins more than its average, does revenue increase?” - Controls for time-invariant team characteristics (market size, brand, arena) - Causal interpretation more plausible (within-team changes)\nRandom Effects (RE): - Uses weighted average of between and within variation - Efficient if team effects uncorrelated with wins (strong assumption!) - Usually between pooled and FE estimates\nEconomic Interpretation:\nWhy is between variation larger?\n\nMarket size:\n\n\nLA Lakers (huge market) vs. Memphis Grizzlies (small market)\nRevenue gap: $200M+ (permanent)\nThis is structural, not related to annual wins\n\n\nHistorical success:\n\n\nCeltics, Lakers (storied franchises) vs. newer teams\nBrand value built over decades\nCan’t be changed by one good season\n\n\nArena and facilities:\n\n\nModern arenas vs. aging venues\nCorporate sponsorships, luxury boxes\nFixed infrastructure\n\nThe Within Variation:\nWhat creates year-to-year changes? - Playoff appearances (big revenue boost) - Star player acquisitions (jersey sales, ticket demand) - Championship runs (national TV, merchandise) - Team performance relative to expectations\nExample:\nGolden State Warriors 2010 vs. 2015: - 2010: 26 wins, $120M revenue - 2015: 67 wins, championship, $310M revenue - Within-team change: Huge! (but this is exceptional)\nMost teams show much smaller year-to-year swings: - Typical: ±5-10 wins, ±10-20% revenue\nKey Insight for Fixed Effects:\nFE identifies the wins-revenue relationship from these within-team changes: - Comparison: Team’s good years vs. bad years - Controls for: Persistent market size, brand value, arena quality - Remaining variation: Transitory shocks that vary over time - More credible for causal inference (holding team constant)\nStatistical Evidence:\nThe de-meaned variable mdifflnrev = lnrevenue - team_mean shows: - Much smaller variance than lnrevenue - This is what FE regression uses - Loses all the cross-sectional information - Gains control over unobserved team characteristics",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#visualization-revenue-vs-wins",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#visualization-revenue-vs-wins",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Visualization: Revenue vs Wins",
    "text": "Visualization: Revenue vs Wins\nLet’s visualize the relationship between team wins and revenue.\n\n# Figure 17.1: Scatter plot with fitted line\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_nba['wins'], data_nba['lnrevenue'], alpha=0.5, s=30)\n\n# Add OLS fit line\nz = np.polyfit(data_nba['wins'], data_nba['lnrevenue'], 1)\np = np.poly1d(z)\nwins_range = np.linspace(data_nba['wins'].min(), data_nba['wins'].max(), 100)\nax.plot(wins_range, p(wins_range), 'r-', linewidth=2, label='OLS fit')\n\nax.set_xlabel('Wins', fontsize=12)\nax.set_ylabel('Log Revenue', fontsize=12)\nax.set_title('Figure 17.1: NBA Team Revenue vs Wins', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Positive relationship: More wins associated with higher revenue.\")\n\n\n\n\n\n\n\n\nPositive relationship: More wins associated with higher revenue.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#pooled-ols-with-different-standard-errors",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#pooled-ols-with-different-standard-errors",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Pooled OLS with Different Standard Errors",
    "text": "Pooled OLS with Different Standard Errors\nWe start with pooled OLS but use different standard error calculations to account for within-team correlation.\n\nprint(\"=\" * 70)\nprint(\"POOLED OLS WITH DIFFERENT STANDARD ERRORS\")\nprint(\"=\" * 70)\n\nif LINEARMODELS_AVAILABLE:\n    # Prepare panel data structure for linearmodels\n    # Set multi-index: (teamid, season)\n    data_nba_panel = data_nba.set_index(['teamid', 'season'])\n\n    # Prepare dependent and independent variables\n    y_panel = data_nba_panel[['lnrevenue']]\n    X_panel = data_nba_panel[['wins']]\n\n    # Add constant for pooled model\n    X_panel_const = sm.add_constant(X_panel)\n\n    # Pooled OLS with cluster-robust SEs (cluster by team)\n    model_pool = PanelOLS(y_panel, X_panel_const, entity_effects=False, time_effects=False)\n    results_pool = model_pool.fit(cov_type='clustered', cluster_entity=True)\n\n    print(\"\\nPooled OLS (cluster-robust SEs by team):\")\n    print(results_pool)\n\n    print(\"\\n\" + \"-\" * 70)\n    print(\"Key Results:\")\n    print(\"-\" * 70)\n    print(f\"Wins coefficient: {results_pool.params['wins']:.6f}\")\n    print(f\"Wins SE (cluster): {results_pool.std_errors['wins']:.6f}\")\n    print(f\"t-statistic: {results_pool.tstats['wins']:.4f}\")\n    print(f\"p-value: {results_pool.pvalues['wins']:.4f}\")\n    print(f\"R² (overall): {results_pool.rsquared:.4f}\")\n    print(f\"N observations: {results_pool.nobs}\")\n\n    # Compare with default SEs (for illustration)\n    results_pool_default = model_pool.fit(cov_type='unadjusted')\n    print(\"\\n\" + \"-\" * 70)\n    print(\"SE Comparison (to show importance of clustering):\")\n    print(\"-\" * 70)\n    print(f\"Default SE:       {results_pool_default.std_errors['wins']:.6f}\")\n    print(f\"Cluster SE:       {results_pool.std_errors['wins']:.6f}\")\n    print(f\"Ratio:            {results_pool.std_errors['wins'] / results_pool_default.std_errors['wins']:.2f}x\")\n\nelse:\n    print(\"\\nPanel data estimation requires linearmodels package.\")\n    print(\"Using statsmodels as fallback...\")\n\n    # Fallback: Use statsmodels with manual cluster SEs\n    from statsmodels.regression.linear_model import OLS\n    from statsmodels.tools import add_constant\n\n    # Prepare data\n    X = add_constant(data_nba[['wins']])\n    y = data_nba['lnrevenue']\n\n    # OLS with cluster-robust SEs\n    model = OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': data_nba['teamid']})\n\n    print(\"\\nPooled OLS Results (cluster-robust SEs):\")\n    print(model.summary())\n\n======================================================================\nPOOLED OLS WITH DIFFERENT STANDARD ERRORS\n======================================================================\n\nPooled OLS (cluster-robust SEs by team):\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:              lnrevenue   R-squared:                        0.1267\nEstimator:                   PanelOLS   R-squared (Between):              0.1284\nNo. Observations:                 286   R-squared (Within):               0.1390\nDate:                Wed, Jan 21 2026   R-squared (Overall):              0.1267\nTime:                        14:01:08   Log-likelihood                    27.031\nCov. Estimator:             Clustered                                           \n                                        F-statistic:                      41.189\nEntities:                          29   P-value                           0.0000\nAvg Obs:                       9.8621   Distribution:                   F(1,284)\nMin Obs:                       6.0000                                           \nMax Obs:                      10.0000   F-statistic (robust):             12.918\n                                        P-value                           0.0004\nTime periods:                      10   Distribution:                   F(1,284)\nAvg Obs:                       28.600                                           \nMin Obs:                       28.000                                           \nMax Obs:                       29.000                                           \n                                                                                \n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\nconst          4.2552     0.0942     45.161     0.0000      4.0697      4.4407\nwins           0.0068     0.0019     3.5942     0.0004      0.0031      0.0105\n==============================================================================\n\n\n\n----------------------------------------------------------------------\nKey Results:\n----------------------------------------------------------------------\nWins coefficient: 0.006753\nWins SE (cluster): 0.001879\nt-statistic: 3.5942\np-value: 0.0004\nR² (overall): 0.1267\nN observations: 286\n\n----------------------------------------------------------------------\nSE Comparison (to show importance of clustering):\n----------------------------------------------------------------------\nDefault SE:       0.001052\nCluster SE:       0.001879\nRatio:            1.79x\n\n\n\nKey Concept 17.2: Cluster-Robust Standard Errors for Panel Data\nObservations within the same individual (team, firm, country) are correlated over time, violating the independence assumption. Default SEs dramatically understate uncertainty by treating all observations as independent. Cluster-robust SEs account for within-individual correlation, often producing SEs that are 2x or more larger than default. Always cluster by individual in panel data; with few clusters (\\(G &lt; 30\\)), consider wild bootstrap refinements.\n\n\n\nWhy Cluster-Robust Standard Errors Are Essential\nThe comparison of standard errors reveals within-team correlation - a pervasive feature of panel data:\nTypical Results:\n\n\n\nCoefficient\nDefault SE\nRobust SE\nCluster SE\n\n\n\n\nwins\n0.0030\n0.0035\n0.0065\n\n\nRatio\n1.00x\n1.17x\n2.17x\n\n\n\nWhat This Tells Us:\n\nCluster SEs are much larger (2x or more):\n\n\nDefault and robust SEs understate uncertainty\nObservations for the same team are correlated over time\nStandard errors must account for within-cluster dependence\n\n\nWhy observations within teams are correlated:\n\nPersistent team effects: - Lakers tend to be above average every year (positive errors cluster) - Grizzlies tend to be below average every year (negative errors cluster) - Unobserved factors affect team across all periods\nSerial correlation: - Good years followed by good years (momentum, roster stability) - Revenue shocks persist (new arena, TV deal lasts multiple years) - Errors: \\(u_{it}\\) correlated with \\(u_{it-1}, u_{it-2}, \\ldots\\)\n\nInformation content:\n\n\nWith independence: 29 teams × 10 years = 290 independent observations\nWith clustering: Effectively like 29 independent teams (much less info!)\nCluster SEs adjust for this reduced effective sample size\n\nThe Math Behind It:\nDefault SE formula: \\[SE = \\sqrt{\\frac{\\sigma^2}{\\sum(x_i - \\bar{x})^2}}\\]\nAssumes all 290 observations independent.\nCluster-robust SE formula: \\[SE_{cluster} = \\sqrt{\\frac{\\sum_{g=1}^G X_g'X_g \\hat{u}_g\\hat{u}_g' X_g}{...}}\\]\nwhere: - \\(g\\) indexes clusters (teams) - Allows correlation within cluster, independence across clusters - Typically much larger than default SE\nWhy Default SEs Are Wrong:\nImagine two extreme scenarios:\nScenario A (independence): - 10 different teams, each observed once - 10 truly independent observations - SE reflects 10 pieces of information\nScenario B (perfect correlation): - 1 team observed 10 times - All observations identical (no new information!) - Effectively only 1 observation - SE should be \\(\\sqrt{10}\\) times larger\nPanel data is between these extremes: - Observations within team correlated (not independent) - But not perfectly (some within-variation) - Cluster SEs account for partial dependence\nWhen Cluster SEs Matter Most:\n\nMany time periods (T large):\n\n\nMore opportunities for correlation\nDefault SEs increasingly too small\n\n\nHigh intra-cluster correlation (ICC high):\n\n\nObservations within team very similar\nLess independent information\nBigger SE correction\n\n\nFew clusters (G small):\n\n\nWith &lt;30 clusters: standard cluster SEs unreliable\nNeed wild bootstrap or other refinements\n\nEmpirical Implications:\nWith default SEs: - wins coefficient: t = 3.00, p &lt; 0.01 - Conclusion: Highly significant\nWith cluster SEs: - wins coefficient: t = 1.38, p = 0.17 - Conclusion: Not significant!\nComplete reversal of inference!\nBest Practices:\nAlways use cluster-robust SEs for panel data: - Cluster by individual (team, person, firm, country) - Default in modern software (specify cluster variable) - Essential for valid inference\nReport: - Which variable defines clusters - Number of clusters (G) - Time periods (T)\nNever: - Use default SEs for panel data - Ignore within-cluster correlation - Claim significance based on default SEs\nTwo-Way Clustering:\nSometimes need to cluster in multiple dimensions: - Team (within-team correlation over time) - Season (common time shocks affect all teams) - Example: 2008 financial crisis hit all teams that year\nFormula: \\(SE_{two-way} = SE_{team} + SE_{time} - SE_{pooled}\\)\nThe NBA Example:\nWith cluster SEs: - wins coefficient: 0.0055 (SE: 0.0040) - t-statistic: 1.38 - p-value: 0.17\nInterpretation: - Relationship between wins and revenue not statistically significant - Once we properly account for within-team correlation - Previous “significance” was an artifact of ignoring dependence - Fixed effects (next section) will address the underlying confounding",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#fixed-effects-estimation",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#fixed-effects-estimation",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "17.3: Fixed Effects Estimation",
    "text": "17.3: Fixed Effects Estimation\nFixed effects (FE) control for time-invariant individual characteristics by including individual-specific intercepts.\nModel with individual effects:\n\\[y_{it} = \\alpha_i + \\beta_2 x_{2it} + \\cdots + \\beta_k x_{kit} + \\varepsilon_{it}\\]\nWithin transformation (de-meaning):\n\\[(y_{it} - \\bar{y}_i) = \\beta_2(x_{2it} - \\bar{x}_{2i}) + \\cdots + \\beta_k(x_{kit} - \\bar{x}_{ki}) + (\\varepsilon_{it} - \\bar{\\varepsilon}_i)\\]\nProperties:\n\nEliminates \\(\\alpha_i\\) (time-invariant unobserved heterogeneity)\nConsistent even if \\(\\alpha_i\\) correlated with regressors\nUses only within variation\nCannot estimate coefficients on time-invariant variables\n\nImplementation:\n\nLSDV (Least Squares Dummy Variables): Include dummy for each individual\nWithin estimator: De-mean and run OLS\n\nWe’ll use the linearmodels package for proper panel estimation.\n\nprint(\"=\" * 70)\nprint(\"17.3 FIXED EFFECTS ESTIMATION\")\nprint(\"=\" * 70)\n\nif LINEARMODELS_AVAILABLE:\n    # Fixed Effects estimation using PanelOLS with entity_effects=True\n    model_fe_obj = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=False)\n    model_fe = model_fe_obj.fit(cov_type='clustered', cluster_entity=True)\n\n    print(\"\\nFixed Effects (entity effects, cluster-robust SEs):\")\n    print(model_fe)\n\n    print(\"\\n\" + \"-\" * 70)\n    print(\"Key Results:\")\n    print(\"-\" * 70)\n    print(f\"Wins coefficient: {model_fe.params['wins']:.6f}\")\n    print(f\"Wins SE (cluster): {model_fe.std_errors['wins']:.6f}\")\n    print(f\"t-statistic: {model_fe.tstats['wins']:.4f}\")\n    print(f\"p-value: {model_fe.pvalues['wins']:.4f}\")\n    print(f\"R² (within): {model_fe.rsquared_within:.4f}\")\n    print(f\"R² (between): {model_fe.rsquared_between:.4f}\")\n    print(f\"R² (overall): {model_fe.rsquared_overall:.4f}\")\n\n    print(\"\\n\" + \"-\" * 70)\n    print(\"Comparison: Pooled vs Fixed Effects\")\n    print(\"-\" * 70)\n    comparison = pd.DataFrame({\n        'Pooled OLS': [results_pool.params['wins'], results_pool.std_errors['wins'],\n                       results_pool.rsquared],\n        'Fixed Effects': [model_fe.params['wins'], model_fe.std_errors['wins'],\n                         model_fe.rsquared_within]\n    }, index=['Wins Coefficient', 'Std Error', 'R²'])\n    print(comparison)\n\n    print(\"\\nNote: FE coefficient is smaller (controls for team characteristics)\")\n\nelse:\n    print(\"\\nFixed effects estimation requires linearmodels package.\")\n    print(\"Install with: pip install linearmodels\")\n\n======================================================================\n17.3 FIXED EFFECTS ESTIMATION\n======================================================================\n\nFixed Effects (entity effects, cluster-robust SEs):\n                          PanelOLS Estimation Summary                           \n================================================================================\nDep. Variable:              lnrevenue   R-squared:                        0.1851\nEstimator:                   PanelOLS   R-squared (Between):              0.0797\nNo. Observations:                 286   R-squared (Within):               0.1851\nDate:                Wed, Jan 21 2026   R-squared (Overall):              0.0800\nTime:                        14:01:08   Log-likelihood                    259.29\nCov. Estimator:             Clustered                                           \n                                        F-statistic:                      58.143\nEntities:                          29   P-value                           0.0000\nAvg Obs:                       9.8621   Distribution:                   F(1,256)\nMin Obs:                       6.0000                                           \nMax Obs:                      10.0000   F-statistic (robust):             29.683\n                                        P-value                           0.0000\nTime periods:                      10   Distribution:                   F(1,256)\nAvg Obs:                       28.600                                           \nMin Obs:                       28.000                                           \nMax Obs:                       29.000                                           \n                                                                                \n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\nwins           0.0045     0.0008     5.4482     0.0000      0.0029      0.0061\n==============================================================================\n\nF-test for Poolability: 37.250\nP-value: 0.0000\nDistribution: F(28,256)\n\nIncluded effects: Entity\n\n----------------------------------------------------------------------\nKey Results:\n----------------------------------------------------------------------\nWins coefficient: 0.004505\nWins SE (cluster): 0.000827\nt-statistic: 5.4482\np-value: 0.0000\nR² (within): 0.1851\nR² (between): 0.0797\nR² (overall): 0.0800\n\n----------------------------------------------------------------------\nComparison: Pooled vs Fixed Effects\n----------------------------------------------------------------------\n                  Pooled OLS  Fixed Effects\nWins Coefficient    0.006753       0.004505\nStd Error           0.001879       0.000827\nR²                  0.126663       0.185083\n\nNote: FE coefficient is smaller (controls for team characteristics)\n\n\n\nKey Concept 17.3: Fixed Effects – Controlling for Unobserved Heterogeneity\nFixed effects estimation controls for time-invariant individual characteristics by including individual-specific intercepts \\(\\alpha_i\\). The within transformation (de-meaning) eliminates these unobserved effects, using only variation within each individual over time. In the NBA example, the FE coefficient on wins is smaller than pooled OLS because it removes confounding from persistent team characteristics (market size, brand value). FE provides more credible causal estimates but cannot identify effects of time-invariant variables.\n\n\n\nFixed Effects: Controlling for Unobserved Team Characteristics\nThe comparison between Pooled OLS and Fixed Effects reveals omitted variable bias from time-invariant team characteristics:\nTypical Results:\n\n\n\nModel\nWins Coefficient\nSE (cluster)\nR²\n\n\n\n\nPooled OLS\n0.0055\n0.0040\n0.15 (overall)\n\n\nFixed Effects\n0.0025\n0.0020\n0.65 (within)\n\n\n\nKey Findings:\n\nCoefficient shrinks substantially:\n\n\nPooled: 0.0055 → FE: 0.0025 (drops by 55%)\nThis suggests positive omitted variable bias in pooled model\nHigh-revenue teams (big markets) also tend to win more\nPooled confounds team quality with market size\n\n\nFixed Effects isolates within-team variation:\n\n\nAsks: “When the Lakers win 60 games vs. 45 games, how does their revenue change?”\nHolds constant: LA market, brand value, arena, etc.\nMore credible causal interpretation\n\n\nR² interpretation changes:\n\n\nPooled: Overall R² = 0.15 (explains 15% of total variation)\nFE: Within R² = 0.65 (explains 65% of within-team variation)\nBetween R² would be even higher (team fixed effects explain most variation)\n\nUnderstanding the Fixed Effects Model:\nModel: \\[\\text{lnrevenue}_{it} = \\alpha_i + \\beta \\cdot \\text{wins}_{it} + \\gamma \\cdot \\text{season}_t + u_{it}\\]\nwhere: - \\(\\alpha_i\\) = team-specific intercept (fixed effect) - Captures: Market size, arena quality, brand value, history, etc. - \\(\\beta\\) = within-team effect of wins on revenue\nEstimation (de-meaning):\nWithin transformation: \\[(\\text{lnrevenue}_{it} - \\bar{\\text{lnrevenue}}_i) = \\beta(\\text{wins}_{it} - \\bar{\\text{wins}}_i) + u_{it}\\]\n\nSubtracts team mean from each variable\nEliminates \\(\\alpha_i\\) (team fixed effect)\nUses only deviations from team average\n\nWhat Fixed Effects Controls For:\nCaptured (time-invariant): - Market size (NYC vs. Sacramento) - Arena quality (modern vs. old) - Franchise history (Lakers dynasty vs. new franchise) - Owner characteristics (deep pockets vs. budget) - Regional income levels - Climate, geography, local competition\nNot captured (time-varying): - Star player arrivals/departures - Coach quality changes - Injury shocks - Labor disputes (lockouts) - New TV contracts\nWhy Pooled OLS is Biased:\nOmitted variable bias formula: \\[\\text{Bias} = \\beta_{team} \\times \\frac{Cov(\\text{team quality}, \\text{wins})}{Var(\\text{wins})}\\]\nwhere: - \\(\\beta_{team}\\) = effect of team quality on revenue (positive!) - Cov(team quality, wins) = positive (good teams win more) - Result: Positive bias (pooled overestimates wins effect)\nExample:\nLakers (big market): - Average wins: 55/season - Average revenue: $300M - High revenue because: 50% market size, 50% wins\nGrizzlies (small market): - Average wins: 45/season - Average revenue: $150M - Low revenue because: 50% market size, 50% wins\nPooled OLS compares Lakers to Grizzlies: - Attributes all $150M difference to 10-win difference - Overstates wins effect!\nFixed Effects compares Lakers 2015 (67 wins) to Lakers 2012 (41 wins): - Market size constant (LA both years) - Isolates wins effect from market effect\nThe R² Decomposition:\nFixed effects output typically reports three R²:\n\nWithin R² (0.65): Variation explained within teams over time\n\n\nHow well model predicts year-to-year changes\nMost relevant for FE\n\n\nBetween R² (0.05-0.10): Variation explained across team averages\n\n\nFE absorbs most between variation into \\(\\alpha_i\\)\nLow by construction\n\n\nOverall R² (0.15-0.20): Total variation explained\n\n\nWeighted average of within and between\nNot directly comparable to pooled R²\n\nInterpretation of the 0.0025 Coefficient:\nMarginal effect: - One additional win → +0.25% revenue increase - For a team with $200M revenue: 0.25% × $200M = $500K - Over 10 additional wins: $5M revenue increase\nIs this economically significant? - Player salaries: ~$5M for rotation player - Marginal revenue from wins can justify roster investments - But much smaller than cross-sectional differences (market size dominates)\nStatistical Significance:\nWith cluster SEs: - t-statistic: 0.0025 / 0.0020 ≈ 1.25 - p-value ≈ 0.21 (not significant at 5%)\nSurprisingly not significant! Why?\n\nSmall within-variation (teams don’t vary hugely in wins year-to-year)\nRevenue smoothing (multi-year contracts, season tickets)\nOnly 29 teams (small number of clusters → large SEs)\nShort panel (10 years → limited within-variation per team)\n\nPractical Implications:\n\nPooled OLS: “High-revenue teams win more” (true, but confounded)\nFixed Effects: “Winning more games increases revenue” ? (effect exists but imprecisely estimated)\nNeed longer panel or more teams for precise FE estimates",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#random-effects-estimation",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#random-effects-estimation",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "17.4: Random Effects Estimation",
    "text": "17.4: Random Effects Estimation\nRandom effects (RE) models individual-specific effects as random draws from a distribution.\nModel:\n\\[y_{it} = \\beta_1 + \\beta_2 x_{2it} + \\cdots + \\beta_k x_{kit} + (\\alpha_i + \\varepsilon_{it})\\]\nwhere:\n\n\\(\\alpha_i \\sim (0, \\sigma_\\alpha^2)\\) is the individual-specific random effect\n\\(\\varepsilon_{it} \\sim (0, \\sigma_\\varepsilon^2)\\) is the idiosyncratic error\n\nKey assumption: \\(\\alpha_i\\) uncorrelated with all regressors\nEstimation: Feasible GLS (FGLS)\nComparison with FE:\n\nRE: More efficient if assumption holds; uses both within and between variation\nFE: Consistent even if \\(\\alpha_i\\) correlated with regressors; uses only within variation\n\nHausman test: Test whether RE assumption is valid\n\n\\(H_0\\): \\(\\alpha_i\\) uncorrelated with regressors (RE consistent and efficient)\n\\(H_a\\): \\(\\alpha_i\\) correlated with regressors (FE consistent, RE inconsistent)\n\n\nprint(\"=\" * 70)\nprint(\"17.4 RANDOM EFFECTS ESTIMATION\")\nprint(\"=\" * 70)\n\nif LINEARMODELS_AVAILABLE:\n    # Random Effects with robust SEs\n    model_re_obj = RandomEffects(y_panel, X_panel_const)\n    model_re = model_re_obj.fit(cov_type='robust')\n\n    print(\"\\nRandom Effects (robust SEs):\")\n    print(model_re)\n\n    print(\"\\n\" + \"-\" * 70)\n    print(\"Key Results:\")\n    print(\"-\" * 70)\n    print(f\"Wins coefficient: {model_re.params['wins']:.6f}\")\n    print(f\"Wins SE (robust): {model_re.std_errors['wins']:.6f}\")\n    print(f\"R² (overall): {model_re.rsquared_overall:.4f}\")\n    print(f\"R² (between): {model_re.rsquared_between:.4f}\")\n    print(f\"R² (within): {model_re.rsquared_within:.4f}\")\n\n    # Model comparison\n    print(\"\\n\" + \"=\" * 70)\n    print(\"Model Comparison: Pooled, RE, and FE\")\n    print(\"=\" * 70)\n\n    comparison_table = pd.DataFrame({\n        'Pooled OLS': [results_pool.params['wins'], results_pool.std_errors['wins'],\n                       results_pool.rsquared, results_pool.nobs],\n        'Random Effects': [model_re.params['wins'], model_re.std_errors['wins'],\n                          model_re.rsquared_overall, model_re.nobs],\n        'Fixed Effects': [model_fe.params['wins'], model_fe.std_errors['wins'],\n                         model_fe.rsquared_within, model_fe.nobs]\n    }, index=['Wins Coefficient', 'Wins Std Error', 'R²', 'N'])\n\n    print(\"\\n\", comparison_table)\n\n    print(\"\\n\" + \"-\" * 70)\n    print(\"Interpretation\")\n    print(\"-\" * 70)\n    print(\"- Pooled: Largest coefficient (confounded by team characteristics)\")\n    print(\"- FE: Controls for time-invariant team effects (within-team variation)\")\n    print(\"- RE: Between pooled and FE (uses both within and between variation)\")\n    print(\"- FE preferred if team effects correlated with wins\")\n\nelse:\n    print(\"\\nRandom effects estimation requires linearmodels package.\")\n    print(\"Install with: pip install linearmodels\")\n\n======================================================================\n17.4 RANDOM EFFECTS ESTIMATION\n======================================================================\n\nRandom Effects (robust SEs):\n                        RandomEffects Estimation Summary                        \n================================================================================\nDep. Variable:              lnrevenue   R-squared:                        0.2100\nEstimator:              RandomEffects   R-squared (Between):              0.0983\nNo. Observations:                 286   R-squared (Within):               0.1850\nDate:                Wed, Jan 21 2026   R-squared (Overall):              0.1137\nTime:                        14:01:08   Log-likelihood                    243.95\nCov. Estimator:                Robust                                           \n                                        F-statistic:                      75.496\nEntities:                          29   P-value                           0.0000\nAvg Obs:                       9.8621   Distribution:                   F(1,284)\nMin Obs:                       6.0000                                           \nMax Obs:                      10.0000   F-statistic (robust):             54.209\n                                        P-value                           0.0000\nTime periods:                      10   Distribution:                   F(1,284)\nAvg Obs:                       28.600                                           \nMin Obs:                       28.000                                           \nMax Obs:                       29.000                                           \n                                                                                \n                             Parameter Estimates                              \n==============================================================================\n            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n------------------------------------------------------------------------------\nconst          4.3417     0.0492     88.293     0.0000      4.2449      4.4385\nwins           0.0046     0.0006     7.3627     0.0000      0.0034      0.0058\n==============================================================================\n\n----------------------------------------------------------------------\nKey Results:\n----------------------------------------------------------------------\nWins coefficient: 0.004597\nWins SE (robust): 0.000624\nR² (overall): 0.1137\nR² (between): 0.0983\nR² (within): 0.1850\n\n======================================================================\nModel Comparison: Pooled, RE, and FE\n======================================================================\n\n                   Pooled OLS  Random Effects  Fixed Effects\nWins Coefficient    0.006753        0.004597       0.004505\nWins Std Error      0.001879        0.000624       0.000827\nR²                  0.126663        0.113682       0.185083\nN                 286.000000      286.000000     286.000000\n\n----------------------------------------------------------------------\nInterpretation\n----------------------------------------------------------------------\n- Pooled: Largest coefficient (confounded by team characteristics)\n- FE: Controls for time-invariant team effects (within-team variation)\n- RE: Between pooled and FE (uses both within and between variation)\n- FE preferred if team effects correlated with wins\n\n\n\nKey Concept 17.4: Fixed Effects vs. Random Effects\nFixed effects (FE) and random effects (RE) differ in a key assumption: RE requires that individual effects \\(\\alpha_i\\) are uncorrelated with regressors, while FE allows arbitrary correlation. FE is consistent in either case but uses only within variation; RE is more efficient but inconsistent if the assumption fails. The Hausman test compares FE and RE estimates – a significant difference indicates RE is inconsistent and FE should be preferred. In practice, FE is the safer choice for most observational studies.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#nonlinear-models-logit-example",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#nonlinear-models-logit-example",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Nonlinear Models: Logit Example",
    "text": "Nonlinear Models: Logit Example\nBefore moving to time series, let’s briefly cover nonlinear models using a logit example.\nBinary outcome model:\n\\[Pr(y=1|X) = \\frac{\\exp(X\\beta)}{1 + \\exp(X\\beta)}\\]\nMarginal effects: Change in probability from one-unit change in \\(x_j\\)\n\\[ME_j = \\frac{\\partial Pr(y=1)}{\\partial x_j} = \\hat{p}(1-\\hat{p})\\beta_j\\]\nWe’ll use earnings data to model the probability of high earnings.\n\nprint(\"=\" * 70)\nprint(\"NONLINEAR MODELS: LOGIT EXAMPLE\")\nprint(\"=\" * 70)\n\n# Load earnings data\ndata_earnings = pd.read_stata(GITHUB_DATA_URL + 'AED_EARNINGS_COMPLETE.DTA')\n\n# Create binary indicator for high earnings\ndata_earnings['dbigearn'] = (data_earnings['earnings'] &gt; 60000).astype(int)\n\nprint(f\"\\nBinary dependent variable: High earnings (&gt; $60,000)\")\nprint(f\"Proportion with high earnings: {data_earnings['dbigearn'].mean():.4f}\")\n\n# Logit model\nmodel_logit = logit('dbigearn ~ age + education', data=data_earnings).fit(cov_type='HC1', disp=0)\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Logit Model Results\")\nprint(\"-\" * 70)\nprint(model_logit.summary())\n\n# Marginal effects\nmarginal_effects = model_logit.get_margeff()\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Marginal Effects (at means)\")\nprint(\"-\" * 70)\nprint(marginal_effects.summary())\n\n# Linear Probability Model for comparison\nmodel_lpm = ols('dbigearn ~ age + education', data=data_earnings).fit(cov_type='HC1')\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Linear Probability Model (for comparison)\")\nprint(\"-\" * 70)\nprint(f\"Age coefficient: {model_lpm.params['age']:.6f} (SE: {model_lpm.bse['age']:.6f})\")\nprint(f\"Education coefficient: {model_lpm.params['education']:.6f} (SE: {model_lpm.bse['education']:.6f})\")\n\nprint(\"\\nNote: Logit marginal effects and LPM coefficients are similar in magnitude.\")\n\n======================================================================\nNONLINEAR MODELS: LOGIT EXAMPLE\n======================================================================\n\nBinary dependent variable: High earnings (&gt; $60,000)\nProportion with high earnings: 0.2729\n\n----------------------------------------------------------------------\nLogit Model Results\n----------------------------------------------------------------------\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               dbigearn   No. Observations:                  872\nModel:                          Logit   Df Residuals:                      869\nMethod:                           MLE   Df Model:                            2\nDate:                Wed, 21 Jan 2026   Pseudo R-squ.:                  0.1447\nTime:                        14:01:09   Log-Likelihood:                -437.15\nconverged:                       True   LL-Null:                       -511.13\nCovariance Type:                  HC1   LLR p-value:                 7.406e-33\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -8.0651      0.691    -11.666      0.000      -9.420      -6.710\nage            0.0385      0.008      4.845      0.000       0.023       0.054\neducation      0.3742      0.037     10.224      0.000       0.302       0.446\n==============================================================================\n\n----------------------------------------------------------------------\nMarginal Effects (at means)\n----------------------------------------------------------------------\n        Logit Marginal Effects       \n=====================================\nDep. Variable:               dbigearn\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nage            0.0064      0.001      5.023      0.000       0.004       0.009\neducation      0.0618      0.005     13.025      0.000       0.052       0.071\n==============================================================================\n\n----------------------------------------------------------------------\nLinear Probability Model (for comparison)\n----------------------------------------------------------------------\nAge coefficient: 0.006420 (SE: 0.001277)\nEducation coefficient: 0.054036 (SE: 0.005020)\n\nNote: Logit marginal effects and LPM coefficients are similar in magnitude.\n\n\nHaving explored panel data methods for cross-sectional units observed over time, we now turn to pure time series analysis where the focus shifts to temporal dynamics, autocorrelation, and stationarity.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#time-series-data",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#time-series-data",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "17.5: Time Series Data",
    "text": "17.5: Time Series Data\nTime series data consist of observations ordered over time: \\(y_1, y_2, \\ldots, y_T\\)\nKey concepts:\n\nAutocorrelation: Correlation between \\(y_t\\) and \\(y_{t-k}\\) (lag \\(k\\))\n\nSample autocorrelation at lag \\(k\\): \\(r_k = \\frac{\\sum_{t=k+1}^T (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^T (y_t - \\bar{y})^2}\\)\n\nStationarity: Statistical properties (mean, variance) constant over time\n\nMany economic time series are non-stationary (trending)\n\nSpurious regression: High \\(R^2\\) without true relationship (both series trending)\n\nSolution: First differencing or detrending\n\nHAC standard errors (Newey-West): Heteroskedasticity and Autocorrelation Consistent\n\nValid inference in presence of autocorrelation\n\n\nU.S. Treasury Interest Rates Example:\nMonthly data from January 1982 to January 2015 on 1-year and 10-year rates.\n\nprint(\"=\" * 70)\nprint(\"17.5 TIME SERIES DATA\")\nprint(\"=\" * 70)\n\n# Load interest rates data\ndata_rates = pd.read_stata(GITHUB_DATA_URL + 'AED_INTERESTRATES.DTA')\n\nprint(\"\\nInterest Rates Data Summary:\")\nprint(data_rates[['gs10', 'gs1', 'dgs10', 'dgs1']].describe())\n\nprint(\"\\nVariable definitions:\")\nprint(\"  gs10: 10-year Treasury rate (level)\")\nprint(\"  gs1: 1-year Treasury rate (level)\")\nprint(\"  dgs10: Change in 10-year rate (first difference)\")\nprint(\"  dgs1: Change in 1-year rate (first difference)\")\n\nprint(\"\\nFirst observations:\")\nprint(data_rates[['gs10', 'gs1', 'dgs10', 'dgs1']].head(10))\n\n======================================================================\n17.5 TIME SERIES DATA\n======================================================================\n\nInterest Rates Data Summary:\n             gs10         gs1       dgs10        dgs1\ncount  397.000000  397.000000  396.000000  396.000000\nmean     6.186020    4.691209   -0.032096   -0.035657\nstd      2.878117    3.283398    0.274015    0.287611\nmin      1.530000    0.100000   -1.430000   -1.810000\n25%      4.100000    1.780000   -0.180000   -0.142500\n50%      5.800000    4.960000   -0.040000   -0.010000\n75%      7.960000    6.640000    0.150000    0.100000\nmax     14.590000   14.730000    0.780000    0.760000\n\nVariable definitions:\n  gs10: 10-year Treasury rate (level)\n  gs1: 1-year Treasury rate (level)\n  dgs10: Change in 10-year rate (first difference)\n  dgs1: Change in 1-year rate (first difference)\n\nFirst observations:\n    gs10    gs1  dgs10  dgs1\n0  14.59  14.32    NaN   NaN\n1  14.43  14.73  -0.16  0.41\n2  13.86  13.95  -0.57 -0.78\n3  13.87  13.98   0.01  0.03\n4  13.62  13.34  -0.25 -0.64\n5  14.30  14.07   0.68  0.73\n6  13.95  13.24  -0.35 -0.83\n7  13.06  11.43  -0.89 -1.81\n8  12.34  10.85  -0.72 -0.58\n9  10.91   9.32  -1.43 -1.53\n\n\n\nKey Concept 17.5: Time Series Stationarity and Spurious Regression\nA time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time. Many economic series are non-stationary (trending), which can produce spurious regressions: high \\(R^2\\) and significant coefficients even when variables are unrelated. Solutions include first differencing (removing trends), detrending, and cointegration analysis. Always check whether your time series are stationary before interpreting regression results.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#time-series-visualization",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#time-series-visualization",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Time Series Visualization",
    "text": "Time Series Visualization\nPlotting time series helps identify trends, seasonality, and structural breaks.\n\n# Figure: Time series plots\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Panel 1: Levels\naxes[0].plot(data_rates.index, data_rates['gs10'], label='10-year rate', linewidth=1.5)\naxes[0].plot(data_rates.index, data_rates['gs1'], label='1-year rate', linewidth=1.5)\naxes[0].set_xlabel('Observation', fontsize=11)\naxes[0].set_ylabel('Interest Rate (%)', fontsize=11)\naxes[0].set_title('Interest Rates (Levels)', fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel 2: Scatter plot\naxes[1].scatter(data_rates['gs1'], data_rates['gs10'], alpha=0.5, s=20)\nz = np.polyfit(data_rates['gs1'].dropna(), data_rates['gs10'].dropna(), 1)\np = np.poly1d(z)\ngs1_range = np.linspace(data_rates['gs1'].min(), data_rates['gs1'].max(), 100)\naxes[1].plot(gs1_range, p(gs1_range), 'r-', linewidth=2, label='OLS fit')\naxes[1].set_xlabel('1-year rate (%)', fontsize=11)\naxes[1].set_ylabel('10-year rate (%)', fontsize=11)\naxes[1].set_title('10-year vs 1-year Rate', fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Both series show strong downward trend over time (non-stationary).\")\nprint(\"Strong positive correlation between 1-year and 10-year rates.\")\n\n\n\n\n\n\n\n\nBoth series show strong downward trend over time (non-stationary).\nStrong positive correlation between 1-year and 10-year rates.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#regression-in-levels-vs.-changes",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#regression-in-levels-vs.-changes",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Regression in Levels vs. Changes",
    "text": "Regression in Levels vs. Changes\nWith trending data, we should be careful about spurious regression.\n\nprint(\"=\" * 70)\nprint(\"Regression in Levels with Time Trend\")\nprint(\"=\" * 70)\n\n# Create time variable\ndata_rates['time'] = np.arange(len(data_rates))\n\n# Regression in levels\nmodel_levels = ols('gs10 ~ gs1 + time', data=data_rates).fit()\nprint(\"\\nLevels regression (default SEs):\")\nprint(f\"  gs1 coef: {model_levels.params['gs1']:.6f}\")\nprint(f\"  R²: {model_levels.rsquared:.6f}\")\n\n# HAC standard errors (Newey-West)\nmodel_levels_hac = ols('gs10 ~ gs1 + time', data=data_rates).fit(cov_type='HAC', cov_kwds={'maxlags': 24})\nprint(\"\\nLevels regression (HAC SEs with 24 lags):\")\nprint(f\"  gs1 coef: {model_levels_hac.params['gs1']:.6f}\")\nprint(f\"  gs1 SE (default): {model_levels.bse['gs1']:.6f}\")\nprint(f\"  gs1 SE (HAC): {model_levels_hac.bse['gs1']:.6f}\")\nprint(f\"\\n  HAC SE is {model_levels_hac.bse['gs1'] / model_levels.bse['gs1']:.2f}x larger!\")\n\n======================================================================\nRegression in Levels with Time Trend\n======================================================================\n\nLevels regression (default SEs):\n  gs1 coef: 0.507550\n  R²: 0.946883\n\nLevels regression (HAC SEs with 24 lags):\n  gs1 coef: 0.507550\n  gs1 SE (default): 0.022147\n  gs1 SE (HAC): 0.080452\n\n  HAC SE is 3.63x larger!\n\n\nNow that we have visualized the time series patterns and estimated regressions in levels, let’s formally examine autocorrelation in the residuals and its consequences for inference.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#autocorrelation",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#autocorrelation",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "17.6: Autocorrelation",
    "text": "17.6: Autocorrelation\nAutocorrelation (serial correlation) violates the independence assumption of OLS.\nConsequences:\n\nOLS remains unbiased and consistent\nStandard errors are incorrect (typically too small)\nHypothesis tests invalid\n\nDetection:\n\nCorrelogram: Plot of autocorrelations at different lags\nBreusch-Godfrey test: LM test for serial correlation\nDurbin-Watson statistic: Tests for AR(1) errors\n\nSolutions:\n\nHAC standard errors (Newey-West)\nModel the autocorrelation (AR, ARMA models)\nFirst differencing (if series are non-stationary)\n\n\nprint(\"=\" * 70)\nprint(\"17.6 AUTOCORRELATION\")\nprint(\"=\" * 70)\n\n# Check residual autocorrelation from levels regression\ndata_rates['uhatgs10'] = model_levels.resid\n\n# Correlogram\nprint(\"\\nAutocorrelations of residuals (levels regression):\")\nacf_resid = acf(data_rates['uhatgs10'].dropna(), nlags=10)\nfor i in range(min(11, len(acf_resid))):\n    print(f\"  Lag {i}: {acf_resid[i]:.6f}\")\n\nprint(\"\\nStrong autocorrelation evident (lag 1 = {:.4f})\".format(acf_resid[1]))\n\n======================================================================\n17.6 AUTOCORRELATION\n======================================================================\n\nAutocorrelations of residuals (levels regression):\n  Lag 0: 1.000000\n  Lag 1: 0.953418\n  Lag 2: 0.888093\n  Lag 3: 0.829507\n  Lag 4: 0.769449\n  Lag 5: 0.708815\n  Lag 6: 0.651059\n  Lag 7: 0.596161\n  Lag 8: 0.537987\n  Lag 9: 0.477552\n  Lag 10: 0.424660\n\nStrong autocorrelation evident (lag 1 = 0.9534)\n\n\n\nKey Concept 17.6: Detecting and Correcting Autocorrelation\nThe correlogram (ACF plot) reveals autocorrelation patterns in residuals. Slowly decaying autocorrelations (e.g., \\(\\rho_1 = 0.95\\), \\(\\rho_{10} = 0.42\\)) indicate non-stationarity and persistent shocks. With autocorrelation, default SEs are too small – HAC (Newey-West) SEs can be 3-8 times larger. Always check residual autocorrelation after estimating time series regressions and use HAC SEs or model the dynamics explicitly.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#correlogram-visualization",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#correlogram-visualization",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Correlogram Visualization",
    "text": "Correlogram Visualization\n\n# Plot correlogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_acf(data_rates['uhatgs10'].dropna(), lags=24, ax=ax, alpha=0.05)\nax.set_title('Correlogram: Residuals from Levels Regression', fontsize=14, fontweight='bold')\nax.set_xlabel('Lag', fontsize=12)\nax.set_ylabel('Autocorrelation', fontsize=12)\nplt.tight_layout()\nplt.show()\n\nprint(\"Autocorrelations decay very slowly (characteristic of non-stationary series).\")\n\n\n\n\n\n\n\n\nAutocorrelations decay very slowly (characteristic of non-stationary series).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#first-differencing",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#first-differencing",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "First Differencing",
    "text": "First Differencing\nFirst differencing can remove trends and reduce autocorrelation.\n\nprint(\"=\" * 70)\nprint(\"Regression in Changes (First Differences)\")\nprint(\"=\" * 70)\n\n# Regression in changes\nmodel_changes = ols('dgs10 ~ dgs1', data=data_rates).fit()\nprint(\"\\nChanges regression:\")\nprint(f\"  dgs1 coef: {model_changes.params['dgs1']:.6f}\")\nprint(f\"  dgs1 SE: {model_changes.bse['dgs1']:.6f}\")\nprint(f\"  R²: {model_changes.rsquared:.6f}\")\n\n# Check residual autocorrelation\nuhat_dgs10 = model_changes.resid\nacf_dgs10_resid = acf(uhat_dgs10.dropna(), nlags=10)\n\nprint(\"\\nAutocorrelations of residuals (changes regression):\")\nfor i in range(min(11, len(acf_dgs10_resid))):\n    print(f\"  Lag {i}: {acf_dgs10_resid[i]:.6f}\")\n\nprint(\"\\nMuch lower autocorrelation after differencing!\")\n\n======================================================================\nRegression in Changes (First Differences)\n======================================================================\n\nChanges regression:\n  dgs1 coef: 0.719836\n  dgs1 SE: 0.031443\n  R²: 0.570860\n\nAutocorrelations of residuals (changes regression):\n  Lag 0: 1.000000\n  Lag 1: 0.254801\n  Lag 2: -0.038743\n  Lag 3: 0.060813\n  Lag 4: 0.023676\n  Lag 5: -0.027540\n  Lag 6: -0.011310\n  Lag 7: 0.042843\n  Lag 8: 0.081094\n  Lag 9: -0.001712\n  Lag 10: -0.019733\n\nMuch lower autocorrelation after differencing!\n\n\n\nKey Concept 17.7: First Differencing for Nonstationary Data\nFirst differencing (\\(\\Delta y_t = y_t - y_{t-1}\\)) transforms non-stationary trending series into stationary ones, eliminating spurious regression problems. After differencing, the residual autocorrelation drops dramatically (from \\(\\rho_1 \\approx 0.95\\) to \\(\\rho_1 \\approx 0.25\\) in the interest rate example). The coefficient interpretation changes from levels to changes: a 1-percentage-point change in the 1-year rate is associated with a 0.72-percentage-point change in the 10-year rate.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#autoregressive-models",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#autoregressive-models",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Autoregressive Models",
    "text": "Autoregressive Models\nAR(p) model: Include lagged dependent variables\n\\[y_t = \\beta_1 + \\beta_2 y_{t-1} + \\cdots + \\beta_{p+1} y_{t-p} + u_t\\]\nADL(p, q) model: Autoregressive Distributed Lag\n\\[y_t = \\beta_1 + \\sum_{j=1}^p \\alpha_j y_{t-j} + \\sum_{j=0}^q \\gamma_j x_{t-j} + u_t\\]\n\n\nAutoregressive Models: Interest Rates Have Memory\nThe ADL model results reveal how interest rates evolve over time with strong persistence:\nTypical ADL(2,2) Results:\nFrom: \\(\\Delta gs10_t = \\beta_0 + \\beta_1 \\Delta gs10_{t-1} + \\beta_2 \\Delta gs10_{t-2} + \\gamma_0 \\Delta gs1_t + \\gamma_1 \\Delta gs1_{t-1} + \\gamma_2 \\Delta gs1_{t-2} + u_t\\)\nAutoregressive terms (own lags): - Lag 1 (\\(\\Delta gs10_{t-1}\\)): ≈ -0.15 to -0.25 (negative!) - Lag 2 (\\(\\Delta gs10_{t-2}\\)): ≈ -0.05 to -0.10 (negative)\nDistributed lag terms (1-year rate): - Contemporary (\\(\\Delta gs1_t\\)): ≈ +0.45 to +0.55 (strong positive!) - Lag 1 (\\(\\Delta gs1_{t-1}\\)): ≈ +0.15 to +0.25 - Lag 2 (\\(\\Delta gs1_{t-2}\\)): ≈ +0.05 to +0.10\nInterpretation:\n\nNegative autocorrelation in changes:\n\n\nCoefficient on \\(\\Delta gs10_{t-1}\\) is negative\nIf 10-year rate increased last month, it tends to partially reverse this month\nThis is mean reversion in changes\nNot mean reversion in levels (levels are highly persistent)\n\n\nStrong contemporary relationship:\n\n\nCoefficient ≈ 0.50 on \\(\\Delta gs1_t\\)\nWhen 1-year rate increases 1%, 10-year rate increases 0.50% same month\nExpectations hypothesis: Long rates reflect expected future short rates\nLess than 1-to-1 because 10-year rate is average over many periods\n\n\nDistributed lag structure:\n\n\nEffects of 1-year rate changes persist over multiple months\nTotal effect: 0.50 + 0.20 + 0.08 ≈ 0.78\nAlmost 80% of 1-year rate change eventually passes through to 10-year rate\n\n\nR² increases substantially:\n\n\nSimple model (no lags): R² ≈ 0.20\nADL(2,2): R² ≈ 0.40-0.50\nDynamics matter! Past values have strong predictive power\n\nWhy ADL Models Are Important:\nForecasting: - Can predict next month’s 10-year rate using: - Past 10-year rates - Current and past 1-year rates - Better forecasts than static models\nPolicy analysis: - Fed controls short rates (1-year) - ADL shows transmission to long rates (10-year) - Speed of adjustment: How quickly long rates respond to policy changes\nEconomic theory testing: - Expectations hypothesis: Long rate = weighted average of expected future short rates - Term structure of interest rates - Market efficiency\nThe Residual ACF:\nAfter fitting ADL(2,2): - Lag 1 autocorrelation: ρ₁ ≈ 0.05-0.10 (much lower!) - Compare to levels regression: ρ₁ ≈ 0.95 - Model captures most autocorrelation\nThis suggests: - ADL(2,2) is adequate specification - No need for higher-order lags - Remaining autocorrelation is small\nComparing Models:\n\n\n\n\n\n\n\n\n\nModel\nR²\nResidual ρ₁\nBIC\n\n\n\n\nStatic (\\(\\Delta gs10 \\sim \\Delta gs1\\))\n0.25\n0.25\nHigher\n\n\nAR(2) (\\(\\Delta gs10 \\sim \\Delta gs10_{t-1,t-2}\\))\n0.10\n0.15\nHigher\n\n\nADL(2,2)\n0.45\n0.08\nLower\n\n\n\nADL(2,2) dominates on all criteria!\nInterpretation of Dynamics:\nShort-run effect (impact multiplier): - Immediate response to \\(\\Delta gs1_t\\): γ₀ ≈ 0.50 - Half of shock passes through contemporaneously\nMedium-run effect (interim multipliers): - After 1 month: γ₀ + γ₁ ≈ 0.70 - After 2 months: γ₀ + γ₁ + γ₂ ≈ 0.78\nLong-run effect (total multiplier): - In levels regression: coefficient ≈ 0.90-0.95 - This is the long-run equilibrium relationship - ADL estimates dynamics of adjustment to this equilibrium\nWhy Negative Own-Lag Coefficients?\nAt first, this seems counterintuitive: - Interest rates are persistent in levels - But changes show mean reversion\nExplanation: - Levels are I(1): Random walk with drift - Changes are I(0): Stationary, but with negative serial correlation - Overshooting: Markets overreact to news, then partially correct\nExample:\nMonth 1: Fed unexpectedly raises 1-year rate by 1% - 10-year rate increases by 0.60% (overshoots equilibrium)\nMonth 2: Market reassesses - 10-year rate decreases by 0.10% (partial reversal)\nMonth 3: Further adjustment - 10-year rate changes by -0.02% (approaching equilibrium)\nLong run: 10-year rate settles at +0.85% (new equilibrium)\nPractical Value:\n\nCentral banks:\n\n\nUnderstand how policy rate changes affect long rates\nTiming and magnitude of transmission\n\n\nBond traders:\n\n\nPredict interest rate movements\nArbitrage opportunities if model predicts well\n\n\nEconomists:\n\n\nTest theories (expectations hypothesis, term premium)\nUnderstand financial market dynamics\n\nModel Selection:\nChose ADL(2,2) based on: - Information criteria (AIC, BIC) - Residual diagnostics (low autocorrelation) - Economic theory (2 lags reasonable for monthly data) - Parsimony (not too many parameters)\nCould try ADL(3,3), but gains typically minimal",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#visualization-changes-in-interest-rates",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#visualization-changes-in-interest-rates",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Visualization: Changes in Interest Rates",
    "text": "Visualization: Changes in Interest Rates\n\n# Figure: Changes\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Panel 1: Time series of changes\naxes[0].plot(data_rates.index, data_rates['dgs10'], label='Δ 10-year rate', linewidth=1)\naxes[0].plot(data_rates.index, data_rates['dgs1'], label='Δ 1-year rate', linewidth=1)\naxes[0].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\naxes[0].set_xlabel('Observation', fontsize=11)\naxes[0].set_ylabel('Change in Rate (pct points)', fontsize=11)\naxes[0].set_title('Changes in Interest Rates', fontsize=12, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Panel 2: Scatter plot of changes\naxes[1].scatter(data_rates['dgs1'], data_rates['dgs10'], alpha=0.5, s=20)\nvalid_idx = data_rates[['dgs1', 'dgs10']].dropna().index\nz = np.polyfit(data_rates.loc[valid_idx, 'dgs1'], data_rates.loc[valid_idx, 'dgs10'], 1)\np = np.poly1d(z)\ndgs1_range = np.linspace(data_rates['dgs1'].min(), data_rates['dgs1'].max(), 100)\naxes[1].plot(dgs1_range, p(dgs1_range), 'r-', linewidth=2, label='OLS fit')\naxes[1].axhline(y=0, color='k', linestyle='--', linewidth=0.5)\naxes[1].axvline(x=0, color='k', linestyle='--', linewidth=0.5)\naxes[1].set_xlabel('Δ 1-year rate', fontsize=11)\naxes[1].set_ylabel('Δ 10-year rate', fontsize=11)\naxes[1].set_title('Change in 10-year vs 1-year Rate', fontsize=12, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Changes fluctuate around zero (stationary-looking).\")\nprint(\"Positive correlation between changes (rates move together).\")\n\n\n\n\n\n\n\n\nChanges fluctuate around zero (stationary-looking).\nPositive correlation between changes (rates move together).\n\n\nHaving developed tools for handling panel data and time series, we now address the fundamental question of causality – how to move from correlation to causal inference using econometric methods.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#causality-and-instrumental-variables",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#causality-and-instrumental-variables",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "17.7: Causality and Instrumental Variables",
    "text": "17.7: Causality and Instrumental Variables\nEstablishing causality is central to econometrics. Correlation does not imply causation!\nThe fundamental problem:\nIn regression \\(y = \\beta_1 + \\beta_2 x + u\\), OLS is biased if \\(E[u|x] \\neq 0\\)\nSources of endogeneity:\n\nOmitted variables\nMeasurement error\nSimultaneity (reverse causation)\n\nInstrumental Variables (IV) solution:\nFind an instrument \\(z\\) that:\n\nRelevance: Correlated with \\(x\\) (can be tested)\nExogeneity: Uncorrelated with \\(u\\) (cannot be tested - must argue)\n\nIV estimator:\n\\[\\hat{\\beta}_{IV} = \\frac{Cov(z,y)}{Cov(z,x)}\\]\nCausal inference methods:\n\nRandomized experiments (RCT)\nInstrumental variables (IV)\nDifference-in-differences (DID)\nRegression discontinuity (RD)\nFixed effects (control for unobserved heterogeneity)\nMatching and propensity scores\n\nKey insight: Need credible identification strategy, not just controls!\n\nprint(\"=\" * 70)\nprint(\"17.7 CAUSALITY AND INSTRUMENTAL VARIABLES\")\nprint(\"=\" * 70)\n\nprint(\"\\nKey Points on Causality:\")\nprint(\"-\" * 70)\nprint(\"\\n1. Correlation ≠ Causation\")\nprint(\"   - Regression shows association, not necessarily causation\")\nprint(\"   - Need to rule out confounding, reverse causation, selection\")\n\nprint(\"\\n2. Randomized Controlled Trials (RCT)\")\nprint(\"   - Gold standard: Randomly assign treatment\")\nprint(\"   - Ensures treatment uncorrelated with potential outcomes\")\nprint(\"   - Causal effect = difference in means\")\n\nprint(\"\\n3. Observational Data Methods\")\nprint(\"   - Instrumental Variables: Use variation from instrument\")\nprint(\"   - Fixed Effects: Control for time-invariant unobservables\")\nprint(\"   - Difference-in-Differences: Compare treatment vs control over time\")\nprint(\"   - Regression Discontinuity: Exploit threshold for treatment\")\n\nprint(\"\\n4. Potential Outcomes Framework\")\nprint(\"   - Y₁ᵢ: Outcome if treated\")\nprint(\"   - Y₀ᵢ: Outcome if not treated\")\nprint(\"   - Individual treatment effect: Y₁ᵢ - Y₀ᵢ\")\nprint(\"   - Problem: Only observe one potential outcome!\")\nprint(\"   - ATE = E[Y₁ᵢ - Y₀ᵢ]: Average Treatment Effect\")\n\nprint(\"\\n5. Instrumental Variables\")\nprint(\"   - Requires valid instrument z:\")\nprint(\"     (a) Relevant: Corr(z,x) ≠ 0\")\nprint(\"     (b) Exogenous: Corr(z,u) = 0\")\nprint(\"   - Example: Distance to college as IV for education\")\nprint(\"   - Weak instruments: Large standard errors\")\n\nprint(\"\\n6. Panel Data and Causality\")\nprint(\"   - Fixed Effects: Controls for αᵢ (unobserved heterogeneity)\")\nprint(\"   - Causal if: Conditional on αᵢ, X exogenous\")\nprint(\"   - NBA example: FE controls for team characteristics\")\nprint(\"   - Identifies within-team effect of wins on revenue\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Practical Recommendations\")\nprint(\"=\" * 70)\nprint(\"\\n1. Always think about potential confounders\")\nprint(\"2. Use robust/cluster standard errors\")\nprint(\"3. Test multiple specifications\")\nprint(\"4. Report both OLS and IV/FE when appropriate\")\nprint(\"5. Be transparent about identification assumptions\")\nprint(\"6. Causal claims require strong justification!\")\n\n======================================================================\n17.7 CAUSALITY AND INSTRUMENTAL VARIABLES\n======================================================================\n\nKey Points on Causality:\n----------------------------------------------------------------------\n\n1. Correlation ≠ Causation\n   - Regression shows association, not necessarily causation\n   - Need to rule out confounding, reverse causation, selection\n\n2. Randomized Controlled Trials (RCT)\n   - Gold standard: Randomly assign treatment\n   - Ensures treatment uncorrelated with potential outcomes\n   - Causal effect = difference in means\n\n3. Observational Data Methods\n   - Instrumental Variables: Use variation from instrument\n   - Fixed Effects: Control for time-invariant unobservables\n   - Difference-in-Differences: Compare treatment vs control over time\n   - Regression Discontinuity: Exploit threshold for treatment\n\n4. Potential Outcomes Framework\n   - Y₁ᵢ: Outcome if treated\n   - Y₀ᵢ: Outcome if not treated\n   - Individual treatment effect: Y₁ᵢ - Y₀ᵢ\n   - Problem: Only observe one potential outcome!\n   - ATE = E[Y₁ᵢ - Y₀ᵢ]: Average Treatment Effect\n\n5. Instrumental Variables\n   - Requires valid instrument z:\n     (a) Relevant: Corr(z,x) ≠ 0\n     (b) Exogenous: Corr(z,u) = 0\n   - Example: Distance to college as IV for education\n   - Weak instruments: Large standard errors\n\n6. Panel Data and Causality\n   - Fixed Effects: Controls for αᵢ (unobserved heterogeneity)\n   - Causal if: Conditional on αᵢ, X exogenous\n   - NBA example: FE controls for team characteristics\n   - Identifies within-team effect of wins on revenue\n\n======================================================================\nPractical Recommendations\n======================================================================\n\n1. Always think about potential confounders\n2. Use robust/cluster standard errors\n3. Test multiple specifications\n4. Report both OLS and IV/FE when appropriate\n5. Be transparent about identification assumptions\n6. Causal claims require strong justification!\n\n\n\nKey Concept 17.8: Instrumental Variables and Causal Inference\nEndogeneity (regressors correlated with errors) biases OLS estimates. Sources include omitted variables, measurement error, and simultaneity. Instrumental variables (IV) provide a solution: find a variable \\(z\\) that is correlated with the endogenous regressor (relevance) but uncorrelated with the error (exogeneity). The IV estimator \\(\\hat{\\beta}_{IV} = \\text{Cov}(z,y)/\\text{Cov}(z,x)\\) is consistent even when OLS is biased. Complementary causal methods include RCTs, DiD, RD, and matching.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#key-takeaways",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#key-takeaways",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nPanel Data Methods: - Panel data combines cross-sectional and time series dimensions, observing multiple individuals over multiple periods - Variance decomposition separates total variation into within (over time) and between (across individuals) components - Pooled OLS ignores panel structure; always use cluster-robust standard errors clustered by individual - Fixed effects controls for time-invariant unobserved heterogeneity by using only within-individual variation - Random effects is more efficient than FE but assumes individual effects are uncorrelated with regressors - FE is preferred when individual effects are likely correlated with regressors (use Hausman test to decide)\nNonlinear Models: - Logit models estimate the probability of binary outcomes using the logistic function - Marginal effects (\\(\\hat{p}(1-\\hat{p})\\beta_j\\)) give the change in probability from a one-unit change in \\(x_j\\) - Logit marginal effects and linear probability model coefficients are typically similar in magnitude\nTime Series Analysis: - Time series data exhibit autocorrelation, where observations are correlated with their past values - Non-stationary series (trending) can produce spurious regressions with misleadingly high \\(R^2\\) - First differencing removes trends and reduces autocorrelation, transforming non-stationary series to stationary - HAC (Newey-West) standard errors account for both heteroskedasticity and autocorrelation in time series - Default SEs can be dramatically too small with autocorrelation (3-8x understatement is common)\nDynamic Models: - Autoregressive (AR) models capture persistence by including lagged dependent variables - Autoregressive distributed lag (ADL) models include lags of both the dependent and independent variables - The correlogram (ACF plot) helps determine the appropriate number of lags - Total multiplier from an ADL model gives the long-run effect of a permanent change in \\(x\\)\nCausality and Instrumental Variables: - Correlation does not imply causation; endogeneity (omitted variables, reverse causation, measurement error) biases OLS - Instrumental variables require relevance (\\(\\text{Corr}(z,x) \\neq 0\\)) and exogeneity (\\(\\text{Corr}(z,u) = 0\\)) - Fixed effects, difference-in-differences, regression discontinuity, and matching are complementary causal methods - Credible causal inference requires a convincing identification strategy, not just adding control variables\nPython tools: linearmodels (PanelOLS, RandomEffects), statsmodels (OLS, logit, HAC, ACF), matplotlib/seaborn (visualization)\nNext steps: Apply these methods to your own research questions. Panel data methods, time series models, and causal inference strategies are essential tools for any applied econometrician working with observational data.\n\nCongratulations! You’ve completed Chapter 17, the final chapter covering panel data, time series, and causal inference. You now have a comprehensive toolkit of econometric methods for analyzing real-world data.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#practice-exercises",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#practice-exercises",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nExercise 1: Panel Data Variance Decomposition\nA panel dataset of 50 firms over 5 years shows: - Overall standard deviation of log revenue: 0.80 - Between standard deviation: 0.70 - Within standard deviation: 0.30\n\nWhich source of variation dominates? What does this imply about the importance of firm-specific characteristics?\nIf you run fixed effects, what proportion of the total variation are you using for estimation?\nWould you expect the FE coefficient to be larger or smaller than pooled OLS? Explain using the omitted variables bias formula.\n\nExercise 2: Cluster-Robust Standard Errors\nYou estimate a panel regression of test scores on class size using data from 100 schools over 3 years (300 observations). The coefficient on class size has: - Default SE: 0.15 (t = 3.33) - Cluster-robust SE (by school): 0.45 (t = 1.11)\n\nWhy is the cluster SE three times larger than the default SE?\nDoes your conclusion about the significance of class size change? At what significance level?\nWhat is the effective number of independent observations in this panel?\n\nExercise 3: Fixed Effects vs. Random Effects\nYou estimate a wage equation using panel data on 500 workers over 10 years. The Hausman test yields \\(\\chi^2 = 25.4\\) with 3 degrees of freedom (\\(p &lt; 0.001\\)).\n\nState the null and alternative hypotheses of the Hausman test.\nWhat do you conclude? Which estimator should you use?\nGive an economic reason why the RE assumption might fail in a wage equation (hint: think about unobserved ability).\n\nExercise 4: Time Series Autocorrelation\nA regression of the 10-year interest rate on the 1-year rate using monthly data yields residuals with: - Lag 1 autocorrelation: 0.95 - Lag 5 autocorrelation: 0.75 - Default SE on the 1-year rate coefficient: 0.022 - HAC SE (24 lags): 0.080\n\nIs there evidence of autocorrelation? What does the slowly decaying ACF pattern suggest about the data?\nBy what factor do the HAC SEs differ from default SEs? What are the implications for hypothesis testing?\nWould first differencing help? What would you expect the lag 1 autocorrelation of the differenced residuals to be?\n\nExercise 5: Spurious Regression\nYou regress GDP on the number of mobile phone subscriptions over 30 years and find \\(R^2 = 0.97\\) with a highly significant coefficient.\n\nWhy might this be a spurious regression? What is the key characteristic of both series?\nDescribe two methods to address this problem.\nIf you first-difference both series, what economic relationship (if any) would the regression estimate?\n\nExercise 6: Identifying Causal Effects\nFor each scenario, identify the main threat to causal inference and suggest an appropriate method:\n\nEstimating the effect of police spending on crime rates across cities (cross-sectional data).\nEstimating the effect of a minimum wage increase on employment (state-level panel data with staggered adoption).\nEstimating the effect of class size on student achievement (students assigned to classes based on a cutoff rule).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  },
  {
    "objectID": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#case-studies",
    "href": "../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html#case-studies",
    "title": "Chapter 17: Panel Data, Time Series Data, Causation",
    "section": "Case Studies",
    "text": "Case Studies\n\nCase Study 1: Panel Data Analysis of Cross-Country Productivity\nIn this case study, you will apply panel data methods from this chapter to analyze labor productivity dynamics across countries using the Mendez convergence clubs dataset.\nDataset: Mendez (2020) convergence clubs data\n\nSource: https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\nSample: 108 countries, 1990-2014 (panel structure: country \\(\\times\\) year)\nVariables: lp (labor productivity), rk (physical capital), hc (human capital), rgdppc (real GDP per capita), tfp (total factor productivity), region, country\n\nResearch question: How do physical and human capital affect labor productivity across countries, and does controlling for unobserved country characteristics change the estimates?\n\n\nTask 1: Panel Data Structure (Guided)\nLoad the dataset and explore its panel structure. Calculate the within and between variation for log labor productivity.\nimport pandas as pd\nimport numpy as np\n\nurl = \"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv\"\ndat = pd.read_csv(url)\ndat['ln_lp'] = np.log(dat['lp'])\ndat['ln_rk'] = np.log(dat['rk'])\n\n# Panel structure\nprint(f\"Countries: {dat['country'].nunique()}\")\nprint(f\"Years: {dat['year'].nunique()}\")\nprint(f\"Total observations: {len(dat)}\")\n\n# Variance decomposition\noverall_var = dat['ln_lp'].var()\nbetween_var = dat.groupby('country')['ln_lp'].mean().var()\nwithin_var = dat.groupby('country')['ln_lp'].apply(lambda x: x - x.mean()).var()\nprint(f\"Overall variance: {overall_var:.4f}\")\nprint(f\"Between variance: {between_var:.4f}\")\nprint(f\"Within variance: {within_var:.4f}\")\nWhich source of variation dominates? What does this imply for the choice between pooled OLS and fixed effects?\n\n\n\nTask 2: Pooled OLS with Cluster-Robust SEs (Guided)\nEstimate a pooled OLS regression of log productivity on log physical capital and human capital. Compare default and cluster-robust standard errors (clustered by country).\nfrom statsmodels.formula.api import ols\n\nmodel_default = ols('ln_lp ~ ln_rk + hc', data=dat).fit()\nmodel_cluster = ols('ln_lp ~ ln_rk + hc', data=dat).fit(\n    cov_type='cluster', cov_kwds={'groups': dat['country']}\n)\n\nprint(\"Default SE:\", model_default.bse.round(4).to_dict())\nprint(\"Cluster SE:\", model_cluster.bse.round(4).to_dict())\nprint(\"Ratio:\", (model_cluster.bse / model_default.bse).round(2).to_dict())\nHow much larger are cluster SEs? What does this tell you about within-country correlation?\n\n\n\nTask 3: Fixed Effects Estimation (Semi-guided)\nEstimate a fixed effects model controlling for country-specific characteristics. Compare the FE coefficients with the pooled OLS coefficients.\nHint: Use linearmodels.panel. PanelOLS with entity_effects=True, or use the within transformation manually by de-meaning the variables by country.\nWhich coefficients change most? What unobserved country characteristics might be driving the difference?\n\n\n\nTask 4: Time Trends in Productivity (Semi-guided)\nAdd a time trend or year fixed effects to the panel model. Test whether productivity growth rates differ across regions.\nHint: Use time_effects=True in PanelOLS for year fixed effects, or create region-year interaction terms.\nIs there evidence of convergence (faster growth in initially poorer countries)?\n\n\n\nTask 5: Regional Heterogeneity with Interactions (Independent)\nEstimate models that allow the returns to physical and human capital to vary by region:\n\nAdd region dummy variables\nAdd region-capital interaction terms\nTest the joint significance of regional interactions\n\nDo returns to capital differ significantly across regions? Which regions show the highest returns to human capital?\n\n\n\nTask 6: Policy Brief on Capital and Productivity (Independent)\nWrite a 200-300 word policy brief addressing: What are the most effective channels for increasing labor productivity across countries? Your brief should:\n\nCompare pooled OLS and fixed effects estimates of capital returns\nDiscuss whether the relationship is causal (what are the threats to identification?)\nEvaluate whether returns to capital differ by region\nRecommend policies based on the relative importance of physical vs. human capital\n\n\nKey Concept 17.9: Panel Data for Cross-Country Analysis\nCross-country panel data enables controlling for time-invariant country characteristics (institutions, geography, culture) that confound cross-sectional estimates. Fixed effects absorb these permanent differences, identifying the relationship between capital accumulation and productivity growth from within-country variation over time. The typical finding is that FE coefficients are smaller than pooled OLS, indicating positive omitted variable bias in cross-sectional estimates.\n\n\nKey Concept 17.10: Choosing Between Panel Data Estimators\nThe choice between pooled OLS, fixed effects, and random effects depends on the research question and data structure. Use pooled OLS with cluster SEs for descriptive associations; use FE when unobserved individual heterogeneity is likely correlated with regressors (the common case); use RE only when individual effects are plausibly random and uncorrelated with regressors (e.g., randomized experiments). The Hausman test helps decide between FE and RE, but economic reasoning should guide the choice.\n\nWhat You’ve Learned: In this case study, you applied the complete panel data toolkit to cross-country productivity analysis. You decomposed variation into within and between components, compared pooled OLS with fixed effects, examined cluster-robust standard errors, and explored regional heterogeneity. These methods are essential for any empirical analysis using panel data.\n\n\n\nCase Study 2: Luminosity and Development Over Time: A Panel Approach\nResearch Question: How does nighttime luminosity evolve across Bolivia’s municipalities over time, and what does within-municipality variation reveal about development dynamics?\nBackground: Throughout this textbook, we have analyzed cross-sectional satellite-development relationships. But the DS4Bolivia dataset includes nighttime lights data for 2012-2020, creating a natural panel dataset. In this case study, we apply Chapter 17’s panel data tools to track how luminosity evolves across municipalities over time, using fixed effects to control for time-invariant characteristics.\nThe Data: The DS4Bolivia dataset covers 339 Bolivian municipalities with annual nighttime lights and population data for 2012-2020, yielding a potential panel of 3,051 municipality-year observations.\nKey Variables:\n\nmun: Municipality name\ndep: Department (administrative region)\nasdf_id: Unique municipality identifier\nln_NTLpc2012 through ln_NTLpc2020: Log nighttime lights per capita (annual)\npop2012 through pop2020: Population (annual)\n\n\nLoad the DS4Bolivia Data and Create Panel Structure\n\n# Load the DS4Bolivia dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\n\nurl_bol = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv\"\nbol = pd.read_csv(url_bol)\n\n# Display available NTL and population variables\nntl_vars = [c for c in bol.columns if c.startswith('ln_NTLpc')]\npop_vars = [c for c in bol.columns if c.startswith('pop') and c[3:].isdigit()]\n\nprint(\"=\" * 70)\nprint(\"DS4BOLIVIA: PANEL DATA CASE STUDY\")\nprint(\"=\" * 70)\nprint(f\"Cross-sectional units: {len(bol)} municipalities\")\nprint(f\"\\nNTL variables available: {ntl_vars}\")\nprint(f\"Population variables available: {pop_vars}\")\n\n\n\nTask 1: Create Panel Dataset (Guided)\nObjective: Reshape the wide-format DS4Bolivia data into a long panel dataset.\nInstructions:\n\nSelect municipality identifiers (mun, dep, asdf_id) and annual NTL/population variables\nReshape from wide to long format using pd.melt() or manual reshaping\nCreate columns: municipality, year, ln_NTLpc, pop, ln_pop\nVerify the panel structure: 339 municipalities x 9 years = 3,051 observations\nShow head() and describe() for the panel dataset\n\n\n# Your code here: Reshape to panel format\n#\n# Example structure:\n# # Reshape NTL variables\n# ntl_cols = {f'ln_NTLpc{y}': y for y in range(2012, 2021)}\n# pop_cols = {f'pop{y}': y for y in range(2012, 2021)}\n#\n# # Melt NTL\n# ntl_long = bol[['mun', 'dep', 'asdf_id'] + list(ntl_cols.keys())].melt(\n#     id_vars=['mun', 'dep', 'asdf_id'],\n#     var_name='ntl_var', value_name='ln_NTLpc'\n# )\n# ntl_long['year'] = ntl_long['ntl_var'].str.extract(r'(\\d{4})').astype(int)\n#\n# # Melt Population\n# pop_long = bol[['asdf_id'] + list(pop_cols.keys())].melt(\n#     id_vars=['asdf_id'],\n#     var_name='pop_var', value_name='pop'\n# )\n# pop_long['year'] = pop_long['pop_var'].str.extract(r'(\\d{4})').astype(int)\n#\n# # Merge\n# panel = ntl_long.merge(pop_long[['asdf_id', 'year', 'pop']],\n#                        on=['asdf_id', 'year'], how='left')\n# panel['ln_pop'] = np.log(panel['pop'])\n# panel = panel.sort_values(['asdf_id', 'year']).reset_index(drop=True)\n#\n# print(f\"Panel shape: {panel.shape}\")\n# print(f\"Municipalities: {panel['asdf_id'].nunique()}\")\n# print(f\"Years: {sorted(panel['year'].unique())}\")\n# print(panel.head(18))  # Show 2 municipalities\n# print(panel[['ln_NTLpc', 'ln_pop', 'pop']].describe().round(3))\n\n\nKey Concept 17.10: Satellite Panel Data\nAnnual nighttime lights observations create panel datasets even where traditional economic surveys are unavailable or infrequent. For Bolivia’s 339 municipalities over 2012-2020, the NTL panel provides 3,051 municipality-year observations. This temporal dimension allows us to move beyond cross-sectional associations and study changes within municipalities over time—a crucial step toward understanding development dynamics rather than just static patterns.\n\n\n\nTask 2: Pooled OLS with Cluster-Robust SEs (Guided)\nObjective: Estimate a pooled OLS regression of NTL on population with cluster-robust standard errors.\nInstructions:\n\nEstimate ln_NTLpc ~ ln_pop + year using all panel observations\nUse cluster-robust standard errors clustered by municipality\nCompare default and cluster-robust SEs\nInterpret: How does population relate to NTL? Is there a significant time trend?\n\n\n# Your code here: Pooled OLS with cluster-robust SEs\n#\n# Example structure:\n# panel_reg = panel[['ln_NTLpc', 'ln_pop', 'year', 'mun', 'asdf_id']].dropna()\n#\n# model_pooled = ols('ln_NTLpc ~ ln_pop + year', data=panel_reg).fit(\n#     cov_type='cluster', cov_kwds={'groups': panel_reg['asdf_id']}\n# )\n# print(\"POOLED OLS WITH CLUSTER-ROBUST SEs\")\n# print(model_pooled.summary())\n\n\n\nTask 3: Fixed Effects (Semi-guided)\nObjective: Estimate a fixed effects model controlling for time-invariant municipality characteristics.\nInstructions:\n\nAdd municipality fixed effects using C(asdf_id) or entity demeaning\nCompare FE coefficients with pooled OLS coefficients\nHow does controlling for time-invariant municipality characteristics change the population-NTL relationship?\nDiscuss: What unobserved factors do municipality fixed effects absorb (altitude, remoteness, climate)?\n\nHint: You can use C(asdf_id) in the formula, or manually demean variables by subtracting municipality means. For large datasets, demeaning is more computationally efficient.\n\n# Your code here: Fixed effects estimation\n#\n# Example structure (demeaning approach):\n# # Demean variables within each municipality\n# for col in ['ln_NTLpc', 'ln_pop']:\n#     panel_reg[f'{col}_dm'] = panel_reg.groupby('asdf_id')[col].transform(\n#         lambda x: x - x.mean()\n#     )\n# panel_reg['year_dm'] = panel_reg['year'] - panel_reg.groupby('asdf_id')['year'].transform('mean')\n#\n# model_fe = ols('ln_NTLpc_dm ~ ln_pop_dm + year_dm - 1', data=panel_reg).fit(\n#     cov_type='cluster', cov_kwds={'groups': panel_reg['asdf_id']}\n# )\n# print(\"FIXED EFFECTS (WITHIN ESTIMATOR)\")\n# print(model_fe.summary())\n#\n# print(\"\\nCOMPARISON:\")\n# print(f\"  Pooled OLS ln_pop coef: {model_pooled.params['ln_pop']:.4f}\")\n# print(f\"  Fixed Effects ln_pop coef: {model_fe.params['ln_pop_dm']:.4f}\")\n\n\nKey Concept 17.11: Fixed Effects for Spatial Heterogeneity\nMunicipality fixed effects absorb all time-invariant characteristics: altitude, remoteness, climate, historical infrastructure, cultural factors. After removing these fixed differences, the remaining variation identifies how changes in population (or other time-varying factors) relate to changes in NTL within the same municipality. This within-municipality analysis is more credible for causal interpretation than cross-sectional regressions, because it eliminates bias from unobserved time-invariant confounders.\n\n\n\nTask 4: Time Trends (Semi-guided)\nObjective: Examine how nighttime lights evolve over time using year fixed effects.\nInstructions:\n\nReplace the linear time trend with year dummy variables: C(year)\nTest whether the year effects are jointly significant\nPlot the estimated year coefficients to visualize the NTL trajectory\nDiscuss: Did NTL grow steadily, or were there jumps or declines?\n\nHint: Use one year as the reference category. The year coefficients show NTL changes relative to the base year.\n\n# Your code here: Year fixed effects\n#\n# Example structure:\n# # Estimate with year dummies (demeaned for municipality FE)\n# panel_reg['year_cat'] = panel_reg['year'].astype(str)\n#\n# # Simple approach: use year means of demeaned NTL\n# year_means = panel_reg.groupby('year')['ln_NTLpc'].mean()\n# year_means_dm = year_means - year_means.iloc[0]  # relative to base year\n#\n# fig, ax = plt.subplots(figsize=(10, 6))\n# ax.plot(year_means.index, year_means.values, 'o-', color='navy', linewidth=2)\n# ax.set_xlabel('Year')\n# ax.set_ylabel('Mean Log NTL per Capita')\n# ax.set_title('Average Municipal NTL Over Time (2012-2020)')\n# ax.grid(True, alpha=0.3)\n# plt.tight_layout()\n# plt.show()\n\n\n\nTask 5: First Differences (Independent)\nObjective: Estimate the relationship using first differences as an alternative to fixed effects.\nInstructions:\n\nCompute first differences: \\(\\Delta ln\\_NTLpc = ln\\_NTLpc_t - ln\\_NTLpc_{t-1}\\) and \\(\\Delta ln\\_pop = ln\\_pop_t - ln\\_pop_{t-1}\\)\nEstimate \\(\\Delta ln\\_NTLpc \\sim \\Delta ln\\_pop\\)\nCompare the first-difference coefficient with the fixed effects estimate\nDiscuss: When do FE and FD give different results? What assumptions does each require?\n\n\n# Your code here: First differences estimation\n#\n# Example structure:\n# panel_reg = panel_reg.sort_values(['asdf_id', 'year'])\n# panel_reg['d_ln_NTLpc'] = panel_reg.groupby('asdf_id')['ln_NTLpc'].diff()\n# panel_reg['d_ln_pop'] = panel_reg.groupby('asdf_id')['ln_pop'].diff()\n#\n# # Drop first year (no difference available)\n# fd_data = panel_reg.dropna(subset=['d_ln_NTLpc', 'd_ln_pop'])\n#\n# model_fd = ols('d_ln_NTLpc ~ d_ln_pop', data=fd_data).fit(\n#     cov_type='cluster', cov_kwds={'groups': fd_data['asdf_id']}\n# )\n# print(\"FIRST DIFFERENCES ESTIMATION\")\n# print(model_fd.summary())\n#\n# print(f\"\\nFD coefficient on ln_pop: {model_fd.params['d_ln_pop']:.4f}\")\n\n\n\nTask 6: Panel Brief (Independent)\nObjective: Write a 200-300 word brief summarizing your panel data analysis.\nYour brief should address:\n\nWhat do within-municipality changes in NTL reveal about development dynamics?\nHow does the FE population coefficient differ from the cross-sectional (pooled OLS) one?\nWhat time trends in NTL are visible across 2012-2020?\nHow do FE and FD estimates compare? Which assumptions matter most?\nWhat are the advantages and limitations of satellite panel data for tracking municipal development over time?\nWhat additional time-varying variables would improve the analysis?\n\n\n# Your code here: Additional analysis for the panel brief\n#\n# You might want to:\n# 1. Decompose variance into between and within components\n# 2. Plot NTL trajectories for selected municipalities\n# 3. Compare growth rates across departments\n# 4. Create a summary table of all estimation results\n#\n# Example: Variance decomposition\n# overall_var = panel_reg['ln_NTLpc'].var()\n# between_var = panel_reg.groupby('asdf_id')['ln_NTLpc'].mean().var()\n# within_var = panel_reg.groupby('asdf_id')['ln_NTLpc'].apply(lambda x: x - x.mean()).var()\n# print(f\"Overall variance: {overall_var:.4f}\")\n# print(f\"Between variance: {between_var:.4f}\")\n# print(f\"Within variance:  {within_var:.4f}\")\n# print(f\"Between share:    {between_var/overall_var:.1%}\")\n\n\n\nWhat You’ve Learned from This Case Study\nThis is the final DS4Bolivia case study in the textbook. Across 12 chapters, you have applied the complete econometric toolkit—from simple descriptive statistics (Chapter 1) through panel data methods (Chapter 17)—to study how satellite data can predict and monitor local economic development. The DS4Bolivia project demonstrates that modern data science and econometrics, combined with freely available satellite imagery, can contribute to SDG monitoring even in data-scarce contexts.\nIn this panel data analysis, you’ve applied Chapter 17’s complete toolkit:\n\nPanel construction: Reshaped cross-sectional data into a municipality-year panel\nPooled OLS: Estimated baseline relationships with cluster-robust standard errors\nFixed effects: Controlled for time-invariant municipality characteristics\nTime trends: Examined the trajectory of nighttime lights across 2012-2020\nFirst differences: Used an alternative estimation strategy and compared with FE\nCritical thinking: Assessed what within-municipality variation reveals about development dynamics\n\nCongratulations! You have now completed the full DS4Bolivia case study arc across the textbook.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chapter 17: Panel Data, Time Series Data, Causation</span>"
    ]
  }
]