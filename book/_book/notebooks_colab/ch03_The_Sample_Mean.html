<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3. The Sample Mean ‚Äì Econometrics Powered by AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html" rel="next">
<link href="../notebooks_colab/ch02_Univariate_Data_Summary.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-8b4baf804e461d9b72633f0de59a0cac.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-0da87e08732b0aad79cc276a1be62fe2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../custom.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks_colab/ch01_Analysis_of_Economics_Data.html">Statistical Foundations</a></li><li class="breadcrumb-item"><a href="../notebooks_colab/ch03_The_Sample_Mean.html"><span class="chapter-title">3. The Sample Mean</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Econometrics Powered by AI</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch00_Preface.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch01_Analysis_of_Economics_Data.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">1. Analysis of Economics Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch02_Univariate_Data_Summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">2. Univariate Data Summary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch03_The_Sample_Mean.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">3. The Sample Mean</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">4. Statistical Inference for the Mean</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Bivariate Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch05_Bivariate_Data_Summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">5. Bivariate Data Summary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch06_The_Least_Squares_Estimator.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">6. The Least Squares Estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">7. Statistical Inference for Bivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">8. Case Studies for Bivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch09_Models_with_Natural_Logarithms.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">9. Models with Natural Logarithms</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Multiple Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">10. Data Summary for Multiple Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">11. Statistical Inference for Multiple Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">12. Further Topics in Multiple Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">13. Case Studies for Multiple Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch14_Regression_with_Indicator_Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">14. Regression with Indicator Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch15_Regression_with_Transformed_Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">15. Regression with Transformed Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch16_Checking_the_Model_and_Data.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">16. Checking the Model and Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">17. Panel Data, Time Series Data, Causation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#chapter-overview" id="toc-chapter-overview" class="nav-link active" data-scroll-target="#chapter-overview">Chapter Overview</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables">3.1 Random Variables</a></li>
  <li><a href="#experiment-coin-tosses" id="toc-experiment-coin-tosses" class="nav-link" data-scroll-target="#experiment-coin-tosses">3.2 Experiment: Coin Tosses</a></li>
  <li><a href="#properties-of-the-sample-mean" id="toc-properties-of-the-sample-mean" class="nav-link" data-scroll-target="#properties-of-the-sample-mean">3.3 Properties of the Sample Mean</a></li>
  <li><a href="#real-data-example---1880-u.s.-census" id="toc-real-data-example---1880-u.s.-census" class="nav-link" data-scroll-target="#real-data-example---1880-u.s.-census">3.4 Real Data Example - 1880 U.S. Census</a></li>
  <li><a href="#estimator-properties" id="toc-estimator-properties" class="nav-link" data-scroll-target="#estimator-properties">3.5 Estimator Properties</a></li>
  <li><a href="#computer-simulation-of-random-samples" id="toc-computer-simulation-of-random-samples" class="nav-link" data-scroll-target="#computer-simulation-of-random-samples">3.6 Computer Simulation of Random Samples</a></li>
  <li><a href="#samples-other-than-simple-random-samples" id="toc-samples-other-than-simple-random-samples" class="nav-link" data-scroll-target="#samples-other-than-simple-random-samples">3.7 Samples other than Simple Random Samples</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#practice-exercises" id="toc-practice-exercises" class="nav-link" data-scroll-target="#practice-exercises">Practice Exercises</a></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies">Case Studies</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<div id="google_translate_element" style="padding: 8px 0;"></div>
<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'en',
    layout: google.translate.TranslateElement.InlineLayout.HORIZONTAL
  }, 'google_translate_element');
}
</script>
<script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks_colab/ch01_Analysis_of_Economics_Data.html">Statistical Foundations</a></li><li class="breadcrumb-item"><a href="../notebooks_colab/ch03_The_Sample_Mean.html"><span class="chapter-title">3. The Sample Mean</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">3. The Sample Mean</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>metricsAI: An Introduction to Econometrics with Python and AI in the Cloud</strong></p>
<p><em><a href="https://carlos-mendez.org">Carlos Mendez</a></em></p>
<p><img src="https://raw.githubusercontent.com/quarcs-lab/metricsai/main/images/ch03_visual_summary.jpg" alt="Chapter 03 Visual Summary" width="65%"></p>
<p>This notebook provides an interactive introduction to one of the most important concepts in statistics: the <strong>sampling distribution of the sample mean</strong>. You‚Äôll explore how sample means behave through experiments and simulations, building intuition for the Central Limit Theorem. All code runs directly in Google Colab without any local setup.</p>
<a href="https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch03_The_Sample_Mean.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Colab"></a>
<div class="chapter-resources">
<p><a href="https://www.youtube.com/watch?v=pnv9ff_3hrI" target="_blank" class="resource-btn">üé¨ AI Video</a> <a href="https://carlos-mendez.my.canva.site/s03-the-sample-mean-pdf" target="_blank" class="resource-btn">‚ú® AI Slides</a> <a href="https://cameron.econ.ucdavis.edu/aed/traedv1_03" target="_blank" class="resource-btn">üìä Cameron Slides</a> <a href="https://app.edcafe.ai/quizzes/697864e12f5d08069e0471b5" target="_blank" class="resource-btn">‚úèÔ∏è Quiz</a> <a href="https://app.edcafe.ai/chatbots/69789d252f5d08069e06fdad" target="_blank" class="resource-btn">ü§ñ AI Tutor</a></p>
</div>
<section id="chapter-overview" class="level2">
<h2 class="anchored" data-anchor-id="chapter-overview">Chapter Overview</h2>
<p>This chapter bridges the gap between descriptive statistics (Chapter 2) and inferential statistics (Chapter 4). The key insight: when we calculate a sample mean <span class="math inline">\(\bar{x}\)</span> from data, we‚Äôre observing one realization of a <strong>random variable</strong> <span class="math inline">\(\bar{X}\)</span> that has its own probability distribution.</p>
<p><strong>What you‚Äôll learn:</strong></p>
<ul>
<li>Understand sample values as realizations of random variables</li>
<li>Derive the mean and variance of the sample mean: <span class="math inline">\(E[\bar{X}] = \mu\)</span>, <span class="math inline">\(Var[\bar{X}] = \sigma^2/n\)</span></li>
<li>Explore the <strong>sampling distribution</strong> of <span class="math inline">\(\bar{X}\)</span> through experiments</li>
<li>Discover the <strong>Central Limit Theorem</strong>: <span class="math inline">\(\bar{X}\)</span> is approximately normal for large <span class="math inline">\(n\)</span></li>
<li>Learn properties of good estimators (unbiasedness, efficiency, consistency)</li>
<li>Compute the <strong>standard error</strong> of the mean: <span class="math inline">\(se(\bar{X}) = s/\sqrt{n}\)</span></li>
</ul>
<p><strong>Datasets used:</strong></p>
<ul>
<li><strong>AED_COINTOSSMEANS.DTA</strong>: 400 sample means from coin toss experiments (n=30 each)</li>
<li><strong>AED_CENSUSAGEMEANS.DTA</strong>: 100 sample means from 1880 U.S. Census ages (n=25 each)</li>
</ul>
<p><strong>Key economic relevance:</strong> This chapter provides the theoretical foundation for ALL statistical inference in economics. Whether estimating average income, unemployment rates, or regression coefficients, understanding the sampling distribution of <span class="math inline">\(\bar{X}\)</span> is essential.</p>
<p><strong>Chapter outline:</strong></p>
<ul>
<li>3.1 Random Variables</li>
<li>3.2 Experiment - Single Sample of Coin Tosses</li>
<li>3.3 Properties of the Sample Mean</li>
<li>3.4 Real Data Example - 1880 U.S. Census</li>
<li>3.5 Estimator Properties</li>
<li>3.6 Computer Simulation of Random Samples</li>
<li>3.7 Samples other than Simple Random Samples</li>
</ul>
<p><strong>Estimated time:</strong> 50-60 minutes</p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>First, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.</p>
<div id="cell-3" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seeds for reproducibility</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>random.seed(RANDOM_SEED)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>np.random.seed(RANDOM_SEED)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'PYTHONHASHSEED'</span>] <span class="op">=</span> <span class="bu">str</span>(RANDOM_SEED)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># GitHub data URL</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>GITHUB_DATA_URL <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Set plotting style</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">6</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Setup complete! Ready to explore the sample mean."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Setup complete! Ready to explore the sample mean.</code></pre>
</div>
</div>
</section>
<section id="random-variables" class="level2">
<h2 class="anchored" data-anchor-id="random-variables">3.1 Random Variables</h2>
<p>A <strong>random variable</strong> is a variable whose value is determined by the outcome of an experiment. The connection between data and randomness:</p>
<ul>
<li><strong>Random variable notation:</strong> <span class="math inline">\(X\)</span> (uppercase) represents the random variable</li>
<li><strong>Realized value notation:</strong> <span class="math inline">\(x\)</span> (lowercase) represents the observed value</li>
</ul>
<p><strong>Example - Coin Toss:</strong></p>
<ul>
<li>Experiment: Toss a fair coin</li>
<li>Random variable: <span class="math inline">\(X = 1\)</span> if heads, <span class="math inline">\(X = 0\)</span> if tails</li>
<li>Each outcome has probability 0.5</li>
</ul>
<p><strong>Key properties:</strong></p>
<p><strong>Mean (Expected Value):</strong> <span class="math display">\[\mu = E[X] = \sum_x x \cdot Pr[X = x]\]</span></p>
<p>For fair coin: <span class="math inline">\(\mu = 0 \times 0.5 + 1 \times 0.5 = 0.5\)</span></p>
<p><strong>Variance:</strong> <span class="math display">\[\sigma^2 = E[(X - \mu)^2] = \sum_x (x - \mu)^2 \cdot Pr[X = x]\]</span></p>
<p>For fair coin: <span class="math inline">\(\sigma^2 = (0-0.5)^2 \times 0.5 + (1-0.5)^2 \times 0.5 = 0.25\)</span></p>
<p><strong>Standard Deviation:</strong> <span class="math inline">\(\sigma = \sqrt{0.25} = 0.5\)</span></p>
<div id="cell-5" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Illustrate coin toss random variable</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"COIN TOSS RANDOM VARIABLE"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fair coin properties</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Fair coin (p = 0.5):"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>mu_fair <span class="op">=</span> <span class="dv">0</span> <span class="op">*</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="dv">1</span> <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>var_fair <span class="op">=</span> (<span class="dv">0</span> <span class="op">-</span> mu_fair)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> <span class="fl">0.5</span> <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> mu_fair)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>sigma_fair <span class="op">=</span> np.sqrt(var_fair)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Mean (Œº):              </span><span class="sc">{</span>mu_fair<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Variance (œÉ¬≤):         </span><span class="sc">{</span>var_fair<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Standard deviation (œÉ): </span><span class="sc">{</span>sigma_fair<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Unfair coin for comparison</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Unfair coin (p = 0.6 for heads):"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>mu_unfair <span class="op">=</span> <span class="dv">0</span> <span class="op">*</span> <span class="fl">0.4</span> <span class="op">+</span> <span class="dv">1</span> <span class="op">*</span> <span class="fl">0.6</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>var_unfair <span class="op">=</span> (<span class="dv">0</span> <span class="op">-</span> mu_unfair)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> <span class="fl">0.4</span> <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> mu_unfair)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> <span class="fl">0.6</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>sigma_unfair <span class="op">=</span> np.sqrt(var_unfair)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Mean (Œº):              </span><span class="sc">{</span>mu_unfair<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Variance (œÉ¬≤):         </span><span class="sc">{</span>var_unfair<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Standard deviation (œÉ): </span><span class="sc">{</span>sigma_unfair<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
COIN TOSS RANDOM VARIABLE
======================================================================

Fair coin (p = 0.5):
  Mean (Œº):              0.5000
  Variance (œÉ¬≤):         0.2500
  Standard deviation (œÉ): 0.5000

Unfair coin (p = 0.6 for heads):
  Mean (Œº):              0.6000
  Variance (œÉ¬≤):         0.2400
  Standard deviation (œÉ): 0.4899</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.1: Random Variables</strong></p>
<p>A random variable <span class="math inline">\(X\)</span> is a variable whose value is determined by the outcome of an unpredictable experiment. The mean <span class="math inline">\(\mu = \mathrm{E}[X]\)</span> is the probability-weighted average of all possible values, while the variance <span class="math inline">\(\sigma^2 = \mathrm{E}[(X-\mu)^2]\)</span> measures variability around the mean. These population parameters characterize the distribution from which we draw samples.</p>
</blockquote>
<p><strong>Transition:</strong> Now that we understand random variables theoretically, let‚Äôs see them in action through a simple experiment: coin tosses. We‚Äôll discover how the sample mean behaves when we repeat the experiment many times.</p>
</section>
<section id="experiment-coin-tosses" class="level2">
<h2 class="anchored" data-anchor-id="experiment-coin-tosses">3.2 Experiment: Coin Tosses</h2>
<section id="one-sample" class="level3">
<h3 class="anchored" data-anchor-id="one-sample">One Sample</h3>
<p>Now we conduct an actual experiment: toss a coin 30 times and record the results. This gives us a <strong>sample</strong> of size <span class="math inline">\(n = 30\)</span>.</p>
<p><strong>Key insight:</strong> The observed values <span class="math inline">\(x_1, x_2, ..., x_{30}\)</span> are realizations of random variables <span class="math inline">\(X_1, X_2, ..., X_{30}\)</span>.</p>
<p>The <strong>sample mean</strong> is: <span class="math display">\[\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\]</span></p>
<p>This <span class="math inline">\(\bar{x}\)</span> is itself a realization of the random variable: <span class="math display">\[\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\]</span></p>
<div id="cell-9" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate single sample of 30 coin tosses</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10101</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.where(u <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)  <span class="co"># 1 if heads, 0 if tails</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SINGLE COIN TOSS SAMPLE (n = 30)"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Number of heads (x=1):  </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(x)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of tails (x=0):  </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(<span class="dv">1</span><span class="op">-</span>x)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample mean (xÃÑ):        </span><span class="sc">{</span>np<span class="sc">.</span>mean(x)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample std dev (s):     </span><span class="sc">{</span>np<span class="sc">.</span>std(x, ddof<span class="op">=</span><span class="dv">1</span>)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Population values:"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Population mean (Œº):   0.5000"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Population std (œÉ):    0.5000"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize single sample</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>ax.hist(x, bins<span class="op">=</span>[<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1.5</span>], edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'steelblue'</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Outcome (0 = Tails, 1 = Heads)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Single Sample of 30 Coin Tosses'</span>,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'Tails (0)'</span>, <span class="st">'Heads (1)'</span>])</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Note: This is just ONE realization of the random variable XÃÑ."</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"To understand XÃÑ's distribution, we need MANY samples..."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
SINGLE COIN TOSS SAMPLE (n = 30)
======================================================================

Number of heads (x=1):  12
Number of tails (x=0):  18
Sample mean (xÃÑ):        0.4000
Sample std dev (s):     0.4983

Population values:
  Population mean (Œº):   0.5000
  Population std (œÉ):    0.5000</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ch03_The_Sample_Mean_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Note: This is just ONE realization of the random variable XÃÑ.
To understand XÃÑ's distribution, we need MANY samples...</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.2: Sample Mean as Random Variable</strong></p>
<p>The observed sample mean <span class="math inline">\(\bar{x}\)</span> is a realization of the random variable <span class="math inline">\(\bar{X} = (X_1 + \cdots + X_n)/n\)</span>. This fundamental insight means that <span class="math inline">\(\bar{x}\)</span> varies from sample to sample in a predictable way‚Äîits distribution can be characterized mathematically, allowing us to perform statistical inference about the population mean <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<p><strong>Key findings from our coin toss experiment (n = 30):</strong></p>
<p><strong>1. Sample mean = 0.4000 (vs.&nbsp;theoretical Œº = 0.5)</strong></p>
<ul>
<li>We got 12 heads and 18 tails (40% vs.&nbsp;expected 50%)</li>
<li>This difference (0.10 or 10 percentage points) is completely normal</li>
<li>With only 30 tosses, random variation of this magnitude is expected</li>
<li>If we flipped 1000 times, we‚Äôd expect to get much closer to 50%</li>
</ul>
<p><strong>2. Sample standard deviation = 0.4983 (vs.&nbsp;theoretical œÉ = 0.5000)</strong></p>
<ul>
<li>Nearly perfect match with population value</li>
<li>This confirms the theoretical formula: œÉ¬≤ = p(1-p) = 0.5(0.5) = 0.25, so œÉ = 0.5</li>
</ul>
<p><strong>3. Why the sample mean differs from 0.5:</strong></p>
<ul>
<li>The sample mean xÃÑ is itself a random variable</li>
<li>Just like one coin toss doesn‚Äôt always give heads, one sample mean doesn‚Äôt always equal Œº</li>
<li>This single value (0.4000) is one realization from the sampling distribution of XÃÑ</li>
<li>The next experiment would likely give a different value (maybe 0.4667 or 0.5333)</li>
</ul>
<p><strong>Economic interpretation:</strong> When we estimate average income from a survey or unemployment rate from a sample, we get one realization that will differ from the true population value. Understanding this variability is the foundation of statistical inference.</p>
</section>
<section id="samples-and-the-distribution-of-sample-means" class="level3">
<h3 class="anchored" data-anchor-id="samples-and-the-distribution-of-sample-means">400 Samples and The Distribution of Sample Means</h3>
<p>To understand the <strong>sampling distribution</strong> of <span class="math inline">\(\bar{X}\)</span>, we repeat the experiment 400 times:</p>
<ul>
<li>Each experiment: 30 coin tosses ‚Üí one sample mean <span class="math inline">\(\bar{x}_i\)</span></li>
<li>After 400 experiments: we have 400 sample means (<span class="math inline">\(\bar{x}_1, \bar{x}_2, ..., \bar{x}_{400}\)</span>)</li>
<li>The histogram of these 400 values approximates the <strong>sampling distribution</strong> of <span class="math inline">\(\bar{X}\)</span></li>
</ul>
<p><strong>What we expect to see:</strong></p>
<ul>
<li>Sample means centered near <span class="math inline">\(\mu = 0.5\)</span> (population mean)</li>
<li>Much less variability than individual coin tosses</li>
<li>Approximately normal distribution (Central Limit Theorem!)</li>
</ul>
<div id="cell-13" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load precomputed coin toss means data (400 samples of size 30)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>data_cointoss <span class="op">=</span> pd.read_stata(GITHUB_DATA_URL <span class="op">+</span> <span class="st">'AED_COINTOSSMEANS.DTA'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"400 COIN TOSS EXPERIMENTS (each n = 30)"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>xbar <span class="op">=</span> data_cointoss[<span class="st">'xbar'</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Summary of 400 sample means:"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_cointoss.describe())</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 5 sample means: </span><span class="sc">{</span>xbar<span class="sc">.</span>head()<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mean of the 400 sample means: </span><span class="sc">{</span>xbar<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Std dev of the 400 sample means: </span><span class="sc">{</span>xbar<span class="sc">.</span>std()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Theoretical predictions:"</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  E[XÃÑ] = Œº = 0.5000"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  œÉ(XÃÑ) = œÉ/‚àön = ‚àö(0.25/30) = </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(<span class="fl">0.25</span><span class="op">/</span><span class="dv">30</span>)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Comparison:"</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Empirical mean: </span><span class="sc">{</span>xbar<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss"> vs Theoretical: 0.5000"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Empirical std:  </span><span class="sc">{</span>xbar<span class="sc">.</span>std()<span class="sc">:.4f}</span><span class="ss"> vs Theoretical: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(<span class="fl">0.25</span><span class="op">/</span><span class="dv">30</span>)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Excellent agreement between theory and experiment!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
400 COIN TOSS EXPERIMENTS (each n = 30)
======================================================================

Summary of 400 sample means:
             xbar       stdev  numobs
count  400.000000  400.000000   400.0
mean     0.499417    0.500826    30.0
std      0.086307    0.010360     0.0
min      0.266667    0.449776    30.0
25%      0.433333    0.498273    30.0
50%      0.500000    0.504007    30.0
75%      0.566667    0.507416    30.0
max      0.733333    0.508548    30.0

First 5 sample means: [0.3333333432674408, 0.5, 0.5333333611488342, 0.5666666626930237, 0.5]

Mean of the 400 sample means: 0.4994
Std dev of the 400 sample means: 0.0863

Theoretical predictions:
  E[XÃÑ] = Œº = 0.5000
  œÉ(XÃÑ) = œÉ/‚àön = ‚àö(0.25/30) = 0.0913

Comparison:
  Empirical mean: 0.4994 vs Theoretical: 0.5000
  Empirical std:  0.0863 vs Theoretical: 0.0913

Excellent agreement between theory and experiment!</code></pre>
</div>
</div>
<p><strong>Key findings from 400 coin toss experiments:</strong></p>
<p><strong>1. Mean of sample means = 0.4994 (vs.&nbsp;theoretical Œº = 0.5)</strong></p>
<ul>
<li>This demonstrates <strong>unbiasedness</strong>: E[XÃÑ] = Œº</li>
<li>The tiny difference (0.0006) is just random variation</li>
<li>With more replications, this would get even closer to 0.5</li>
<li>On average across many samples, XÃÑ equals the true population mean</li>
</ul>
<p><strong>2. Standard deviation of sample means = 0.0863 (vs.&nbsp;theoretical = 0.0913)</strong></p>
<ul>
<li>The theoretical standard error is œÉ/‚àön = 0.5/‚àö30 = 0.0913</li>
<li>Our empirical SD (0.0863) is very close to this prediction</li>
<li>This confirms the variance formula: Var(XÃÑ) = œÉ¬≤/n works in practice</li>
</ul>
<p><strong>3. Range of sample means: 0.2667 to 0.7333</strong></p>
<ul>
<li>Individual sample means vary considerably (from 26.7% to 73.3% heads)</li>
<li>This shows why we need statistical theory - any single sample could be misleading</li>
<li>But the distribution is predictable and centered correctly</li>
</ul>
<p><strong>4. Comparison with single coin tosses:</strong></p>
<ul>
<li>Individual coin tosses have œÉ = 0.5</li>
<li>Sample means have œÉ(XÃÑ) = 0.0863</li>
<li>Sample means are 5.8√ó less variable than individual tosses</li>
<li>This is the power of averaging: ‚àö30 ‚âà 5.5</li>
</ul>
<p><strong>Economic interpretation:</strong> When estimating economic parameters (average wage, inflation rate, GDP growth), individual survey responses vary widely, but the sample mean is much more precise. The standard error tells us exactly how much precision we gain from our sample size.</p>
</section>
<section id="visualizing-the-sampling-distribution-of-barx" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-sampling-distribution-of-barx">Visualizing the Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></h3>
<p>The histogram below shows the distribution of the 400 sample means. Notice:</p>
<ol type="1">
<li><strong>Center:</strong> Near 0.5 (the population mean <span class="math inline">\(\mu\)</span>)</li>
<li><strong>Spread:</strong> Much narrower than the original population (œÉ = 0.5)</li>
<li><strong>Shape:</strong> Approximately bell-shaped (normal distribution)</li>
</ol>
<p>The red curve is the <strong>theoretical normal distribution</strong> with mean <span class="math inline">\(\mu = 0.5\)</span> and standard deviation <span class="math inline">\(\sigma/\sqrt{n} = 0.091\)</span>.</p>
<div id="cell-16" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize distribution of sample means</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Histogram of 400 sample means</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>n_hist, bins, patches <span class="op">=</span> ax.hist(xbar, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                                 edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'steelblue'</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                                 label<span class="op">=</span><span class="st">'400 sample means'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay theoretical normal distribution</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>xbar_range <span class="op">=</span> np.linspace(xbar.<span class="bu">min</span>(), xbar.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>theoretical_pdf <span class="op">=</span> stats.norm.pdf(xbar_range, <span class="fl">0.5</span>, np.sqrt(<span class="fl">0.25</span><span class="op">/</span><span class="dv">30</span>))</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>ax.plot(xbar_range, theoretical_pdf, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f'Theoretical N(0.5, </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(<span class="fl">0.25</span><span class="op">/</span><span class="dv">30</span>)<span class="sc">:.3f}</span><span class="ss">)'</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sample mean from each of 400 samples'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Figure 3.1B: Distribution of Sample Means (Central Limit Theorem)'</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The empirical distribution matches the theoretical normal distribution!"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This is the Central Limit Theorem in action."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ch03_The_Sample_Mean_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>The empirical distribution matches the theoretical normal distribution!
This is the Central Limit Theorem in action.</code></pre>
</div>
</div>
<p><strong>Transition:</strong> Having observed the sampling distribution empirically through coin tosses, we can now derive its properties mathematically and understand why it behaves the way it does.</p>
</section>
</section>
<section id="properties-of-the-sample-mean" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-sample-mean">3.3 Properties of the Sample Mean</h2>
<p>Under the assumption of a <strong>simple random sample</strong> where:</p>
<ul>
<li>A. Each <span class="math inline">\(X_i\)</span> has common mean: <span class="math inline">\(E[X_i] = \mu\)</span></li>
<li>B. Each <span class="math inline">\(X_i\)</span> has common variance: <span class="math inline">\(Var[X_i] = \sigma^2\)</span></li>
<li>C. The <span class="math inline">\(X_i\)</span> are statistically independent</li>
</ul>
<p>We can mathematically prove:</p>
<p><strong>Mean of the sample mean:</strong> <span class="math display">\[E[\bar{X}] = \mu\]</span></p>
<p>This says <span class="math inline">\(\bar{X}\)</span> is <strong>unbiased</strong> for <span class="math inline">\(\mu\)</span> (its expected value equals the parameter we‚Äôre estimating).</p>
<p><strong>Variance of the sample mean:</strong> <span class="math display">\[Var[\bar{X}] = \frac{\sigma^2}{n}\]</span></p>
<p><strong>Standard deviation of the sample mean:</strong> <span class="math display">\[SD[\bar{X}] = \frac{\sigma}{\sqrt{n}}\]</span></p>
<p><strong>Key insights:</strong></p>
<ol type="1">
<li>As sample size <span class="math inline">\(n\)</span> increases, <span class="math inline">\(Var[\bar{X}]\)</span> decreases (<span class="math inline">\(\propto 1/n\)</span>)</li>
<li>Larger samples give more precise estimates (smaller variability)</li>
<li>Standard deviation decreases at rate $1/$ (to halve uncertainty, need 4√ó the sample size)</li>
</ol>
<div id="cell-19" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate how variance of XÃÑ depends on sample size n</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"HOW SAMPLE SIZE AFFECTS PRECISION"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For coin toss: Œº = 0.5, œÉ¬≤ = 0.25, œÉ = 0.5</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>sigma_sq <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">100</span>, <span class="dv">400</span>, <span class="dv">1000</span>]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Population: Œº = </span><span class="sc">{</span>mu<span class="sc">}</span><span class="ss">, œÉ = </span><span class="sc">{</span>sigma<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'n'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'œÉ(XÃÑ) = œÉ/‚àön'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Var(XÃÑ) = œÉ¬≤/n'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> sample_sizes:</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    sd_xbar <span class="op">=</span> sigma <span class="op">/</span> np.sqrt(n)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    var_xbar <span class="op">=</span> sigma_sq <span class="op">/</span> n</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>n<span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span>sd_xbar<span class="sc">:&lt;20.6f}</span><span class="ss"> </span><span class="sc">{</span>var_xbar<span class="sc">:&lt;20.6f}</span><span class="ss">"</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Key observation: Doubling n reduces œÉ(XÃÑ) by factor of ‚àö2 ‚âà 1.41"</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"To halve uncertainty, need to quadruple sample size."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
HOW SAMPLE SIZE AFFECTS PRECISION
======================================================================

Population: Œº = 0.5, œÉ = 0.5

n          œÉ(XÃÑ) = œÉ/‚àön         Var(XÃÑ) = œÉ¬≤/n      
--------------------------------------------------
10         0.158114             0.025000            
30         0.091287             0.008333            
100        0.050000             0.002500            
400        0.025000             0.000625            
1000       0.015811             0.000250            

Key observation: Doubling n reduces œÉ(XÃÑ) by factor of ‚àö2 ‚âà 1.41
To halve uncertainty, need to quadruple sample size.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.3: Properties of the Sample Mean</strong></p>
<p>Under simple random sampling (common mean <span class="math inline">\(\mu\)</span>, common variance <span class="math inline">\(\sigma^2\)</span>, independence), the sample mean <span class="math inline">\(\bar{X}\)</span> has mean <span class="math inline">\(\mathrm{E}[\bar{X}] = \mu\)</span> (unbiased) and variance <span class="math inline">\(\operatorname{Var}[\bar{X}] = \sigma^2/n\)</span> (decreases with sample size). The standard deviation <span class="math inline">\(\sigma_{\bar{X}} = \sigma/\sqrt{n}\)</span> shrinks as <span class="math inline">\(n\)</span> increases, meaning larger samples produce more precise estimates of <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<div id="cell-21" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Illustrate standard error calculation</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STANDARD ERROR CALCULATION"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use our single sample from earlier</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>x_mean <span class="op">=</span> np.mean(x)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>x_std <span class="op">=</span> np.std(x, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Sample standard deviation</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>se_xbar <span class="op">=</span> x_std <span class="op">/</span> np.sqrt(n)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample statistics (n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Sample mean (xÃÑ):          </span><span class="sc">{</span>x_mean<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Sample std dev (s):        </span><span class="sc">{</span>x_std<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Standard error se(XÃÑ):     </span><span class="sc">{</span>se_xbar<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Population values:"</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Population mean (Œº):       0.5000"</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Population std (œÉ):        0.5000"</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  True œÉ/‚àön:                 </span><span class="sc">{</span><span class="fl">0.5</span><span class="op">/</span>np<span class="sc">.</span>sqrt(n)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation:"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  The standard error </span><span class="sc">{</span>se_xbar<span class="sc">:.4f}</span><span class="ss"> tells us the typical distance"</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  between our sample mean (</span><span class="sc">{</span>x_mean<span class="sc">:.4f}</span><span class="ss">) and the true population mean (0.5)."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
STANDARD ERROR CALCULATION
======================================================================

Sample statistics (n = 30):
  Sample mean (xÃÑ):          0.4000
  Sample std dev (s):        0.4983
  Standard error se(XÃÑ):     0.0910

Population values:
  Population mean (Œº):       0.5000
  Population std (œÉ):        0.5000
  True œÉ/‚àön:                 0.0913

Interpretation:
  The standard error 0.0910 tells us the typical distance
  between our sample mean (0.4000) and the true population mean (0.5).</code></pre>
</div>
</div>
<section id="interpreting-the-standard-error" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-the-standard-error">Interpreting the Standard Error</h3>
<p><strong>Key findings from our standard error calculation (n = 30):</strong></p>
<p><strong>1. Sample mean = 0.4000 with standard error = 0.0910</strong></p>
<ul>
<li>The standard error tells us the typical distance between xÃÑ and Œº</li>
<li>Our sample mean (0.40) is about 1.1 standard errors below the true mean (0.50)</li>
<li>This is well within normal sampling variation (within 2 standard errors)</li>
</ul>
<p><strong>2. Estimated SE = 0.0910 vs.&nbsp;True œÉ/‚àön = 0.0913</strong></p>
<ul>
<li>We used sample standard deviation s = 0.4983 instead of œÉ = 0.5</li>
<li>Our estimate is remarkably accurate (only 0.0003 difference)</li>
<li>In practice, we never know œÉ, so we always use s to compute the standard error</li>
</ul>
<p><strong>3. What the standard error means:</strong></p>
<ul>
<li>If we repeated this experiment many times, our sample means would typically differ from 0.5 by about 0.091</li>
<li>About 68% of sample means would fall within ¬±0.091 of 0.5 (between 0.409 and 0.591)</li>
<li>About 95% would fall within ¬±0.182 of 0.5 (between 0.318 and 0.682)</li>
</ul>
<p><strong>4. How to reduce the standard error:</strong></p>
<ul>
<li>To halve the SE, we‚Äôd need to quadruple the sample size (n = 120)</li>
<li>To cut SE by 10√ó, we‚Äôd need 100√ó the sample size (n = 3000)</li>
<li>This is why larger surveys are more precise but also more expensive</li>
</ul>
<p><strong>Economic interpretation:</strong> When a poll reports ‚Äúmargin of error ¬±3%‚Äù, they‚Äôre referring to approximately 2 standard errors. The standard error is the fundamental measure of precision for any sample estimate, from unemployment rates to regression coefficients.</p>
<blockquote class="blockquote">
<p><strong>Key Concept 3.4: Standard Error of the Mean</strong></p>
<p>The standard error se(<span class="math inline">\(\bar{X}\)</span>) = <span class="math inline">\(s/\sqrt{n}\)</span> is the estimated standard deviation of the sample mean. It measures the precision of <span class="math inline">\(\bar{x}\)</span> as an estimate of <span class="math inline">\(\mu\)</span>. Since <span class="math inline">\(\sigma\)</span> is unknown in practice, we replace it with the sample standard deviation <span class="math inline">\(s\)</span>. The standard error decreases with <span class="math inline">\(\sqrt{n}\)</span>, so doubling precision requires quadrupling the sample size.</p>
</blockquote>
</section>
<section id="central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="central-limit-theorem">Central Limit Theorem</h3>
<p>The <strong>Central Limit Theorem (CLT)</strong> is one of the most important results in statistics:</p>
<p><strong>Statement:</strong> If <span class="math inline">\(X_1, ..., X_n\)</span> are independent random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then as <span class="math inline">\(n \to \infty\)</span>:</p>
<p><span class="math display">\[\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right) \text{ approximately}\]</span></p>
<p>Or equivalently, the <strong>standardized</strong> sample mean:</p>
<p><span class="math display">\[Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1) \text{ approximately}\]</span></p>
<p><strong>Remarkable facts:</strong></p>
<ol type="1">
<li>This holds <strong>regardless of the distribution of <span class="math inline">\(X\)</span></strong> (doesn‚Äôt have to be normal!)</li>
<li>Works well even for moderate sample sizes (<span class="math inline">\(n \geq 30\)</span> is common rule of thumb)</li>
<li>Provides justification for using normal-based inference</li>
</ol>
<p><strong>Standard error:</strong> Since <span class="math inline">\(\sigma\)</span> is typically unknown, we estimate it with sample standard deviation <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[se(\bar{X}) = \frac{s}{\sqrt{n}}\]</span></p>
<blockquote class="blockquote">
<p><strong>Key Concept 3.5: The Central Limit Theorem</strong></p>
<p>The Central Limit Theorem states that the standardized sample mean <span class="math inline">\(Z = (\bar{X} - \mu)/(\sigma/\sqrt{n})\)</span> converges to a standard normal distribution N(0,1) as <span class="math inline">\(n \rightarrow \infty\)</span>. This remarkable result holds regardless of the distribution of <span class="math inline">\(X\)</span> (as long as it has finite mean and variance), making normal-based inference applicable to a wide variety of problems.</p>
</blockquote>
<p><strong>Transition:</strong> The coin toss example showed us the Central Limit Theorem with a simple binary variable. Now let‚Äôs see if it works with real-world data that has a complex, non-normal distribution‚Äîthe ages from the 1880 U.S. Census.</p>
</section>
</section>
<section id="real-data-example---1880-u.s.-census" class="level2">
<h2 class="anchored" data-anchor-id="real-data-example---1880-u.s.-census">3.4 Real Data Example - 1880 U.S. Census</h2>
<p>Now we move from coin tosses to real economic/demographic data. The 1880 U.S. Census recorded ages of all 50,169,452 people in the United States.</p>
<p><strong>Population parameters (known because we have full census):</strong></p>
<ul>
<li>Population mean age: <span class="math inline">\(\mu = 24.13\)</span> years</li>
<li>Population std dev: <span class="math inline">\(\sigma = 18.61\)</span> years</li>
<li>Distribution: Highly non-normal (skewed, peaks at multiples of 5 due to rounding)</li>
</ul>
<p><strong>Experiment:</strong></p>
<ul>
<li>Draw 100 random samples, each of size <span class="math inline">\(n = 25\)</span></li>
<li>Calculate sample mean age for each sample</li>
<li>Examine distribution of these 100 sample means</li>
</ul>
<p><strong>Question:</strong> Even though population ages are NOT normally distributed, will the sample means be approximately normal? (CLT says yes!)</p>
<div id="cell-28" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load census age means data</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>data_census <span class="op">=</span> pd.read_stata(GITHUB_DATA_URL <span class="op">+</span> <span class="st">'AED_CENSUSAGEMEANS.DTA'</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"1880 U.S. CENSUS - 100 SAMPLES OF SIZE 25"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the mean variable</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">'mean'</span> <span class="kw">in</span> data_census.columns:</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    age_means <span class="op">=</span> data_census[<span class="st">'mean'</span>]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> <span class="st">'xmean'</span> <span class="kw">in</span> data_census.columns:</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    age_means <span class="op">=</span> data_census[<span class="st">'xmean'</span>]</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    age_means <span class="op">=</span> data_census.iloc[:, <span class="dv">0</span>]</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Summary of 100 sample means:"</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_census.describe())</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 5 sample means: </span><span class="sc">{</span>age_means<span class="sc">.</span>head()<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mean of the 100 sample means:   </span><span class="sc">{</span>age_means<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss"> years"</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Std dev of the 100 sample means: </span><span class="sc">{</span>age_means<span class="sc">.</span>std()<span class="sc">:.2f}</span><span class="ss"> years"</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Theoretical predictions:"</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  E[XÃÑ] = Œº = 24.13 years"</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  œÉ(XÃÑ) = œÉ/‚àön = 18.61/‚àö25 = </span><span class="sc">{</span><span class="fl">18.61</span><span class="op">/</span>np<span class="sc">.</span>sqrt(<span class="dv">25</span>)<span class="sc">:.2f}</span><span class="ss"> years"</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Comparison:"</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Empirical mean: </span><span class="sc">{</span>age_means<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss"> vs Theoretical: 24.13"</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Empirical std:  </span><span class="sc">{</span>age_means<span class="sc">.</span>std()<span class="sc">:.2f}</span><span class="ss"> vs Theoretical: </span><span class="sc">{</span><span class="fl">18.61</span><span class="op">/</span>np<span class="sc">.</span>sqrt(<span class="dv">25</span>)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Close agreement, despite non-normal population distribution!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
1880 U.S. CENSUS - 100 SAMPLES OF SIZE 25
======================================================================

Summary of 100 sample means:
             mean       stdev  numobs
count  100.000000  100.000000   100.0
mean    23.782001   18.245018    25.0
std      3.760694    2.890753     0.0
min     14.600000   12.362847    25.0
25%     22.020000   16.148388    25.0
50%     23.759999   18.434547    25.0
75%     26.190000   20.387874    25.0
max     33.439999   25.306587    25.0

First 5 sample means: [27.84000015258789, 19.399999618530273, 23.280000686645508, 26.84000015258789, 26.559999465942383]

Mean of the 100 sample means:   23.78 years
Std dev of the 100 sample means: 3.76 years

Theoretical predictions:
  E[XÃÑ] = Œº = 24.13 years
  œÉ(XÃÑ) = œÉ/‚àön = 18.61/‚àö25 = 3.72 years

Comparison:
  Empirical mean: 23.78 vs Theoretical: 24.13
  Empirical std:  3.76 vs Theoretical: 3.72

Close agreement, despite non-normal population distribution!</code></pre>
</div>
</div>
<p><strong>Key findings from 100 samples of 1880 U.S. Census ages (n = 25 each):</strong></p>
<p><strong>1. Mean of sample means = 23.78 years (vs.&nbsp;true population Œº = 24.13)</strong></p>
<ul>
<li>Difference of only 0.35 years (less than 1.5% error)</li>
<li>This again demonstrates unbiasedness</li>
<li>With only 100 samples, some sampling error is expected</li>
</ul>
<p><strong>2. Standard deviation of sample means = 3.76 years (vs.&nbsp;theoretical = 3.72)</strong></p>
<ul>
<li>Theoretical: œÉ/‚àön = 18.61/‚àö25 = 3.72 years</li>
<li>Empirical: 3.76 years</li>
<li>Excellent agreement between theory and data (within 1%)</li>
</ul>
<p><strong>3. Range of sample means: 14.6 to 33.4 years</strong></p>
<ul>
<li>Individual sample means vary by almost 19 years</li>
<li>But most cluster tightly around 24 years</li>
<li>This wide range shows why statistical theory matters</li>
</ul>
<p><strong>4. The power of the Central Limit Theorem:</strong></p>
<ul>
<li>The population distribution of ages in 1880 was highly non-normal:
<ul>
<li>Many young children (high frequency at low ages)</li>
<li>Heaping at multiples of 5 (people rounded their ages)</li>
<li>Long right tail (elderly people)</li>
</ul></li>
<li>Yet the distribution of sample means IS approximately normal</li>
<li>This is the CLT‚Äôs remarkable power - normality emerges from averaging</li>
</ul>
<p><strong>5. Practical implications for sample size:</strong></p>
<ul>
<li>With n = 25, the standard error is 3.72 years</li>
<li>To estimate average age within ¬±1 year (95% confidence), we‚Äôd need about 4 times larger samples</li>
<li>The Census Bureau uses this logic to design survey sizes</li>
</ul>
<p><strong>Economic interpretation:</strong> Real economic data (income, age, consumption) is rarely normally distributed - often highly skewed or irregular. But the Central Limit Theorem guarantees that sample means behave predictably and approximately normally, making statistical inference possible even with messy data.</p>
<section id="visualization-census-sample-means-distribution" class="level3">
<h3 class="anchored" data-anchor-id="visualization-census-sample-means-distribution">Visualization: Census Sample Means Distribution</h3>
<p>This figure demonstrates the Central Limit Theorem with real data. Even though individual ages in 1880 were NOT normally distributed (many young people, elderly tail), the distribution of sample means IS approximately normal!</p>
<div id="cell-31" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize distribution of census age means</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Histogram</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>n_hist, bins, patches <span class="op">=</span> ax.hist(age_means, bins<span class="op">=</span><span class="dv">20</span>, density<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                                 edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'coral'</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                                 label<span class="op">=</span><span class="st">'100 sample means'</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay theoretical normal distribution</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>age_range <span class="op">=</span> np.linspace(age_means.<span class="bu">min</span>(), age_means.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>theoretical_pdf <span class="op">=</span> stats.norm.pdf(age_range, <span class="fl">24.13</span>, <span class="fl">18.61</span><span class="op">/</span>np.sqrt(<span class="dv">25</span>))</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>ax.plot(age_range, theoretical_pdf, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f'Theoretical N(24.13, </span><span class="sc">{</span><span class="fl">18.61</span><span class="op">/</span>np<span class="sc">.</span>sqrt(<span class="dv">25</span>)<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sample mean age (years)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Figure 3.3: Distribution of Sample Means from 1880 U.S. Census'</span>,</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Central Limit Theorem validated with real census data!"</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample means are approximately normal, even though ages are not."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ch03_The_Sample_Mean_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Central Limit Theorem validated with real census data!
Sample means are approximately normal, even though ages are not.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.6: CLT in Practice</strong></p>
<p>The Central Limit Theorem is not just a mathematical curiosity‚Äîit works with real data. Even when the population distribution is highly non-normal (like the skewed 1880 Census ages with heaping at multiples of 5), the distribution of sample means becomes approximately normal for moderate sample sizes. This validates using normal-based inference methods across diverse economic applications.</p>
</blockquote>
</section>
<section id="interpreting-the-simulation-results" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-the-simulation-results">Interpreting the Simulation Results</h3>
<p><strong>Key findings from 400 simulated coin toss samples:</strong></p>
<p><strong>1. Mean of simulated sample means = 0.5004 (vs.&nbsp;theoretical Œº = 0.5)</strong></p>
<ul>
<li>Perfect agreement (difference of only 0.0004)</li>
<li>This validates our simulation code</li>
<li>Confirms the theoretical prediction E[XÃÑ] = Œº</li>
</ul>
<p><strong>2. Standard deviation of simulated means = 0.0887 (vs.&nbsp;theoretical = 0.0913)</strong></p>
<ul>
<li>Close agreement (within 3%)</li>
<li>Theoretical: œÉ/‚àön = ‚àö(0.25/30) = 0.0913</li>
<li>The small difference is random simulation noise</li>
</ul>
<p><strong>3. Range of simulated means: 0.2667 to 0.7667</strong></p>
<ul>
<li>Matches the theoretical range well</li>
<li>About 95% of values fall within Œº ¬± 2œÉ(XÃÑ) = 0.5 ¬± 0.183</li>
<li>This is exactly what we‚Äôd expect from a normal distribution</li>
</ul>
<p><strong>4. Why simulation matters:</strong></p>
<ul>
<li><strong>Validation:</strong> We‚Äôve confirmed that theory matches practice</li>
<li><strong>Intuition:</strong> We can see the CLT in action, not just read about it</li>
<li><strong>Flexibility:</strong> We can simulate complex scenarios where theory is hard</li>
<li><strong>Modern econometrics:</strong> Bootstrap, Monte Carlo methods rely on simulation</li>
</ul>
<p><strong>5. Reproducibility with random seeds:</strong></p>
<ul>
<li>By setting <code>np.random.seed(10101)</code>, we get identical results every time</li>
<li>Essential for scientific reproducibility</li>
<li>In research, always document your random seed</li>
</ul>
<p><strong>6. The simulation matches the pre-computed data:</strong></p>
<ul>
<li>Earlier we loaded AED_COINTOSSMEANS.DTA with mean 0.4994, sd 0.0863</li>
<li>Our simulation gave mean 0.5004, sd 0.0887</li>
<li>These match closely (differences are just from different random seeds)</li>
</ul>
<p><strong>Economic interpretation:</strong> Modern econometric research heavily uses simulation methods (bootstrap standard errors, Monte Carlo integration, Bayesian MCMC). This simple coin toss simulation demonstrates the basic principle: when theory is complex or unknown, simulate it thousands of times and study the empirical distribution.</p>
<p><em>Having seen the Central Limit Theorem at work with both coins and census data, let‚Äôs formalize what makes the sample mean a good estimator.</em></p>
</section>
</section>
<section id="estimator-properties" class="level2">
<h2 class="anchored" data-anchor-id="estimator-properties">3.5 Estimator Properties</h2>
<p>Why use the sample mean <span class="math inline">\(\bar{X}\)</span> to estimate the population mean <span class="math inline">\(\mu\)</span>? Because it has desirable statistical properties:</p>
<p><strong>1. Unbiasedness:</strong> An estimator is <strong>unbiased</strong> if its expected value equals the parameter: <span class="math display">\[E[\bar{X}] = \mu\]</span></p>
<p>This means on average (across many samples), <span class="math inline">\(\bar{X}\)</span> equals <span class="math inline">\(\mu\)</span> (no systematic over- or under-estimation).</p>
<p><strong>2. Efficiency (Minimum Variance):</strong> Among all unbiased estimators, <span class="math inline">\(\bar{X}\)</span> has the <strong>smallest variance</strong> for many distributions (normal, Bernoulli, binomial, Poisson). An estimator with minimum variance is called <strong>efficient</strong> or <strong>best</strong>.</p>
<p><strong>3. Consistency:</strong> An estimator is <strong>consistent</strong> if it converges to the true parameter as <span class="math inline">\(n \to \infty\)</span>. For <span class="math inline">\(\bar{X}\)</span>:</p>
<ul>
<li><span class="math inline">\(E[\bar{X}] = \mu\)</span> (unbiased, no bias to disappear)</li>
<li><span class="math inline">\(Var[\bar{X}] = \sigma^2/n \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span> (variance shrinks to zero)</li>
</ul>
<p>Therefore <span class="math inline">\(\bar{X}\)</span> is consistent for <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Economic application:</strong> These properties justify using sample means to estimate average income, unemployment rates, GDP per capita, etc.</p>
<div id="cell-36" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Illustrate consistency: variance shrinks as n increases</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CONSISTENCY: VARIANCE SHRINKS AS n INCREASES"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">18.61</span>  <span class="co"># Census population std dev</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Population std deviation: œÉ = </span><span class="sc">{</span>sigma<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'Sample size n'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Var(XÃÑ) = œÉ¬≤/n'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'SD(XÃÑ) = œÉ/‚àön'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">55</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> sample_sizes:</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    var_xbar <span class="op">=</span> sigma<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    sd_xbar <span class="op">=</span> sigma <span class="op">/</span> np.sqrt(n)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>n<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>var_xbar<span class="sc">:&lt;20.2f}</span><span class="ss"> </span><span class="sc">{</span>sd_xbar<span class="sc">:&lt;20.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">As n ‚Üí ‚àû, Var(XÃÑ) ‚Üí 0 and SD(XÃÑ) ‚Üí 0"</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This guarantees XÃÑ converges to Œº (consistency)."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
CONSISTENCY: VARIANCE SHRINKS AS n INCREASES
======================================================================

Population std deviation: œÉ = 18.61

Sample size n   Var(XÃÑ) = œÉ¬≤/n       SD(XÃÑ) = œÉ/‚àön       
-------------------------------------------------------
5               69.27                8.3226              
10              34.63                5.8850              
25              13.85                3.7220              
50              6.93                 2.6319              
100             3.46                 1.8610              
500             0.69                 0.8323              
1000            0.35                 0.5885              
5000            0.07                 0.2632              

As n ‚Üí ‚àû, Var(XÃÑ) ‚Üí 0 and SD(XÃÑ) ‚Üí 0
This guarantees XÃÑ converges to Œº (consistency).</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.7: Properties of Good Estimators</strong></p>
<p>A good estimator should be unbiased (E[<span class="math inline">\(\bar{X}\)</span>] = <span class="math inline">\(\mu\)</span>), consistent (converges to <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>), and efficient (minimum variance among unbiased estimators). The sample mean <span class="math inline">\(\bar{X}\)</span> satisfies all three properties under simple random sampling, making it the preferred estimator of <span class="math inline">\(\mu\)</span> for most distributions.</p>
</blockquote>
</section>
<section id="computer-simulation-of-random-samples" class="level2">
<h2 class="anchored" data-anchor-id="computer-simulation-of-random-samples">3.6 Computer Simulation of Random Samples</h2>
<p>Modern statistics relies heavily on <strong>computer simulation</strong> to:</p>
<ol type="1">
<li>Generate random samples from known distributions</li>
<li>Study properties of estimators</li>
<li>Validate theoretical results</li>
</ol>
<p><strong>How computers generate randomness:</strong></p>
<p>Computers use <strong>pseudo-random number generators</strong> (PRNGs):</p>
<ul>
<li>Not truly random, but appear random for practical purposes</li>
<li>Generate values between 0 and 1 (uniform distribution)</li>
<li>Any value between 0 and 1 is equally likely</li>
<li>Successive values appear independent</li>
</ul>
<p><strong>Transforming uniform random numbers:</strong></p>
<p>From uniform U(0,1) random numbers, we can generate any distribution:</p>
<ul>
<li><strong>Coin toss:</strong> If <span class="math inline">\(U &gt; 0.5\)</span>, then <span class="math inline">\(X = 1\)</span> (heads), else <span class="math inline">\(X = 0\)</span> (tails)</li>
<li><strong>Normal distribution:</strong> Use Box-Muller transform or inverse CDF method</li>
<li><strong>Any distribution:</strong> Inverse transform sampling</li>
</ul>
<p><strong>Example - Coin toss simulation:</strong></p>
<ul>
<li>Draw uniform random number <span class="math inline">\(U \sim    ext{Uniform}(0,1)\)</span></li>
<li>If <span class="math inline">\(U &gt; 0.5\)</span>: heads (<span class="math inline">\(X=1\)</span>)</li>
<li>If <span class="math inline">\(U \leq 0.5\)</span>: tails (<span class="math inline">\(X=0\)</span>)</li>
<li>Repeat 30 times to simulate 30 coin tosses</li>
</ul>
<p><strong>Example - Census sampling:</strong></p>
<ul>
<li>Population: <span class="math inline">\(N = 50,169,452\)</span> people</li>
<li>If uniform draw falls in <span class="math inline">\([0, 1/N)\)</span>, select person 1</li>
<li>If uniform draw falls in <span class="math inline">\([1/N, 2/N)\)</span>, select person 2</li>
<li>Continue for all <span class="math inline">\(N\)</span> people</li>
</ul>
<p><strong>The importance of seeds:</strong></p>
<p>The <strong>seed</strong> is the starting value that determines the entire sequence:</p>
<ul>
<li>Same seed ‚Üí identical ‚Äúrandom‚Äù sequence (reproducibility)</li>
<li>Different seed ‚Üí different sequence</li>
<li><strong>Best practice:</strong> Always set seed in research code</li>
<li>Example: <code>np.random.seed(10101)</code></li>
</ul>
<p><strong>Why reproducibility matters:</strong></p>
<ul>
<li>Scientific research must be verifiable</li>
<li>Debugging requires consistent results</li>
<li>Publication standards demand reproducible results</li>
</ul>
<p>Let‚Äôs simulate the coin toss experiment ourselves!</p>
<div id="cell-39" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate 400 samples of 30 coin tosses</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SIMULATION: 400 SAMPLES OF 30 COIN TOSSES"</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10101</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>result_mean <span class="op">=</span> np.zeros(n_simulations)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>result_std <span class="op">=</span> np.zeros(n_simulations)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate sample of coin tosses (Bernoulli with p=0.5)</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, sample_size)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    result_mean[i] <span class="op">=</span> sample.mean()</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    result_std[i] <span class="op">=</span> sample.std(ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Simulation results:"</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Mean of 400 sample means:  </span><span class="sc">{</span>result_mean<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Std dev of 400 means:      </span><span class="sc">{</span>result_mean<span class="sc">.</span>std()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Min sample mean:           </span><span class="sc">{</span>result_mean<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Max sample mean:           </span><span class="sc">{</span>result_mean<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Theoretical values:"</span>)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  E[XÃÑ] = Œº:                  0.5000"</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  œÉ(XÃÑ) = œÉ/‚àön:               </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(<span class="fl">0.25</span><span class="op">/</span><span class="dv">30</span>)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Perfect match between simulation and theory!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
SIMULATION: 400 SAMPLES OF 30 COIN TOSSES
======================================================================

Simulation results:
  Mean of 400 sample means:  0.5004
  Std dev of 400 means:      0.0887
  Min sample mean:           0.2667
  Max sample mean:           0.7667

Theoretical values:
  E[XÃÑ] = Œº:                  0.5000
  œÉ(XÃÑ) = œÉ/‚àön:               0.0913

Perfect match between simulation and theory!</code></pre>
</div>
</div>
<p>This figure shows our simulated distribution (green) overlaid with the theoretical normal distribution (red). They match almost perfectly!</p>
<div id="cell-41" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize simulated distribution</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>ax.hist(result_mean, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'lightgreen'</span>,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">'Simulated (400 samples)'</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay theoretical normal distribution</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(result_mean.<span class="bu">min</span>(), result_mean.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>theoretical_pdf <span class="op">=</span> stats.norm.pdf(x_range, <span class="fl">0.5</span>, np.sqrt(<span class="fl">0.25</span><span class="op">/</span><span class="dv">30</span>))</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>ax.plot(x_range, theoretical_pdf, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">'Theoretical N(0.5, 0.091)'</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sample mean'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Simulated vs Theoretical Sampling Distribution'</span>,</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Simulation perfectly replicates theoretical predictions!"</span>)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"This validates both our code and the underlying theory."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ch03_The_Sample_Mean_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Simulation perfectly replicates theoretical predictions!
This validates both our code and the underlying theory.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.9: Monte Carlo Simulation</strong></p>
<p>Computers generate pseudo-random numbers using deterministic algorithms that produce sequences appearing random. Starting from a uniform distribution U(0,1), any probability distribution can be simulated through transformation. The seed determines the sequence, making results reproducible‚Äîcritical for scientific research. Always set and document your random seed.</p>
</blockquote>
<p><strong>Transition:</strong> So far we‚Äôve assumed simple random sampling where all observations are independent and identically distributed. But what happens when this assumption is violated? Let‚Äôs explore alternative sampling methods.</p>
</section>
<section id="samples-other-than-simple-random-samples" class="level2">
<h2 class="anchored" data-anchor-id="samples-other-than-simple-random-samples">3.7 Samples other than Simple Random Samples</h2>
<p>The simple random sample assumptions (A-C from Section 3.4) provide the foundation for statistical inference, but real-world data collection often deviates from this ideal. Understanding these deviations is critical for proper analysis.</p>
<p><strong>Recall simple random sample assumptions:</strong></p>
<ul>
<li>A. Common mean: <span class="math inline">\(\mathrm{E}[X_i] = \mu\)</span> for all <span class="math inline">\(i\)</span></li>
<li>B. Common variance: <span class="math inline">\(\operatorname{Var}[X_i] = \sigma^2\)</span> for all <span class="math inline">\(i\)</span></li>
<li>C. Statistical independence: <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are independent</li>
</ul>
<p><strong>Two types of deviations:</strong></p>
<ol type="1">
<li><strong>Representative samples (relaxes assumption C only):</strong>
<ul>
<li>Still from the same distribution (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> constant)</li>
<li>But observations are NO LONGER independent</li>
<li>Example: Cluster sampling (surveying all students in randomly selected schools)</li>
<li><strong>Solution:</strong> Adjust the standard error formula to account for dependence</li>
</ul></li>
<li><strong>Nonrepresentative samples (violates assumption A):</strong>
<ul>
<li>Different observations have DIFFERENT population means</li>
<li>Example: Surveying Golf Digest readers to estimate average U.S. income</li>
<li>Golf magazine readers have higher income than general population (<span class="math inline">\(\mu_i \neq \mu\)</span>)</li>
<li><strong>Big problem</strong> - standard inference methods fail completely</li>
<li><strong>Solution:</strong> Use weighted means if inclusion probabilities are known</li>
</ul></li>
</ol>
<p><strong>Weighted Mean Approach:</strong></p>
<p>When inclusion probabilities <span class="math inline">\(\pi_i\)</span> are known, construct weighted estimates:</p>
<ul>
<li><span class="math inline">\(\pi_i\)</span> = probability that observation <span class="math inline">\(i\)</span> is included in the sample</li>
<li>Sample weight: <span class="math inline">\(w_i = 1/\pi_i\)</span> (inverse probability weighting)</li>
<li>Weighted mean: <span class="math display">\[\bar{x}_w = \frac{\sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i}\]</span></li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>Suppose women have 70% probability of inclusion (<span class="math inline">\(\pi_{female} = 0.7\)</span>, <span class="math inline">\(w_{female} = 1.43\)</span>)</li>
<li>Men have 30% probability of inclusion (<span class="math inline">\(\pi_{male} = 0.3\)</span>, <span class="math inline">\(w_{male} = 3.33\)</span>)</li>
<li>Weighted mean corrects for oversampling of women</li>
</ul>
<p><strong>Economic applications:</strong></p>
<ul>
<li>Household surveys often oversample certain groups (low-income, minorities)</li>
<li>Survey weights correct for unequal sampling probabilities</li>
<li>Major surveys (CPS, ACS, PSID) provide sampling weights in datasets</li>
</ul>
<div id="cell-44" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate weighted vs. unweighted mean</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WEIGHTED MEAN EXAMPLE"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate population with two groups</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>n_men <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>n_women <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Men have higher average income</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>income_men <span class="op">=</span> np.random.normal(<span class="dv">60000</span>, <span class="dv">15000</span>, n_men)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>income_women <span class="op">=</span> np.random.normal(<span class="dv">50000</span>, <span class="dv">15000</span>, n_women)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>true_pop_mean <span class="op">=</span> (income_men.mean() <span class="op">+</span> income_women.mean()) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">True population means:"</span>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Men:   $</span><span class="sc">{</span>income_men<span class="sc">.</span>mean()<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Women: $</span><span class="sc">{</span>income_women<span class="sc">.</span>mean()<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Overall: $</span><span class="sc">{</span>true_pop_mean<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Biased sample: oversample women (70% women, 30% men)</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>sample_men <span class="op">=</span> np.random.choice(income_men, size<span class="op">=</span><span class="dv">15</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>sample_women <span class="op">=</span> np.random.choice(income_women, size<span class="op">=</span><span class="dv">35</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> np.concatenate([sample_men, sample_women])</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Unweighted mean (WRONG - biased toward women)</span></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>unweighted_mean <span class="op">=</span> sample.mean()</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Weighted mean (CORRECT - accounts for oversampling)</span></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.concatenate([np.repeat(<span class="dv">1</span><span class="op">/</span><span class="fl">0.3</span>, <span class="dv">15</span>), np.repeat(<span class="dv">1</span><span class="op">/</span><span class="fl">0.7</span>, <span class="dv">35</span>)])</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>weighted_mean <span class="op">=</span> np.average(sample, weights<span class="op">=</span>weights)</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample estimates:"</span>)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Unweighted mean: $</span><span class="sc">{</span>unweighted_mean<span class="sc">:,.0f}</span><span class="ss"> (biased toward women)"</span>)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Weighted mean:   $</span><span class="sc">{</span>weighted_mean<span class="sc">:,.0f}</span><span class="ss"> (corrected)"</span>)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Bias:"</span>)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Unweighted bias: $</span><span class="sc">{</span>unweighted_mean <span class="op">-</span> true_pop_mean<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Weighted bias:   $</span><span class="sc">{</span>weighted_mean <span class="op">-</span> true_pop_mean<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Weighting corrects for nonrepresentative sampling!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 3.8: Simple Random Sampling Assumptions</strong></p>
<p>Simple random sampling assumes all observations come from the same distribution with common mean <span class="math inline">\(\mu\)</span>. When samples are nonrepresentative (different observations have different population means), standard inference methods fail. Weighted means can correct for this if inclusion probabilities <span class="math inline">\(\pi_i\)</span> are known, with weights <span class="math inline">\(w_i = 1/\pi_i\)</span> applied to each observation.</p>
</blockquote>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<p><strong>Random Variables and Sampling Distributions:</strong></p>
<ul>
<li>A random variable <span class="math inline">\(X\)</span> (uppercase) represents an uncertain outcome; its realization <span class="math inline">\(x\)</span> (lowercase) is the observed value</li>
<li>The sample mean <span class="math inline">\(\bar{x}\)</span> is ONE realization of the random variable <span class="math inline">\(\bar{X} = (X_1 + \cdots + X_n)/n\)</span></li>
<li>The sampling distribution of <span class="math inline">\(\bar{X}\)</span> describes how <span class="math inline">\(\bar{x}\)</span> varies across different samples from the same population</li>
<li>Understanding that statistics are random variables is the foundation of statistical inference</li>
<li>Example: Drawing 400 samples of coin tosses (n=30 each) produces 400 different sample means, revealing <span class="math inline">\(\bar{X}\)</span>‚Äôs distribution</li>
</ul>
<p><strong>Properties of the Sample Mean (Theoretical Results):</strong></p>
<ul>
<li>Under simple random sampling (common mean <span class="math inline">\(\mu\)</span>, common variance <span class="math inline">\(\sigma^2\)</span>, independence):</li>
<li>Mean: <span class="math inline">\(\mathrm{E}[\bar{X}] = \mu\)</span> (unbiased estimator)</li>
<li>Variance: <span class="math inline">\(\operatorname{Var}[\bar{X}] = \sigma^2/n\)</span> (precision increases with sample size)</li>
<li>Standard deviation: <span class="math inline">\(\sigma_{\bar{X}} = \sigma/\sqrt{n}\)</span> (decreases at rate <span class="math inline">\(1/\sqrt{n}\)</span>)</li>
<li>To halve the standard error, you must quadruple the sample size (e.g., from n=100 to n=400)</li>
<li>The sample mean is less variable than individual observations since <span class="math inline">\(\sigma^2/n &lt; \sigma^2\)</span></li>
<li>As <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\bar{X}\)</span> converges to <span class="math inline">\(\mu\)</span> because <span class="math inline">\(\operatorname{Var}[\bar{X}] \rightarrow 0\)</span></li>
</ul>
<p><strong>Central Limit Theorem (Most Important Result):</strong></p>
<ul>
<li>For large <span class="math inline">\(n\)</span>, <span class="math inline">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span> approximately</li>
<li>Equivalently: <span class="math inline">\(Z = (\bar{X} - \mu)/(\sigma/\sqrt{n}) \sim N(0,1)\)</span> approximately</li>
<li>This holds <strong>regardless of the distribution of <span class="math inline">\(X\)</span></strong> (as long as finite mean and variance exist)</li>
<li>Rule of thumb: CLT works well for <span class="math inline">\(n \geq 30\)</span> in most cases</li>
<li>Empirical evidence: Coin tosses (binary) ‚Üí normal distribution of means; Census ages (highly skewed) ‚Üí normal distribution of means</li>
<li>Why this matters: Justifies using normal-based inference methods (confidence intervals, hypothesis tests) for almost any problem</li>
</ul>
<p><strong>Standard Error (Estimated Standard Deviation):</strong></p>
<ul>
<li>Population standard deviation <span class="math inline">\(\sigma_{\bar{X}} = \sigma/\sqrt{n}\)</span> is unknown because <span class="math inline">\(\sigma\)</span> is unknown</li>
<li>Standard error: se(<span class="math inline">\(\bar{X}\)</span>) = <span class="math inline">\(s/\sqrt{n}\)</span> where <span class="math inline">\(s\)</span> is sample standard deviation</li>
<li>‚ÄúStandard error‚Äù means ‚Äúestimated standard deviation‚Äù (applies to any estimator, not just <span class="math inline">\(\bar{X}\)</span>)</li>
<li>Measures precision of <span class="math inline">\(\bar{x}\)</span> as an estimate of <span class="math inline">\(\mu\)</span>‚Äîsmaller is better</li>
<li>Example: Coin toss with n=30 gives se ‚âà 0.091; Census with n=25 gives se ‚âà 3.72 years</li>
<li>Used to construct confidence intervals and conduct hypothesis tests (Chapter 4)</li>
</ul>
<p><strong>Desirable Estimator Properties:</strong></p>
<ul>
<li><strong>Unbiased:</strong> <span class="math inline">\(\mathrm{E}[\bar{X}] = \mu\)</span> (correct on average, no systematic error)</li>
<li><strong>Efficient:</strong> Minimum variance among unbiased estimators (most precise)</li>
<li><strong>Consistent:</strong> Converges to <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> (guaranteed by unbiasedness + variance ‚Üí 0)</li>
<li>The sample mean <span class="math inline">\(\bar{X}\)</span> satisfies all three properties under simple random sampling</li>
<li>For normal, Bernoulli, binomial, and Poisson distributions, <span class="math inline">\(\bar{X}\)</span> is the best unbiased estimator</li>
<li>Sample median is also unbiased (for symmetric distributions) but has higher variance than <span class="math inline">\(\bar{X}\)</span></li>
</ul>
<p><strong>Empirical Validation:</strong></p>
<ul>
<li>Coin toss experiment (400 samples, n=30 each):</li>
<li>Mean of sample means: 0.4994 vs.&nbsp;theoretical 0.5000</li>
<li>SD of sample means: 0.0863 vs.&nbsp;theoretical 0.0913</li>
<li>Approximately normal distribution</li>
<li>Census ages (100 samples, n=25 each):</li>
<li>Mean of sample means: 23.78 vs.&nbsp;theoretical 24.13</li>
<li>SD of sample means: 3.76 vs.&nbsp;theoretical 3.72</li>
<li>Normal distribution despite highly non-normal population</li>
<li>Computer simulation replicates theoretical results perfectly</li>
</ul>
<p><strong>Economic Applications:</strong></p>
<ul>
<li>Estimating average income, consumption, wages from household surveys</li>
<li>Public opinion polling (sample proportion is a special case of sample mean)</li>
<li>Macroeconomic indicators: unemployment rate, inflation, GDP growth (all based on samples)</li>
<li>Quality control: manufacturing processes use sample means to monitor production</li>
<li>Clinical trials: comparing average outcomes between treatment and control groups</li>
<li>All regression coefficients (Chapters 6-7) have sampling distributions just like <span class="math inline">\(\bar{X}\)</span></li>
</ul>
<p><strong>Connection to Statistical Inference (Chapter 4):</strong></p>
<ul>
<li>This chapter provides the theoretical foundation for confidence intervals</li>
<li>We know <span class="math inline">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span> approximately</li>
<li>This allows us to make probability statements about how far <span class="math inline">\(\bar{x}\)</span> is from <span class="math inline">\(\mu\)</span></li>
<li>Example: Pr(<span class="math inline">\(\mu - 1.96 \cdot \sigma/\sqrt{n} &lt; \bar{X} &lt; \mu + 1.96 \cdot \sigma/\sqrt{n}\)</span>) ‚âà 0.95</li>
<li>Rearranging gives 95% confidence interval: <span class="math inline">\(\bar{x} \pm 1.96 \cdot s/\sqrt{n}\)</span></li>
</ul>
<p><strong>Key Formulas to Remember:</strong></p>
<ol type="1">
<li>Mean of random variable: <span class="math inline">\(\mu = \mathrm{E}[X] = \sum_x x \cdot \mathrm{Pr}[X=x]\)</span></li>
<li>Variance: <span class="math inline">\(\sigma^2 = \mathrm{E}[(X-\mu)^2] = \sum_x (x-\mu)^2 \cdot \mathrm{Pr}[X=x]\)</span></li>
<li>Sample mean: <span class="math inline">\(\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\)</span></li>
<li>Sample variance: <span class="math inline">\(s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2\)</span></li>
<li>Mean of sample mean: <span class="math inline">\(\mathrm{E}[\bar{X}] = \mu\)</span></li>
<li>Variance of sample mean: <span class="math inline">\(\operatorname{Var}[\bar{X}] = \sigma^2/n\)</span></li>
<li>Standard error: se(<span class="math inline">\(\bar{X}\)</span>) = <span class="math inline">\(s/\sqrt{n}\)</span></li>
<li>Standardized sample mean: <span class="math inline">\(Z = (\bar{X} - \mu)/(\sigma/\sqrt{n}) \sim N(0,1)\)</span></li>
</ol>
<hr>
<p><strong>Next Steps:</strong></p>
<ul>
<li><strong>Chapter 4</strong> uses these results to construct confidence intervals and test hypotheses about <span class="math inline">\(\mu\)</span></li>
<li><strong>Chapters 6-7</strong> extend the same logic to regression coefficients (which are also sample statistics with sampling distributions)</li>
<li>The conceptual framework developed here applies to ALL statistical inference in econometrics</li>
</ul>
</section>
<section id="practice-exercises" class="level2">
<h2 class="anchored" data-anchor-id="practice-exercises">Practice Exercises</h2>
<p>Test your understanding of the sample mean and sampling distributions:</p>
<p><strong>Exercise 1:</strong> Random variable properties</p>
<ul>
<li>Suppose <span class="math inline">\(X = 100\)</span> with probability 0.8 and <span class="math inline">\(X = 600\)</span> with probability 0.2</li>
<li><ol type="a">
<li>Calculate the mean <span class="math inline">\(\mu = \mathrm{E}[X]\)</span></li>
</ol></li>
<li><ol start="2" type="a">
<li>Calculate the variance <span class="math inline">\(\sigma^2 = \mathrm{E}[(X-\mu)^2]\)</span></li>
</ol></li>
<li><ol start="3" type="a">
<li>Calculate the standard deviation <span class="math inline">\(\sigma\)</span></li>
</ol></li>
</ul>
<p><strong>Exercise 2:</strong> Sample mean properties</p>
<ul>
<li>Consider random samples of size <span class="math inline">\(n = 25\)</span> from a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu = 100\)</span> and variance <span class="math inline">\(\sigma^2 = 400\)</span></li>
<li><ol type="a">
<li>What is the mean of the sample mean <span class="math inline">\(\bar{X}\)</span>?</li>
</ol></li>
<li><ol start="2" type="a">
<li>What is the variance of the sample mean <span class="math inline">\(\bar{X}\)</span>?</li>
</ol></li>
<li><ol start="3" type="a">
<li>What is the standard deviation (standard error) of the sample mean?</li>
</ol></li>
</ul>
<p><strong>Exercise 3:</strong> Central Limit Theorem application</p>
<ul>
<li>A population has mean <span class="math inline">\(\mu = 50\)</span> and standard deviation <span class="math inline">\(\sigma = 12\)</span></li>
<li>You draw a random sample of size <span class="math inline">\(n = 64\)</span></li>
<li><ol type="a">
<li>What is the approximate distribution of <span class="math inline">\(\bar{X}\)</span> (by the CLT)?</li>
</ol></li>
<li><ol start="2" type="a">
<li>What is the probability that <span class="math inline">\(\bar{X}\)</span> falls between 48 and 52?</li>
</ol></li>
<li><ol start="3" type="a">
<li>Would this probability be larger or smaller if <span class="math inline">\(n = 144\)</span>? Why?</li>
</ol></li>
</ul>
<p><strong>Exercise 4:</strong> Standard error interpretation</p>
<ul>
<li>Two researchers estimate average income in a city
<ul>
<li>Researcher A uses <span class="math inline">\(n = 100\)</span>, gets <span class="math inline">\(\bar{x}_A = \$52,000\)</span>, <span class="math inline">\(s_A = \$15,000\)</span></li>
<li>Researcher B uses <span class="math inline">\(n = 400\)</span>, gets <span class="math inline">\(\bar{x}_B = \$54,000\)</span>, <span class="math inline">\(s_B = \$16,000\)</span></li>
</ul></li>
<li><ol type="a">
<li>Calculate the standard error for each researcher</li>
</ol></li>
<li><ol start="2" type="a">
<li>Which estimate is more precise? Why?</li>
</ol></li>
<li><ol start="3" type="a">
<li>Are these estimates statistically different? (Compare the difference to the standard errors)</li>
</ol></li>
</ul>
<p><strong>Exercise 5:</strong> Consistency</p>
<ul>
<li>Explain why the sample mean <span class="math inline">\(\bar{X}\)</span> is a consistent estimator of <span class="math inline">\(\mu\)</span></li>
<li>Show that both conditions for consistency are satisfied: bias ‚Üí 0 and variance ‚Üí 0 as <span class="math inline">\(n \rightarrow \infty\)</span></li>
</ul>
<p><strong>Exercise 6:</strong> Simulation</p>
<ul>
<li>Using Python, simulate 1,000 samples of size <span class="math inline">\(n = 50\)</span> from a uniform distribution U(0, 10)</li>
<li><ol type="a">
<li>Calculate the sample mean for each of the 1,000 samples</li>
</ol></li>
<li><ol start="2" type="a">
<li>Compute the mean and standard deviation of these 1,000 sample means</li>
</ol></li>
<li><ol start="3" type="a">
<li>Compare with theoretical values: <span class="math inline">\(\mu = 5\)</span>, <span class="math inline">\(\sigma^2/n = (100/12)/50 = 0.1667\)</span></li>
</ol></li>
<li><ol start="4" type="a">
<li>Create a histogram and verify approximate normality</li>
</ol></li>
</ul>
<p><strong>Exercise 7:</strong> Sample size calculation</p>
<ul>
<li>You want to estimate average household expenditure on food with a standard error of $10</li>
<li>From pilot data, you know <span class="math inline">\(\sigma \approx \$80\)</span></li>
<li><ol type="a">
<li>What sample size <span class="math inline">\(n\)</span> do you need?</li>
</ol></li>
<li><ol start="2" type="a">
<li>If you double the desired precision (se = $5), how does the required sample size change?</li>
</ol></li>
</ul>
<p><strong>Exercise 8:</strong> Unbiasedness vs.&nbsp;efficiency</p>
<ul>
<li>The sample median is also an unbiased estimator of <span class="math inline">\(\mu\)</span> when the population is symmetric</li>
<li><ol type="a">
<li>Explain what ‚Äúunbiased‚Äù means in this context</li>
</ol></li>
<li><ol start="2" type="a">
<li>Why do we prefer the sample mean to the sample median for estimating <span class="math inline">\(\mu\)</span>?</li>
</ol></li>
<li><ol start="3" type="a">
<li>For what type of population distribution might the median be preferable?</li>
</ol></li>
</ul>
<hr>
</section>
<section id="case-studies" class="level2">
<h2 class="anchored" data-anchor-id="case-studies">Case Studies</h2>
<p>Now that you‚Äôve learned about the sample mean, sampling distributions, and the Central Limit Theorem, let‚Äôs apply these concepts to real economic data using the <strong>Economic Convergence Clubs</strong> dataset.</p>
<p><strong>Why case studies matter:</strong></p>
<ul>
<li>Bridge theory and practice: Move from abstract sampling concepts to real data analysis</li>
<li>Build analytical skills: Practice computing sample statistics and understanding variability</li>
<li>Develop statistical intuition: See how sample size affects precision and distribution shape</li>
<li>Connect to research: Apply fundamental concepts to cross-country economic comparisons</li>
</ul>
<section id="case-study-1-sampling-distributions-of-labor-productivity" class="level3">
<h3 class="anchored" data-anchor-id="case-study-1-sampling-distributions-of-labor-productivity">Case Study 1: Sampling Distributions of Labor Productivity</h3>
<p><strong>Research Question:</strong> How does average labor productivity vary across different samples of countries? How does sample size affect the precision of our estimates?</p>
<p><strong>Background:</strong> In Chapter 1-2, you explored productivity levels across countries. Now we shift to understanding <strong>sampling variability</strong>‚Äîhow sample means vary when we draw different samples from the population.</p>
<p>This is fundamental to statistical inference: if we only observe a sample of countries (say, 20 out of 108), how confident can we be that our sample mean approximates the true population mean? The Central Limit Theorem tells us that sample means follow a normal distribution (even if the underlying data don‚Äôt), with variability decreasing as sample size increases.</p>
<p><strong>The Data:</strong> We‚Äôll use the convergence clubs dataset (Mendez, 2020) to explore sampling distributions:</p>
<ul>
<li><strong>Population:</strong> 108 countries observed from 1990-2014</li>
<li><strong>Key variable:</strong> <code>lp</code> (labor productivity, output per worker)</li>
<li><strong>Task:</strong> Draw multiple random samples, compute sample means, and observe the distribution</li>
</ul>
<p><strong>Your Task:</strong> Use Chapter 3‚Äôs tools (sample mean, sample variance, Central Limit Theorem) to understand how sample statistics vary and how sample size affects precision.</p>
<hr>
<blockquote class="blockquote">
<p><strong>Key Concept 3.10: Sampling Distribution and the Central Limit Theorem</strong></p>
<p>The <strong>sampling distribution of the mean</strong> shows how sample means <span class="math inline">\(\bar{y}\)</span> vary across different random samples drawn from the same population. Key properties:</p>
<ol type="1">
<li><strong>Mean of sampling distribution = population mean</strong>: <span class="math inline">\(E[\bar{y}] = \mu\)</span></li>
<li><strong>Standard error decreases with sample size</strong>: <span class="math inline">\(SE(\bar{y}) = \sigma/\sqrt{n}\)</span></li>
<li><strong>Central Limit Theorem</strong>: For large <span class="math inline">\(n\)</span> (typically <span class="math inline">\(n \geq 30\)</span>), <span class="math inline">\(\bar{y}\)</span> is approximately normally distributed, <strong>regardless</strong> of the population distribution</li>
</ol>
<p>This is why we can use normal-based inference methods even for non-normal economic data like earnings and wealth distributions.</p>
</blockquote>
<hr>
</section>
<section id="how-to-use-these-tasks" class="level3">
<h3 class="anchored" data-anchor-id="how-to-use-these-tasks">How to Use These Tasks</h3>
<p><strong>Progressive difficulty:</strong></p>
<ul>
<li><strong>Tasks 1-2:</strong> Guided (detailed instructions, code provided)</li>
<li><strong>Tasks 3-4:</strong> Semi-guided (moderate guidance, you write most code)</li>
<li><strong>Tasks 5-6:</strong> Independent (minimal guidance, design your own analysis)</li>
</ul>
<p><strong>Work incrementally:</strong> Complete tasks in order. Each builds on previous concepts.</p>
<hr>
<section id="task-1-explore-the-population-distribution-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-1-explore-the-population-distribution-guided">Task 1: Explore the Population Distribution (Guided)</h4>
<p><strong>Objective:</strong> Load the convergence clubs data and examine the population distribution of labor productivity.</p>
<p><strong>Instructions:</strong> Run the code below to load data and visualize the population distribution.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import required libraries</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load convergence clubs dataset</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a> <span class="st">"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv"</span>,</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a> index_col<span class="op">=</span>[<span class="st">"country"</span>, <span class="st">"year"</span>]</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>).sort_index()</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract 2014 data (most recent year) for cross-sectional analysis</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>df_2014 <span class="op">=</span> df.loc[(<span class="bu">slice</span>(<span class="va">None</span>), <span class="dv">2014</span>), <span class="st">'lp'</span>].dropna()</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Population size: </span><span class="sc">{</span><span class="bu">len</span>(df_2014)<span class="sc">}</span><span class="ss"> countries"</span>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Population mean: $</span><span class="sc">{</span>df_2014<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Population std dev: $</span><span class="sc">{</span>df_2014<span class="sc">.</span>std()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize population distribution</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>ax.hist(df_2014, bins<span class="op">=</span><span class="dv">20</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>ax.axvline(df_2014.mean(), color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Mean = $</span><span class="sc">{</span>df_2014<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Labor Productivity (thousands, 2011 USD PPP)'</span>)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Population Distribution of Labor Productivity (2014, 108 countries)'</span>)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Check normality</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Skewness: </span><span class="sc">{</span>stats<span class="sc">.</span>skew(df_2014)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Note: Population distribution is right-skewed (not normal)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What to observe:</strong></p>
<ul>
<li>Is the population distribution normal or skewed?</li>
<li>What is the population mean and standard deviation?</li>
<li>Note: Despite non-normality, the CLT will ensure sample means are approximately normal for large samples!</li>
</ul>
<hr>
</section>
<section id="task-2-draw-a-single-sample-and-compute-the-sample-mean-semi-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-2-draw-a-single-sample-and-compute-the-sample-mean-semi-guided">Task 2: Draw a Single Sample and Compute the Sample Mean (Semi-guided)</h4>
<p><strong>Objective:</strong> Draw a random sample of size <span class="math inline">\(n=30\)</span> and compute the sample mean.</p>
<p><strong>Instructions:</strong></p>
<ol type="1">
<li>Use <code>np.random.choice()</code> to draw a random sample of size 30 from the population</li>
<li>Compute the sample mean using <code>.mean()</code></li>
<li>Compare the sample mean to the population mean</li>
<li>Repeat this process 2-3 times (with different random seeds) to see how the sample mean varies</li>
</ol>
<p><strong>Starter code:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a random sample of size 30</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> np.random.choice(df_2014, size<span class="op">=</span>n, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sample mean</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> sample.mean()</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample mean: $</span><span class="sc">{</span>sample_mean<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Population mean: $</span><span class="sc">{</span>df_2014<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Difference: $</span><span class="sc">{</span>sample_mean <span class="op">-</span> df_2014<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Question: How close is the sample mean to the population mean?</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Questions:</strong></p>
<ul>
<li>How much does the sample mean differ from the population mean?</li>
<li>If you draw another sample, will you get the same sample mean?</li>
<li>What does this variability tell you about using samples for inference?</li>
</ul>
<hr>
</section>
<section id="task-3-simulate-the-sampling-distribution-semi-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-3-simulate-the-sampling-distribution-semi-guided">Task 3: Simulate the Sampling Distribution (Semi-guided)</h4>
<p><strong>Objective:</strong> Draw 1000 random samples of size <span class="math inline">\(n=30\)</span> and plot the distribution of sample means.</p>
<p><strong>Instructions:</strong></p>
<ol type="1">
<li>Use a loop to draw 1000 samples, each of size <span class="math inline">\(n=30\)</span></li>
<li>For each sample, compute and store the sample mean</li>
<li>Plot a histogram of the 1000 sample means</li>
<li>Compare this sampling distribution to the theoretical prediction from the CLT</li>
</ol>
<p><strong>Hint:</strong> The CLT predicts that sample means should be normally distributed with:</p>
<ul>
<li>Mean = <span class="math inline">\(\mu\)</span> (population mean)</li>
<li>Standard error = <span class="math inline">\(\sigma/\sqrt{n}\)</span> (population std / sqrt(sample size))</li>
</ul>
<p><strong>Example structure:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate sampling distribution</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>sample_means <span class="op">=</span> []</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a> sample <span class="op">=</span> np.random.choice(df_2014, size<span class="op">=</span>n, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a> sample_means.append(sample.mean())</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>sample_means <span class="op">=</span> np.array(sample_means)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot sampling distribution</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>ax.hist(sample_means, bins<span class="op">=</span><span class="dv">30</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>ax.axvline(df_2014.mean(), color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Population mean'</span>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>ax.axvline(sample_means.mean(), color<span class="op">=</span><span class="st">'blue'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Mean of sample means'</span>)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sample Mean (thousands, 2011 USD PPP)'</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'Sampling Distribution of the Mean (n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss"> samples)'</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare empirical vs theoretical</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Population mean (Œº): $</span><span class="sc">{</span>df_2014<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean of sample means: $</span><span class="sc">{</span>sample_means<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Theoretical SE (œÉ/‚àön): $</span><span class="sc">{</span>df_2014<span class="sc">.</span>std()<span class="op">/</span>np<span class="sc">.</span>sqrt(n)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Empirical SE (std of sample means): $</span><span class="sc">{</span>sample_means<span class="sc">.</span>std()<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Questions:</strong></p>
<ul>
<li>Does the distribution of sample means look approximately normal (even though the population distribution was skewed)?</li>
<li>How close is the mean of sample means to the population mean?</li>
<li>How close is the empirical standard error to the theoretical prediction?</li>
</ul>
<hr>
<blockquote class="blockquote">
<p><strong>Key Concept 3.11: Standard Error and Precision</strong></p>
<p>The <strong>standard error</strong> <span class="math inline">\(SE(\bar{y}) = \sigma/\sqrt{n}\)</span> measures the typical distance between a sample mean and the population mean. Key insights:</p>
<ol type="1">
<li><strong>Decreases with sample size</strong>: Doubling the sample size reduces SE by factor of <span class="math inline">\(\sqrt{2} \approx 1.41\)</span></li>
<li><strong>Trade-off</strong>: Larger samples cost more (time/money) but provide more precise estimates</li>
<li><strong>Diminishing returns</strong>: Going from <span class="math inline">\(n=25\)</span> to <span class="math inline">\(n=100\)</span> reduces SE by half, but <span class="math inline">\(n=100\)</span> to <span class="math inline">\(n=400\)</span> also reduces by half</li>
</ol>
<p>In economic research, sample size is often limited by data availability, requiring careful attention to precision.</p>
</blockquote>
<hr>
</section>
<section id="task-4-investigate-the-effect-of-sample-size-more-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-4-investigate-the-effect-of-sample-size-more-independent">Task 4: Investigate the Effect of Sample Size (More Independent)</h4>
<p><strong>Objective:</strong> Compare sampling distributions for different sample sizes (<span class="math inline">\(n = 10, 30, 50, 100\)</span>).</p>
<p><strong>Instructions:</strong></p>
<ol type="1">
<li>For each sample size, simulate 1000 samples and compute sample means</li>
<li>Plot the four sampling distributions on the same graph (or use subplots)</li>
<li>Compare the standard errors across sample sizes</li>
<li>Verify that <span class="math inline">\(SE\)</span> decreases as <span class="math inline">\(1/\sqrt{n}\)</span></li>
</ol>
<p><strong>Key questions to answer:</strong></p>
<ul>
<li>How does the shape of the sampling distribution change with sample size?</li>
<li>How much does precision improve when you quadruple the sample size (e.g., <span class="math inline">\(n=25\)</span> to <span class="math inline">\(n=100\)</span>)?</li>
<li>At what sample size does the distribution start to look clearly normal?</li>
</ul>
<hr>
</section>
<section id="task-5-comparing-high-income-vs-developing-countries-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-5-comparing-high-income-vs-developing-countries-independent">Task 5: Comparing High-Income vs Developing Countries (Independent)</h4>
<p><strong>Objective:</strong> Investigate whether sampling distributions differ for subpopulations (high-income vs developing countries).</p>
<p><strong>Instructions:</strong></p>
<ol type="1">
<li>Split the 2014 data by <code>hi1990</code> (high-income indicator)</li>
<li>For each group, simulate the sampling distribution of the mean (use <span class="math inline">\(n=20\)</span>, 1000 samples)</li>
<li>Plot both sampling distributions on the same graph</li>
<li>Compare means and standard errors between groups</li>
</ol>
<p><strong>Research question:</strong> Do high-income and developing countries have different average productivity levels? How confident can we be in this difference based on samples?</p>
<p><strong>Hints:</strong></p>
<ul>
<li>Use <code>df.loc[(slice(None), 2014), ['lp', 'hi1990']].dropna()</code> to get both variables</li>
<li>Filter by <code>hi1990 == 'yes'</code> and <code>hi1990 == 'no'</code></li>
<li>Compare population means and sampling distribution characteristics</li>
</ul>
<hr>
</section>
<section id="task-6-design-your-own-sampling-experiment-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-6-design-your-own-sampling-experiment-independent">Task 6: Design Your Own Sampling Experiment (Independent)</h4>
<p><strong>Objective:</strong> Explore a question of your choice using sampling distributions.</p>
<p><strong>Choose ONE of the following:</strong></p>
<p><strong>Option A:</strong> Effect of outliers on sample means</p>
<ul>
<li>Remove the top 5% most productive countries (outliers)</li>
<li>Compare sampling distributions with vs without outliers</li>
<li>Question: How sensitive is the sample mean to extreme values?</li>
</ul>
<p><strong>Option B:</strong> Time trends in sampling distributions</p>
<ul>
<li>Compare sampling distributions for years 1990, 2000, 2010, 2014</li>
<li>Question: Has average productivity increased over time? Has variability changed?</li>
</ul>
<p><strong>Option C:</strong> Regional sampling distributions</p>
<ul>
<li>Split countries by region (use <code>region</code> variable)</li>
<li>Compare sampling distributions across regions</li>
<li>Question: Which regions show the highest/lowest productivity? Most/least variability?</li>
</ul>
<p><strong>Your analysis should include:</strong></p>
<ol type="1">
<li>Clear research question</li>
<li>Appropriate sample size(s)</li>
<li>Simulated sampling distribution(s) with visualizations</li>
<li>Statistical summary (means, standard errors)</li>
<li>Economic interpretation: What does this tell us about global productivity patterns?</li>
</ol>
<hr>
</section>
</section>
<section id="what-youve-learned-from-this-case-study" class="level3">
<h3 class="anchored" data-anchor-id="what-youve-learned-from-this-case-study">What You‚Äôve Learned from This Case Study</h3>
<p>Through this hands-on exploration of sampling distributions using cross-country productivity data, you‚Äôve applied all Chapter 3 concepts:</p>
<p><strong>Population vs sample</strong>: Understood the difference and why we use samples for inference</p>
<p><strong>Sample mean</strong>: Computed point estimates from random samples</p>
<p><strong>Sampling variability</strong>: Observed how sample means vary across different samples</p>
<p><strong>Sampling distribution</strong>: Simulated and visualized the distribution of sample means</p>
<p><strong>Central Limit Theorem</strong>: Verified that sample means are approximately normal even for skewed populations</p>
<p><strong>Standard error</strong>: Quantified precision and understood how it decreases with sample size (<span class="math inline">\(\sigma/\sqrt{n}\)</span>)</p>
<p><strong>Effect of sample size</strong>: Compared sampling distributions for different <span class="math inline">\(n\)</span> values</p>
<p><strong>Subpopulation analysis</strong>: Explored differences across country groups</p>
<hr>
<p><strong>Connection to the Research:</strong> Understanding sampling distributions is fundamental to the convergence clubs analysis. When researchers estimate average productivity for a club, they‚Äôre working with samples and must account for sampling variability. The tools you practiced here‚Äîcomputing sample means, quantifying precision, understanding the CLT‚Äîare essential for all statistical inference in economics.</p>
<p><strong>Looking ahead:</strong></p>
<ul>
<li><strong>Chapter 4:</strong> Statistical inference (confidence intervals, hypothesis tests) builds directly on sampling distributions</li>
<li><strong>Chapter 5-9:</strong> Regression analysis extends these concepts to relationships between variables</li>
<li><strong>Chapter 10-17:</strong> Advanced methods for causal inference and panel data</li>
</ul>
<hr>
<p><strong>Congratulations!</strong> You‚Äôve completed Chapter 3 and applied sampling theory to real cross-country data. Continue to Chapter 4 to learn how to use sampling distributions for statistical inference!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks_colab/ch02_Univariate_Data_Summary.html" class="pagination-link" aria-label="2. Univariate Data Summary">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">2. Univariate Data Summary</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html" class="pagination-link" aria-label="4. Statistical Inference for the Mean">
        <span class="nav-page-text"><span class="chapter-title">4. Statistical Inference for the Mean</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>