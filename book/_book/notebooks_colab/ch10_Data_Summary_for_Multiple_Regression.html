<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics Powered by AI - 10. Data Summary for Multiple Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html" rel="next">
<link href="../notebooks_colab/ch09_Models_with_Natural_Logarithms.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html">Multiple Regression</a></li><li class="breadcrumb-item"><a href="../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">10. Data Summary for Multiple Regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Econometrics Powered by AI</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Statistical Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch01_Analysis_of_Economics_Data.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">1. Analysis of Economics Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch02_Univariate_Data_Summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">2. Univariate Data Summary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch03_The_Sample_Mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">3. The Sample Mean</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch04_Statistical_Inference_for_the_Mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">4. Statistical Inference for the Mean</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Bivariate Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch05_Bivariate_Data_Summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">5. Bivariate Data Summary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch06_The_Least_Squares_Estimator.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">6. The Least Squares Estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch07_Statistical_Inference_for_Bivariate_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">7. Statistical Inference for Bivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch08_Case_Studies_for_Bivariate_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">8. Case Studies for Bivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch09_Models_with_Natural_Logarithms.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">9. Models with Natural Logarithms</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Multiple Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">10. Data Summary for Multiple Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">11. Statistical Inference for Multiple Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch12_Further_Topics_in_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">12. Further Topics in Multiple Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch13_Case_Studies_for_Multiple_Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">13. Case Studies for Multiple Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch14_Regression_with_Indicator_Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">14. Regression with Indicator Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch15_Regression_with_Transformed_Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">15. Regression with Transformed Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch16_Checking_the_Model_and_Data.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">16. Checking the Model and Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks_colab/ch17_Panel_Data_Time_Series_Data_Causation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">17. Panel Data, Time Series Data, Causation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#chapter-overview" id="toc-chapter-overview" class="nav-link active" data-scroll-target="#chapter-overview">Chapter Overview</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#example---house-price-and-characteristics" id="toc-example---house-price-and-characteristics" class="nav-link" data-scroll-target="#example---house-price-and-characteristics">10.1: Example - House Price and Characteristics</a></li>
  <li><a href="#two-way-scatterplots" id="toc-two-way-scatterplots" class="nav-link" data-scroll-target="#two-way-scatterplots">10.2: Two-Way Scatterplots</a></li>
  <li><a href="#correlation-analysis" id="toc-correlation-analysis" class="nav-link" data-scroll-target="#correlation-analysis">10.3: Correlation Analysis</a></li>
  <li><a href="#multiple-regression-estimation" id="toc-multiple-regression-estimation" class="nav-link" data-scroll-target="#multiple-regression-estimation">10.4: Multiple Regression Estimation</a></li>
  <li><a href="#partial-effects---the-fwl-theorem" id="toc-partial-effects---the-fwl-theorem" class="nav-link" data-scroll-target="#partial-effects---the-fwl-theorem">10.5: Partial Effects - The FWL Theorem</a></li>
  <li><a href="#model-fit-statistics" id="toc-model-fit-statistics" class="nav-link" data-scroll-target="#model-fit-statistics">10.6: Model Fit Statistics</a></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison">10.7: Model Comparison</a></li>
  <li><a href="#inestimable-models-and-multicollinearity" id="toc-inestimable-models-and-multicollinearity" class="nav-link" data-scroll-target="#inestimable-models-and-multicollinearity">10.8: Inestimable Models and Multicollinearity</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#practice-exercises" id="toc-practice-exercises" class="nav-link" data-scroll-target="#practice-exercises">Practice Exercises</a></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies">Case Studies</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<div id="google_translate_element" style="padding: 8px 0;"></div>
<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'en',
    layout: google.translate.TranslateElement.InlineLayout.HORIZONTAL
  }, 'google_translate_element');
}
</script>
<script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">10. Data Summary for Multiple Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><strong>metricsAI: An Introduction to Econometrics with Python and AI in the Cloud</strong></p>
<p><em><a href="https://carlos-mendez.org">Carlos Mendez</a></em></p>
<p><img src="https://raw.githubusercontent.com/quarcs-lab/metricsai/main/images/ch10_visual_summary.jpg" alt="Chapter 10 Visual Summary" width="65%"></p>
<p>This notebook provides an interactive introduction to multiple regression analysis. You‚Äôll learn how to work with multiple explanatory variables, interpret partial effects, assess model fit, and detect multicollinearity. All code runs directly in Google Colab without any local setup.</p>
<a href="https://colab.research.google.com/github/quarcs-lab/metricsai/blob/main/notebooks_colab/ch10_Data_Summary_for_Multiple_Regression.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Colab"></a>
<div class="chapter-resources">
<p><a href="https://www.youtube.com/watch?v=9398KshS_JA" target="_blank" class="resource-btn">üé¨ AI Video</a> <a href="https://carlos-mendez.my.canva.site/s10-data-summary-for-multiple-regression-pdf" target="_blank" class="resource-btn">‚ú® AI Slides</a> <a href="https://cameron.econ.ucdavis.edu/aed/traedv1_10" target="_blank" class="resource-btn">üìä Cameron Slides</a> <a href="https://app.edcafe.ai/quizzes/697868212f5d08069e04add5" target="_blank" class="resource-btn">‚úèÔ∏è Quiz</a> <a href="https://app.edcafe.ai/chatbots/6978a0fd2f5d08069e0715f8" target="_blank" class="resource-btn">ü§ñ AI Tutor</a></p>
</div>
<section id="chapter-overview" class="level2">
<h2 class="anchored" data-anchor-id="chapter-overview">Chapter Overview</h2>
<p>This chapter extends bivariate regression to the more realistic case where we want to predict an outcome using <strong>multiple explanatory variables</strong> simultaneously. Multiple regression allows us to estimate the partial effect of each variable while controlling for others‚Äîa crucial feature for empirical economic analysis.</p>
<p><strong>Learning Objectives:</strong></p>
<p>By the end of this chapter, you will be able to:</p>
<ol type="1">
<li>Extend bivariate regression concepts to multiple regression with several regressors</li>
<li>Interpret pairwise correlations and use them for exploratory data analysis</li>
<li>Understand the ordinary least squares (OLS) method for multiple regression</li>
<li>Interpret partial effects: how one regressor affects <span class="math inline">\(y\)</span> while holding others constant</li>
<li>Distinguish between partial effects and total effects</li>
<li>Evaluate model fit using R-squared and adjusted R-squared</li>
<li>Understand information criteria (AIC, BIC) for model selection</li>
<li>Recognize when regression coefficients cannot be estimated (perfect collinearity)</li>
</ol>
<p><strong>Dataset used:</strong></p>
<ul>
<li><strong>AED_HOUSE.DTA</strong>: 29 houses sold in Davis, California (1999) with price, size, bedrooms, bathrooms, lot size, age, and month sold</li>
</ul>
<p><strong>Key economic question:</strong> What is the effect of house size on price <strong>after controlling</strong> for other characteristics like bedrooms, bathrooms, and age?</p>
<p><strong>Chapter outline:</strong></p>
<ul>
<li>10.1 Example: House Price and Characteristics</li>
<li>10.2 Two-Way Scatterplots</li>
<li>10.3 Correlation Analysis</li>
<li>10.4 Multiple Regression Estimation</li>
<li>10.5 Partial Effects ‚Äî The FWL Theorem</li>
<li>10.6 Model Fit Statistics</li>
<li>10.7 Model Comparison</li>
<li>10.8 Inestimable Models and Multicollinearity</li>
<li>Key Takeaways</li>
<li>Practice Exercises</li>
<li>Case Studies</li>
</ul>
<p><strong>Estimated time:</strong> 60-75 minutes</p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>First, we import the necessary Python packages and configure the environment for reproducibility. All data will stream directly from GitHub.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seeds for reproducibility</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>random.seed(RANDOM_SEED)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>np.random.seed(RANDOM_SEED)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'PYTHONHASHSEED'</span>] <span class="op">=</span> <span class="bu">str</span>(RANDOM_SEED)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># GitHub data URL</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>GITHUB_DATA_URL <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/quarcs-lab/data-open/master/AED/"</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Set plotting style</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">6</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Setup complete! Ready to analyze house price data."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Setup complete! Ready to analyze house price data.</code></pre>
</div>
</div>
</section>
<section id="example---house-price-and-characteristics" class="level2">
<h2 class="anchored" data-anchor-id="example---house-price-and-characteristics">10.1: Example - House Price and Characteristics</h2>
<p>We begin with a real estate dataset from Davis, California. Understanding house prices is a classic economic application because prices reflect both fundamental characteristics (size, bedrooms) and market conditions.</p>
<p><strong>The dataset contains:</strong></p>
<ul>
<li><strong>Price</strong>: Sale price in dollars</li>
<li><strong>Size</strong>: House size in square feet</li>
<li><strong>Bedrooms</strong>: Number of bedrooms</li>
<li><strong>Bathrooms</strong>: Number of bathrooms</li>
<li><strong>Lotsize</strong>: Size of lot (1=small, 2=medium, 3=large)</li>
<li><strong>Age</strong>: House age in years</li>
<li><strong>Monthsold</strong>: Month of year house was sold</li>
</ul>
<p><strong>Economic motivation:</strong> A simple regression of price on bedrooms might find a positive relationship, but is this because bedrooms directly add value, or because houses with more bedrooms tend to be larger? Multiple regression helps us disentangle these effects.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load house price data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>data_house <span class="op">=</span> pd.read_stata(GITHUB_DATA_URL <span class="op">+</span> <span class="st">'AED_HOUSE.DTA'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first few observations</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 10 observations:"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_house[[<span class="st">'price'</span>, <span class="st">'size'</span>, <span class="st">'bedrooms'</span>, <span class="st">'bathrooms'</span>, <span class="st">'lotsize'</span>, <span class="st">'age'</span>, <span class="st">'monthsold'</span>]].head(<span class="dv">10</span>))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Display data summary</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Summary statistics:"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_house.describe())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>First 10 observations:
    price  size  bedrooms  bathrooms  lotsize   age  monthsold
0  204000  1400         3        2.0        1  31.0          7
1  212000  1600         3        3.0        2  33.0          5
2  213000  1800         3        2.0        2  51.0          4
3  220000  1600         3        2.0        1  49.0          4
4  224500  2100         4        2.5        2  47.0          6
5  229000  1700         4        2.5        2  35.0          3
6  230000  2100         4        2.0        2  34.0          8
7  233000  1700         3        2.0        1  40.0          6
8  235000  1700         4        2.0        2  29.0          7
9  235000  1600         3        2.0        3  35.0          5

Summary statistics:
               price         size   bedrooms  bathrooms    lotsize        age  \
count      29.000000    29.000000  29.000000  29.000000  29.000000  29.000000   
mean   253910.344828  1882.758621   3.793103   2.206897   2.137931  36.413792   
std     37390.710695   398.272130   0.675030   0.341144   0.693034   7.118975   
min    204000.000000  1400.000000   3.000000   2.000000   1.000000  23.000000   
25%    233000.000000  1600.000000   3.000000   2.000000   2.000000  31.000000   
50%    244000.000000  1800.000000   4.000000   2.000000   2.000000  35.000000   
75%    270000.000000  2000.000000   4.000000   2.500000   3.000000  39.000000   
max    375000.000000  3300.000000   6.000000   3.000000   3.000000  51.000000   

       monthsold           list  
count  29.000000      29.000000  
mean    5.965517  257824.137931  
std     1.679344   40860.264099  
min     3.000000  199900.000000  
25%     5.000000  239000.000000  
50%     6.000000  245000.000000  
75%     7.000000  269000.000000  
max     8.000000  386000.000000  </code></pre>
</div>
</div>
<section id="bivariate-vs.-multiple-regression" class="level3">
<h3 class="anchored" data-anchor-id="bivariate-vs.-multiple-regression">Bivariate vs.&nbsp;Multiple Regression</h3>
<p>Let‚Äôs compare a simple regression (price on bedrooms only) with a multiple regression (price on bedrooms AND size). This illustrates how controlling for other variables changes coefficient estimates.</p>
<p><strong>Key insight:</strong> In the bivariate regression, the bedrooms coefficient captures both the direct effect of bedrooms and the indirect effect through correlation with size. In multiple regression, we isolate the <strong>partial effect</strong> of bedrooms holding size constant.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bivariate regression: price ~ bedrooms</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model_bivariate <span class="op">=</span> ols(<span class="st">'price ~ bedrooms'</span>, data<span class="op">=</span>data_house).fit()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BIVARIATE REGRESSION: price ~ bedrooms"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_bivariate.summary())</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple regression: price ~ bedrooms + size</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>model_multiple <span class="op">=</span> ols(<span class="st">'price ~ bedrooms + size'</span>, data<span class="op">=</span>data_house).fit()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MULTIPLE REGRESSION: price ~ bedrooms + size"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_multiple.summary())</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare bedrooms coefficient</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"COEFFICIENT COMPARISON"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bedrooms coefficient (bivariate):  $</span><span class="sc">{</span>model_bivariate<span class="sc">.</span>params[<span class="st">'bedrooms'</span>]<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bedrooms coefficient (multiple):   $</span><span class="sc">{</span>model_multiple<span class="sc">.</span>params[<span class="st">'bedrooms'</span>]<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Change: $</span><span class="sc">{</span>model_multiple<span class="sc">.</span>params[<span class="st">'bedrooms'</span>] <span class="op">-</span> model_bivariate<span class="sc">.</span>params[<span class="st">'bedrooms'</span>]<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">The coefficient drops dramatically because bedrooms was capturing size effects."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
BIVARIATE REGRESSION: price ~ bedrooms
======================================================================
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R-squared:                       0.183
Model:                            OLS   Adj. R-squared:                  0.152
Method:                 Least Squares   F-statistic:                     6.030
Date:                Wed, 21 Jan 2026   Prob (F-statistic):             0.0208
Time:                        15:16:42   Log-Likelihood:                -343.06
No. Observations:                  29   AIC:                             690.1
Df Residuals:                      27   BIC:                             692.9
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept   1.641e+05   3.71e+04      4.423      0.000     8.8e+04     2.4e+05
bedrooms    2.367e+04   9637.976      2.456      0.021    3891.805    4.34e+04
==============================================================================
Omnibus:                       23.468   Durbin-Watson:                   0.345
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               35.285
Skew:                           1.939   Prob(JB):                     2.18e-08
Kurtosis:                       6.764   Cond. No.                         23.8
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

======================================================================
MULTIPLE REGRESSION: price ~ bedrooms + size
======================================================================
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R-squared:                       0.618
Model:                            OLS   Adj. R-squared:                  0.589
Method:                 Least Squares   F-statistic:                     21.03
Date:                Wed, 21 Jan 2026   Prob (F-statistic):           3.68e-06
Time:                        15:16:42   Log-Likelihood:                -332.03
No. Observations:                  29   AIC:                             670.1
Df Residuals:                      26   BIC:                             674.2
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept   1.117e+05   2.76e+04      4.048      0.000     5.5e+04    1.68e+05
bedrooms    1553.4580   7846.866      0.198      0.845   -1.46e+04    1.77e+04
size          72.4081     13.300      5.444      0.000      45.070      99.746
==============================================================================
Omnibus:                        0.516   Durbin-Watson:                   1.230
Prob(Omnibus):                  0.773   Jarque-Bera (JB):                0.609
Skew:                          -0.086   Prob(JB):                        0.737
Kurtosis:                       2.311   Cond. No.                     1.21e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.21e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

======================================================================
COEFFICIENT COMPARISON
======================================================================
Bedrooms coefficient (bivariate):  $23,667.30
Bedrooms coefficient (multiple):   $1,553.46
Change: $-22,113.84

The coefficient drops dramatically because bedrooms was capturing size effects.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.1: Partial Effects vs.&nbsp;Total Effects in Multiple Regression</strong></p>
<p>In bivariate regression, the bedrooms coefficient ($23,667) captures both the direct effect of bedrooms and the indirect effect through correlation with size. In multiple regression, the bedrooms coefficient drops to $1,553 ‚Äî the <strong>partial effect</strong> holding size constant. This dramatic change illustrates why controlling for confounders is essential for isolating individual variable effects.</p>
</blockquote>
</section>
</section>
<section id="two-way-scatterplots" class="level2">
<h2 class="anchored" data-anchor-id="two-way-scatterplots">10.2: Two-Way Scatterplots</h2>
<p>Before running multiple regression, it‚Äôs useful to visualize pairwise relationships between variables. A <strong>scatterplot matrix</strong> shows all two-way scatterplots simultaneously.</p>
<p><strong>What to look for:</strong></p>
<ul>
<li>Strong linear relationships (potential predictors of price)</li>
<li>Correlation between explanatory variables (potential multicollinearity)</li>
<li>Outliers or non-linear patterns</li>
</ul>
<p>The diagonal shows the distribution of each variable using kernel density estimates (KDE).</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scatterplot matrix</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plot_vars <span class="op">=</span> [<span class="st">'price'</span>, <span class="st">'size'</span>, <span class="st">'bedrooms'</span>, <span class="st">'age'</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> sns.pairplot(data_house[plot_vars], diag_kind<span class="op">=</span><span class="st">'kde'</span>, plot_kws<span class="op">=</span>{<span class="st">'alpha'</span>: <span class="fl">0.6</span>, <span class="st">'s'</span>: <span class="dv">50</span>})</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>g.fig.suptitle(<span class="st">'Figure 10.1: Scatterplot Matrix - House Price Data'</span>, </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>               fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, y<span class="op">=</span><span class="fl">1.00</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Scatterplot matrix created."</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Notice: Price shows strongest relationship with Size."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch10_Data_Summary_for_Multiple_Regression_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Scatterplot matrix created.
Notice: Price shows strongest relationship with Size.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.2: Exploratory Data Analysis with Scatterplot Matrices</strong></p>
<p>Pairwise scatterplot matrices display all two-way relationships simultaneously, revealing linear associations, nonlinearities, clusters, and outliers before formal modeling. They also highlight potential multicollinearity: if two regressors are tightly correlated (e.g., size and bedrooms), their individual effects may be hard to separate in a regression.</p>
</blockquote>
</section>
<section id="correlation-analysis" class="level2">
<h2 class="anchored" data-anchor-id="correlation-analysis">10.3: Correlation Analysis</h2>
<p>The <strong>correlation coefficient</strong> measures the strength of linear association between two variables, ranging from -1 (perfect negative) to +1 (perfect positive).</p>
<p><strong>Correlation matrix insights:</strong></p>
<ul>
<li><strong>Price is most correlated with Size</strong> (r = 0.79), then Bedrooms (r = 0.43)</li>
<li><strong>Size and Bedrooms are correlated</strong> (r = 0.52), which can cause multicollinearity</li>
<li>Correlation ‚â† causation (merely shows association)</li>
</ul>
<p>A correlation heatmap provides visual representation with color intensity showing strength of correlation.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate correlation matrix</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>corr_vars <span class="op">=</span> [<span class="st">'price'</span>, <span class="st">'size'</span>, <span class="st">'bedrooms'</span>, <span class="st">'bathrooms'</span>, <span class="st">'lotsize'</span>, <span class="st">'age'</span>, <span class="st">'monthsold'</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>corr_matrix <span class="op">=</span> data_house[corr_vars].corr()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Correlation Matrix:"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corr_matrix)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize correlation matrix as heatmap</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'.3f'</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="dv">1</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">0.8</span>})</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Correlation Matrix Heatmap'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Key observations:"</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Price-Size correlation: </span><span class="sc">{</span>corr_matrix<span class="sc">.</span>loc[<span class="st">'price'</span>, <span class="st">'size'</span>]<span class="sc">:.3f}</span><span class="ss"> (strongest predictor)"</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Price-Bedrooms correlation: </span><span class="sc">{</span>corr_matrix<span class="sc">.</span>loc[<span class="st">'price'</span>, <span class="st">'bedrooms'</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Size-Bedrooms correlation: </span><span class="sc">{</span>corr_matrix<span class="sc">.</span>loc[<span class="st">'size'</span>, <span class="st">'bedrooms'</span>]<span class="sc">:.3f}</span><span class="ss"> (multicollinearity concern)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation Matrix:
              price      size  bedrooms  bathrooms   lotsize       age  \
price      1.000000  0.785782  0.427275   0.329793  0.153479 -0.068015   
size       0.785782  1.000000  0.517630   0.316338  0.112437  0.076925   
bedrooms   0.427275  0.517630  1.000000   0.037435  0.292206 -0.026140   
bathrooms  0.329793  0.316338  0.037435   1.000000  0.101575  0.037018   
lotsize    0.153479  0.112437  0.292206   0.101575  1.000000 -0.019220   
age       -0.068015  0.076925 -0.026140   0.037018 -0.019220  1.000000   
monthsold -0.209985 -0.214511  0.182512  -0.392310 -0.057140 -0.366207   

           monthsold  
price      -0.209985  
size       -0.214511  
bedrooms    0.182512  
bathrooms  -0.392310  
lotsize    -0.057140  
age        -0.366207  
monthsold   1.000000  </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="ch10_Data_Summary_for_Multiple_Regression_files/figure-html/cell-6-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Key observations:
  - Price-Size correlation: 0.786 (strongest predictor)
  - Price-Bedrooms correlation: 0.427
  - Size-Bedrooms correlation: 0.518 (multicollinearity concern)</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.3: Correlation vs.&nbsp;Causation in Multivariate Analysis</strong></p>
<p>High bivariate correlation (e.g., bedrooms-price, <span class="math inline">\(r = 0.43\)</span>) may diminish or vanish after controlling for confounders. In our data, bedrooms correlate with price largely because bigger houses have more bedrooms. Multiple regression isolates each variable‚Äôs partial contribution, revealing that size ‚Äî not bedrooms ‚Äî drives most of the price variation.</p>
</blockquote>
<p>Having explored the data visually and through correlations, we now estimate the formal multiple regression model to quantify partial effects.</p>
</section>
<section id="multiple-regression-estimation" class="level2">
<h2 class="anchored" data-anchor-id="multiple-regression-estimation">10.4: Multiple Regression Estimation</h2>
<p>Now we estimate the <strong>full multiple regression model</strong> with all available predictors. The regression equation is:</p>
<p><span class="math display">\[\widehat{\text{price}} = b_1 + b_2 \times \text{size} + b_3 \times \text{bedrooms} + b_4 \times \text{bathrooms} + b_5 \times \text{lotsize} + b_6 \times \text{age} + b_7 \times \text{monthsold}\]</span></p>
<p><strong>Ordinary Least Squares (OLS)</strong> chooses coefficients <span class="math inline">\(b_1, ..., b_7\)</span> to minimize the sum of squared residuals:</p>
<p><span class="math display">\[\min \sum_{i=1}^{n} (y_i - \widehat{y}_i)^2\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the actual price and <span class="math inline">\(\widehat{y}_i\)</span> is the predicted price.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate full multiple regression model</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>model_full <span class="op">=</span> ols(<span class="st">'price ~ size + bedrooms + bathrooms + lotsize + age + monthsold'</span>,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                 data<span class="op">=</span>data_house).fit()</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"FULL MULTIPLE REGRESSION MODEL"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_full.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
FULL MULTIPLE REGRESSION MODEL
======================================================================
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R-squared:                       0.651
Model:                            OLS   Adj. R-squared:                  0.555
Method:                 Least Squares   F-statistic:                     6.826
Date:                Wed, 21 Jan 2026   Prob (F-statistic):           0.000342
Time:                        15:16:45   Log-Likelihood:                -330.74
No. Observations:                  29   AIC:                             675.5
Df Residuals:                      22   BIC:                             685.1
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept   1.378e+05   6.15e+04      2.242      0.035    1.03e+04    2.65e+05
size          68.3694     15.389      4.443      0.000      36.454     100.285
bedrooms    2685.3151   9192.526      0.292      0.773   -1.64e+04    2.17e+04
bathrooms   6832.8800   1.57e+04      0.435      0.668   -2.58e+04    3.94e+04
lotsize     2303.2214   7226.535      0.319      0.753   -1.27e+04    1.73e+04
age         -833.0386    719.335     -1.158      0.259   -2324.847     658.770
monthsold  -2088.5036   3520.898     -0.593      0.559   -9390.399    5213.392
==============================================================================
Omnibus:                        1.317   Durbin-Watson:                   1.259
Prob(Omnibus):                  0.518   Jarque-Bera (JB):                0.980
Skew:                           0.151   Prob(JB):                        0.612
Kurtosis:                       2.152   Cond. No.                     2.59e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.59e+04. This might indicate that there are
strong multicollinearity or other numerical problems.</code></pre>
</div>
</div>
<section id="coefficient-interpretation-with-confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="coefficient-interpretation-with-confidence-intervals">Coefficient Interpretation with Confidence Intervals</h3>
<p>Each regression coefficient represents a <strong>partial effect</strong>: the expected change in price when that variable increases by one unit, <strong>holding all other variables constant</strong>.</p>
<p><strong>Example interpretation (Size coefficient):</strong></p>
<ul>
<li>Coefficient ‚âà $68.37 per square foot</li>
<li>Interpretation: A one square foot increase in house size is associated with a $68.37 increase in price, holding bedrooms, bathrooms, lot size, age, and month sold constant.</li>
</ul>
<p>The 95% confidence interval tells us the range of plausible values for each coefficient.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display coefficients with 95% confidence intervals</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>conf_int <span class="op">=</span> model_full.conf_int(alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>coef_table <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Coefficient'</span>: model_full.params,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Std. Error'</span>: model_full.bse,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CI Lower'</span>: conf_int.iloc[:, <span class="dv">0</span>],</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CI Upper'</span>: conf_int.iloc[:, <span class="dv">1</span>],</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'t-statistic'</span>: model_full.tvalues,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'p-value'</span>: model_full.pvalues</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients with 95% Confidence Intervals:"</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef_table)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Key result: Size is the only statistically significant predictor (p &lt; 0.05)."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Coefficients with 95% Confidence Intervals:
             Coefficient    Std. Error      CI Lower       CI Upper  \
Intercept  137791.065699  61464.951869  10320.557398  265261.573999   
size           68.369419     15.389472     36.453608     100.285230   
bedrooms     2685.315122   9192.525674 -16378.816300   21749.446543   
bathrooms    6832.880015  15721.191544 -25770.875723   39436.635753   
lotsize      2303.221371   7226.535205 -12683.695364   17290.138107   
age          -833.038602    719.334544  -2324.847139     658.769936   
monthsold   -2088.503625   3520.897859  -9390.398871    5213.391620   

           t-statistic   p-value  
Intercept     2.241783  0.035387  
size          4.442610  0.000205  
bedrooms      0.292119  0.772932  
bathrooms     0.434629  0.668065  
lotsize       0.318717  0.752947  
age          -1.158068  0.259254  
monthsold    -0.593174  0.559114  

Key result: Size is the only statistically significant predictor (p &lt; 0.05).</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.4: Interpreting Partial Effects in Multiple Regression</strong></p>
<p>Each coefficient <span class="math inline">\(b_j\)</span> measures the expected change in <span class="math inline">\(y\)</span> when <span class="math inline">\(x_j\)</span> increases by one unit, <strong>holding all other regressors constant</strong>. For example, a size coefficient of $68.37 means each additional square foot is associated with a $68.37 price increase, controlling for bedrooms, bathrooms, lot size, age, and month sold. Statistical significance is assessed through confidence intervals and <span class="math inline">\(t\)</span>-tests.</p>
</blockquote>
</section>
</section>
<section id="partial-effects---the-fwl-theorem" class="level2">
<h2 class="anchored" data-anchor-id="partial-effects---the-fwl-theorem">10.5: Partial Effects - The FWL Theorem</h2>
<p>The <strong>Frisch-Waugh-Lovell (FWL) Theorem</strong> states that the coefficient on any variable in multiple regression equals the coefficient from a bivariate regression of <span class="math inline">\(y\)</span> on the <strong>residualized</strong> version of that variable.</p>
<p><strong>Demonstration:</strong></p>
<ol type="1">
<li>Regress <code>size</code> on all other variables, obtain residuals <span class="math inline">\(\widetilde{\text{size}}\)</span></li>
<li>Regress <code>price</code> on <span class="math inline">\(\widetilde{\text{size}}\)</span> only</li>
<li>The coefficient will exactly match the <code>size</code> coefficient from the full multiple regression</li>
</ol>
<p><strong>Intuition:</strong> The residual <span class="math inline">\(\widetilde{\text{size}}\)</span> represents the variation in size that is <strong>not explained</strong> by other variables. This is why multiple regression isolates partial effects.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Regress size on all other variables</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>model_size_on_others <span class="op">=</span> ols(<span class="st">'size ~ bedrooms + bathrooms + lotsize + age + monthsold'</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                            data<span class="op">=</span>data_house).fit()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>resid_size <span class="op">=</span> model_size_on_others.resid</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Regress price on residualized size</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>data_house[<span class="st">'resid_size'</span>] <span class="op">=</span> resid_size</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>model_price_on_resid <span class="op">=</span> ols(<span class="st">'price ~ resid_size'</span>, data<span class="op">=</span>data_house).fit()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare coefficients</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DEMONSTRATION: FWL THEOREM (Partial Effects)"</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size coefficient from FULL multiple regression:  </span><span class="sc">{</span>model_full<span class="sc">.</span>params[<span class="st">'size'</span>]<span class="sc">:.10f}</span><span class="ss">"</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coefficient on residualized size (bivariate):    </span><span class="sc">{</span>model_price_on_resid<span class="sc">.</span>params[<span class="st">'resid_size'</span>]<span class="sc">:.10f}</span><span class="ss">"</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Difference (numerical precision):                 </span><span class="sc">{</span><span class="bu">abs</span>(model_full.params[<span class="st">'size'</span>] <span class="op">-</span> model_price_on_resid.params[<span class="st">'resid_size'</span>])<span class="sc">:.15f}</span><span class="ss">"</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">These coefficients are identical! This proves the partial effect interpretation."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
DEMONSTRATION: FWL THEOREM (Partial Effects)
======================================================================
Size coefficient from FULL multiple regression:  68.3694189767
Coefficient on residualized size (bivariate):    68.3694189767
Difference (numerical precision):                 0.000000000000057

These coefficients are identical! This proves the partial effect interpretation.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.5: The Frisch-Waugh-Lovell (FWL) Theorem</strong></p>
<p>The partial effect of <span class="math inline">\(x_j\)</span> in a multiple regression equals the slope from a bivariate regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(\widetilde{x}_j\)</span>, where <span class="math inline">\(\widetilde{x}_j\)</span> is the residual from regressing <span class="math inline">\(x_j\)</span> on all other regressors. Intuitively, <span class="math inline">\(\widetilde{x}_j\)</span> captures the variation in <span class="math inline">\(x_j\)</span> that is independent of the other variables ‚Äî this is exactly what multiple regression uses to estimate partial effects.</p>
</blockquote>
<p>Now that we understand partial effects and the FWL theorem, let‚Äôs evaluate how well the overall regression model fits the data.</p>
</section>
<section id="model-fit-statistics" class="level2">
<h2 class="anchored" data-anchor-id="model-fit-statistics">10.6: Model Fit Statistics</h2>
<p>Several statistics summarize how well the regression model fits the data:</p>
<p><strong>R-squared (<span class="math inline">\(R^2\)</span>):</strong></p>
<ul>
<li>Fraction of variation in <span class="math inline">\(y\)</span> explained by the regressors</li>
<li>Formula: <span class="math inline">\(R^2 = \frac{\text{Explained SS}}{\text{Total SS}} = 1 - \frac{\text{Residual SS}}{\text{Total SS}}\)</span></li>
<li>Range: 0 to 1 (higher is better fit)</li>
<li><strong>Problem:</strong> Always increases when adding variables (even irrelevant ones)</li>
</ul>
<p><strong>Adjusted R-squared (<span class="math inline">\(\bar{R}^2\)</span>):</strong></p>
<ul>
<li>Penalizes model complexity (number of parameters <span class="math inline">\(k\)</span>)</li>
<li>Formula: <span class="math inline">\(\bar{R}^2 = 1 - \frac{\text{RSS}/(n-k)}{\text{TSS}/(n-1)}\)</span></li>
<li>Can decrease when adding unhelpful variables</li>
<li>Preferred for model comparison</li>
</ul>
<p><strong>Root MSE (Standard error of regression):</strong></p>
<ul>
<li>Typical size of prediction error</li>
<li>Formula: <span class="math inline">\(s_e = \sqrt{\frac{1}{n-k}\sum (y_i - \widehat{y}_i)^2}\)</span></li>
<li>Same units as <span class="math inline">\(y\)</span> (dollars in our case)</li>
</ul>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and display model fit statistics</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data_house)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="bu">len</span>(model_full.params)  <span class="co"># includes intercept</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> n <span class="op">-</span> k</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL FIT STATISTICS"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size (n):               </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of parameters (k):      </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Degrees of freedom (n-k):      </span><span class="sc">{</span>df<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">R-squared:                     </span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Adjusted R-squared:            </span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared_adj<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Root MSE:                      $</span><span class="sc">{</span>np<span class="sc">.</span>sqrt(model_full.mse_resid)<span class="sc">:,</span><span class="fl">.2</span><span class="er">f</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify R¬≤ = [Corr(y, ≈∑)]¬≤</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> model_full.fittedvalues</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>corr_y_yhat <span class="op">=</span> np.corrcoef(data_house[<span class="st">'price'</span>], predicted)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Verification: R¬≤ = [Corr(y, ≈∑)]¬≤"</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Correlation(y, ≈∑):           </span><span class="sc">{</span>corr_y_yhat<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  [Correlation(y, ≈∑)]¬≤:        </span><span class="sc">{</span>corr_y_yhat<span class="op">**</span><span class="dv">2</span><span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  R¬≤ from model:                </span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Match: </span><span class="sc">{</span>np<span class="sc">.</span>isclose(corr_y_yhat<span class="op">**</span><span class="dv">2</span>, model_full.rsquared)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
MODEL FIT STATISTICS
======================================================================
Sample size (n):               29
Number of parameters (k):      7
Degrees of freedom (n-k):      22

R-squared:                     0.650553
Adjusted R-squared:            0.555249
Root MSE:                      $24,935.73

Verification: R¬≤ = [Corr(y, ≈∑)]¬≤
  Correlation(y, ≈∑):           0.806569
  [Correlation(y, ≈∑)]¬≤:        0.650553
  R¬≤ from model:                0.650553
  Match: True</code></pre>
</div>
</div>
<section id="information-criteria-aic-and-bic" class="level3">
<h3 class="anchored" data-anchor-id="information-criteria-aic-and-bic">Information Criteria (AIC and BIC)</h3>
<p><strong>Akaike Information Criterion (AIC)</strong> and <strong>Bayesian Information Criterion (BIC)</strong> are more sophisticated measures that penalize model complexity:</p>
<p><span class="math display">\[\text{AIC} = n \times \ln(\widehat{\sigma}_e^2) + n(1 + \ln 2\pi) + 2k\]</span> <span class="math display">\[\text{BIC} = n \times \ln(\widehat{\sigma}_e^2) + n(1 + \ln 2\pi) + k \times \ln(n)\]</span></p>
<p><strong>Key points:</strong></p>
<ul>
<li><strong>Lower values are better</strong> (unlike R¬≤)</li>
<li>BIC penalizes complexity more heavily than AIC</li>
<li>Useful for comparing non-nested models</li>
<li>Different software packages may use different conventions (scaling by <span class="math inline">\(n\)</span>)</li>
</ul>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate information criteria</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"INFORMATION CRITERIA"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># AIC and BIC from statsmodels</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"AIC (statsmodels):             </span><span class="sc">{</span>model_full<span class="sc">.</span>aic<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"BIC (statsmodels):             </span><span class="sc">{</span>model_full<span class="sc">.</span>bic<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual calculation (Stata convention)</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>rss <span class="op">=</span> np.<span class="bu">sum</span>(model_full.resid <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>aic_stata <span class="op">=</span> n <span class="op">*</span> np.log(rss<span class="op">/</span>n) <span class="op">+</span> n <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi)) <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>k</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>bic_stata <span class="op">=</span> n <span class="op">*</span> np.log(rss<span class="op">/</span>n) <span class="op">+</span> n <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> np.log(<span class="dv">2</span><span class="op">*</span>np.pi)) <span class="op">+</span> k<span class="op">*</span>np.log(n)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">AIC (Stata convention):        </span><span class="sc">{</span>aic_stata<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"BIC (Stata convention):        </span><span class="sc">{</span>bic_stata<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Note: Different conventions yield different values, but ranking is consistent."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
INFORMATION CRITERIA
======================================================================
AIC (statsmodels):             675.4824
BIC (statsmodels):             685.0535

AIC (Stata convention):        675.4824
BIC (Stata convention):        685.0535

Note: Different conventions yield different values, but ranking is consistent.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.6: Model Selection with Adjusted R-squared vs.&nbsp;Information Criteria</strong></p>
<p>Adjusted <span class="math inline">\(R^2\)</span> penalizes complexity mildly by dividing sums of squares by degrees of freedom. Information criteria (AIC, BIC) impose stronger penalties: BIC = <span class="math inline">\(n \ln(\hat{\sigma}_e^2) + k \ln(n)\)</span>. Smaller AIC/BIC values indicate better models. BIC is generally preferred because its penalty grows with sample size, favoring more parsimonious specifications.</p>
</blockquote>
</section>
</section>
<section id="model-comparison" class="level2">
<h2 class="anchored" data-anchor-id="model-comparison">10.7: Model Comparison</h2>
<p>It‚Äôs often useful to compare multiple model specifications side-by-side. Here we compare:</p>
<ul>
<li><strong>Simple model:</strong> Price predicted by size only</li>
<li><strong>Full model:</strong> Price predicted by all variables</li>
</ul>
<p><strong>Key comparison points:</strong></p>
<ul>
<li>How much does R¬≤ improve?</li>
<li>Does adjusted R¬≤ improve (accounting for added complexity)?</li>
<li>How do coefficient estimates change?</li>
</ul>
<p><strong>Economic interpretation:</strong> If adding 5 more variables only modestly improves fit, the simple model may be preferred (parsimony principle).</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple model (size only)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>model_simple <span class="op">=</span> ols(<span class="st">'price ~ size'</span>, data<span class="op">=</span>data_house).fit()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create comparison table</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL COMPARISON: Simple vs. Full"</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>comparison_stats <span class="op">=</span> pd.DataFrame({</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Model'</span>: [<span class="st">'Simple (size only)'</span>, <span class="st">'Full (all variables)'</span>],</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'R¬≤'</span>: [model_simple.rsquared, model_full.rsquared],</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Adj R¬≤'</span>: [model_simple.rsquared_adj, model_full.rsquared_adj],</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'AIC'</span>: [model_simple.aic, model_full.aic],</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'BIC'</span>: [model_simple.bic, model_full.bic],</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'N'</span>: [n, n]</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(comparison_stats.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Interpretation:"</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - R¬≤ increases from </span><span class="sc">{</span>model_simple<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss"> to </span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss"> (+</span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared <span class="op">-</span> model_simple<span class="sc">.</span>rsquared<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Adj R¬≤ DECREASES from </span><span class="sc">{</span>model_simple<span class="sc">.</span>rsquared_adj<span class="sc">:.3f}</span><span class="ss"> to </span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared_adj<span class="sc">:.3f}</span><span class="ss"> (</span><span class="sc">{</span>model_full<span class="sc">.</span>rsquared_adj <span class="op">-</span> model_simple<span class="sc">.</span>rsquared_adj<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - This suggests the added variables don't improve fit enough to justify complexity."</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Simple model may be preferred (parsimony principle)."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
MODEL COMPARISON: Simple vs. Full
======================================================================
               Model       R¬≤   Adj R¬≤        AIC        BIC  N
  Simple (size only) 0.617453 0.603285 668.106844 670.841436 29
Full (all variables) 0.650553 0.555249 675.482401 685.053472 29

Interpretation:
  - R¬≤ increases from 0.617 to 0.651 (+0.033)
  - Adj R¬≤ DECREASES from 0.603 to 0.555 (-0.048)
  - This suggests the added variables don't improve fit enough to justify complexity.
  - Simple model may be preferred (parsimony principle).</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.7: The Parsimony Principle</strong></p>
<p>Simpler models are preferred unless additional variables meaningfully improve fit. In our house price example, adding five variables beyond size barely increased <span class="math inline">\(R^2\)</span> (0.618 <span class="math inline">\(\to\)</span> 0.651) while adjusted <span class="math inline">\(R^2\)</span> actually fell (0.603 <span class="math inline">\(\to\)</span> 0.555). Check adjusted <span class="math inline">\(R^2\)</span>, AIC, and BIC ‚Äî if they don‚Äôt improve, the simpler model is preferred.</p>
</blockquote>
<p>Having compared models, we now turn to a critical pitfall: when regressors are too closely related to separate their individual effects.</p>
</section>
<section id="inestimable-models-and-multicollinearity" class="level2">
<h2 class="anchored" data-anchor-id="inestimable-models-and-multicollinearity">10.8: Inestimable Models and Multicollinearity</h2>
<p><strong>Perfect multicollinearity</strong> occurs when one regressor is an exact linear combination of others. In this case, OLS cannot estimate all coefficients (the model is ‚Äúinestimable‚Äù).</p>
<p><strong>Example:</strong> If we create <code>size_twice = 2 √ó size</code> and include both <code>size</code> and <code>size_twice</code> as regressors, the model is perfectly collinear.</p>
<p><strong>Variance Inflation Factor (VIF)</strong> detects multicollinearity:</p>
<ul>
<li>VIF measures how much a variable‚Äôs variance is inflated due to correlation with other regressors</li>
<li>Formula: <span class="math inline">\(VIF_j = \frac{1}{1 - R_j^2}\)</span> where <span class="math inline">\(R_j^2\)</span> is from regressing <span class="math inline">\(x_j\)</span> on all other <span class="math inline">\(x\)</span>‚Äôs</li>
<li><strong>Rule of thumb:</strong> VIF &gt; 10 indicates problematic multicollinearity</li>
</ul>
<p><strong>Consequences of high multicollinearity:</strong></p>
<ul>
<li>Large standard errors (imprecise estimates)</li>
<li>Unstable coefficients (small data changes ‚Üí big estimate changes)</li>
<li>Difficulty interpreting individual effects</li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate perfect multicollinearity</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DEMONSTRATION: Perfect Multicollinearity"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a perfectly collinear variable</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>data_house[<span class="st">'size_twice'</span>] <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> data_house[<span class="st">'size'</span>]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Creating variable: size_twice = 2 √ó size"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attempting to estimate: price ~ size + size_twice + bedrooms</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    model_collinear <span class="op">=</span> ols(<span class="st">'price ~ size + size_twice + bedrooms'</span>, data<span class="op">=</span>data_house).fit()</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Model estimated (software automatically dropped one variable):"</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(model_collinear.summary())</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error: </span><span class="sc">{</span><span class="bu">type</span>(e)<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Message: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
DEMONSTRATION: Perfect Multicollinearity
======================================================================
Creating variable: size_twice = 2 √ó size
Attempting to estimate: price ~ size + size_twice + bedrooms

Model estimated (software automatically dropped one variable):
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R-squared:                       0.618
Model:                            OLS   Adj. R-squared:                  0.589
Method:                 Least Squares   F-statistic:                     21.03
Date:                Wed, 21 Jan 2026   Prob (F-statistic):           3.68e-06
Time:                        15:16:45   Log-Likelihood:                -332.03
No. Observations:                  29   AIC:                             670.1
Df Residuals:                      26   BIC:                             674.2
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept   1.117e+05   2.76e+04      4.048      0.000     5.5e+04    1.68e+05
size          14.4816      2.660      5.444      0.000       9.014      19.949
size_twice    28.9633      5.320      5.444      0.000      18.028      39.898
bedrooms    1553.4580   7846.866      0.198      0.845   -1.46e+04    1.77e+04
==============================================================================
Omnibus:                        0.516   Durbin-Watson:                   1.230
Prob(Omnibus):                  0.773   Jarque-Bera (JB):                0.609
Skew:                          -0.086   Prob(JB):                        0.737
Kurtosis:                       2.311   Cond. No.                     1.96e+17
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 1.39e-26. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.</code></pre>
</div>
</div>
<section id="variance-inflation-factors-vif" class="level3">
<h3 class="anchored" data-anchor-id="variance-inflation-factors-vif">Variance Inflation Factors (VIF)</h3>
<p>Now let‚Äôs calculate VIF for each variable in our full model to check for multicollinearity problems.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate VIF for each variable in the full model</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_house[[<span class="st">'size'</span>, <span class="st">'bedrooms'</span>, <span class="st">'bathrooms'</span>, <span class="st">'lotsize'</span>, <span class="st">'age'</span>, <span class="st">'monthsold'</span>]]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>vif_data <span class="op">=</span> pd.DataFrame()</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">"Variable"</span>] <span class="op">=</span> X.columns</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(X.values, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>])]</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"VARIANCE INFLATION FACTORS (VIF)"</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vif_data.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Interpretation:"</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - VIF &gt; 10: Problematic multicollinearity"</span>)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - VIF 5-10: Moderate multicollinearity"</span>)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - VIF &lt; 5: Low multicollinearity"</span>)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>max_vif <span class="op">=</span> vif_data[<span class="st">'VIF'</span>].<span class="bu">max</span>()</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> max_vif <span class="op">&gt;</span> <span class="dv">10</span>:</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Warning: Maximum VIF = </span><span class="sc">{</span>max_vif<span class="sc">:.2f}</span><span class="ss"> indicates multicollinearity issues."</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> max_vif <span class="op">&gt;</span> <span class="dv">5</span>:</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Note: Maximum VIF = </span><span class="sc">{</span>max_vif<span class="sc">:.2f}</span><span class="ss"> shows moderate multicollinearity."</span>)</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Good: Maximum VIF = </span><span class="sc">{</span>max_vif<span class="sc">:.2f}</span><span class="ss"> - no serious multicollinearity detected."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>======================================================================
VARIANCE INFLATION FACTORS (VIF)
======================================================================
 Variable       VIF
     size 40.133832
 bedrooms 57.819254
bathrooms 34.741906
  lotsize 11.969605
      age 21.024092
monthsold 12.795557

Interpretation:
  - VIF &gt; 10: Problematic multicollinearity
  - VIF 5-10: Moderate multicollinearity
  - VIF &lt; 5: Low multicollinearity

Warning: Maximum VIF = 57.82 indicates multicollinearity issues.</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.8: Detecting Multicollinearity with VIF</strong></p>
<p>The Variance Inflation Factor (VIF) quantifies how much a coefficient‚Äôs variance is inflated due to correlation with other regressors. VIF<span class="math inline">\(_j = 1/(1 - R_j^2)\)</span>, where <span class="math inline">\(R_j^2\)</span> is from regressing <span class="math inline">\(x_j\)</span> on all other regressors. A VIF above 5 suggests moderate concern; above 10 indicates severe multicollinearity that inflates standard errors and destabilizes estimates. Perfect collinearity (VIF <span class="math inline">\(\to \infty\)</span>) makes coefficients inestimable.</p>
</blockquote>
</section>
<section id="visualization-actual-vs.-predicted-values" class="level3">
<h3 class="anchored" data-anchor-id="visualization-actual-vs.-predicted-values">Visualization: Actual vs.&nbsp;Predicted Values</h3>
<p>A plot of actual vs.&nbsp;predicted values helps visualize model fit. Points close to the 45-degree line indicate accurate predictions.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create actual vs predicted plot</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>ax.scatter(data_house[<span class="st">'price'</span>], model_full.fittedvalues, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">50</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>ax.plot([data_house[<span class="st">'price'</span>].<span class="bu">min</span>(), data_house[<span class="st">'price'</span>].<span class="bu">max</span>()],</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        [data_house[<span class="st">'price'</span>].<span class="bu">min</span>(), data_house[<span class="st">'price'</span>].<span class="bu">max</span>()],</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Perfect prediction (45¬∞ line)'</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Actual Price ($1000s)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Predicted Price ($1000s)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Actual vs Predicted House Prices'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Points close to the red line indicate accurate predictions."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch10_Data_Summary_for_Multiple_Regression_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Points close to the red line indicate accurate predictions.</code></pre>
</div>
</div>
</section>
<section id="visualization-residual-plot" class="level3">
<h3 class="anchored" data-anchor-id="visualization-residual-plot">Visualization: Residual Plot</h3>
<p>A <strong>residual plot</strong> (residuals vs.&nbsp;fitted values) helps diagnose model problems:</p>
<ul>
<li><strong>Random scatter around zero:</strong> Good (model assumptions satisfied)</li>
<li><strong>Patterns (curves, funnels):</strong> Bad (model misspecification or heteroskedasticity)</li>
<li><strong>Outliers:</strong> Investigate unusual observations</li>
</ul>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create residual plot</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>ax.scatter(model_full.fittedvalues, model_full.resid, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">50</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Zero residual line'</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Fitted values ($1000s)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Residuals ($1000s)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Residual Plot: Multiple Regression'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Random scatter around zero suggests model assumptions are reasonable."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch10_Data_Summary_for_Multiple_Regression_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Random scatter around zero suggests model assumptions are reasonable.</code></pre>
</div>
</div>
</section>
<section id="visualization-coefficient-plot-with-confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="visualization-coefficient-plot-with-confidence-intervals">Visualization: Coefficient Plot with Confidence Intervals</h3>
<p>A <strong>coefficient plot</strong> displays estimated coefficients with their 95% confidence intervals. This makes it easy to see:</p>
<ul>
<li>Which coefficients are significantly different from zero (CI doesn‚Äôt include zero)</li>
<li>Relative magnitude of effects</li>
<li>Precision of estimates (narrow vs.&nbsp;wide CIs)</li>
</ul>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create coefficient plot with confidence intervals</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Exclude intercept for better visualization</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>params_no_int <span class="op">=</span> model_full.params[<span class="dv">1</span>:]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>ci_no_int <span class="op">=</span> conf_int.iloc[<span class="dv">1</span>:, :]</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>y_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(params_no_int))</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>ax.errorbar(params_no_int.values, y_pos,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>            xerr<span class="op">=</span>[params_no_int.values <span class="op">-</span> ci_no_int.iloc[:, <span class="dv">0</span>].values,</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>                  ci_no_int.iloc[:, <span class="dv">1</span>].values <span class="op">-</span> params_no_int.values],</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>            fmt<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">8</span>, capsize<span class="op">=</span><span class="dv">5</span>, capthick<span class="op">=</span><span class="dv">2</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(y_pos)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(params_no_int.index)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Zero effect'</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Coefficient Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Coefficient Estimates with 95% Confidence Intervals'</span>,</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients whose CI crosses zero are not statistically significant."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch10_Data_Summary_for_Multiple_Regression_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Coefficients whose CI crosses zero are not statistically significant.</code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<p><strong>Data Exploration and Correlation:</strong></p>
<ul>
<li>Multiple regression extends bivariate regression to include several regressors simultaneously</li>
<li>Summary statistics and two-way scatterplots provide initial overview of variable relationships</li>
<li>Correlation matrices reveal pairwise associations (e.g., price-size correlation of 0.79)</li>
<li>Correlation can be misleading ‚Äî bedrooms correlate with price, but may simply reflect house size</li>
</ul>
<p><strong>OLS Estimation:</strong></p>
<ul>
<li>Multiple regression model: <span class="math inline">\(\widehat{y} = b_1 + b_2 x_2 + b_3 x_3 + \cdots + b_k x_k\)</span></li>
<li>OLS minimizes the sum of squared residuals: <span class="math inline">\(\min \sum (y_i - \widehat{y}_i)^2\)</span></li>
<li>The coefficient <span class="math inline">\(b_j\)</span> can be computed by bivariate regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(\widetilde{x}_j\)</span> (residualized <span class="math inline">\(x_j\)</span>)</li>
</ul>
<p><strong>Partial Effects:</strong></p>
<ul>
<li>Each coefficient <span class="math inline">\(b_j\)</span> measures the partial effect: the change in <span class="math inline">\(\widehat{y}\)</span> when <span class="math inline">\(x_j\)</span> changes by one unit, <strong>holding all other regressors constant</strong></li>
<li>Partial effects differ from total effects (which allow other regressors to vary)</li>
<li>OLS measures association, not causation ‚Äî use careful language (‚Äúassociated with‚Äù)</li>
</ul>
<p><strong>Model Fit:</strong></p>
<ul>
<li><span class="math inline">\(R^2\)</span> measures the fraction of variation in <span class="math inline">\(y\)</span> explained by all regressors (0 to 1)</li>
<li>Adjusted <span class="math inline">\(R^2\)</span> penalizes model complexity: can decrease when adding weak regressors</li>
<li>Standard error of regression <span class="math inline">\(s_e\)</span> measures typical prediction error in units of <span class="math inline">\(y\)</span></li>
<li>Example: Adding 5 regressors increased <span class="math inline">\(R^2\)</span> from 0.618 to 0.651 but decreased <span class="math inline">\(\bar{R}^2\)</span> from 0.603 to 0.555</li>
</ul>
<p><strong>Information Criteria:</strong></p>
<ul>
<li>AIC, BIC, and HQIC penalize larger models more heavily than adjusted <span class="math inline">\(R^2\)</span></li>
<li>Smaller values indicate better models</li>
<li>BIC is generally preferred (stronger complexity penalty than AIC)</li>
</ul>
<p><strong>Multicollinearity:</strong></p>
<ul>
<li>Perfect collinearity makes some coefficients inestimable (computer shows ‚Äúomitted‚Äù)</li>
<li>VIF detects multicollinearity: VIF &gt; 10 indicates problematic, VIF &gt; 5 moderate concern</li>
<li>High multicollinearity inflates standard errors and makes estimates unstable</li>
</ul>
<p><strong>Python tools used:</strong> <code>statsmodels</code> (OLS, VIF), <code>seaborn</code> (pairplot, heatmap), <code>pandas</code> (DataFrames), <code>matplotlib</code> (coefficient plots, diagnostics)</p>
<p><strong>Next steps:</strong> Chapter 11 covers <strong>statistical inference</strong> for multiple regression ‚Äî hypothesis tests, confidence intervals, and overall F-tests for model significance.</p>
<p>Congratulations on completing Chapter 10! You now have the tools to estimate and evaluate multiple regression models, a foundation for the rest of the course.</p>
</section>
<section id="practice-exercises" class="level2">
<h2 class="anchored" data-anchor-id="practice-exercises">Practice Exercises</h2>
<p>Test your understanding of multiple regression concepts with these exercises.</p>
<hr>
<p><strong>Exercise 1: Residuals and Fitted Values</strong></p>
<p>A regression leads to the fitted equation <span class="math inline">\(\widehat{y} = 2 + 3x_2 + 4x_3\)</span>.</p>
<ol type="a">
<li><p>What is the predicted value for observation <span class="math inline">\((x_2, x_3) = (2, 1)\)</span>?</p></li>
<li><p>If the actual value is <span class="math inline">\(y = 9\)</span>, what is the residual?</p></li>
<li><p>Is this observation over-predicted or under-predicted?</p></li>
</ol>
<hr>
<p><strong>Exercise 2: Partial vs.&nbsp;Total Effects</strong></p>
<p>Suppose OLS regression on the same dataset leads to:</p>
<ul>
<li>Bivariate: <span class="math inline">\(\widehat{y} = 6 + 5x_2\)</span></li>
<li>Multiple: <span class="math inline">\(\widehat{y} = 2 + 3x_2 + 4x_3\)</span></li>
</ul>
<ol type="a">
<li><p>Why does the coefficient on <span class="math inline">\(x_2\)</span> change from 5 to 3?</p></li>
<li><p>Which coefficient (5 or 3) represents the partial effect of <span class="math inline">\(x_2\)</span>? Explain.</p></li>
<li><p>Under what condition would the bivariate and multiple regression coefficients on <span class="math inline">\(x_2\)</span> be equal?</p></li>
</ol>
<hr>
<p><strong>Exercise 3: R-squared and Adjusted R-squared</strong></p>
<p>OLS regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> for a sample of size <span class="math inline">\(n = 53\)</span> leads to Residual SS = 20 and Total SS = 50.</p>
<ol type="a">
<li><p>Compute <span class="math inline">\(R^2\)</span>.</p></li>
<li><p>Compute the standard error of the regression <span class="math inline">\(s_e\)</span>.</p></li>
<li><p>Compute the correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(\widehat{y}\)</span>.</p></li>
<li><p>If we add 3 more regressors and the Residual SS drops to 18, compute the new <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span>. Does the model improve?</p></li>
</ol>
<hr>
<p><strong>Exercise 4: Information Criteria</strong></p>
<p>Consider two models estimated on <span class="math inline">\(n = 29\)</span> observations:</p>
<ul>
<li>Model A (2 parameters): AIC = 668.1, BIC = 670.8</li>
<li>Model B (7 parameters): AIC = 675.5, BIC = 685.1</li>
</ul>
<ol type="a">
<li><p>Which model is preferred by AIC? By BIC?</p></li>
<li><p>Why does BIC penalize Model B more heavily than AIC?</p></li>
<li><p>What economic reasoning supports the simpler model in this case?</p></li>
</ol>
<hr>
<p><strong>Exercise 5: Multicollinearity Detection</strong></p>
<p>A multiple regression of house prices yields the following VIF values:</p>
<table class="table">
<thead>
<tr class="header">
<th>Variable</th>
<th>VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Size</td>
<td>40.1</td>
</tr>
<tr class="even">
<td>Bedrooms</td>
<td>57.8</td>
</tr>
<tr class="odd">
<td>Bathrooms</td>
<td>34.7</td>
</tr>
<tr class="even">
<td>Age</td>
<td>21.0</td>
</tr>
</tbody>
</table>
<ol type="a">
<li><p>Which variables show the most severe multicollinearity?</p></li>
<li><p>What consequences does high VIF have for coefficient estimates and inference?</p></li>
<li><p>Suggest a strategy to reduce multicollinearity in this model.</p></li>
</ol>
<hr>
<p><strong>Exercise 6: Model Building</strong></p>
<p>You have data on employee wages with variables: education (years), experience (years), age, tenure (years at current firm), and gender.</p>
<ol type="a">
<li><p>Why might including both age and experience create multicollinearity?</p></li>
<li><p>Propose two alternative model specifications and explain the trade-offs.</p></li>
<li><p>How would you use adjusted <span class="math inline">\(R^2\)</span> and BIC to select the preferred specification?</p></li>
</ol>
</section>
<section id="case-studies" class="level2">
<h2 class="anchored" data-anchor-id="case-studies">Case Studies</h2>
<section id="case-study-1-multiple-regression-for-cross-country-productivity" class="level3">
<h3 class="anchored" data-anchor-id="case-study-1-multiple-regression-for-cross-country-productivity">Case Study 1: Multiple Regression for Cross-Country Productivity</h3>
<p>In this case study, you will apply multiple regression techniques to investigate how human capital and physical capital jointly explain cross-country differences in labor productivity. Using data from 108 countries over 1990-2014, you will explore correlations, estimate multiple regression models, interpret partial effects, and compare model specifications.</p>
<p><strong>Dataset:</strong> Mendez Convergence Clubs Data</p>
<ul>
<li><strong>Source:</strong> Mendez (2020), 108 countries, 1990-2014</li>
<li><strong>Key variables:</strong>
<ul>
<li><code>lp</code> ‚Äî Labor productivity (GDP per worker)</li>
<li><code>rk</code> ‚Äî Physical capital per worker</li>
<li><code>hc</code> ‚Äî Human capital index</li>
<li><code>rgdppc</code> ‚Äî Real GDP per capita</li>
<li><code>region</code> ‚Äî Geographic region</li>
</ul></li>
</ul>
<p><strong>Research question:</strong> How do human capital and physical capital jointly explain cross-country productivity differences?</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Mendez convergence clubs dataset</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/quarcs-lab/mendez2020-convergence-clubs-code-data/master/assets/dat.csv"</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>dat <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>dat_2014 <span class="op">=</span> dat[dat[<span class="st">'year'</span>] <span class="op">==</span> <span class="dv">2014</span>].dropna(subset<span class="op">=</span>[<span class="st">'lp'</span>, <span class="st">'rk'</span>, <span class="st">'hc'</span>]).copy()</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cross-section sample: </span><span class="sc">{</span><span class="bu">len</span>(dat_2014)<span class="sc">}</span><span class="ss"> countries (year 2014)"</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>dat_2014.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="task-1-explore-productivity-data-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-1-explore-productivity-data-guided">Task 1: Explore Productivity Data (Guided)</h4>
<p>Explore the cross-country productivity dataset.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Create log-transformed variables</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>dat_2014[<span class="st">'ln_lp'</span>] <span class="op">=</span> np.log(dat_2014[<span class="st">'lp'</span>])</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>dat_2014[<span class="st">'ln_rk'</span>] <span class="op">=</span> np.log(dat_2014[<span class="st">'rk'</span>])</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Summary statistics for key variables</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dat_2014[[<span class="st">'lp'</span>, <span class="st">'rk'</span>, <span class="st">'hc'</span>, <span class="st">'ln_lp'</span>, <span class="st">'ln_rk'</span>]].describe())</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Create a scatterplot matrix</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>sns.pairplot(dat_2014[[<span class="st">'ln_lp'</span>, <span class="st">'ln_rk'</span>, <span class="st">'hc'</span>]], diag_kind<span class="op">=</span><span class="st">'kde'</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">'Scatterplot Matrix: Productivity Variables'</span>, y<span class="op">=</span><span class="fl">1.02</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Questions:</strong></p>
<ul>
<li>Which variable shows the strongest visual association with <span class="math inline">\(\ln(\text{lp})\)</span>?</li>
<li>Do you see any nonlinear patterns or outliers?</li>
</ul>
</section>
<section id="task-2-correlation-analysis-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-2-correlation-analysis-guided">Task 2: Correlation Analysis (Guided)</h4>
<p>Compute and visualize the correlation matrix.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Compute correlation matrix</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>corr <span class="op">=</span> dat_2014[[<span class="st">'ln_lp'</span>, <span class="st">'ln_rk'</span>, <span class="st">'hc'</span>]].corr()</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corr)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Visualize as heatmap</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'.3f'</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, center<span class="op">=</span><span class="dv">0</span>, square<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Correlation Matrix: Productivity Variables'</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Questions:</strong></p>
<ul>
<li>Which predictor is most strongly correlated with <span class="math inline">\(\ln(\text{lp})\)</span>?</li>
<li>Are <span class="math inline">\(\ln(\text{rk})\)</span> and hc correlated with each other? What implications does this have for multiple regression?</li>
</ul>
<blockquote class="blockquote">
<p><strong>Key Concept 10.9: Functional Form and Cross-Country Comparisons</strong></p>
<p>When comparing countries with vastly different income levels, logarithmic transformations are essential. Using <span class="math inline">\(\ln(\text{lp})\)</span> and <span class="math inline">\(\ln(\text{rk})\)</span> compresses the scale so that both Luxembourg and Malawi can be meaningfully compared. Coefficients on log-transformed variables have elasticity interpretations, while coefficients on level variables (like hc) represent semi-elasticities.</p>
</blockquote>
</section>
<section id="task-3-multiple-regression-estimation-semi-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-3-multiple-regression-estimation-semi-guided">Task 3: Multiple Regression Estimation (Semi-guided)</h4>
<p>Estimate bivariate and multiple regression models for labor productivity.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 1: ln(lp) ~ ln(rk) only</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> ols(<span class="st">'ln_lp ~ ln_rk'</span>, data<span class="op">=</span>dat_2014).fit()</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model1.summary())</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 2: ln(lp) ~ hc only</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> ols(<span class="st">'ln_lp ~ hc'</span>, data<span class="op">=</span>dat_2014).fit()</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model2.summary())</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 3: ln(lp) ~ ln(rk) + hc (multiple regression)</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> ols(<span class="st">'ln_lp ~ ln_rk + hc'</span>, data<span class="op">=</span>dat_2014).fit()</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model3.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Questions:</strong></p>
<ul>
<li>How do the coefficients on <span class="math inline">\(\ln(\text{rk})\)</span> change between Model 1 and Model 3?</li>
<li>Does adding human capital improve the model? Compare <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span>.</li>
</ul>
</section>
<section id="task-4-interpret-partial-effects-semi-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-4-interpret-partial-effects-semi-guided">Task 4: Interpret Partial Effects (Semi-guided)</h4>
<p>Interpret the coefficients from the multiple regression model.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display coefficients with confidence intervals</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model 3 Coefficients:"</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model3.params)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">95% Confidence Intervals:"</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model3.conf_int())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Questions:</strong></p>
<ul>
<li>Interpret the coefficient on <span class="math inline">\(\ln(\text{rk})\)</span> in Model 3. What does it mean in economic terms?</li>
<li>Interpret the coefficient on hc. How does a one-unit increase in human capital relate to productivity?</li>
<li>Are both coefficients statistically significant? How do you know?</li>
</ul>
</section>
<section id="task-5-model-selection-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-5-model-selection-independent">Task 5: Model Selection (Independent)</h4>
<p>Compare models using fit statistics and information criteria.</p>
<p><strong>Your tasks:</strong></p>
<ol type="1">
<li>Create a comparison table with <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, and BIC for all three models</li>
<li>Which model is preferred by adjusted <span class="math inline">\(R^2\)</span>? By AIC? By BIC?</li>
<li>Does the parsimony principle favor one model over another?</li>
<li>Calculate VIF for Model 3 ‚Äî is multicollinearity a concern?</li>
</ol>
<p><em>Hint: Use <code>model.rsquared</code>, <code>model.rsquared_adj</code>, <code>model.aic</code>, <code>model.bic</code> and <code>variance_inflation_factor()</code> from statsmodels.</em></p>
</section>
<section id="task-6-development-policy-brief-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-6-development-policy-brief-independent">Task 6: Development Policy Brief (Independent)</h4>
<p>Write a 200-300 word policy brief summarizing your findings.</p>
<p><strong>Your brief should address:</strong></p>
<ol type="1">
<li>What are the key determinants of cross-country labor productivity?</li>
<li>What is the relative importance of physical capital vs.&nbsp;human capital?</li>
<li>How do partial effects differ from bivariate associations?</li>
<li>What policy implications follow from your analysis?</li>
<li>What limitations should policymakers keep in mind (association vs.&nbsp;causation)?</li>
</ol>
<blockquote class="blockquote">
<p><strong>Key Concept 10.10: Multiple Regression in Development Economics</strong></p>
<p>Cross-country productivity regressions are central to development economics. By controlling for both physical capital (<span class="math inline">\(\ln(\text{rk})\)</span>) and human capital (hc), we can assess each factor‚Äôs <strong>partial contribution</strong> to productivity. This addresses a key policy question: should developing countries invest in machines or education? Multiple regression helps disentangle these effects, though causal claims require careful identification strategies.</p>
</blockquote>
</section>
</section>
<section id="what-youve-learned" class="level3">
<h3 class="anchored" data-anchor-id="what-youve-learned">What You‚Äôve Learned</h3>
<p>In this case study, you applied the full multiple regression toolkit to cross-country productivity data:</p>
<ul>
<li>Explored data with scatterplot matrices and correlation analysis</li>
<li>Estimated bivariate and multiple regression models, observing how coefficients change</li>
<li>Interpreted partial effects of physical and human capital on productivity</li>
<li>Compared model specifications using adjusted <span class="math inline">\(R^2\)</span>, AIC, and BIC</li>
<li>Connected statistical findings to development policy questions</li>
</ul>
<p>These skills form the foundation for more advanced empirical analysis in the chapters ahead.</p>
</section>
<section id="case-study-2-multiple-satellite-predictors-of-development" class="level3">
<h3 class="anchored" data-anchor-id="case-study-2-multiple-satellite-predictors-of-development">Case Study 2: Multiple Satellite Predictors of Development</h3>
<p><strong>Research Question</strong>: Do satellite image embeddings improve our ability to predict municipal development beyond nighttime lights alone?</p>
<p>In Chapter 1, we introduced the DS4Bolivia project and estimated a bivariate regression of development on nighttime lights. Now we extend this analysis using Chapter 10‚Äôs <strong>multiple regression</strong> tools, adding satellite image embeddings as additional predictors to test whether they improve our ability to predict municipal development.</p>
<p><strong>Background</strong>: Nighttime lights (NTL) are a well-established proxy for economic activity, but they capture only one dimension of a municipality‚Äôs characteristics ‚Äî nocturnal luminosity. Daytime satellite imagery contains far richer information: building density, road networks, agricultural patterns, vegetation cover. Deep learning models can extract this information as <strong>64-dimensional embedding vectors</strong>, where each dimension captures abstract visual patterns learned automatically from the data.</p>
<p><strong>The key question for multiple regression</strong>: Does adding these satellite embeddings as extra regressors significantly improve explanatory power compared to NTL alone? And if so, which embeddings matter most?</p>
<p><strong>Variables for this case study:</strong></p>
<ul>
<li><code>imds</code> ‚Äî Municipal Sustainable Development Index (0-100, outcome variable)</li>
<li><code>ln_NTLpc2017</code> ‚Äî Log nighttime lights per capita (established predictor from Chapter 1)</li>
<li><code>A00</code>, <code>A10</code>, <code>A20</code>, <code>A30</code>, <code>A40</code> ‚Äî Selected satellite image embedding dimensions (new predictors)</li>
<li><code>mun</code>, <code>dep</code> ‚Äî Municipality and department identifiers</li>
</ul>
<p><strong>Your Task</strong>: Use correlation analysis, multiple regression, the FWL theorem, and model comparison tools from Chapter 10 to evaluate whether satellite embeddings add predictive value beyond nighttime lights.</p>
<section id="load-the-ds4bolivia-data" class="level4">
<h4 class="anchored" data-anchor-id="load-the-ds4bolivia-data">Load the DS4Bolivia Data</h4>
<p>Let‚Äôs load the DS4Bolivia dataset and select the variables needed for our multiple regression analysis. We focus on the development index, nighttime lights, and five selected satellite embedding dimensions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the DS4Bolivia dataset</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>url_bol <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master/ds4bolivia_v20250523.csv"</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>bol <span class="op">=</span> pd.read_csv(url_bol)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Select key variables for this case study</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>key_vars <span class="op">=</span> [<span class="st">'mun'</span>, <span class="st">'dep'</span>, <span class="st">'imds'</span>, <span class="st">'ln_NTLpc2017'</span>, <span class="st">'A00'</span>, <span class="st">'A10'</span>, <span class="st">'A20'</span>, <span class="st">'A30'</span>, <span class="st">'A40'</span>]</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>bol_sat <span class="op">=</span> bol[key_vars].copy()</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DS4BOLIVIA DATASET ‚Äî SATELLITE PREDICTORS"</span>)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset shape: </span><span class="sc">{</span>bol_sat<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> municipalities, </span><span class="sc">{</span>bol_sat<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> variables"</span>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Departments: </span><span class="sc">{</span>bol[<span class="st">'dep'</span>]<span class="sc">.</span>nunique()<span class="sc">}</span><span class="ss"> unique departments"</span>)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Complete cases: </span><span class="sc">{</span>bol_sat<span class="sc">.</span>dropna()<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"FIRST 10 MUNICIPALITIES"</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bol_sat.head(<span class="dv">10</span>).to_string(index<span class="op">=</span><span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="task-1-explore-variables-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-1-explore-variables-guided">Task 1: Explore Variables (Guided)</h4>
<p><strong>Objective</strong>: Understand the satellite embedding variables and how they compare to nighttime lights.</p>
<p><strong>Instructions</strong>:</p>
<ol type="1">
<li>Generate summary statistics for all predictor variables using <code>describe()</code></li>
<li>Compare the scale and distribution of NTL vs.&nbsp;embedding variables</li>
<li>Check for missing values across all selected variables</li>
<li>Consider: What are satellite embeddings? How do they differ from NTL?</li>
</ol>
<p><strong>Key insight</strong>: Unlike NTL (which has a clear physical interpretation ‚Äî light intensity), embedding dimensions are <strong>abstract features</strong> extracted by neural networks. Dimension <code>A00</code> doesn‚Äôt mean ‚Äúvegetation‚Äù or ‚Äúroads‚Äù ‚Äî it captures a learned combination of visual patterns.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Your code here: Explore the satellite predictor variables</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Summary statistics for all numeric variables</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DESCRIPTIVE STATISTICS: DEVELOPMENT AND SATELLITE PREDICTORS"</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bol_sat[[<span class="st">'imds'</span>, <span class="st">'ln_NTLpc2017'</span>, <span class="st">'A00'</span>, <span class="st">'A10'</span>, <span class="st">'A20'</span>, <span class="st">'A30'</span>, <span class="st">'A40'</span>]].describe().<span class="bu">round</span>(<span class="dv">3</span>))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Check for missing values</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MISSING VALUES"</span>)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bol_sat.isnull().<span class="bu">sum</span>())</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Compare variable ranges</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"VARIABLE RANGES"</span>)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> [<span class="st">'imds'</span>, <span class="st">'ln_NTLpc2017'</span>, <span class="st">'A00'</span>, <span class="st">'A10'</span>, <span class="st">'A20'</span>, <span class="st">'A30'</span>, <span class="st">'A40'</span>]:</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    col <span class="op">=</span> bol_sat[var].dropna()</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>var<span class="sc">:16s}</span><span class="ss">  range: [</span><span class="sc">{</span>col<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>col<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">]  std: </span><span class="sc">{</span>col<span class="sc">.</span>std()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="task-2-correlation-analysis-guided-1" class="level4">
<h4 class="anchored" data-anchor-id="task-2-correlation-analysis-guided-1">Task 2: Correlation Analysis (Guided)</h4>
<p><strong>Objective</strong>: Compute and visualize the correlation structure among all predictors and the outcome variable.</p>
<p><strong>Instructions</strong>:</p>
<ol type="1">
<li>Compute the correlation matrix for <code>imds</code>, <code>ln_NTLpc2017</code>, and all five embedding variables</li>
<li>Display the correlation matrix as a heatmap</li>
<li>Identify: Which embeddings correlate most strongly with <code>imds</code>?</li>
<li>Identify: Do the embeddings correlate with each other (potential multicollinearity)?</li>
<li>Do the embeddings correlate with NTL, or do they capture different information?</li>
</ol>
<p><strong>Apply what you learned in section 10.3</strong>: Use <code>seaborn.heatmap()</code> with annotated correlation values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Your code here: Correlation analysis of satellite predictors</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Compute correlation matrix</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>corr_vars <span class="op">=</span> [<span class="st">'imds'</span>, <span class="st">'ln_NTLpc2017'</span>, <span class="st">'A00'</span>, <span class="st">'A10'</span>, <span class="st">'A20'</span>, <span class="st">'A30'</span>, <span class="st">'A40'</span>]</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>corr_sat <span class="op">=</span> bol_sat[corr_vars].dropna().corr()</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CORRELATION MATRIX: DEVELOPMENT AND SATELLITE PREDICTORS"</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corr_sat.<span class="bu">round</span>(<span class="dv">3</span>))</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Visualize as heatmap</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr_sat, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'.3f'</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>            square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="dv">1</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">0.8</span>})</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Correlation Matrix: IMDS, NTL, and Satellite Embeddings'</span>,</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Highlight strongest correlations with IMDS</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Correlations with IMDS (development index):"</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>imds_corr <span class="op">=</span> corr_sat[<span class="st">'imds'</span>].drop(<span class="st">'imds'</span>).sort_values(key<span class="op">=</span><span class="bu">abs</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var, r <span class="kw">in</span> imds_corr.items():</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>var<span class="sc">:16s}</span><span class="ss">  r = </span><span class="sc">{</span>r<span class="sc">:+.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.12: High-Dimensional Satellite Features</strong></p>
<p>Satellite embeddings are <strong>64 abstract features</strong> extracted by deep learning models (convolutional neural networks) from daytime satellite imagery. Unlike handcrafted variables (e.g., NDVI for vegetation), each embedding dimension captures complex visual patterns ‚Äî road density, building structures, agricultural layouts ‚Äî learned automatically from the data. These features are not directly interpretable (dimension A00 doesn‚Äôt have a specific meaning), but they collectively encode rich information about a municipality‚Äôs physical landscape.</p>
</blockquote>
</section>
<section id="task-3-multiple-regression-semi-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-3-multiple-regression-semi-guided">Task 3: Multiple Regression (Semi-guided)</h4>
<p><strong>Objective</strong>: Estimate a multiple regression model with NTL and satellite embeddings as predictors.</p>
<p><strong>Instructions</strong>:</p>
<ol type="1">
<li>Estimate the full model: <code>imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40</code></li>
<li>Display the regression summary</li>
<li>Compare <span class="math inline">\(R^2\)</span> with the bivariate NTL-only model from Chapter 1</li>
<li>Interpret: How much does adding embeddings improve explanatory power?</li>
<li>Which embedding coefficients are statistically significant?</li>
</ol>
<p><strong>Apply what you learned in section 10.4</strong>: Use <code>ols()</code> from statsmodels and interpret partial effects.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Your code here: Multiple regression with satellite predictors</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Prepare data (drop missing values)</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>reg_data <span class="op">=</span> bol_sat[[<span class="st">'imds'</span>, <span class="st">'ln_NTLpc2017'</span>, <span class="st">'A00'</span>, <span class="st">'A10'</span>, <span class="st">'A20'</span>, <span class="st">'A30'</span>, <span class="st">'A40'</span>]].dropna()</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Regression sample: </span><span class="sc">{</span><span class="bu">len</span>(reg_data)<span class="sc">}</span><span class="ss"> municipalities (complete cases)"</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Bivariate model (NTL only ‚Äî baseline from Chapter 1)</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>model_ntl <span class="op">=</span> ols(<span class="st">'imds ~ ln_NTLpc2017'</span>, data<span class="op">=</span>reg_data).fit()</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL 1: BIVARIATE ‚Äî imds ~ ln_NTLpc2017"</span>)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_ntl.summary())</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Multiple regression (NTL + all 5 embeddings)</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>model_full_sat <span class="op">=</span> ols(<span class="st">'imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40'</span>,</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>                     data<span class="op">=</span>reg_data).fit()</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MODEL 2: MULTIPLE ‚Äî imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40"</span>)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_full_sat.summary())</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Compare R-squared</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"R-SQUARED COMPARISON"</span>)</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NTL only:           R¬≤ = </span><span class="sc">{</span>model_ntl<span class="sc">.</span>rsquared<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NTL + 5 embeddings: R¬≤ = </span><span class="sc">{</span>model_full_sat<span class="sc">.</span>rsquared<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Improvement:        ŒîR¬≤ = </span><span class="sc">{</span>model_full_sat<span class="sc">.</span>rsquared <span class="op">-</span> model_ntl<span class="sc">.</span>rsquared<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="task-4-partial-effects-via-fwl-semi-guided" class="level4">
<h4 class="anchored" data-anchor-id="task-4-partial-effects-via-fwl-semi-guided">Task 4: Partial Effects via FWL (Semi-guided)</h4>
<p><strong>Objective</strong>: Demonstrate the Frisch-Waugh-Lovell theorem by showing that the NTL coefficient in the multiple regression equals the coefficient from regressing residualized IMDS on residualized NTL.</p>
<p><strong>Instructions</strong>:</p>
<ol type="1">
<li>Regress <code>imds</code> on all embedding variables (<code>A00</code>, <code>A10</code>, <code>A20</code>, <code>A30</code>, <code>A40</code>), save residuals <span class="math inline">\(e_y\)</span></li>
<li>Regress <code>ln_NTLpc2017</code> on all embedding variables, save residuals <span class="math inline">\(e_x\)</span></li>
<li>Regress <span class="math inline">\(e_y\)</span> on <span class="math inline">\(e_x\)</span> (bivariate regression of residuals)</li>
<li>Verify that the coefficient matches the NTL coefficient from the full multiple regression</li>
</ol>
<p><strong>Apply what you learned in section 10.5</strong>: This demonstrates that the partial effect of NTL is computed from the variation in NTL that is <em>independent</em> of the satellite embeddings.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Your code here: FWL theorem demonstration</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Regress imds on embeddings only, save residuals e_y</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>model_y_on_emb <span class="op">=</span> ols(<span class="st">'imds ~ A00 + A10 + A20 + A30 + A40'</span>, data<span class="op">=</span>reg_data).fit()</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>e_y <span class="op">=</span> model_y_on_emb.resid</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Regress ln_NTLpc2017 on embeddings only, save residuals e_x</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>model_x_on_emb <span class="op">=</span> ols(<span class="st">'ln_NTLpc2017 ~ A00 + A10 + A20 + A30 + A40'</span>, data<span class="op">=</span>reg_data).fit()</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>e_x <span class="op">=</span> model_x_on_emb.resid</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Regress e_y on e_x</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>fwl_data <span class="op">=</span> pd.DataFrame({<span class="st">'e_y'</span>: e_y, <span class="st">'e_x'</span>: e_x})</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>model_fwl <span class="op">=</span> ols(<span class="st">'e_y ~ e_x'</span>, data<span class="op">=</span>fwl_data).fit()</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Compare coefficients</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"FWL THEOREM DEMONSTRATION"</span>)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NTL coefficient from FULL multiple regression:  </span><span class="sc">{</span>model_full_sat<span class="sc">.</span>params[<span class="st">'ln_NTLpc2017'</span>]<span class="sc">:.10f}</span><span class="ss">"</span>)</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coefficient from FWL residual regression:       </span><span class="sc">{</span>model_fwl<span class="sc">.</span>params[<span class="st">'e_x'</span>]<span class="sc">:.10f}</span><span class="ss">"</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Difference (numerical precision):                </span><span class="sc">{</span><span class="bu">abs</span>(model_full_sat.params[<span class="st">'ln_NTLpc2017'</span>] <span class="op">-</span> model_fwl.params[<span class="st">'e_x'</span>])<span class="sc">:.15f}</span><span class="ss">"</span>)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">The coefficients are identical ‚Äî confirming the FWL theorem!"</span>)</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Interpretation: The partial effect of NTL on IMDS is computed using"</span>)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"only the variation in NTL that is NOT explained by the satellite embeddings."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="task-5-model-comparison-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-5-model-comparison-independent">Task 5: Model Comparison (Independent)</h4>
<p><strong>Objective</strong>: Compare multiple model specifications using fit statistics and information criteria.</p>
<p><strong>Your tasks</strong>:</p>
<ol type="1">
<li>Estimate three models:
<ul>
<li><strong>Model 1</strong>: <code>imds ~ ln_NTLpc2017</code> (NTL only)</li>
<li><strong>Model 2</strong>: <code>imds ~ ln_NTLpc2017 + A00 + A10</code> (NTL + 2 embeddings)</li>
<li><strong>Model 3</strong>: <code>imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40</code> (NTL + 5 embeddings)</li>
</ul></li>
<li>Create a comparison table reporting <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, and BIC for each model</li>
<li>Use <code>model.rsquared</code>, <code>model.rsquared_adj</code>, <code>model.aic</code>, <code>model.bic</code></li>
<li>Which model is ‚Äúbest‚Äù by each criterion?</li>
<li>Does the parsimony principle favor fewer or more embedding variables?</li>
</ol>
<p><em>Hint: Use <code>pd.DataFrame()</code> to create a clean comparison table.</em></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Your code here: Model comparison</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Estimate three models</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model_1 = ols('imds ~ ln_NTLpc2017', data=reg_data).fit()</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model_2 = ols('imds ~ ln_NTLpc2017 + A00 + A10', data=reg_data).fit()</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model_3 = ols('imds ~ ln_NTLpc2017 + A00 + A10 + A20 + A30 + A40', data=reg_data).fit()</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Create comparison table</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co"># comparison = pd.DataFrame({</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co">#     'Model': ['NTL only', 'NTL + 2 embeddings', 'NTL + 5 embeddings'],</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     'R¬≤': [model_1.rsquared, model_2.rsquared, model_3.rsquared],</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="co">#     'Adj R¬≤': [model_1.rsquared_adj, model_2.rsquared_adj, model_3.rsquared_adj],</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="co">#     'AIC': [model_1.aic, model_2.aic, model_3.aic],</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co">#     'BIC': [model_1.bic, model_2.bic, model_3.bic],</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     'N': [len(reg_data)] * 3</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="co"># })</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co"># print(comparison.to_string(index=False))</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Interpret ‚Äî which model is preferred by each criterion?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p><strong>Key Concept 10.13: Incremental Predictive Power</strong></p>
<p>When adding predictors to a regression model, <span class="math inline">\(R^2\)</span> can only increase or stay the same ‚Äî it never decreases. This makes <span class="math inline">\(R^2\)</span> misleading for model comparison when models have different numbers of predictors. <strong>Adjusted <span class="math inline">\(R^2\)</span></strong> penalizes for additional variables, while <strong>AIC</strong> and <strong>BIC</strong> balance fit against complexity. In the satellite prediction context, adding all 64 embeddings would maximize <span class="math inline">\(R^2\)</span> but might overfit; information criteria help identify the most parsimonious model.</p>
</blockquote>
</section>
<section id="task-6-policy-brief-on-satellite-prediction-independent" class="level4">
<h4 class="anchored" data-anchor-id="task-6-policy-brief-on-satellite-prediction-independent">Task 6: Policy Brief on Satellite Prediction (Independent)</h4>
<p><strong>Objective</strong>: Write a 200-300 word policy brief summarizing the value of satellite embeddings for development prediction.</p>
<p><strong>Your brief should address:</strong></p>
<ol type="1">
<li><strong>Improvement</strong>: How much does adding satellite embeddings improve development prediction compared to NTL alone?</li>
<li><strong>Complexity trade-off</strong>: Is the improvement worth the added model complexity? What do adjusted <span class="math inline">\(R^2\)</span>, AIC, and BIC suggest?</li>
<li><strong>Partial effects</strong>: After controlling for embeddings, does NTL remain a significant predictor? What does the FWL theorem reveal about NTL‚Äôs independent contribution?</li>
<li><strong>SDG monitoring implications</strong>: How could multi-source satellite data enhance SDG monitoring in data-scarce countries like Bolivia?</li>
<li><strong>Limitations</strong>: What can satellite data <em>not</em> capture about development? What are the risks of relying on abstract embedding features for policy decisions?</li>
</ol>
<p><strong>Connection to Research</strong>: The DS4Bolivia project uses all 64 embedding dimensions plus machine learning methods (Random Forest, XGBoost) to predict SDG indicators, achieving meaningful predictive accuracy. Your multiple regression analysis provides a transparent, interpretable baseline for comparison.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Your code here: Additional analysis for the policy brief</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># You might want to:</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a summary table of key results across models</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Generate a visualization comparing model fit</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Calculate the percentage improvement in R¬≤ from adding embeddings</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Discuss which embeddings contribute most to prediction</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Example:</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print("KEY RESULTS FOR POLICY BRIEF")</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"NTL-only R¬≤:          {model_1.rsquared:.4f}")</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"NTL + 5 embed R¬≤:     {model_3.rsquared:.4f}")</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"R¬≤ improvement:       {(model_3.rsquared - model_1.rsquared) / model_1.rsquared * 100:.1f}%")</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"NTL coef (bivariate): {model_1.params['ln_NTLpc2017']:.4f}")</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"NTL coef (multiple):  {model_3.params['ln_NTLpc2017']:.4f}")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="what-youve-learned-from-this-case-study" class="level4">
<h4 class="anchored" data-anchor-id="what-youve-learned-from-this-case-study">What You‚Äôve Learned from This Case Study</h4>
<p>Through this analysis of multiple satellite predictors of Bolivian municipal development, you‚Äôve applied the full Chapter 10 toolkit to a cutting-edge research application:</p>
<ul>
<li><strong>Correlation analysis</strong>: Explored the correlation structure among NTL, satellite embeddings, and development outcomes</li>
<li><strong>Multiple regression</strong>: Estimated models with multiple satellite predictors and interpreted partial effects</li>
<li><strong>FWL theorem</strong>: Demonstrated that the partial effect of NTL equals the coefficient from regressing residualized IMDS on residualized NTL</li>
<li><strong>Model comparison</strong>: Evaluated competing specifications using <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, and BIC</li>
<li><strong>Critical assessment</strong>: Weighed the predictive gains from additional satellite features against model complexity</li>
</ul>
<p><strong>Connection to upcoming chapters</strong>: In Chapter 11, we‚Äôll test whether the satellite embeddings are <em>statistically</em> significant using F-tests for joint significance. In Chapter 12, we‚Äôll address robust inference and prediction intervals.</p>
<p><strong>This dataset returns throughout the textbook</strong>: Each subsequent chapter applies its specific econometric tools to the DS4Bolivia data, building progressively toward a comprehensive satellite-based development prediction framework.</p>
<hr>
<p><strong>Well done!</strong> You‚Äôve now analyzed how multiple satellite data sources can predict development outcomes, moving from simple bivariate regression (Chapter 1) to the richer multiple regression framework of Chapter 10.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks_colab/ch09_Models_with_Natural_Logarithms.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">9. Models with Natural Logarithms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks_colab/ch11_Statistical_Inference_for_Multiple_Regression.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">11. Statistical Inference for Multiple Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  var tocNav = document.getElementById('TOC');
  if (tocNav) {
    var coverDiv = document.createElement('div');
    coverDiv.id = 'toc-cover-image';
    var img = document.createElement('img');
    var path = window.location.pathname;
    if (path.indexOf('/notebooks_colab/') !== -1) {
      img.src = '../images/book1cover.jpg';
    } else {
      img.src = 'images/book1cover.jpg';
    }
    img.alt = 'Econometrics Powered by AI';
    coverDiv.appendChild(img);
    tocNav.parentNode.insertBefore(coverDiv, tocNav);
  }
});
</script>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>